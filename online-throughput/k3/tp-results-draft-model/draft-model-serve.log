[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:28 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:28 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'draft_model', 'model': 'meta-llama/Llama-3.2-1B', 'num_speculative_tokens': 3, 'max_model_len': 5000}}
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:29 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:29 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:32 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:32 [model.py:1559] Using max model len 131072
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:32 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=44263)[0;0m WARNING 01-27 14:34:32 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:34:32 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=44468)[0;0m INFO 01-27 14:34:40 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='draft_model', model='meta-llama/Llama-3.2-1B', num_spec_tokens=3), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8448], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=44468)[0;0m WARNING 01-27 14:34:40 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 14:34:45 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43307 backend=nccl
INFO 01-27 14:34:45 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43307 backend=nccl
INFO 01-27 14:34:46 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 14:34:46 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 14:34:46 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 14:34:46 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 14:34:46 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 14:34:46 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 14:34:46 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 14:34:47 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 14:34:47 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:34:47 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:34:48 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP1 pid=44629)[0;0m INFO 01-27 14:35:11 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=44629)[0;0m INFO 01-27 14:35:11 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP1 pid=44629)[0;0m INFO 01-27 14:35:11 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=1
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:12 [default_loader.py:291] Loading weights took 21.89 seconds
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:12 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:12 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:12 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=0
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:13 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:13 [default_loader.py:291] Loading weights took 0.25 seconds
[0;36m(Worker_TP1 pid=44629)[0;0m INFO 01-27 14:35:13 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:14 [gpu_model_runner.py:3921] Model loading took 66.91 GiB memory and 26.285896 seconds
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:23 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:23 [backends.py:704] Dynamo bytecode transform time: 8.58 s
[0;36m(Worker_TP1 pid=44629)[0;0m INFO 01-27 14:35:36 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.787 s
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:36 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.741 s
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:36 [monitor.py:34] torch.compile takes 17.32 s in total
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:38 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:38 [backends.py:704] Dynamo bytecode transform time: 1.81 s
[0;36m(Worker_TP1 pid=44629)[0;0m INFO 01-27 14:35:40 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.307 s
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:40 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.284 s
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:40 [monitor.py:34] torch.compile takes 19.42 s in total
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:35:41 [gpu_worker.py:355] Available KV cache memory: 2.5 GiB
[0;36m(EngineCore_DP0 pid=44468)[0;0m INFO 01-27 14:35:41 [kv_cache_utils.py:1307] GPU KV cache size: 14,864 tokens
[0;36m(EngineCore_DP0 pid=44468)[0;0m INFO 01-27 14:35:41 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.97x
[0;36m(Worker_TP0 pid=44628)[0;0m INFO 01-27 14:36:00 [gpu_model_runner.py:4880] Graph capturing finished in 19 secs, took 4.32 GiB
[0;36m(EngineCore_DP0 pid=44468)[0;0m INFO 01-27 14:36:00 [core.py:272] init engine (profile, create kv cache, warmup model) took 46.20 seconds
[0;36m(EngineCore_DP0 pid=44468)[0;0m INFO 01-27 14:36:03 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:03 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=44263)[0;0m WARNING 01-27 14:36:03 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:03 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:03 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:03 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:06 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:06 [serving.py:221] Chat template warmup completed in 2574.8ms
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:06 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:07 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:27 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.97, Accepted throughput: 3.79 tokens/s, Drafted throughput: 3.83 tokens/s, Accepted: 92 tokens, Drafted: 93 tokens, Per-position acceptance rate: 1.000, 1.000, 0.968, Avg Draft acceptance rate: 98.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:37 [loggers.py:257] Engine 000: Avg prompt throughput: 70.3 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 41.90 tokens/s, Drafted throughput: 51.00 tokens/s, Accepted: 419 tokens, Drafted: 510 tokens, Per-position acceptance rate: 0.876, 0.829, 0.759, Avg Draft acceptance rate: 82.2%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:47 [loggers.py:257] Engine 000: Avg prompt throughput: 69.3 tokens/s, Avg generation throughput: 60.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 41.90 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 419 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.859, 0.745, 0.674, Avg Draft acceptance rate: 75.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:57 [loggers.py:257] Engine 000: Avg prompt throughput: 59.9 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:36:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 44.70 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 447 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.924, 0.795, 0.697, Avg Draft acceptance rate: 80.5%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:07 [loggers.py:257] Engine 000: Avg prompt throughput: 86.6 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 39.60 tokens/s, Drafted throughput: 54.89 tokens/s, Accepted: 396 tokens, Drafted: 549 tokens, Per-position acceptance rate: 0.825, 0.716, 0.623, Avg Draft acceptance rate: 72.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:17 [loggers.py:257] Engine 000: Avg prompt throughput: 50.1 tokens/s, Avg generation throughput: 65.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 47.20 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 472 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.919, 0.849, 0.769, Avg Draft acceptance rate: 84.6%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:27 [loggers.py:257] Engine 000: Avg prompt throughput: 52.0 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 41.70 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 417 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.839, 0.753, 0.651, Avg Draft acceptance rate: 74.7%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:37 [loggers.py:257] Engine 000: Avg prompt throughput: 89.2 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 42.00 tokens/s, Drafted throughput: 54.90 tokens/s, Accepted: 420 tokens, Drafted: 549 tokens, Per-position acceptance rate: 0.858, 0.760, 0.678, Avg Draft acceptance rate: 76.5%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:47 [loggers.py:257] Engine 000: Avg prompt throughput: 73.9 tokens/s, Avg generation throughput: 66.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 48.00 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 480 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.935, 0.865, 0.795, Avg Draft acceptance rate: 86.5%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:57 [loggers.py:257] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:37:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 42.70 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 427 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.870, 0.761, 0.690, Avg Draft acceptance rate: 77.4%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:07 [loggers.py:257] Engine 000: Avg prompt throughput: 83.0 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 40.90 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 409 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.810, 0.728, 0.685, Avg Draft acceptance rate: 74.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:17 [loggers.py:257] Engine 000: Avg prompt throughput: 97.3 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 45.80 tokens/s, Drafted throughput: 54.29 tokens/s, Accepted: 458 tokens, Drafted: 543 tokens, Per-position acceptance rate: 0.923, 0.851, 0.757, Avg Draft acceptance rate: 84.3%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:27 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 56.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 39.80 tokens/s, Drafted throughput: 48.60 tokens/s, Accepted: 398 tokens, Drafted: 486 tokens, Per-position acceptance rate: 0.895, 0.821, 0.741, Avg Draft acceptance rate: 81.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:37 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 5.10 tokens/s, Drafted throughput: 5.10 tokens/s, Accepted: 51 tokens, Drafted: 51 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:47 [loggers.py:257] Engine 000: Avg prompt throughput: 108.7 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 74.10 tokens/s, Drafted throughput: 91.20 tokens/s, Accepted: 741 tokens, Drafted: 912 tokens, Per-position acceptance rate: 0.882, 0.812, 0.743, Avg Draft acceptance rate: 81.2%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:57 [loggers.py:257] Engine 000: Avg prompt throughput: 149.6 tokens/s, Avg generation throughput: 111.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:38:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 77.09 tokens/s, Drafted throughput: 101.99 tokens/s, Accepted: 771 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.871, 0.744, 0.653, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:07 [loggers.py:257] Engine 000: Avg prompt throughput: 116.3 tokens/s, Avg generation throughput: 119.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 84.49 tokens/s, Drafted throughput: 104.99 tokens/s, Accepted: 845 tokens, Drafted: 1050 tokens, Per-position acceptance rate: 0.886, 0.811, 0.717, Avg Draft acceptance rate: 80.5%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:17 [loggers.py:257] Engine 000: Avg prompt throughput: 119.7 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 79.39 tokens/s, Drafted throughput: 104.39 tokens/s, Accepted: 794 tokens, Drafted: 1044 tokens, Per-position acceptance rate: 0.845, 0.767, 0.670, Avg Draft acceptance rate: 76.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:27 [loggers.py:257] Engine 000: Avg prompt throughput: 134.2 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 82.49 tokens/s, Drafted throughput: 103.19 tokens/s, Accepted: 825 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.884, 0.788, 0.727, Avg Draft acceptance rate: 79.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:37 [loggers.py:257] Engine 000: Avg prompt throughput: 168.1 tokens/s, Avg generation throughput: 115.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 80.70 tokens/s, Drafted throughput: 100.80 tokens/s, Accepted: 807 tokens, Drafted: 1008 tokens, Per-position acceptance rate: 0.872, 0.798, 0.732, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:47 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 40.70 tokens/s, Drafted throughput: 50.10 tokens/s, Accepted: 407 tokens, Drafted: 501 tokens, Per-position acceptance rate: 0.892, 0.820, 0.725, Avg Draft acceptance rate: 81.2%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:57 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:39:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 11.40 tokens/s, Drafted throughput: 12.30 tokens/s, Accepted: 114 tokens, Drafted: 123 tokens, Per-position acceptance rate: 0.951, 0.927, 0.902, Avg Draft acceptance rate: 92.7%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:07 [loggers.py:257] Engine 000: Avg prompt throughput: 258.3 tokens/s, Avg generation throughput: 200.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 139.69 tokens/s, Drafted throughput: 178.19 tokens/s, Accepted: 1397 tokens, Drafted: 1782 tokens, Per-position acceptance rate: 0.877, 0.779, 0.695, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:17 [loggers.py:257] Engine 000: Avg prompt throughput: 236.0 tokens/s, Avg generation throughput: 209.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 146.18 tokens/s, Drafted throughput: 190.18 tokens/s, Accepted: 1462 tokens, Drafted: 1902 tokens, Per-position acceptance rate: 0.856, 0.771, 0.678, Avg Draft acceptance rate: 76.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:27 [loggers.py:257] Engine 000: Avg prompt throughput: 258.7 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 147.69 tokens/s, Drafted throughput: 188.99 tokens/s, Accepted: 1477 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.865, 0.771, 0.708, Avg Draft acceptance rate: 78.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:37 [loggers.py:257] Engine 000: Avg prompt throughput: 97.1 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 79.79 tokens/s, Drafted throughput: 94.19 tokens/s, Accepted: 798 tokens, Drafted: 942 tokens, Per-position acceptance rate: 0.914, 0.857, 0.771, Avg Draft acceptance rate: 84.7%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:47 [loggers.py:257] Engine 000: Avg prompt throughput: 160.3 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 33.40 tokens/s, Drafted throughput: 39.60 tokens/s, Accepted: 334 tokens, Drafted: 396 tokens, Per-position acceptance rate: 0.902, 0.841, 0.788, Avg Draft acceptance rate: 84.3%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:57 [loggers.py:257] Engine 000: Avg prompt throughput: 411.7 tokens/s, Avg generation throughput: 400.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:40:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 279.79 tokens/s, Drafted throughput: 359.98 tokens/s, Accepted: 2798 tokens, Drafted: 3600 tokens, Per-position acceptance rate: 0.869, 0.776, 0.687, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:07 [loggers.py:257] Engine 000: Avg prompt throughput: 516.2 tokens/s, Avg generation throughput: 400.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 280.99 tokens/s, Drafted throughput: 351.58 tokens/s, Accepted: 2810 tokens, Drafted: 3516 tokens, Per-position acceptance rate: 0.877, 0.793, 0.728, Avg Draft acceptance rate: 79.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:17 [loggers.py:257] Engine 000: Avg prompt throughput: 305.6 tokens/s, Avg generation throughput: 322.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 227.07 tokens/s, Drafted throughput: 284.66 tokens/s, Accepted: 2271 tokens, Drafted: 2847 tokens, Per-position acceptance rate: 0.879, 0.791, 0.723, Avg Draft acceptance rate: 79.8%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:27 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 3.90 tokens/s, Drafted throughput: 3.90 tokens/s, Accepted: 39 tokens, Drafted: 39 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:37 [loggers.py:257] Engine 000: Avg prompt throughput: 738.0 tokens/s, Avg generation throughput: 545.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 379.91 tokens/s, Drafted throughput: 487.51 tokens/s, Accepted: 3799 tokens, Drafted: 4875 tokens, Per-position acceptance rate: 0.864, 0.777, 0.697, Avg Draft acceptance rate: 77.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:47 [loggers.py:257] Engine 000: Avg prompt throughput: 822.6 tokens/s, Avg generation throughput: 683.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 478.68 tokens/s, Drafted throughput: 604.17 tokens/s, Accepted: 4787 tokens, Drafted: 6042 tokens, Per-position acceptance rate: 0.871, 0.786, 0.719, Avg Draft acceptance rate: 79.2%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:57 [loggers.py:257] Engine 000: Avg prompt throughput: 842.9 tokens/s, Avg generation throughput: 682.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:41:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 480.36 tokens/s, Drafted throughput: 601.15 tokens/s, Accepted: 4804 tokens, Drafted: 6012 tokens, Per-position acceptance rate: 0.883, 0.789, 0.725, Avg Draft acceptance rate: 79.9%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:07 [loggers.py:257] Engine 000: Avg prompt throughput: 428.7 tokens/s, Avg generation throughput: 443.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 316.57 tokens/s, Drafted throughput: 380.36 tokens/s, Accepted: 3166 tokens, Drafted: 3804 tokens, Per-position acceptance rate: 0.905, 0.831, 0.761, Avg Draft acceptance rate: 83.2%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:17 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 1.20 tokens/s, Drafted throughput: 1.20 tokens/s, Accepted: 12 tokens, Drafted: 12 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1203.9 tokens/s, Avg generation throughput: 724.3 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 48.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 505.79 tokens/s, Drafted throughput: 639.76 tokens/s, Accepted: 5059 tokens, Drafted: 6399 tokens, Per-position acceptance rate: 0.875, 0.788, 0.709, Avg Draft acceptance rate: 79.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1199.4 tokens/s, Avg generation throughput: 1030.9 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 57.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 723.98 tokens/s, Drafted throughput: 910.18 tokens/s, Accepted: 7240 tokens, Drafted: 9102 tokens, Per-position acceptance rate: 0.878, 0.788, 0.720, Avg Draft acceptance rate: 79.5%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1246.4 tokens/s, Avg generation throughput: 1044.0 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 734.34 tokens/s, Drafted throughput: 918.22 tokens/s, Accepted: 7344 tokens, Drafted: 9183 tokens, Per-position acceptance rate: 0.877, 0.798, 0.724, Avg Draft acceptance rate: 80.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1239.1 tokens/s, Avg generation throughput: 1031.9 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 44.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:42:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 729.00 tokens/s, Drafted throughput: 907.20 tokens/s, Accepted: 7290 tokens, Drafted: 9072 tokens, Per-position acceptance rate: 0.878, 0.798, 0.735, Avg Draft acceptance rate: 80.4%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:07 [loggers.py:257] Engine 000: Avg prompt throughput: 742.5 tokens/s, Avg generation throughput: 893.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 632.44 tokens/s, Drafted throughput: 783.83 tokens/s, Accepted: 6325 tokens, Drafted: 7839 tokens, Per-position acceptance rate: 0.884, 0.804, 0.732, Avg Draft acceptance rate: 80.7%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1198.4 tokens/s, Avg generation throughput: 650.5 tokens/s, Running: 49 reqs, Waiting: 13 reqs, GPU KV cache usage: 97.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 226.74 tokens/s, Drafted throughput: 284.24 tokens/s, Accepted: 4535 tokens, Drafted: 5685 tokens, Per-position acceptance rate: 0.878, 0.793, 0.722, Avg Draft acceptance rate: 79.8%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1282.1 tokens/s, Avg generation throughput: 1157.3 tokens/s, Running: 41 reqs, Waiting: 20 reqs, GPU KV cache usage: 93.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 809.83 tokens/s, Drafted throughput: 1027.24 tokens/s, Accepted: 8098 tokens, Drafted: 10272 tokens, Per-position acceptance rate: 0.873, 0.781, 0.711, Avg Draft acceptance rate: 78.8%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1441.4 tokens/s, Avg generation throughput: 1096.3 tokens/s, Running: 50 reqs, Waiting: 12 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 770.19 tokens/s, Drafted throughput: 961.79 tokens/s, Accepted: 7702 tokens, Drafted: 9618 tokens, Per-position acceptance rate: 0.878, 0.796, 0.728, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1383.7 tokens/s, Avg generation throughput: 1128.2 tokens/s, Running: 48 reqs, Waiting: 12 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:43:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 791.70 tokens/s, Drafted throughput: 997.50 tokens/s, Accepted: 7917 tokens, Drafted: 9975 tokens, Per-position acceptance rate: 0.870, 0.791, 0.720, Avg Draft acceptance rate: 79.4%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1343.0 tokens/s, Avg generation throughput: 1068.7 tokens/s, Running: 52 reqs, Waiting: 6 reqs, GPU KV cache usage: 87.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 750.72 tokens/s, Drafted throughput: 938.61 tokens/s, Accepted: 7508 tokens, Drafted: 9387 tokens, Per-position acceptance rate: 0.872, 0.800, 0.728, Avg Draft acceptance rate: 80.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1426.2 tokens/s, Avg generation throughput: 1168.6 tokens/s, Running: 53 reqs, Waiting: 7 reqs, GPU KV cache usage: 93.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 811.66 tokens/s, Drafted throughput: 1057.45 tokens/s, Accepted: 8117 tokens, Drafted: 10575 tokens, Per-position acceptance rate: 0.856, 0.762, 0.684, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1433.2 tokens/s, Avg generation throughput: 1175.1 tokens/s, Running: 55 reqs, Waiting: 7 reqs, GPU KV cache usage: 95.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 817.65 tokens/s, Drafted throughput: 1048.13 tokens/s, Accepted: 8177 tokens, Drafted: 10482 tokens, Per-position acceptance rate: 0.863, 0.775, 0.702, Avg Draft acceptance rate: 78.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1130.1 tokens/s, Avg generation throughput: 1128.7 tokens/s, Running: 47 reqs, Waiting: 16 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 785.30 tokens/s, Drafted throughput: 1017.90 tokens/s, Accepted: 7853 tokens, Drafted: 10179 tokens, Per-position acceptance rate: 0.853, 0.762, 0.699, Avg Draft acceptance rate: 77.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:47 [loggers.py:257] Engine 000: Avg prompt throughput: 504.0 tokens/s, Avg generation throughput: 753.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 522.76 tokens/s, Drafted throughput: 695.95 tokens/s, Accepted: 5228 tokens, Drafted: 6960 tokens, Per-position acceptance rate: 0.832, 0.746, 0.676, Avg Draft acceptance rate: 75.1%
[0;36m(APIServer pid=44263)[0;0m INFO 01-27 14:44:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
