[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:00 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:00 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'draft_model', 'model': 'meta-llama/Llama-3.2-1B', 'num_speculative_tokens': 4, 'max_model_len': 5000}}
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:01 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:01 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:03 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:03 [model.py:1559] Using max model len 131072
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:03 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=22927)[0;0m WARNING 01-26 17:10:03 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:10:03 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=23138)[0;0m INFO 01-26 17:10:11 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='draft_model', model='meta-llama/Llama-3.2-1B', num_spec_tokens=4), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8448], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=23138)[0;0m WARNING 01-26 17:10:11 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-26 17:10:17 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:54249 backend=nccl
INFO 01-26 17:10:17 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:54249 backend=nccl
INFO 01-26 17:10:17 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-26 17:10:17 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-26 17:10:17 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-26 17:10:17 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-26 17:10:17 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-26 17:10:17 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-26 17:10:17 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-26 17:10:17 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-26 17:10:17 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:17 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:19 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:44 [default_loader.py:291] Loading weights took 23.65 seconds
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:44 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:44 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:44 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=0
[0;36m(Worker_TP1 pid=23299)[0;0m INFO 01-26 17:10:44 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=23299)[0;0m INFO 01-26 17:10:44 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP1 pid=23299)[0;0m INFO 01-26 17:10:44 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=1
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:45 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:46 [default_loader.py:291] Loading weights took 0.29 seconds
[0;36m(Worker_TP1 pid=23299)[0;0m INFO 01-26 17:10:46 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:46 [gpu_model_runner.py:3921] Model loading took 66.91 GiB memory and 27.831725 seconds
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:55 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:10:55 [backends.py:704] Dynamo bytecode transform time: 8.62 s
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:01 [backends.py:261] Cache the graph of compile range (1, 8448) for later use
[0;36m(Worker_TP1 pid=23299)[0;0m INFO 01-26 17:11:01 [backends.py:261] Cache the graph of compile range (1, 8448) for later use
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:13 [backends.py:278] Compiling a graph for compile range (1, 8448) takes 12.73 s
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:13 [monitor.py:34] torch.compile takes 21.36 s in total
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:15 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:15 [backends.py:704] Dynamo bytecode transform time: 2.37 s
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:19 [backends.py:278] Compiling a graph for compile range (1, 8448) takes 2.39 s
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:19 [monitor.py:34] torch.compile takes 26.12 s in total
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:20 [gpu_worker.py:355] Available KV cache memory: 2.5 GiB
[0;36m(EngineCore_DP0 pid=23138)[0;0m INFO 01-26 17:11:20 [kv_cache_utils.py:1307] GPU KV cache size: 14,864 tokens
[0;36m(EngineCore_DP0 pid=23138)[0;0m INFO 01-26 17:11:20 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.97x
[0;36m(Worker_TP0 pid=23298)[0;0m INFO 01-26 17:11:40 [gpu_model_runner.py:4880] Graph capturing finished in 19 secs, took 4.22 GiB
[0;36m(EngineCore_DP0 pid=23138)[0;0m INFO 01-26 17:11:40 [core.py:272] init engine (profile, create kv cache, warmup model) took 53.46 seconds
[0;36m(EngineCore_DP0 pid=23138)[0;0m INFO 01-26 17:11:42 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:43 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=22927)[0;0m WARNING 01-26 17:11:43 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:43 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:43 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:43 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [serving.py:221] Chat template warmup completed in 2920.2ms
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:11:46 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:17 [loggers.py:257] Engine 000: Avg prompt throughput: 79.7 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.28, Accepted throughput: 5.23 tokens/s, Drafted throughput: 6.38 tokens/s, Accepted: 492 tokens, Drafted: 600 tokens, Per-position acceptance rate: 0.907, 0.847, 0.793, 0.733, Avg Draft acceptance rate: 82.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:27 [loggers.py:257] Engine 000: Avg prompt throughput: 80.6 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.79, Accepted throughput: 48.90 tokens/s, Drafted throughput: 70.00 tokens/s, Accepted: 489 tokens, Drafted: 700 tokens, Per-position acceptance rate: 0.817, 0.726, 0.657, 0.594, Avg Draft acceptance rate: 69.9%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:37 [loggers.py:257] Engine 000: Avg prompt throughput: 59.9 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.85, Accepted throughput: 50.50 tokens/s, Drafted throughput: 70.80 tokens/s, Accepted: 505 tokens, Drafted: 708 tokens, Per-position acceptance rate: 0.887, 0.746, 0.650, 0.571, Avg Draft acceptance rate: 71.3%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:47 [loggers.py:257] Engine 000: Avg prompt throughput: 86.6 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 52.20 tokens/s, Drafted throughput: 69.59 tokens/s, Accepted: 522 tokens, Drafted: 696 tokens, Per-position acceptance rate: 0.851, 0.787, 0.707, 0.655, Avg Draft acceptance rate: 75.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:57 [loggers.py:257] Engine 000: Avg prompt throughput: 65.4 tokens/s, Avg generation throughput: 70.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:13:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.93, Accepted throughput: 52.20 tokens/s, Drafted throughput: 71.20 tokens/s, Accepted: 522 tokens, Drafted: 712 tokens, Per-position acceptance rate: 0.876, 0.742, 0.680, 0.635, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:07 [loggers.py:257] Engine 000: Avg prompt throughput: 79.5 tokens/s, Avg generation throughput: 69.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.97, Accepted throughput: 51.90 tokens/s, Drafted throughput: 70.00 tokens/s, Accepted: 519 tokens, Drafted: 700 tokens, Per-position acceptance rate: 0.834, 0.777, 0.714, 0.640, Avg Draft acceptance rate: 74.1%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:17 [loggers.py:257] Engine 000: Avg prompt throughput: 81.1 tokens/s, Avg generation throughput: 69.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.95, Accepted throughput: 52.00 tokens/s, Drafted throughput: 70.40 tokens/s, Accepted: 520 tokens, Drafted: 704 tokens, Per-position acceptance rate: 0.864, 0.807, 0.665, 0.619, Avg Draft acceptance rate: 73.9%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:27 [loggers.py:257] Engine 000: Avg prompt throughput: 96.8 tokens/s, Avg generation throughput: 74.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.26, Accepted throughput: 56.79 tokens/s, Drafted throughput: 69.59 tokens/s, Accepted: 568 tokens, Drafted: 696 tokens, Per-position acceptance rate: 0.920, 0.839, 0.776, 0.730, Avg Draft acceptance rate: 81.6%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:37 [loggers.py:257] Engine 000: Avg prompt throughput: 90.4 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 47.30 tokens/s, Drafted throughput: 70.00 tokens/s, Accepted: 473 tokens, Drafted: 700 tokens, Per-position acceptance rate: 0.823, 0.686, 0.634, 0.560, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:47 [loggers.py:257] Engine 000: Avg prompt throughput: 97.3 tokens/s, Avg generation throughput: 69.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.97, Accepted throughput: 51.60 tokens/s, Drafted throughput: 69.60 tokens/s, Accepted: 516 tokens, Drafted: 696 tokens, Per-position acceptance rate: 0.833, 0.782, 0.695, 0.655, Avg Draft acceptance rate: 74.1%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:57 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:14:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.13, Accepted throughput: 46.30 tokens/s, Drafted throughput: 59.20 tokens/s, Accepted: 463 tokens, Drafted: 592 tokens, Per-position acceptance rate: 0.885, 0.811, 0.736, 0.696, Avg Draft acceptance rate: 78.2%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:17 [loggers.py:257] Engine 000: Avg prompt throughput: 129.4 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 35.45 tokens/s, Drafted throughput: 46.80 tokens/s, Accepted: 709 tokens, Drafted: 936 tokens, Per-position acceptance rate: 0.859, 0.791, 0.718, 0.662, Avg Draft acceptance rate: 75.7%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:27 [loggers.py:257] Engine 000: Avg prompt throughput: 149.6 tokens/s, Avg generation throughput: 124.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.87, Accepted throughput: 91.90 tokens/s, Drafted throughput: 128.00 tokens/s, Accepted: 919 tokens, Drafted: 1280 tokens, Per-position acceptance rate: 0.859, 0.747, 0.672, 0.594, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:37 [loggers.py:257] Engine 000: Avg prompt throughput: 116.3 tokens/s, Avg generation throughput: 132.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.04, Accepted throughput: 99.69 tokens/s, Drafted throughput: 131.19 tokens/s, Accepted: 997 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.872, 0.780, 0.710, 0.677, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:47 [loggers.py:257] Engine 000: Avg prompt throughput: 156.4 tokens/s, Avg generation throughput: 126.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.95, Accepted throughput: 94.29 tokens/s, Drafted throughput: 127.99 tokens/s, Accepted: 943 tokens, Drafted: 1280 tokens, Per-position acceptance rate: 0.850, 0.791, 0.681, 0.625, Avg Draft acceptance rate: 73.7%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:57 [loggers.py:257] Engine 000: Avg prompt throughput: 138.2 tokens/s, Avg generation throughput: 125.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:15:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 93.29 tokens/s, Drafted throughput: 128.78 tokens/s, Accepted: 933 tokens, Drafted: 1288 tokens, Per-position acceptance rate: 0.857, 0.739, 0.683, 0.618, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:07 [loggers.py:257] Engine 000: Avg prompt throughput: 180.9 tokens/s, Avg generation throughput: 130.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.10, Accepted throughput: 97.90 tokens/s, Drafted throughput: 126.40 tokens/s, Accepted: 979 tokens, Drafted: 1264 tokens, Per-position acceptance rate: 0.867, 0.807, 0.731, 0.693, Avg Draft acceptance rate: 77.5%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 9.80 tokens/s, Drafted throughput: 14.00 tokens/s, Accepted: 98 tokens, Drafted: 140 tokens, Per-position acceptance rate: 0.829, 0.743, 0.629, 0.600, Avg Draft acceptance rate: 70.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:27 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.00, Accepted throughput: 1.20 tokens/s, Drafted throughput: 1.20 tokens/s, Accepted: 12 tokens, Drafted: 12 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:37 [loggers.py:257] Engine 000: Avg prompt throughput: 246.9 tokens/s, Avg generation throughput: 198.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.98, Accepted throughput: 147.59 tokens/s, Drafted throughput: 198.39 tokens/s, Accepted: 1476 tokens, Drafted: 1984 tokens, Per-position acceptance rate: 0.867, 0.770, 0.702, 0.637, Avg Draft acceptance rate: 74.4%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:47 [loggers.py:257] Engine 000: Avg prompt throughput: 247.4 tokens/s, Avg generation throughput: 234.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 173.89 tokens/s, Drafted throughput: 244.79 tokens/s, Accepted: 1739 tokens, Drafted: 2448 tokens, Per-position acceptance rate: 0.838, 0.745, 0.654, 0.605, Avg Draft acceptance rate: 71.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:57 [loggers.py:257] Engine 000: Avg prompt throughput: 315.0 tokens/s, Avg generation throughput: 242.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:16:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 181.78 tokens/s, Drafted throughput: 239.98 tokens/s, Accepted: 1818 tokens, Drafted: 2400 tokens, Per-position acceptance rate: 0.868, 0.783, 0.715, 0.663, Avg Draft acceptance rate: 75.8%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:07 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 71.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.17, Accepted throughput: 54.20 tokens/s, Drafted throughput: 68.39 tokens/s, Accepted: 542 tokens, Drafted: 684 tokens, Per-position acceptance rate: 0.889, 0.825, 0.749, 0.708, Avg Draft acceptance rate: 79.2%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:17 [loggers.py:257] Engine 000: Avg prompt throughput: 160.3 tokens/s, Avg generation throughput: 87.2 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.15, Accepted throughput: 65.49 tokens/s, Drafted throughput: 83.19 tokens/s, Accepted: 655 tokens, Drafted: 832 tokens, Per-position acceptance rate: 0.875, 0.822, 0.760, 0.692, Avg Draft acceptance rate: 78.7%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:27 [loggers.py:257] Engine 000: Avg prompt throughput: 488.9 tokens/s, Avg generation throughput: 426.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.89, Accepted throughput: 316.10 tokens/s, Drafted throughput: 437.99 tokens/s, Accepted: 3161 tokens, Drafted: 4380 tokens, Per-position acceptance rate: 0.847, 0.751, 0.673, 0.616, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:37 [loggers.py:257] Engine 000: Avg prompt throughput: 533.7 tokens/s, Avg generation throughput: 433.8 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 324.58 tokens/s, Drafted throughput: 428.77 tokens/s, Accepted: 3246 tokens, Drafted: 4288 tokens, Per-position acceptance rate: 0.864, 0.780, 0.713, 0.672, Avg Draft acceptance rate: 75.7%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:47 [loggers.py:257] Engine 000: Avg prompt throughput: 210.9 tokens/s, Avg generation throughput: 218.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 161.90 tokens/s, Drafted throughput: 223.20 tokens/s, Accepted: 1619 tokens, Drafted: 2232 tokens, Per-position acceptance rate: 0.864, 0.751, 0.674, 0.613, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:17:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:07 [loggers.py:257] Engine 000: Avg prompt throughput: 689.8 tokens/s, Avg generation throughput: 468.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.95, Accepted throughput: 173.93 tokens/s, Drafted throughput: 236.17 tokens/s, Accepted: 3479 tokens, Drafted: 4724 tokens, Per-position acceptance rate: 0.858, 0.765, 0.692, 0.631, Avg Draft acceptance rate: 73.6%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:17 [loggers.py:257] Engine 000: Avg prompt throughput: 877.0 tokens/s, Avg generation throughput: 724.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.98, Accepted throughput: 540.55 tokens/s, Drafted throughput: 726.33 tokens/s, Accepted: 5406 tokens, Drafted: 7264 tokens, Per-position acceptance rate: 0.861, 0.771, 0.699, 0.646, Avg Draft acceptance rate: 74.4%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:27 [loggers.py:257] Engine 000: Avg prompt throughput: 857.2 tokens/s, Avg generation throughput: 736.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 552.22 tokens/s, Drafted throughput: 730.30 tokens/s, Accepted: 5523 tokens, Drafted: 7304 tokens, Per-position acceptance rate: 0.865, 0.784, 0.713, 0.663, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:37 [loggers.py:257] Engine 000: Avg prompt throughput: 428.7 tokens/s, Avg generation throughput: 430.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.15, Accepted throughput: 328.21 tokens/s, Drafted throughput: 416.41 tokens/s, Accepted: 3282 tokens, Drafted: 4164 tokens, Per-position acceptance rate: 0.882, 0.811, 0.749, 0.711, Avg Draft acceptance rate: 78.8%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:47 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.46, Accepted throughput: 12.10 tokens/s, Drafted throughput: 14.00 tokens/s, Accepted: 121 tokens, Drafted: 140 tokens, Per-position acceptance rate: 0.914, 0.886, 0.857, 0.800, Avg Draft acceptance rate: 86.4%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1446.1 tokens/s, Avg generation throughput: 956.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 58.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:18:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.98, Accepted throughput: 711.05 tokens/s, Drafted throughput: 955.53 tokens/s, Accepted: 7111 tokens, Drafted: 9556 tokens, Per-position acceptance rate: 0.863, 0.772, 0.700, 0.642, Avg Draft acceptance rate: 74.4%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1326.0 tokens/s, Avg generation throughput: 1020.2 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 763.00 tokens/s, Drafted throughput: 1017.86 tokens/s, Accepted: 7631 tokens, Drafted: 10180 tokens, Per-position acceptance rate: 0.865, 0.777, 0.702, 0.654, Avg Draft acceptance rate: 75.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1147.2 tokens/s, Avg generation throughput: 1103.4 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 57.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 828.04 tokens/s, Drafted throughput: 1097.52 tokens/s, Accepted: 8281 tokens, Drafted: 10976 tokens, Per-position acceptance rate: 0.866, 0.783, 0.709, 0.660, Avg Draft acceptance rate: 75.4%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1287.6 tokens/s, Avg generation throughput: 1040.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 63.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.04, Accepted throughput: 781.87 tokens/s, Drafted throughput: 1029.96 tokens/s, Accepted: 7819 tokens, Drafted: 10300 tokens, Per-position acceptance rate: 0.871, 0.793, 0.715, 0.658, Avg Draft acceptance rate: 75.9%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:37 [loggers.py:257] Engine 000: Avg prompt throughput: 403.7 tokens/s, Avg generation throughput: 590.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 445.60 tokens/s, Drafted throughput: 590.00 tokens/s, Accepted: 4456 tokens, Drafted: 5900 tokens, Per-position acceptance rate: 0.873, 0.784, 0.708, 0.656, Avg Draft acceptance rate: 75.5%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:47 [loggers.py:257] Engine 000: Avg prompt throughput: 144.6 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.47, Accepted throughput: 12.50 tokens/s, Drafted throughput: 14.40 tokens/s, Accepted: 125 tokens, Drafted: 144 tokens, Per-position acceptance rate: 0.917, 0.889, 0.861, 0.806, Avg Draft acceptance rate: 86.8%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1642.5 tokens/s, Avg generation throughput: 1040.1 tokens/s, Running: 48 reqs, Waiting: 11 reqs, GPU KV cache usage: 87.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:19:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 770.47 tokens/s, Drafted throughput: 1039.56 tokens/s, Accepted: 7705 tokens, Drafted: 10396 tokens, Per-position acceptance rate: 0.858, 0.770, 0.695, 0.641, Avg Draft acceptance rate: 74.1%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1469.1 tokens/s, Avg generation throughput: 1114.2 tokens/s, Running: 51 reqs, Waiting: 8 reqs, GPU KV cache usage: 93.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.97, Accepted throughput: 830.37 tokens/s, Drafted throughput: 1119.16 tokens/s, Accepted: 8304 tokens, Drafted: 11192 tokens, Per-position acceptance rate: 0.855, 0.768, 0.696, 0.649, Avg Draft acceptance rate: 74.2%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1390.1 tokens/s, Avg generation throughput: 1153.5 tokens/s, Running: 56 reqs, Waiting: 6 reqs, GPU KV cache usage: 96.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.94, Accepted throughput: 858.00 tokens/s, Drafted throughput: 1166.67 tokens/s, Accepted: 8581 tokens, Drafted: 11668 tokens, Per-position acceptance rate: 0.853, 0.764, 0.690, 0.635, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1239.8 tokens/s, Avg generation throughput: 1175.0 tokens/s, Running: 48 reqs, Waiting: 12 reqs, GPU KV cache usage: 93.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 881.81 tokens/s, Drafted throughput: 1166.81 tokens/s, Accepted: 8818 tokens, Drafted: 11668 tokens, Per-position acceptance rate: 0.872, 0.789, 0.708, 0.654, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1307.5 tokens/s, Avg generation throughput: 1213.1 tokens/s, Running: 48 reqs, Waiting: 15 reqs, GPU KV cache usage: 97.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.93, Accepted throughput: 901.81 tokens/s, Drafted throughput: 1229.88 tokens/s, Accepted: 9019 tokens, Drafted: 12300 tokens, Per-position acceptance rate: 0.854, 0.764, 0.683, 0.633, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1397.4 tokens/s, Avg generation throughput: 1102.7 tokens/s, Running: 53 reqs, Waiting: 10 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.89, Accepted throughput: 816.69 tokens/s, Drafted throughput: 1130.79 tokens/s, Accepted: 8167 tokens, Drafted: 11308 tokens, Per-position acceptance rate: 0.844, 0.752, 0.676, 0.617, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1381.7 tokens/s, Avg generation throughput: 1121.5 tokens/s, Running: 47 reqs, Waiting: 14 reqs, GPU KV cache usage: 94.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:20:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 830.54 tokens/s, Drafted throughput: 1146.05 tokens/s, Accepted: 8305 tokens, Drafted: 11460 tokens, Per-position acceptance rate: 0.846, 0.750, 0.684, 0.618, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:21:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1169.3 tokens/s, Avg generation throughput: 1182.4 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:21:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.88, Accepted throughput: 876.71 tokens/s, Drafted throughput: 1216.81 tokens/s, Accepted: 8767 tokens, Drafted: 12168 tokens, Per-position acceptance rate: 0.837, 0.744, 0.675, 0.627, Avg Draft acceptance rate: 72.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:21:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 208.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=22927)[0;0m INFO 01-26 17:21:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.93, Accepted throughput: 157.20 tokens/s, Drafted throughput: 214.39 tokens/s, Accepted: 1572 tokens, Drafted: 2144 tokens, Per-position acceptance rate: 0.840, 0.757, 0.692, 0.644, Avg Draft acceptance rate: 73.3%
