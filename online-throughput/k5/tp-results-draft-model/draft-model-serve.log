[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:19 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:19 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'draft_model', 'model': 'meta-llama/Llama-3.2-1B', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:25 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:25 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:27 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:27 [model.py:1559] Using max model len 131072
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:27 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=10961)[0;0m WARNING 01-27 13:36:27 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:36:27 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=12660)[0;0m INFO 01-27 13:36:37 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='draft_model', model='meta-llama/Llama-3.2-1B', num_spec_tokens=5), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8448], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=12660)[0;0m WARNING 01-27 13:36:37 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 13:36:42 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:33275 backend=nccl
INFO 01-27 13:36:42 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:33275 backend=nccl
INFO 01-27 13:36:42 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 13:36:43 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 13:36:43 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 13:36:43 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 13:36:43 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 13:36:43 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 13:36:43 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 13:36:43 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 13:36:43 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:36:43 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:36:45 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:30 [weight_utils.py:510] Time spent downloading weights for meta-llama/Llama-3.3-70B-Instruct: 43.760430 seconds
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:53 [default_loader.py:291] Loading weights took 22.38 seconds
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:53 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:53 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:53 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=0
[0;36m(Worker_TP1 pid=12838)[0;0m INFO 01-27 13:37:55 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=12838)[0;0m INFO 01-27 13:37:55 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP1 pid=12838)[0;0m INFO 01-27 13:37:55 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=1
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:56 [weight_utils.py:510] Time spent downloading weights for meta-llama/Llama-3.2-1B: 2.716741 seconds
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:56 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:56 [default_loader.py:291] Loading weights took 0.24 seconds
[0;36m(Worker_TP1 pid=12838)[0;0m INFO 01-27 13:37:56 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:37:57 [gpu_model_runner.py:3921] Model loading took 66.91 GiB memory and 72.730456 seconds
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:06 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:06 [backends.py:704] Dynamo bytecode transform time: 8.80 s
[0;36m(Worker_TP1 pid=12838)[0;0m INFO 01-27 13:38:14 [backends.py:261] Cache the graph of compile range (1, 8448) for later use
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:14 [backends.py:261] Cache the graph of compile range (1, 8448) for later use
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:29 [backends.py:278] Compiling a graph for compile range (1, 8448) takes 17.28 s
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:29 [monitor.py:34] torch.compile takes 26.09 s in total
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:31 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:31 [backends.py:704] Dynamo bytecode transform time: 1.79 s
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:36 [backends.py:278] Compiling a graph for compile range (1, 8448) takes 4.77 s
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:36 [monitor.py:34] torch.compile takes 32.65 s in total
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:38:43 [gpu_worker.py:355] Available KV cache memory: 2.5 GiB
[0;36m(EngineCore_DP0 pid=12660)[0;0m INFO 01-27 13:38:43 [kv_cache_utils.py:1307] GPU KV cache size: 14,864 tokens
[0;36m(EngineCore_DP0 pid=12660)[0;0m INFO 01-27 13:38:43 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.97x
[0;36m(Worker_TP0 pid=12837)[0;0m INFO 01-27 13:39:03 [gpu_model_runner.py:4880] Graph capturing finished in 20 secs, took 4.22 GiB
[0;36m(EngineCore_DP0 pid=12660)[0;0m INFO 01-27 13:39:03 [core.py:272] init engine (profile, create kv cache, warmup model) took 66.02 seconds
[0;36m(EngineCore_DP0 pid=12660)[0;0m INFO 01-27 13:39:06 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:06 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=10961)[0;0m WARNING 01-27 13:39:06 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:06 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:06 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:06 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [serving.py:221] Chat template warmup completed in 2564.3ms
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:39:09 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:19 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 16.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.23, Accepted throughput: 0.68 tokens/s, Drafted throughput: 0.80 tokens/s, Accepted: 131 tokens, Drafted: 155 tokens, Per-position acceptance rate: 0.903, 0.903, 0.871, 0.774, 0.774, Avg Draft acceptance rate: 84.5%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:29 [loggers.py:257] Engine 000: Avg prompt throughput: 68.0 tokens/s, Avg generation throughput: 73.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.36, Accepted throughput: 56.10 tokens/s, Drafted throughput: 83.50 tokens/s, Accepted: 561 tokens, Drafted: 835 tokens, Per-position acceptance rate: 0.826, 0.719, 0.653, 0.599, 0.563, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:39 [loggers.py:257] Engine 000: Avg prompt throughput: 64.0 tokens/s, Avg generation throughput: 74.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.47, Accepted throughput: 58.30 tokens/s, Drafted throughput: 84.00 tokens/s, Accepted: 583 tokens, Drafted: 840 tokens, Per-position acceptance rate: 0.857, 0.774, 0.667, 0.595, 0.577, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:49 [loggers.py:257] Engine 000: Avg prompt throughput: 105.6 tokens/s, Avg generation throughput: 73.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.52, Accepted throughput: 57.40 tokens/s, Drafted throughput: 81.50 tokens/s, Accepted: 574 tokens, Drafted: 815 tokens, Per-position acceptance rate: 0.883, 0.761, 0.669, 0.626, 0.583, Avg Draft acceptance rate: 70.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:59 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:42:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.63, Accepted throughput: 60.69 tokens/s, Drafted throughput: 83.49 tokens/s, Accepted: 607 tokens, Drafted: 835 tokens, Per-position acceptance rate: 0.874, 0.790, 0.701, 0.665, 0.605, Avg Draft acceptance rate: 72.7%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:09 [loggers.py:257] Engine 000: Avg prompt throughput: 65.3 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.17, Accepted throughput: 52.89 tokens/s, Drafted throughput: 83.49 tokens/s, Accepted: 529 tokens, Drafted: 835 tokens, Per-position acceptance rate: 0.790, 0.707, 0.599, 0.551, 0.521, Avg Draft acceptance rate: 63.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:19 [loggers.py:257] Engine 000: Avg prompt throughput: 92.8 tokens/s, Avg generation throughput: 76.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.64, Accepted throughput: 60.10 tokens/s, Drafted throughput: 82.50 tokens/s, Accepted: 601 tokens, Drafted: 825 tokens, Per-position acceptance rate: 0.891, 0.836, 0.709, 0.624, 0.582, Avg Draft acceptance rate: 72.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:29 [loggers.py:257] Engine 000: Avg prompt throughput: 114.6 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.01, Accepted throughput: 65.29 tokens/s, Drafted throughput: 81.49 tokens/s, Accepted: 653 tokens, Drafted: 815 tokens, Per-position acceptance rate: 0.933, 0.840, 0.761, 0.736, 0.736, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:39 [loggers.py:257] Engine 000: Avg prompt throughput: 90.4 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.08, Accepted throughput: 51.10 tokens/s, Drafted throughput: 83.00 tokens/s, Accepted: 511 tokens, Drafted: 830 tokens, Per-position acceptance rate: 0.807, 0.675, 0.614, 0.530, 0.452, Avg Draft acceptance rate: 61.6%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:49 [loggers.py:257] Engine 000: Avg prompt throughput: 97.3 tokens/s, Avg generation throughput: 75.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.57, Accepted throughput: 58.90 tokens/s, Drafted throughput: 82.50 tokens/s, Accepted: 589 tokens, Drafted: 825 tokens, Per-position acceptance rate: 0.842, 0.800, 0.691, 0.630, 0.606, Avg Draft acceptance rate: 71.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:59 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 60.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:43:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.82, Accepted throughput: 48.50 tokens/s, Drafted throughput: 63.50 tokens/s, Accepted: 485 tokens, Drafted: 635 tokens, Per-position acceptance rate: 0.874, 0.819, 0.756, 0.709, 0.661, Avg Draft acceptance rate: 76.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:09 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 18.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.31, Accepted throughput: 15.10 tokens/s, Drafted throughput: 17.50 tokens/s, Accepted: 151 tokens, Drafted: 175 tokens, Per-position acceptance rate: 0.914, 0.914, 0.886, 0.800, 0.800, Avg Draft acceptance rate: 86.3%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:19 [loggers.py:257] Engine 000: Avg prompt throughput: 132.0 tokens/s, Avg generation throughput: 129.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.38, Accepted throughput: 99.49 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 995 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.830, 0.728, 0.656, 0.599, 0.571, Avg Draft acceptance rate: 67.7%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:29 [loggers.py:257] Engine 000: Avg prompt throughput: 167.6 tokens/s, Avg generation throughput: 129.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.39, Accepted throughput: 100.30 tokens/s, Drafted throughput: 147.99 tokens/s, Accepted: 1003 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.865, 0.757, 0.642, 0.588, 0.537, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:39 [loggers.py:257] Engine 000: Avg prompt throughput: 130.5 tokens/s, Avg generation throughput: 136.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.47, Accepted throughput: 106.30 tokens/s, Drafted throughput: 153.00 tokens/s, Accepted: 1063 tokens, Drafted: 1530 tokens, Per-position acceptance rate: 0.833, 0.768, 0.670, 0.627, 0.575, Avg Draft acceptance rate: 69.5%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:49 [loggers.py:257] Engine 000: Avg prompt throughput: 165.5 tokens/s, Avg generation throughput: 141.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.67, Accepted throughput: 110.79 tokens/s, Drafted throughput: 150.99 tokens/s, Accepted: 1108 tokens, Drafted: 1510 tokens, Per-position acceptance rate: 0.884, 0.798, 0.699, 0.656, 0.632, Avg Draft acceptance rate: 73.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:59 [loggers.py:257] Engine 000: Avg prompt throughput: 180.3 tokens/s, Avg generation throughput: 134.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:44:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.46, Accepted throughput: 103.70 tokens/s, Drafted throughput: 150.00 tokens/s, Accepted: 1037 tokens, Drafted: 1500 tokens, Per-position acceptance rate: 0.837, 0.760, 0.683, 0.610, 0.567, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:09 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.74, Accepted throughput: 46.00 tokens/s, Drafted throughput: 61.50 tokens/s, Accepted: 460 tokens, Drafted: 615 tokens, Per-position acceptance rate: 0.870, 0.813, 0.740, 0.683, 0.634, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:19 [loggers.py:257] Engine 000: Avg prompt throughput: 109.4 tokens/s, Avg generation throughput: 74.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.51, Accepted throughput: 57.49 tokens/s, Drafted throughput: 81.99 tokens/s, Accepted: 575 tokens, Drafted: 820 tokens, Per-position acceptance rate: 0.841, 0.750, 0.689, 0.622, 0.604, Avg Draft acceptance rate: 70.1%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:29 [loggers.py:257] Engine 000: Avg prompt throughput: 299.5 tokens/s, Avg generation throughput: 254.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.45, Accepted throughput: 197.29 tokens/s, Drafted throughput: 285.98 tokens/s, Accepted: 1973 tokens, Drafted: 2860 tokens, Per-position acceptance rate: 0.851, 0.759, 0.666, 0.610, 0.563, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:39 [loggers.py:257] Engine 000: Avg prompt throughput: 311.1 tokens/s, Avg generation throughput: 261.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.58, Accepted throughput: 204.09 tokens/s, Drafted throughput: 284.98 tokens/s, Accepted: 2041 tokens, Drafted: 2850 tokens, Per-position acceptance rate: 0.867, 0.782, 0.696, 0.637, 0.598, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:49 [loggers.py:257] Engine 000: Avg prompt throughput: 150.8 tokens/s, Avg generation throughput: 158.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.54, Accepted throughput: 123.69 tokens/s, Drafted throughput: 174.49 tokens/s, Accepted: 1237 tokens, Drafted: 1745 tokens, Per-position acceptance rate: 0.837, 0.777, 0.693, 0.633, 0.605, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:59 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:45:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.20, Accepted throughput: 12.60 tokens/s, Drafted throughput: 15.00 tokens/s, Accepted: 126 tokens, Drafted: 150 tokens, Per-position acceptance rate: 0.900, 0.900, 0.867, 0.767, 0.767, Avg Draft acceptance rate: 84.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:09 [loggers.py:257] Engine 000: Avg prompt throughput: 542.6 tokens/s, Avg generation throughput: 445.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.47, Accepted throughput: 345.58 tokens/s, Drafted throughput: 497.98 tokens/s, Accepted: 3456 tokens, Drafted: 4980 tokens, Per-position acceptance rate: 0.850, 0.765, 0.671, 0.610, 0.573, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:19 [loggers.py:257] Engine 000: Avg prompt throughput: 598.8 tokens/s, Avg generation throughput: 464.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.64, Accepted throughput: 364.67 tokens/s, Drafted throughput: 500.96 tokens/s, Accepted: 3647 tokens, Drafted: 5010 tokens, Per-position acceptance rate: 0.858, 0.782, 0.711, 0.665, 0.624, Avg Draft acceptance rate: 72.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:29 [loggers.py:257] Engine 000: Avg prompt throughput: 210.9 tokens/s, Avg generation throughput: 240.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.46, Accepted throughput: 187.09 tokens/s, Drafted throughput: 270.49 tokens/s, Accepted: 1871 tokens, Drafted: 2705 tokens, Per-position acceptance rate: 0.852, 0.745, 0.677, 0.619, 0.566, Avg Draft acceptance rate: 69.2%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:39 [loggers.py:257] Engine 000: Avg prompt throughput: 279.0 tokens/s, Avg generation throughput: 114.9 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.56, Accepted throughput: 88.40 tokens/s, Drafted throughput: 124.00 tokens/s, Accepted: 884 tokens, Drafted: 1240 tokens, Per-position acceptance rate: 0.867, 0.762, 0.690, 0.633, 0.613, Avg Draft acceptance rate: 71.3%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:49 [loggers.py:257] Engine 000: Avg prompt throughput: 903.9 tokens/s, Avg generation throughput: 759.5 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.54, Accepted throughput: 590.67 tokens/s, Drafted throughput: 835.46 tokens/s, Accepted: 5907 tokens, Drafted: 8355 tokens, Per-position acceptance rate: 0.855, 0.770, 0.684, 0.632, 0.594, Avg Draft acceptance rate: 70.7%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:59 [loggers.py:257] Engine 000: Avg prompt throughput: 908.7 tokens/s, Avg generation throughput: 776.2 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:46:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.57, Accepted throughput: 607.70 tokens/s, Drafted throughput: 851.51 tokens/s, Accepted: 6077 tokens, Drafted: 8515 tokens, Per-position acceptance rate: 0.860, 0.771, 0.702, 0.641, 0.595, Avg Draft acceptance rate: 71.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:09 [loggers.py:257] Engine 000: Avg prompt throughput: 761.4 tokens/s, Avg generation throughput: 702.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.73, Accepted throughput: 554.81 tokens/s, Drafted throughput: 744.52 tokens/s, Accepted: 5548 tokens, Drafted: 7445 tokens, Per-position acceptance rate: 0.878, 0.800, 0.726, 0.683, 0.639, Avg Draft acceptance rate: 74.5%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.00, Accepted throughput: 2.00 tokens/s, Drafted throughput: 2.00 tokens/s, Accepted: 20 tokens, Drafted: 20 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1102.8 tokens/s, Avg generation throughput: 679.5 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 49.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.50, Accepted throughput: 525.35 tokens/s, Drafted throughput: 751.43 tokens/s, Accepted: 5254 tokens, Drafted: 7515 tokens, Per-position acceptance rate: 0.848, 0.768, 0.675, 0.619, 0.585, Avg Draft acceptance rate: 69.9%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1321.3 tokens/s, Avg generation throughput: 1078.6 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 60.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.60, Accepted throughput: 844.95 tokens/s, Drafted throughput: 1173.43 tokens/s, Accepted: 8450 tokens, Drafted: 11735 tokens, Per-position acceptance rate: 0.865, 0.774, 0.709, 0.651, 0.602, Avg Draft acceptance rate: 72.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1267.8 tokens/s, Avg generation throughput: 1093.1 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 61.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.57, Accepted throughput: 852.46 tokens/s, Drafted throughput: 1193.44 tokens/s, Accepted: 8525 tokens, Drafted: 11935 tokens, Per-position acceptance rate: 0.858, 0.776, 0.695, 0.643, 0.600, Avg Draft acceptance rate: 71.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1404.3 tokens/s, Avg generation throughput: 1063.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:47:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.62, Accepted throughput: 833.61 tokens/s, Drafted throughput: 1152.01 tokens/s, Accepted: 8336 tokens, Drafted: 11520 tokens, Per-position acceptance rate: 0.875, 0.788, 0.706, 0.648, 0.602, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:09 [loggers.py:257] Engine 000: Avg prompt throughput: 556.0 tokens/s, Avg generation throughput: 811.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.55, Accepted throughput: 635.46 tokens/s, Drafted throughput: 895.94 tokens/s, Accepted: 6355 tokens, Drafted: 8960 tokens, Per-position acceptance rate: 0.854, 0.770, 0.691, 0.636, 0.595, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:19 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.72, Accepted throughput: 8.50 tokens/s, Drafted throughput: 9.00 tokens/s, Accepted: 85 tokens, Drafted: 90 tokens, Per-position acceptance rate: 0.944, 0.944, 0.944, 0.944, 0.944, Avg Draft acceptance rate: 94.4%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1574.3 tokens/s, Avg generation throughput: 952.5 tokens/s, Running: 50 reqs, Waiting: 9 reqs, GPU KV cache usage: 95.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.51, Accepted throughput: 734.04 tokens/s, Drafted throughput: 1044.41 tokens/s, Accepted: 7341 tokens, Drafted: 10445 tokens, Per-position acceptance rate: 0.853, 0.764, 0.682, 0.628, 0.588, Avg Draft acceptance rate: 70.3%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1529.6 tokens/s, Avg generation throughput: 1129.0 tokens/s, Running: 52 reqs, Waiting: 9 reqs, GPU KV cache usage: 94.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.58, Accepted throughput: 884.33 tokens/s, Drafted throughput: 1234.40 tokens/s, Accepted: 8844 tokens, Drafted: 12345 tokens, Per-position acceptance rate: 0.859, 0.773, 0.699, 0.646, 0.605, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1287.2 tokens/s, Avg generation throughput: 1121.8 tokens/s, Running: 51 reqs, Waiting: 9 reqs, GPU KV cache usage: 93.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.44, Accepted throughput: 866.71 tokens/s, Drafted throughput: 1260.37 tokens/s, Accepted: 8668 tokens, Drafted: 12605 tokens, Per-position acceptance rate: 0.843, 0.750, 0.668, 0.611, 0.566, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1306.6 tokens/s, Avg generation throughput: 1124.8 tokens/s, Running: 46 reqs, Waiting: 15 reqs, GPU KV cache usage: 92.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:48:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.39, Accepted throughput: 867.79 tokens/s, Drafted throughput: 1279.99 tokens/s, Accepted: 8678 tokens, Drafted: 12800 tokens, Per-position acceptance rate: 0.833, 0.739, 0.658, 0.604, 0.556, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1411.3 tokens/s, Avg generation throughput: 1185.3 tokens/s, Running: 55 reqs, Waiting: 9 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.45, Accepted throughput: 916.75 tokens/s, Drafted throughput: 1327.93 tokens/s, Accepted: 9168 tokens, Drafted: 13280 tokens, Per-position acceptance rate: 0.845, 0.749, 0.669, 0.613, 0.575, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1284.7 tokens/s, Avg generation throughput: 1159.0 tokens/s, Running: 49 reqs, Waiting: 13 reqs, GPU KV cache usage: 94.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.41, Accepted throughput: 895.18 tokens/s, Drafted throughput: 1310.97 tokens/s, Accepted: 8952 tokens, Drafted: 13110 tokens, Per-position acceptance rate: 0.847, 0.747, 0.663, 0.603, 0.554, Avg Draft acceptance rate: 68.3%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1473.4 tokens/s, Avg generation throughput: 1122.3 tokens/s, Running: 52 reqs, Waiting: 11 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.39, Accepted throughput: 863.36 tokens/s, Drafted throughput: 1272.94 tokens/s, Accepted: 8634 tokens, Drafted: 12730 tokens, Per-position acceptance rate: 0.839, 0.745, 0.667, 0.595, 0.545, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1254.0 tokens/s, Avg generation throughput: 1156.4 tokens/s, Running: 49 reqs, Waiting: 0 reqs, GPU KV cache usage: 93.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.44, Accepted throughput: 895.54 tokens/s, Drafted throughput: 1300.41 tokens/s, Accepted: 8956 tokens, Drafted: 13005 tokens, Per-position acceptance rate: 0.842, 0.741, 0.675, 0.621, 0.564, Avg Draft acceptance rate: 68.9%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 372.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.49, Accepted throughput: 294.17 tokens/s, Drafted throughput: 421.95 tokens/s, Accepted: 2942 tokens, Drafted: 4220 tokens, Per-position acceptance rate: 0.827, 0.739, 0.690, 0.636, 0.594, Avg Draft acceptance rate: 69.7%
[0;36m(APIServer pid=10961)[0;0m INFO 01-27 13:49:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
