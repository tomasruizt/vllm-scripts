[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:08 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:08 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'eagle3', 'model': 'yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', 'num_speculative_tokens': 5}}
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:09 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:09 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:13 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=26963)[0;0m WARNING 01-27 14:01:13 [model.py:1883] Casting torch.float16 to torch.bfloat16.
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:13 [model.py:1559] Using max model len 2048
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:18 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:18 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:01:18 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(EngineCore_DP0 pid=27291)[0;0m INFO 01-27 14:01:25 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='eagle3', model='yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', num_spec_tokens=5), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=27291)[0;0m WARNING 01-27 14:01:25 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 14:01:30 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59015 backend=nccl
INFO 01-27 14:01:30 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59015 backend=nccl
INFO 01-27 14:01:31 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 14:01:31 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 14:01:31 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 14:01:31 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 14:01:31 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 14:01:31 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 14:01:31 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 14:01:31 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 14:01:31 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:01:32 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:01:33 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:01:58 [default_loader.py:291] Loading weights took 23.47 seconds
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:01:58 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=27446)[0;0m INFO 01-27 14:01:58 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:05 [weight_utils.py:510] Time spent downloading weights for yuhuili/EAGLE3-LLaMA3.3-Instruct-70B: 6.447558 seconds
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:08 [default_loader.py:291] Loading weights took 3.05 seconds
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:09 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP1 pid=27446)[0;0m INFO 01-27 14:02:09 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:09 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP1 pid=27446)[0;0m INFO 01-27 14:02:10 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:10 [gpu_model_runner.py:3921] Model loading took 67.35 GiB memory and 37.587219 seconds
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:19 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:19 [backends.py:704] Dynamo bytecode transform time: 8.56 s
[0;36m(Worker_TP1 pid=27446)[0;0m INFO 01-27 14:02:25 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:25 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:39 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 14.63 s
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:39 [monitor.py:34] torch.compile takes 23.19 s in total
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:39 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:39 [backends.py:704] Dynamo bytecode transform time: 0.36 s
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:43 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 4.10 s
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:43 [monitor.py:34] torch.compile takes 27.65 s in total
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:02:45 [gpu_worker.py:355] Available KV cache memory: 2.05 GiB
[0;36m(EngineCore_DP0 pid=27291)[0;0m INFO 01-27 14:02:45 [kv_cache_utils.py:1307] GPU KV cache size: 13,280 tokens
[0;36m(EngineCore_DP0 pid=27291)[0;0m INFO 01-27 14:02:45 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.65x
[0;36m(Worker_TP0 pid=27445)[0;0m INFO 01-27 14:03:01 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 4.09 GiB
[0;36m(EngineCore_DP0 pid=27291)[0;0m INFO 01-27 14:03:01 [core.py:272] init engine (profile, create kv cache, warmup model) took 51.32 seconds
[0;36m(EngineCore_DP0 pid=27291)[0;0m INFO 01-27 14:03:04 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:04 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=26963)[0;0m WARNING 01-27 14:03:04 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:04 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:04 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:04 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [serving.py:221] Chat template warmup completed in 2672.0ms
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:03:07 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:08:48 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 10.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:08:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.59, Accepted throughput: 0.23 tokens/s, Drafted throughput: 0.32 tokens/s, Accepted: 79 tokens, Drafted: 110 tokens, Per-position acceptance rate: 0.864, 0.818, 0.773, 0.636, 0.500, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:08:58 [loggers.py:257] Engine 000: Avg prompt throughput: 70.3 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:08:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 46.60 tokens/s, Drafted throughput: 89.00 tokens/s, Accepted: 466 tokens, Drafted: 890 tokens, Per-position acceptance rate: 0.781, 0.596, 0.528, 0.404, 0.309, Avg Draft acceptance rate: 52.4%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:08 [loggers.py:257] Engine 000: Avg prompt throughput: 69.3 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 44.80 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 448 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.797, 0.578, 0.432, 0.307, 0.219, Avg Draft acceptance rate: 46.7%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:18 [loggers.py:257] Engine 000: Avg prompt throughput: 59.9 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 41.70 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 417 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.696, 0.526, 0.412, 0.284, 0.232, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:28 [loggers.py:257] Engine 000: Avg prompt throughput: 86.6 tokens/s, Avg generation throughput: 65.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 46.80 tokens/s, Drafted throughput: 95.00 tokens/s, Accepted: 468 tokens, Drafted: 950 tokens, Per-position acceptance rate: 0.721, 0.611, 0.468, 0.358, 0.305, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:38 [loggers.py:257] Engine 000: Avg prompt throughput: 65.4 tokens/s, Avg generation throughput: 57.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 38.10 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 381 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.641, 0.474, 0.354, 0.281, 0.234, Avg Draft acceptance rate: 39.7%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:48 [loggers.py:257] Engine 000: Avg prompt throughput: 65.3 tokens/s, Avg generation throughput: 71.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 52.60 tokens/s, Drafted throughput: 97.00 tokens/s, Accepted: 526 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.809, 0.639, 0.495, 0.418, 0.351, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:58 [loggers.py:257] Engine 000: Avg prompt throughput: 77.5 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:09:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 45.00 tokens/s, Drafted throughput: 96.50 tokens/s, Accepted: 450 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.741, 0.580, 0.435, 0.326, 0.249, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:08 [loggers.py:257] Engine 000: Avg prompt throughput: 103.2 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 50.10 tokens/s, Drafted throughput: 94.50 tokens/s, Accepted: 501 tokens, Drafted: 945 tokens, Per-position acceptance rate: 0.778, 0.661, 0.460, 0.413, 0.339, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:18 [loggers.py:257] Engine 000: Avg prompt throughput: 18.8 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.66, Accepted throughput: 32.90 tokens/s, Drafted throughput: 98.99 tokens/s, Accepted: 329 tokens, Drafted: 990 tokens, Per-position acceptance rate: 0.626, 0.419, 0.258, 0.202, 0.157, Avg Draft acceptance rate: 33.2%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:28 [loggers.py:257] Engine 000: Avg prompt throughput: 114.1 tokens/s, Avg generation throughput: 72.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 52.90 tokens/s, Drafted throughput: 94.49 tokens/s, Accepted: 529 tokens, Drafted: 945 tokens, Per-position acceptance rate: 0.794, 0.661, 0.577, 0.429, 0.339, Avg Draft acceptance rate: 56.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:38 [loggers.py:257] Engine 000: Avg prompt throughput: 95.8 tokens/s, Avg generation throughput: 72.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 54.10 tokens/s, Drafted throughput: 93.00 tokens/s, Accepted: 541 tokens, Drafted: 930 tokens, Per-position acceptance rate: 0.855, 0.683, 0.548, 0.452, 0.371, Avg Draft acceptance rate: 58.2%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:48 [loggers.py:257] Engine 000: Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 23.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 15.80 tokens/s, Drafted throughput: 40.00 tokens/s, Accepted: 158 tokens, Drafted: 400 tokens, Per-position acceptance rate: 0.762, 0.537, 0.375, 0.175, 0.125, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:58 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:10:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.01, Accepted throughput: 44.60 tokens/s, Drafted throughput: 74.00 tokens/s, Accepted: 446 tokens, Drafted: 740 tokens, Per-position acceptance rate: 0.804, 0.676, 0.622, 0.507, 0.405, Avg Draft acceptance rate: 60.3%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:08 [loggers.py:257] Engine 000: Avg prompt throughput: 106.5 tokens/s, Avg generation throughput: 111.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 76.10 tokens/s, Drafted throughput: 177.50 tokens/s, Accepted: 761 tokens, Drafted: 1775 tokens, Per-position acceptance rate: 0.749, 0.524, 0.400, 0.273, 0.197, Avg Draft acceptance rate: 42.9%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:18 [loggers.py:257] Engine 000: Avg prompt throughput: 143.5 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 81.89 tokens/s, Drafted throughput: 174.98 tokens/s, Accepted: 819 tokens, Drafted: 1750 tokens, Per-position acceptance rate: 0.706, 0.583, 0.437, 0.331, 0.283, Avg Draft acceptance rate: 46.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:28 [loggers.py:257] Engine 000: Avg prompt throughput: 130.5 tokens/s, Avg generation throughput: 121.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 86.70 tokens/s, Drafted throughput: 175.99 tokens/s, Accepted: 867 tokens, Drafted: 1760 tokens, Per-position acceptance rate: 0.756, 0.580, 0.457, 0.372, 0.298, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:38 [loggers.py:257] Engine 000: Avg prompt throughput: 146.7 tokens/s, Avg generation throughput: 125.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 90.09 tokens/s, Drafted throughput: 176.49 tokens/s, Accepted: 901 tokens, Drafted: 1765 tokens, Per-position acceptance rate: 0.762, 0.637, 0.459, 0.380, 0.314, Avg Draft acceptance rate: 51.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:48 [loggers.py:257] Engine 000: Avg prompt throughput: 132.9 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 73.09 tokens/s, Drafted throughput: 174.98 tokens/s, Accepted: 731 tokens, Drafted: 1750 tokens, Per-position acceptance rate: 0.683, 0.511, 0.389, 0.289, 0.217, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:58 [loggers.py:257] Engine 000: Avg prompt throughput: 119.7 tokens/s, Avg generation throughput: 105.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:11:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 76.99 tokens/s, Drafted throughput: 144.99 tokens/s, Accepted: 770 tokens, Drafted: 1450 tokens, Per-position acceptance rate: 0.828, 0.641, 0.510, 0.379, 0.297, Avg Draft acceptance rate: 53.1%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:18 [loggers.py:257] Engine 000: Avg prompt throughput: 220.2 tokens/s, Avg generation throughput: 183.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 65.00 tokens/s, Drafted throughput: 131.50 tokens/s, Accepted: 1300 tokens, Drafted: 2630 tokens, Per-position acceptance rate: 0.772, 0.587, 0.485, 0.354, 0.274, Avg Draft acceptance rate: 49.4%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:28 [loggers.py:257] Engine 000: Avg prompt throughput: 251.3 tokens/s, Avg generation throughput: 222.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 156.29 tokens/s, Drafted throughput: 333.99 tokens/s, Accepted: 1563 tokens, Drafted: 3340 tokens, Per-position acceptance rate: 0.719, 0.557, 0.431, 0.347, 0.286, Avg Draft acceptance rate: 46.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:38 [loggers.py:257] Engine 000: Avg prompt throughput: 279.6 tokens/s, Avg generation throughput: 220.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 155.19 tokens/s, Drafted throughput: 330.47 tokens/s, Accepted: 1552 tokens, Drafted: 3305 tokens, Per-position acceptance rate: 0.737, 0.585, 0.428, 0.331, 0.266, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:48 [loggers.py:257] Engine 000: Avg prompt throughput: 119.7 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 88.60 tokens/s, Drafted throughput: 171.50 tokens/s, Accepted: 886 tokens, Drafted: 1715 tokens, Per-position acceptance rate: 0.805, 0.624, 0.493, 0.373, 0.289, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:58 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:12:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 11.00 tokens/s, Drafted throughput: 20.00 tokens/s, Accepted: 110 tokens, Drafted: 200 tokens, Per-position acceptance rate: 0.700, 0.625, 0.600, 0.475, 0.350, Avg Draft acceptance rate: 55.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:08 [loggers.py:257] Engine 000: Avg prompt throughput: 450.8 tokens/s, Avg generation throughput: 384.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 270.80 tokens/s, Drafted throughput: 567.00 tokens/s, Accepted: 2708 tokens, Drafted: 5670 tokens, Per-position acceptance rate: 0.745, 0.568, 0.450, 0.345, 0.280, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:18 [loggers.py:257] Engine 000: Avg prompt throughput: 540.2 tokens/s, Avg generation throughput: 405.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 288.00 tokens/s, Drafted throughput: 593.50 tokens/s, Accepted: 2880 tokens, Drafted: 5935 tokens, Per-position acceptance rate: 0.752, 0.598, 0.450, 0.352, 0.274, Avg Draft acceptance rate: 48.5%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:28 [loggers.py:257] Engine 000: Avg prompt throughput: 382.1 tokens/s, Avg generation throughput: 358.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 255.20 tokens/s, Drafted throughput: 527.00 tokens/s, Accepted: 2552 tokens, Drafted: 5270 tokens, Per-position acceptance rate: 0.752, 0.589, 0.457, 0.344, 0.278, Avg Draft acceptance rate: 48.4%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 4.60 tokens/s, Drafted throughput: 8.50 tokens/s, Accepted: 46 tokens, Drafted: 85 tokens, Per-position acceptance rate: 0.765, 0.647, 0.529, 0.471, 0.294, Avg Draft acceptance rate: 54.1%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:48 [loggers.py:257] Engine 000: Avg prompt throughput: 584.0 tokens/s, Avg generation throughput: 394.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 277.30 tokens/s, Drafted throughput: 578.99 tokens/s, Accepted: 2773 tokens, Drafted: 5790 tokens, Per-position acceptance rate: 0.736, 0.572, 0.453, 0.353, 0.281, Avg Draft acceptance rate: 47.9%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:58 [loggers.py:257] Engine 000: Avg prompt throughput: 873.7 tokens/s, Avg generation throughput: 695.3 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:13:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 496.86 tokens/s, Drafted throughput: 1003.42 tokens/s, Accepted: 4969 tokens, Drafted: 10035 tokens, Per-position acceptance rate: 0.761, 0.600, 0.466, 0.358, 0.290, Avg Draft acceptance rate: 49.5%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:08 [loggers.py:257] Engine 000: Avg prompt throughput: 822.6 tokens/s, Avg generation throughput: 673.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 472.27 tokens/s, Drafted throughput: 1012.43 tokens/s, Accepted: 4723 tokens, Drafted: 10125 tokens, Per-position acceptance rate: 0.729, 0.566, 0.429, 0.343, 0.265, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:18 [loggers.py:257] Engine 000: Avg prompt throughput: 572.5 tokens/s, Avg generation throughput: 587.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 426.87 tokens/s, Drafted throughput: 814.44 tokens/s, Accepted: 4269 tokens, Drafted: 8145 tokens, Per-position acceptance rate: 0.770, 0.630, 0.497, 0.400, 0.324, Avg Draft acceptance rate: 52.4%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:38 [loggers.py:257] Engine 000: Avg prompt throughput: 949.3 tokens/s, Avg generation throughput: 595.4 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 62.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 210.60 tokens/s, Drafted throughput: 427.75 tokens/s, Accepted: 4212 tokens, Drafted: 8555 tokens, Per-position acceptance rate: 0.751, 0.596, 0.462, 0.362, 0.290, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1183.4 tokens/s, Avg generation throughput: 965.0 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 72.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 677.72 tokens/s, Drafted throughput: 1450.03 tokens/s, Accepted: 6777 tokens, Drafted: 14500 tokens, Per-position acceptance rate: 0.735, 0.572, 0.435, 0.334, 0.260, Avg Draft acceptance rate: 46.7%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1268.2 tokens/s, Avg generation throughput: 1030.1 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 63.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:14:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 745.89 tokens/s, Drafted throughput: 1427.98 tokens/s, Accepted: 7459 tokens, Drafted: 14280 tokens, Per-position acceptance rate: 0.772, 0.624, 0.494, 0.401, 0.321, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1122.9 tokens/s, Avg generation throughput: 1008.2 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 74.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 711.92 tokens/s, Drafted throughput: 1487.83 tokens/s, Accepted: 7120 tokens, Drafted: 14880 tokens, Per-position acceptance rate: 0.725, 0.578, 0.454, 0.351, 0.284, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1128.5 tokens/s, Avg generation throughput: 996.0 tokens/s, Running: 21 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 706.94 tokens/s, Drafted throughput: 1461.88 tokens/s, Accepted: 7070 tokens, Drafted: 14620 tokens, Per-position acceptance rate: 0.752, 0.581, 0.454, 0.353, 0.278, Avg Draft acceptance rate: 48.4%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 93.79 tokens/s, Drafted throughput: 198.48 tokens/s, Accepted: 938 tokens, Drafted: 1985 tokens, Per-position acceptance rate: 0.718, 0.572, 0.446, 0.360, 0.267, Avg Draft acceptance rate: 47.3%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1113.7 tokens/s, Avg generation throughput: 401.7 tokens/s, Running: 50 reqs, Waiting: 13 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 285.64 tokens/s, Drafted throughput: 550.39 tokens/s, Accepted: 2857 tokens, Drafted: 5505 tokens, Per-position acceptance rate: 0.752, 0.618, 0.504, 0.404, 0.317, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1018.6 tokens/s, Avg generation throughput: 1018.7 tokens/s, Running: 43 reqs, Waiting: 19 reqs, GPU KV cache usage: 95.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 713.27 tokens/s, Drafted throughput: 1530.92 tokens/s, Accepted: 7133 tokens, Drafted: 15310 tokens, Per-position acceptance rate: 0.741, 0.572, 0.428, 0.329, 0.260, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1236.3 tokens/s, Avg generation throughput: 1004.9 tokens/s, Running: 47 reqs, Waiting: 16 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:15:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 719.73 tokens/s, Drafted throughput: 1422.37 tokens/s, Accepted: 7198 tokens, Drafted: 14225 tokens, Per-position acceptance rate: 0.760, 0.607, 0.475, 0.382, 0.307, Avg Draft acceptance rate: 50.6%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1181.8 tokens/s, Avg generation throughput: 998.6 tokens/s, Running: 45 reqs, Waiting: 16 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 702.77 tokens/s, Drafted throughput: 1474.44 tokens/s, Accepted: 7028 tokens, Drafted: 14745 tokens, Per-position acceptance rate: 0.727, 0.577, 0.452, 0.349, 0.279, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1226.8 tokens/s, Avg generation throughput: 974.8 tokens/s, Running: 46 reqs, Waiting: 16 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 695.60 tokens/s, Drafted throughput: 1399.49 tokens/s, Accepted: 6956 tokens, Drafted: 13995 tokens, Per-position acceptance rate: 0.755, 0.598, 0.469, 0.369, 0.294, Avg Draft acceptance rate: 49.7%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1167.0 tokens/s, Avg generation throughput: 1039.1 tokens/s, Running: 47 reqs, Waiting: 15 reqs, GPU KV cache usage: 98.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 735.28 tokens/s, Drafted throughput: 1507.45 tokens/s, Accepted: 7353 tokens, Drafted: 15075 tokens, Per-position acceptance rate: 0.752, 0.589, 0.465, 0.357, 0.277, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1201.3 tokens/s, Avg generation throughput: 990.6 tokens/s, Running: 48 reqs, Waiting: 12 reqs, GPU KV cache usage: 97.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 702.36 tokens/s, Drafted throughput: 1438.42 tokens/s, Accepted: 7024 tokens, Drafted: 14385 tokens, Per-position acceptance rate: 0.741, 0.580, 0.466, 0.364, 0.290, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1263.2 tokens/s, Avg generation throughput: 1026.0 tokens/s, Running: 50 reqs, Waiting: 12 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 725.05 tokens/s, Drafted throughput: 1494.90 tokens/s, Accepted: 7251 tokens, Drafted: 14950 tokens, Per-position acceptance rate: 0.730, 0.577, 0.456, 0.364, 0.298, Avg Draft acceptance rate: 48.5%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1023.3 tokens/s, Avg generation throughput: 1000.5 tokens/s, Running: 42 reqs, Waiting: 19 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:16:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 693.94 tokens/s, Drafted throughput: 1536.87 tokens/s, Accepted: 6940 tokens, Drafted: 15370 tokens, Per-position acceptance rate: 0.724, 0.547, 0.418, 0.323, 0.246, Avg Draft acceptance rate: 45.2%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:17:08 [loggers.py:257] Engine 000: Avg prompt throughput: 709.6 tokens/s, Avg generation throughput: 871.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=26963)[0;0m INFO 01-27 14:17:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 609.67 tokens/s, Drafted throughput: 1342.93 tokens/s, Accepted: 6097 tokens, Drafted: 13430 tokens, Per-position acceptance rate: 0.710, 0.551, 0.427, 0.323, 0.259, Avg Draft acceptance rate: 45.4%
