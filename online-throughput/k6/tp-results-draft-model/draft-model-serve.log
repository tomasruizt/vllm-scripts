[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:56 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:56 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'draft_model', 'model': 'meta-llama/Llama-3.2-1B', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:57 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:57 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:59 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:59 [model.py:1559] Using max model len 131072
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:59 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=36194)[0;0m WARNING 01-27 14:19:59 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:19:59 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=36391)[0;0m INFO 01-27 14:20:06 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='draft_model', model='meta-llama/Llama-3.2-1B', num_spec_tokens=6), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8448], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=36391)[0;0m WARNING 01-27 14:20:06 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 14:20:12 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:54919 backend=nccl
INFO 01-27 14:20:12 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:54919 backend=nccl
INFO 01-27 14:20:12 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 14:20:12 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 14:20:12 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 14:20:12 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 14:20:12 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 14:20:12 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 14:20:12 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 14:20:12 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 14:20:12 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:13 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:14 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP1 pid=36556)[0;0m INFO 01-27 14:20:37 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=36556)[0;0m INFO 01-27 14:20:37 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP1 pid=36556)[0;0m INFO 01-27 14:20:37 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=1
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:38 [default_loader.py:291] Loading weights took 21.83 seconds
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:38 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:38 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:38 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=0
[0;36m(Worker_TP1 pid=36556)[0;0m INFO 01-27 14:20:38 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:38 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:39 [default_loader.py:291] Loading weights took 0.29 seconds
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:39 [gpu_model_runner.py:3921] Model loading took 66.91 GiB memory and 25.763717 seconds
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:48 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:20:48 [backends.py:704] Dynamo bytecode transform time: 8.51 s
[0;36m(Worker_TP1 pid=36556)[0;0m INFO 01-27 14:21:01 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.777 s
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:01 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.701 s
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:01 [monitor.py:34] torch.compile takes 17.21 s in total
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:03 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:03 [backends.py:704] Dynamo bytecode transform time: 1.78 s
[0;36m(Worker_TP1 pid=36556)[0;0m INFO 01-27 14:21:04 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.298 s
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:04 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.280 s
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:04 [monitor.py:34] torch.compile takes 19.28 s in total
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:11 [gpu_worker.py:355] Available KV cache memory: 2.5 GiB
[0;36m(EngineCore_DP0 pid=36391)[0;0m INFO 01-27 14:21:11 [kv_cache_utils.py:1307] GPU KV cache size: 14,864 tokens
[0;36m(EngineCore_DP0 pid=36391)[0;0m INFO 01-27 14:21:11 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.97x
[0;36m(Worker_TP0 pid=36555)[0;0m INFO 01-27 14:21:31 [gpu_model_runner.py:4880] Graph capturing finished in 20 secs, took 4.23 GiB
[0;36m(EngineCore_DP0 pid=36391)[0;0m INFO 01-27 14:21:31 [core.py:272] init engine (profile, create kv cache, warmup model) took 52.14 seconds
[0;36m(EngineCore_DP0 pid=36391)[0;0m INFO 01-27 14:21:34 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:34 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=36194)[0;0m WARNING 01-27 14:21:34 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:34 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:34 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:34 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [serving.py:221] Chat template warmup completed in 2788.1ms
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:21:37 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:17 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 7.00, Accepted throughput: 1.65 tokens/s, Drafted throughput: 1.65 tokens/s, Accepted: 72 tokens, Drafted: 72 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:27 [loggers.py:257] Engine 000: Avg prompt throughput: 88.7 tokens/s, Avg generation throughput: 73.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.99, Accepted throughput: 58.19 tokens/s, Drafted throughput: 87.59 tokens/s, Accepted: 582 tokens, Drafted: 876 tokens, Per-position acceptance rate: 0.808, 0.740, 0.685, 0.623, 0.582, 0.548, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:37 [loggers.py:257] Engine 000: Avg prompt throughput: 64.0 tokens/s, Avg generation throughput: 79.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.99, Accepted throughput: 63.90 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 639 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.863, 0.762, 0.650, 0.606, 0.575, 0.537, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:47 [loggers.py:257] Engine 000: Avg prompt throughput: 105.6 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.91, Accepted throughput: 61.00 tokens/s, Drafted throughput: 93.60 tokens/s, Accepted: 610 tokens, Drafted: 936 tokens, Per-position acceptance rate: 0.872, 0.750, 0.660, 0.596, 0.551, 0.481, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:57 [loggers.py:257] Engine 000: Avg prompt throughput: 93.2 tokens/s, Avg generation throughput: 80.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:22:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.04, Accepted throughput: 64.20 tokens/s, Drafted throughput: 95.39 tokens/s, Accepted: 642 tokens, Drafted: 954 tokens, Per-position acceptance rate: 0.855, 0.767, 0.673, 0.616, 0.579, 0.547, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:07 [loggers.py:257] Engine 000: Avg prompt throughput: 65.3 tokens/s, Avg generation throughput: 75.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.72, Accepted throughput: 59.60 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 596 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.812, 0.719, 0.625, 0.550, 0.519, 0.500, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:17 [loggers.py:257] Engine 000: Avg prompt throughput: 95.3 tokens/s, Avg generation throughput: 79.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.99, Accepted throughput: 63.10 tokens/s, Drafted throughput: 94.81 tokens/s, Accepted: 631 tokens, Drafted: 948 tokens, Per-position acceptance rate: 0.842, 0.797, 0.652, 0.608, 0.576, 0.519, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:27 [loggers.py:257] Engine 000: Avg prompt throughput: 96.8 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.40, Accepted throughput: 69.50 tokens/s, Drafted throughput: 94.79 tokens/s, Accepted: 695 tokens, Drafted: 948 tokens, Per-position acceptance rate: 0.886, 0.791, 0.734, 0.703, 0.665, 0.620, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:37 [loggers.py:257] Engine 000: Avg prompt throughput: 90.4 tokens/s, Avg generation throughput: 69.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.35, Accepted throughput: 53.30 tokens/s, Drafted throughput: 95.39 tokens/s, Accepted: 533 tokens, Drafted: 954 tokens, Per-position acceptance rate: 0.761, 0.642, 0.585, 0.509, 0.440, 0.415, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:47 [loggers.py:257] Engine 000: Avg prompt throughput: 126.9 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.43, Accepted throughput: 68.20 tokens/s, Drafted throughput: 92.39 tokens/s, Accepted: 682 tokens, Drafted: 924 tokens, Per-position acceptance rate: 0.903, 0.857, 0.740, 0.669, 0.649, 0.610, Avg Draft acceptance rate: 73.8%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:57 [loggers.py:257] Engine 000: Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:23:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.99, Accepted throughput: 27.90 tokens/s, Drafted throughput: 42.00 tokens/s, Accepted: 279 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.857, 0.786, 0.671, 0.600, 0.557, 0.514, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:07 [loggers.py:257] Engine 000: Avg prompt throughput: 79.7 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.73, Accepted throughput: 45.40 tokens/s, Drafted throughput: 57.60 tokens/s, Accepted: 454 tokens, Drafted: 576 tokens, Per-position acceptance rate: 0.885, 0.833, 0.823, 0.750, 0.729, 0.708, Avg Draft acceptance rate: 78.8%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:17 [loggers.py:257] Engine 000: Avg prompt throughput: 153.4 tokens/s, Avg generation throughput: 135.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.73, Accepted throughput: 106.79 tokens/s, Drafted throughput: 171.58 tokens/s, Accepted: 1068 tokens, Drafted: 1716 tokens, Per-position acceptance rate: 0.832, 0.734, 0.615, 0.556, 0.517, 0.479, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:27 [loggers.py:257] Engine 000: Avg prompt throughput: 162.2 tokens/s, Avg generation throughput: 144.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.95, Accepted throughput: 114.60 tokens/s, Drafted throughput: 174.00 tokens/s, Accepted: 1146 tokens, Drafted: 1740 tokens, Per-position acceptance rate: 0.855, 0.752, 0.666, 0.607, 0.559, 0.514, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:37 [loggers.py:257] Engine 000: Avg prompt throughput: 156.4 tokens/s, Avg generation throughput: 144.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.98, Accepted throughput: 114.60 tokens/s, Drafted throughput: 172.80 tokens/s, Accepted: 1146 tokens, Drafted: 1728 tokens, Per-position acceptance rate: 0.840, 0.781, 0.660, 0.601, 0.569, 0.528, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:47 [loggers.py:257] Engine 000: Avg prompt throughput: 180.0 tokens/s, Avg generation throughput: 142.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.95, Accepted throughput: 113.09 tokens/s, Drafted throughput: 171.58 tokens/s, Accepted: 1131 tokens, Drafted: 1716 tokens, Per-position acceptance rate: 0.839, 0.734, 0.675, 0.622, 0.563, 0.521, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:57 [loggers.py:257] Engine 000: Avg prompt throughput: 139.1 tokens/s, Avg generation throughput: 127.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:24:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.11, Accepted throughput: 101.89 tokens/s, Drafted throughput: 148.79 tokens/s, Accepted: 1019 tokens, Drafted: 1488 tokens, Per-position acceptance rate: 0.855, 0.794, 0.685, 0.617, 0.593, 0.565, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:17 [loggers.py:257] Engine 000: Avg prompt throughput: 267.6 tokens/s, Avg generation throughput: 217.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.00, Accepted throughput: 86.45 tokens/s, Drafted throughput: 129.60 tokens/s, Accepted: 1729 tokens, Drafted: 2592 tokens, Per-position acceptance rate: 0.845, 0.757, 0.674, 0.613, 0.579, 0.535, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:27 [loggers.py:257] Engine 000: Avg prompt throughput: 316.4 tokens/s, Avg generation throughput: 270.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.93, Accepted throughput: 215.20 tokens/s, Drafted throughput: 328.80 tokens/s, Accepted: 2152 tokens, Drafted: 3288 tokens, Per-position acceptance rate: 0.841, 0.757, 0.650, 0.597, 0.560, 0.522, Avg Draft acceptance rate: 65.5%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:37 [loggers.py:257] Engine 000: Avg prompt throughput: 286.8 tokens/s, Avg generation throughput: 255.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.00, Accepted throughput: 203.68 tokens/s, Drafted throughput: 305.37 tokens/s, Accepted: 2037 tokens, Drafted: 3054 tokens, Per-position acceptance rate: 0.839, 0.754, 0.678, 0.617, 0.578, 0.536, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.00, Accepted throughput: 4.50 tokens/s, Drafted throughput: 5.40 tokens/s, Accepted: 45 tokens, Drafted: 54 tokens, Per-position acceptance rate: 1.000, 0.889, 0.889, 0.889, 0.667, 0.667, Avg Draft acceptance rate: 83.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:57 [loggers.py:257] Engine 000: Avg prompt throughput: 422.2 tokens/s, Avg generation throughput: 328.6 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:25:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.97, Accepted throughput: 261.47 tokens/s, Drafted throughput: 394.76 tokens/s, Accepted: 2615 tokens, Drafted: 3948 tokens, Per-position acceptance rate: 0.848, 0.755, 0.667, 0.606, 0.567, 0.530, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:07 [loggers.py:257] Engine 000: Avg prompt throughput: 589.5 tokens/s, Avg generation throughput: 482.8 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.03, Accepted throughput: 385.49 tokens/s, Drafted throughput: 573.59 tokens/s, Accepted: 3855 tokens, Drafted: 5736 tokens, Per-position acceptance rate: 0.846, 0.771, 0.676, 0.620, 0.581, 0.539, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:17 [loggers.py:257] Engine 000: Avg prompt throughput: 382.1 tokens/s, Avg generation throughput: 358.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.98, Accepted throughput: 287.79 tokens/s, Drafted throughput: 433.79 tokens/s, Accepted: 2878 tokens, Drafted: 4338 tokens, Per-position acceptance rate: 0.849, 0.740, 0.667, 0.624, 0.567, 0.534, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:27 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.00, Accepted throughput: 13.00 tokens/s, Drafted throughput: 15.60 tokens/s, Accepted: 130 tokens, Drafted: 156 tokens, Per-position acceptance rate: 0.885, 0.885, 0.885, 0.808, 0.769, 0.769, Avg Draft acceptance rate: 83.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:37 [loggers.py:257] Engine 000: Avg prompt throughput: 987.4 tokens/s, Avg generation throughput: 725.7 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.99, Accepted throughput: 577.58 tokens/s, Drafted throughput: 868.77 tokens/s, Accepted: 5776 tokens, Drafted: 8688 tokens, Per-position acceptance rate: 0.843, 0.758, 0.668, 0.612, 0.576, 0.532, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:47 [loggers.py:257] Engine 000: Avg prompt throughput: 943.7 tokens/s, Avg generation throughput: 786.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.05, Accepted throughput: 630.37 tokens/s, Drafted throughput: 934.75 tokens/s, Accepted: 6304 tokens, Drafted: 9348 tokens, Per-position acceptance rate: 0.850, 0.757, 0.686, 0.632, 0.580, 0.541, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:57 [loggers.py:257] Engine 000: Avg prompt throughput: 880.4 tokens/s, Avg generation throughput: 784.9 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:26:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.22, Accepted throughput: 634.62 tokens/s, Drafted throughput: 903.03 tokens/s, Accepted: 6346 tokens, Drafted: 9030 tokens, Per-position acceptance rate: 0.868, 0.781, 0.706, 0.658, 0.622, 0.581, Avg Draft acceptance rate: 70.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 40.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.93, Accepted throughput: 33.40 tokens/s, Drafted throughput: 51.00 tokens/s, Accepted: 334 tokens, Drafted: 510 tokens, Per-position acceptance rate: 0.882, 0.776, 0.635, 0.600, 0.541, 0.494, Avg Draft acceptance rate: 65.5%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:17 [loggers.py:257] Engine 000: Avg prompt throughput: 672.3 tokens/s, Avg generation throughput: 370.3 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 56.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.10, Accepted throughput: 294.48 tokens/s, Drafted throughput: 431.37 tokens/s, Accepted: 2945 tokens, Drafted: 4314 tokens, Per-position acceptance rate: 0.861, 0.766, 0.684, 0.630, 0.595, 0.559, Avg Draft acceptance rate: 68.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1342.6 tokens/s, Avg generation throughput: 1056.8 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 61.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.97, Accepted throughput: 842.22 tokens/s, Drafted throughput: 1271.88 tokens/s, Accepted: 8423 tokens, Drafted: 12720 tokens, Per-position acceptance rate: 0.840, 0.750, 0.670, 0.618, 0.568, 0.526, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1306.4 tokens/s, Avg generation throughput: 1080.2 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 55.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.16, Accepted throughput: 870.58 tokens/s, Drafted throughput: 1255.17 tokens/s, Accepted: 8706 tokens, Drafted: 12552 tokens, Per-position acceptance rate: 0.860, 0.779, 0.697, 0.647, 0.607, 0.571, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1217.4 tokens/s, Avg generation throughput: 1114.2 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.00, Accepted throughput: 892.69 tokens/s, Drafted throughput: 1337.38 tokens/s, Accepted: 8927 tokens, Drafted: 13374 tokens, Per-position acceptance rate: 0.855, 0.760, 0.678, 0.620, 0.566, 0.527, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1113.3 tokens/s, Avg generation throughput: 1052.8 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:27:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.01, Accepted throughput: 844.97 tokens/s, Drafted throughput: 1265.81 tokens/s, Accepted: 8451 tokens, Drafted: 12660 tokens, Per-position acceptance rate: 0.852, 0.755, 0.669, 0.617, 0.576, 0.537, Avg Draft acceptance rate: 66.8%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.52, Accepted throughput: 43.80 tokens/s, Drafted throughput: 58.20 tokens/s, Accepted: 438 tokens, Drafted: 582 tokens, Per-position acceptance rate: 0.938, 0.814, 0.763, 0.722, 0.660, 0.619, Avg Draft acceptance rate: 75.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1130.5 tokens/s, Avg generation throughput: 350.2 tokens/s, Running: 60 reqs, Waiting: 0 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.99, Accepted throughput: 274.79 tokens/s, Drafted throughput: 412.78 tokens/s, Accepted: 2748 tokens, Drafted: 4128 tokens, Per-position acceptance rate: 0.834, 0.754, 0.670, 0.613, 0.581, 0.541, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1271.0 tokens/s, Avg generation throughput: 1202.3 tokens/s, Running: 51 reqs, Waiting: 12 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.95, Accepted throughput: 956.62 tokens/s, Drafted throughput: 1453.09 tokens/s, Accepted: 9567 tokens, Drafted: 14532 tokens, Per-position acceptance rate: 0.846, 0.749, 0.663, 0.612, 0.562, 0.519, Avg Draft acceptance rate: 65.8%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1365.4 tokens/s, Avg generation throughput: 1075.7 tokens/s, Running: 52 reqs, Waiting: 7 reqs, GPU KV cache usage: 95.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.00, Accepted throughput: 858.47 tokens/s, Drafted throughput: 1286.95 tokens/s, Accepted: 8585 tokens, Drafted: 12870 tokens, Per-position acceptance rate: 0.840, 0.752, 0.673, 0.621, 0.576, 0.541, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1276.7 tokens/s, Avg generation throughput: 1130.3 tokens/s, Running: 50 reqs, Waiting: 10 reqs, GPU KV cache usage: 93.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.84, Accepted throughput: 896.47 tokens/s, Drafted throughput: 1400.35 tokens/s, Accepted: 8965 tokens, Drafted: 14004 tokens, Per-position acceptance rate: 0.829, 0.726, 0.650, 0.595, 0.540, 0.501, Avg Draft acceptance rate: 64.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1287.7 tokens/s, Avg generation throughput: 1114.4 tokens/s, Running: 50 reqs, Waiting: 9 reqs, GPU KV cache usage: 95.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:28:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.90, Accepted throughput: 886.12 tokens/s, Drafted throughput: 1361.88 tokens/s, Accepted: 8862 tokens, Drafted: 13620 tokens, Per-position acceptance rate: 0.830, 0.734, 0.653, 0.596, 0.564, 0.526, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1344.6 tokens/s, Avg generation throughput: 1122.2 tokens/s, Running: 49 reqs, Waiting: 11 reqs, GPU KV cache usage: 92.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.72, Accepted throughput: 882.72 tokens/s, Drafted throughput: 1424.88 tokens/s, Accepted: 8828 tokens, Drafted: 14250 tokens, Per-position acceptance rate: 0.823, 0.723, 0.621, 0.566, 0.514, 0.471, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1334.4 tokens/s, Avg generation throughput: 1101.1 tokens/s, Running: 52 reqs, Waiting: 3 reqs, GPU KV cache usage: 88.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.78, Accepted throughput: 868.65 tokens/s, Drafted throughput: 1380.52 tokens/s, Accepted: 8687 tokens, Drafted: 13806 tokens, Per-position acceptance rate: 0.808, 0.711, 0.634, 0.581, 0.538, 0.503, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1270.0 tokens/s, Avg generation throughput: 1090.1 tokens/s, Running: 49 reqs, Waiting: 10 reqs, GPU KV cache usage: 94.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.72, Accepted throughput: 856.78 tokens/s, Drafted throughput: 1381.76 tokens/s, Accepted: 8568 tokens, Drafted: 13818 tokens, Per-position acceptance rate: 0.808, 0.705, 0.632, 0.569, 0.523, 0.484, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:37 [loggers.py:257] Engine 000: Avg prompt throughput: 861.6 tokens/s, Avg generation throughput: 1136.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.86, Accepted throughput: 908.18 tokens/s, Drafted throughput: 1410.57 tokens/s, Accepted: 9082 tokens, Drafted: 14106 tokens, Per-position acceptance rate: 0.819, 0.724, 0.658, 0.600, 0.550, 0.512, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 7.00 tokens/s, Drafted throughput: 28.80 tokens/s, Accepted: 70 tokens, Drafted: 288 tokens, Per-position acceptance rate: 0.438, 0.333, 0.271, 0.188, 0.125, 0.104, Avg Draft acceptance rate: 24.3%
[0;36m(APIServer pid=36194)[0;0m INFO 01-27 14:29:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
