[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:30 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:30 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'eagle3', 'model': 'yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', 'num_speculative_tokens': 6}}
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:31 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:31 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:33 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=68102)[0;0m WARNING 01-27 15:13:33 [model.py:1883] Casting torch.float16 to torch.bfloat16.
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:33 [model.py:1559] Using max model len 2048
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:33 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:33 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:13:33 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(EngineCore_DP0 pid=68297)[0;0m INFO 01-27 15:13:40 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='eagle3', model='yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', num_spec_tokens=6), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=68297)[0;0m WARNING 01-27 15:13:40 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 15:13:46 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:40441 backend=nccl
INFO 01-27 15:13:46 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:40441 backend=nccl
INFO 01-27 15:13:46 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 15:13:46 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 15:13:46 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 15:13:46 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 15:13:46 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 15:13:46 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 15:13:46 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 15:13:46 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 15:13:46 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:13:46 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:13:48 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP1 pid=68470)[0;0m INFO 01-27 15:14:11 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:11 [default_loader.py:291] Loading weights took 21.95 seconds
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:11 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:15 [default_loader.py:291] Loading weights took 3.02 seconds
[0;36m(Worker_TP1 pid=68470)[0;0m INFO 01-27 15:14:15 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:16 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP1 pid=68470)[0;0m INFO 01-27 15:14:16 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:17 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:17 [gpu_model_runner.py:3921] Model loading took 67.35 GiB memory and 29.971780 seconds
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:26 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:26 [backends.py:704] Dynamo bytecode transform time: 8.69 s
[0;36m(Worker_TP1 pid=68470)[0;0m INFO 01-27 15:14:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 8.770 s
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 8.441 s
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:39 [monitor.py:34] torch.compile takes 17.13 s in total
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:40 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:40 [backends.py:704] Dynamo bytecode transform time: 0.35 s
[0;36m(Worker_TP1 pid=68470)[0;0m INFO 01-27 15:14:40 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.048 s
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:40 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.041 s
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:40 [monitor.py:34] torch.compile takes 17.52 s in total
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:41 [gpu_worker.py:355] Available KV cache memory: 2.05 GiB
[0;36m(EngineCore_DP0 pid=68297)[0;0m INFO 01-27 15:14:41 [kv_cache_utils.py:1307] GPU KV cache size: 13,280 tokens
[0;36m(EngineCore_DP0 pid=68297)[0;0m INFO 01-27 15:14:41 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.65x
[0;36m(Worker_TP0 pid=68469)[0;0m INFO 01-27 15:14:58 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 4.10 GiB
[0;36m(EngineCore_DP0 pid=68297)[0;0m INFO 01-27 15:14:58 [core.py:272] init engine (profile, create kv cache, warmup model) took 40.75 seconds
[0;36m(EngineCore_DP0 pid=68297)[0;0m INFO 01-27 15:15:00 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:01 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=68102)[0;0m WARNING 01-27 15:15:01 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:01 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:01 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:01 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:03 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:03 [serving.py:221] Chat template warmup completed in 2564.6ms
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:15:04 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:14 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 30.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 3.10 tokens/s, Drafted throughput: 6.37 tokens/s, Accepted: 228 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.692, 0.603, 0.577, 0.487, 0.308, 0.256, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:24 [loggers.py:257] Engine 000: Avg prompt throughput: 88.0 tokens/s, Avg generation throughput: 74.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.99, Accepted throughput: 55.60 tokens/s, Drafted throughput: 111.59 tokens/s, Accepted: 556 tokens, Drafted: 1116 tokens, Per-position acceptance rate: 0.855, 0.634, 0.554, 0.403, 0.323, 0.220, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:34 [loggers.py:257] Engine 000: Avg prompt throughput: 44.0 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 37.10 tokens/s, Drafted throughput: 114.59 tokens/s, Accepted: 371 tokens, Drafted: 1146 tokens, Per-position acceptance rate: 0.665, 0.450, 0.325, 0.209, 0.157, 0.136, Avg Draft acceptance rate: 32.4%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:44 [loggers.py:257] Engine 000: Avg prompt throughput: 94.2 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 45.90 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 459 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.716, 0.568, 0.432, 0.311, 0.290, 0.191, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:54 [loggers.py:257] Engine 000: Avg prompt throughput: 56.1 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:16:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 45.00 tokens/s, Drafted throughput: 113.99 tokens/s, Accepted: 450 tokens, Drafted: 1140 tokens, Per-position acceptance rate: 0.663, 0.516, 0.389, 0.316, 0.274, 0.211, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:04 [loggers.py:257] Engine 000: Avg prompt throughput: 71.6 tokens/s, Avg generation throughput: 69.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 50.70 tokens/s, Drafted throughput: 112.79 tokens/s, Accepted: 507 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.750, 0.580, 0.447, 0.362, 0.303, 0.255, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:14 [loggers.py:257] Engine 000: Avg prompt throughput: 76.2 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 50.19 tokens/s, Drafted throughput: 112.79 tokens/s, Accepted: 502 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.718, 0.559, 0.447, 0.378, 0.309, 0.261, Avg Draft acceptance rate: 44.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:24 [loggers.py:257] Engine 000: Avg prompt throughput: 80.2 tokens/s, Avg generation throughput: 69.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.72, Accepted throughput: 51.10 tokens/s, Drafted throughput: 112.79 tokens/s, Accepted: 511 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.766, 0.601, 0.457, 0.367, 0.287, 0.239, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:34 [loggers.py:257] Engine 000: Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 47.40 tokens/s, Drafted throughput: 111.59 tokens/s, Accepted: 474 tokens, Drafted: 1116 tokens, Per-position acceptance rate: 0.758, 0.565, 0.414, 0.344, 0.263, 0.204, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:44 [loggers.py:257] Engine 000: Avg prompt throughput: 83.0 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 36.90 tokens/s, Drafted throughput: 112.80 tokens/s, Accepted: 369 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.628, 0.463, 0.330, 0.234, 0.176, 0.133, Avg Draft acceptance rate: 32.7%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:54 [loggers.py:257] Engine 000: Avg prompt throughput: 97.3 tokens/s, Avg generation throughput: 74.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:17:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.05, Accepted throughput: 55.90 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 559 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.792, 0.667, 0.552, 0.432, 0.333, 0.279, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:04 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 40.50 tokens/s, Drafted throughput: 91.19 tokens/s, Accepted: 405 tokens, Drafted: 912 tokens, Per-position acceptance rate: 0.829, 0.612, 0.467, 0.296, 0.257, 0.204, Avg Draft acceptance rate: 44.4%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:14 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 11.20 tokens/s, Drafted throughput: 22.20 tokens/s, Accepted: 112 tokens, Drafted: 222 tokens, Per-position acceptance rate: 0.703, 0.622, 0.595, 0.514, 0.324, 0.270, Avg Draft acceptance rate: 50.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:24 [loggers.py:257] Engine 000: Avg prompt throughput: 152.7 tokens/s, Avg generation throughput: 118.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.83, Accepted throughput: 87.60 tokens/s, Drafted throughput: 186.00 tokens/s, Accepted: 876 tokens, Drafted: 1860 tokens, Per-position acceptance rate: 0.797, 0.623, 0.523, 0.381, 0.290, 0.213, Avg Draft acceptance rate: 47.1%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:34 [loggers.py:257] Engine 000: Avg prompt throughput: 133.4 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 73.90 tokens/s, Drafted throughput: 204.01 tokens/s, Accepted: 739 tokens, Drafted: 2040 tokens, Per-position acceptance rate: 0.676, 0.482, 0.362, 0.262, 0.232, 0.159, Avg Draft acceptance rate: 36.2%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:44 [loggers.py:257] Engine 000: Avg prompt throughput: 102.1 tokens/s, Avg generation throughput: 120.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 85.99 tokens/s, Drafted throughput: 207.58 tokens/s, Accepted: 860 tokens, Drafted: 2076 tokens, Per-position acceptance rate: 0.708, 0.538, 0.410, 0.327, 0.275, 0.228, Avg Draft acceptance rate: 41.4%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:54 [loggers.py:257] Engine 000: Avg prompt throughput: 142.8 tokens/s, Avg generation throughput: 129.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:18:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 94.90 tokens/s, Drafted throughput: 205.20 tokens/s, Accepted: 949 tokens, Drafted: 2052 tokens, Per-position acceptance rate: 0.775, 0.594, 0.462, 0.380, 0.304, 0.260, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:04 [loggers.py:257] Engine 000: Avg prompt throughput: 138.2 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 76.90 tokens/s, Drafted throughput: 204.00 tokens/s, Accepted: 769 tokens, Drafted: 2040 tokens, Per-position acceptance rate: 0.697, 0.509, 0.371, 0.291, 0.221, 0.174, Avg Draft acceptance rate: 37.7%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:14 [loggers.py:257] Engine 000: Avg prompt throughput: 180.9 tokens/s, Avg generation throughput: 130.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 97.40 tokens/s, Drafted throughput: 197.40 tokens/s, Accepted: 974 tokens, Drafted: 1974 tokens, Per-position acceptance rate: 0.802, 0.660, 0.532, 0.392, 0.316, 0.258, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 10.40 tokens/s, Drafted throughput: 30.60 tokens/s, Accepted: 104 tokens, Drafted: 306 tokens, Per-position acceptance rate: 0.725, 0.490, 0.392, 0.216, 0.137, 0.078, Avg Draft acceptance rate: 34.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:34 [loggers.py:257] Engine 000: Avg prompt throughput: 184.5 tokens/s, Avg generation throughput: 139.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.83, Accepted throughput: 102.79 tokens/s, Drafted throughput: 217.77 tokens/s, Accepted: 1028 tokens, Drafted: 2178 tokens, Per-position acceptance rate: 0.782, 0.614, 0.526, 0.394, 0.300, 0.215, Avg Draft acceptance rate: 47.2%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:44 [loggers.py:257] Engine 000: Avg prompt throughput: 237.7 tokens/s, Avg generation throughput: 222.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 156.69 tokens/s, Drafted throughput: 394.78 tokens/s, Accepted: 1567 tokens, Drafted: 3948 tokens, Per-position acceptance rate: 0.685, 0.518, 0.398, 0.307, 0.267, 0.205, Avg Draft acceptance rate: 39.7%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:54 [loggers.py:257] Engine 000: Avg prompt throughput: 250.2 tokens/s, Avg generation throughput: 229.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:19:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 164.39 tokens/s, Drafted throughput: 393.58 tokens/s, Accepted: 1644 tokens, Drafted: 3936 tokens, Per-position acceptance rate: 0.733, 0.550, 0.418, 0.332, 0.259, 0.213, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:04 [loggers.py:257] Engine 000: Avg prompt throughput: 198.4 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 114.89 tokens/s, Drafted throughput: 260.38 tokens/s, Accepted: 1149 tokens, Drafted: 2604 tokens, Per-position acceptance rate: 0.760, 0.599, 0.472, 0.341, 0.265, 0.210, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:14 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.11, Accepted throughput: 8.70 tokens/s, Drafted throughput: 16.80 tokens/s, Accepted: 87 tokens, Drafted: 168 tokens, Per-position acceptance rate: 0.750, 0.643, 0.607, 0.500, 0.321, 0.286, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:24 [loggers.py:257] Engine 000: Avg prompt throughput: 450.8 tokens/s, Avg generation throughput: 370.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 265.49 tokens/s, Drafted throughput: 625.18 tokens/s, Accepted: 2655 tokens, Drafted: 6252 tokens, Per-position acceptance rate: 0.726, 0.551, 0.441, 0.337, 0.277, 0.215, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:34 [loggers.py:257] Engine 000: Avg prompt throughput: 540.1 tokens/s, Avg generation throughput: 410.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 297.26 tokens/s, Drafted throughput: 688.11 tokens/s, Accepted: 2973 tokens, Drafted: 6882 tokens, Per-position acceptance rate: 0.748, 0.578, 0.446, 0.339, 0.266, 0.214, Avg Draft acceptance rate: 43.2%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:44 [loggers.py:257] Engine 000: Avg prompt throughput: 382.1 tokens/s, Avg generation throughput: 367.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 267.89 tokens/s, Drafted throughput: 609.57 tokens/s, Accepted: 2679 tokens, Drafted: 6096 tokens, Per-position acceptance rate: 0.760, 0.594, 0.455, 0.344, 0.266, 0.219, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:20:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 4.20 tokens/s, Drafted throughput: 8.40 tokens/s, Accepted: 42 tokens, Drafted: 84 tokens, Per-position acceptance rate: 0.786, 0.714, 0.571, 0.500, 0.286, 0.143, Avg Draft acceptance rate: 50.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:04 [loggers.py:257] Engine 000: Avg prompt throughput: 629.6 tokens/s, Avg generation throughput: 447.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 319.89 tokens/s, Drafted throughput: 752.98 tokens/s, Accepted: 3199 tokens, Drafted: 7530 tokens, Per-position acceptance rate: 0.726, 0.551, 0.442, 0.341, 0.277, 0.211, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:14 [loggers.py:257] Engine 000: Avg prompt throughput: 893.2 tokens/s, Avg generation throughput: 684.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 498.70 tokens/s, Drafted throughput: 1123.80 tokens/s, Accepted: 4987 tokens, Drafted: 11238 tokens, Per-position acceptance rate: 0.755, 0.592, 0.461, 0.350, 0.277, 0.227, Avg Draft acceptance rate: 44.4%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:24 [loggers.py:257] Engine 000: Avg prompt throughput: 771.1 tokens/s, Avg generation throughput: 678.9 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 486.75 tokens/s, Drafted throughput: 1161.48 tokens/s, Accepted: 4868 tokens, Drafted: 11616 tokens, Per-position acceptance rate: 0.726, 0.564, 0.421, 0.335, 0.263, 0.206, Avg Draft acceptance rate: 41.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:34 [loggers.py:257] Engine 000: Avg prompt throughput: 558.9 tokens/s, Avg generation throughput: 543.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 403.26 tokens/s, Drafted throughput: 859.70 tokens/s, Accepted: 4033 tokens, Drafted: 8598 tokens, Per-position acceptance rate: 0.760, 0.613, 0.489, 0.386, 0.311, 0.255, Avg Draft acceptance rate: 46.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:44 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.33, Accepted throughput: 2.60 tokens/s, Drafted throughput: 3.60 tokens/s, Accepted: 26 tokens, Drafted: 36 tokens, Per-position acceptance rate: 1.000, 0.833, 0.833, 0.667, 0.500, 0.500, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1177.6 tokens/s, Avg generation throughput: 714.6 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:21:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 517.45 tokens/s, Drafted throughput: 1169.29 tokens/s, Accepted: 5175 tokens, Drafted: 11694 tokens, Per-position acceptance rate: 0.744, 0.582, 0.459, 0.355, 0.287, 0.228, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1072.3 tokens/s, Avg generation throughput: 961.0 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 687.36 tokens/s, Drafted throughput: 1657.10 tokens/s, Accepted: 6874 tokens, Drafted: 16572 tokens, Per-position acceptance rate: 0.729, 0.563, 0.424, 0.323, 0.252, 0.199, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1255.2 tokens/s, Avg generation throughput: 985.8 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 61.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 727.77 tokens/s, Drafted throughput: 1561.73 tokens/s, Accepted: 7278 tokens, Drafted: 15618 tokens, Per-position acceptance rate: 0.757, 0.608, 0.486, 0.388, 0.308, 0.248, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1093.5 tokens/s, Avg generation throughput: 996.3 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 722.67 tokens/s, Drafted throughput: 1654.74 tokens/s, Accepted: 7227 tokens, Drafted: 16548 tokens, Per-position acceptance rate: 0.728, 0.587, 0.457, 0.354, 0.280, 0.214, Avg Draft acceptance rate: 43.7%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1032.8 tokens/s, Avg generation throughput: 957.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 692.22 tokens/s, Drafted throughput: 1615.86 tokens/s, Accepted: 6922 tokens, Drafted: 16158 tokens, Per-position acceptance rate: 0.739, 0.568, 0.447, 0.341, 0.267, 0.209, Avg Draft acceptance rate: 42.8%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 78.09 tokens/s, Drafted throughput: 188.98 tokens/s, Accepted: 781 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.686, 0.540, 0.425, 0.352, 0.270, 0.206, Avg Draft acceptance rate: 41.3%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1113.9 tokens/s, Avg generation throughput: 429.7 tokens/s, Running: 47 reqs, Waiting: 15 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:22:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.76, Accepted throughput: 311.08 tokens/s, Drafted throughput: 677.35 tokens/s, Accepted: 3111 tokens, Drafted: 6774 tokens, Per-position acceptance rate: 0.733, 0.598, 0.492, 0.386, 0.306, 0.240, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1047.3 tokens/s, Avg generation throughput: 937.6 tokens/s, Running: 46 reqs, Waiting: 17 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 670.66 tokens/s, Drafted throughput: 1601.66 tokens/s, Accepted: 6708 tokens, Drafted: 16020 tokens, Per-position acceptance rate: 0.742, 0.562, 0.425, 0.323, 0.256, 0.204, Avg Draft acceptance rate: 41.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1065.3 tokens/s, Avg generation throughput: 985.9 tokens/s, Running: 46 reqs, Waiting: 14 reqs, GPU KV cache usage: 98.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 720.95 tokens/s, Drafted throughput: 1598.90 tokens/s, Accepted: 7210 tokens, Drafted: 15990 tokens, Per-position acceptance rate: 0.749, 0.600, 0.465, 0.368, 0.290, 0.233, Avg Draft acceptance rate: 45.1%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1199.7 tokens/s, Avg generation throughput: 957.5 tokens/s, Running: 43 reqs, Waiting: 20 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 686.66 tokens/s, Drafted throughput: 1626.87 tokens/s, Accepted: 6868 tokens, Drafted: 16272 tokens, Per-position acceptance rate: 0.725, 0.566, 0.437, 0.337, 0.264, 0.205, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1156.8 tokens/s, Avg generation throughput: 1014.0 tokens/s, Running: 44 reqs, Waiting: 16 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 745.85 tokens/s, Drafted throughput: 1625.30 tokens/s, Accepted: 7459 tokens, Drafted: 16254 tokens, Per-position acceptance rate: 0.750, 0.608, 0.482, 0.374, 0.303, 0.236, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1254.7 tokens/s, Avg generation throughput: 939.6 tokens/s, Running: 50 reqs, Waiting: 11 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 676.53 tokens/s, Drafted throughput: 1571.84 tokens/s, Accepted: 6766 tokens, Drafted: 15720 tokens, Per-position acceptance rate: 0.750, 0.573, 0.445, 0.340, 0.263, 0.212, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1016.1 tokens/s, Avg generation throughput: 990.7 tokens/s, Running: 44 reqs, Waiting: 18 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:23:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 714.37 tokens/s, Drafted throughput: 1658.33 tokens/s, Accepted: 7144 tokens, Drafted: 16584 tokens, Per-position acceptance rate: 0.732, 0.568, 0.450, 0.350, 0.272, 0.212, Avg Draft acceptance rate: 43.1%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1203.1 tokens/s, Avg generation throughput: 975.4 tokens/s, Running: 48 reqs, Waiting: 15 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 707.69 tokens/s, Drafted throughput: 1592.75 tokens/s, Accepted: 7078 tokens, Drafted: 15930 tokens, Per-position acceptance rate: 0.741, 0.582, 0.451, 0.359, 0.290, 0.242, Avg Draft acceptance rate: 44.4%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1098.8 tokens/s, Avg generation throughput: 931.3 tokens/s, Running: 44 reqs, Waiting: 19 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 657.95 tokens/s, Drafted throughput: 1630.09 tokens/s, Accepted: 6580 tokens, Drafted: 16302 tokens, Per-position acceptance rate: 0.719, 0.537, 0.417, 0.319, 0.244, 0.185, Avg Draft acceptance rate: 40.4%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:24 [loggers.py:257] Engine 000: Avg prompt throughput: 985.5 tokens/s, Avg generation throughput: 992.6 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 708.16 tokens/s, Drafted throughput: 1727.91 tokens/s, Accepted: 7082 tokens, Drafted: 17280 tokens, Per-position acceptance rate: 0.708, 0.547, 0.424, 0.323, 0.252, 0.205, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 179.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 131.58 tokens/s, Drafted throughput: 320.96 tokens/s, Accepted: 1316 tokens, Drafted: 3210 tokens, Per-position acceptance rate: 0.707, 0.540, 0.434, 0.335, 0.250, 0.194, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=68102)[0;0m INFO 01-27 15:24:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
