[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:54 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:54 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'draft_model', 'model': 'meta-llama/Llama-3.2-1B', 'num_speculative_tokens': 7, 'max_model_len': 5000}}
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:56 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:56 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:57 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:57 [model.py:1559] Using max model len 131072
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:57 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=76062)[0;0m WARNING 01-27 15:31:57 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:31:57 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=76256)[0;0m INFO 01-27 15:32:04 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='draft_model', model='meta-llama/Llama-3.2-1B', num_spec_tokens=7), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8448], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=76256)[0;0m WARNING 01-27 15:32:04 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 15:32:10 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:58401 backend=nccl
INFO 01-27 15:32:10 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:58401 backend=nccl
INFO 01-27 15:32:10 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 15:32:11 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 15:32:11 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 15:32:11 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 15:32:11 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 15:32:11 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 15:32:11 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 15:32:11 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 15:32:11 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:11 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:36 [default_loader.py:291] Loading weights took 21.89 seconds
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:36 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:36 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:36 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=0
[0;36m(Worker_TP1 pid=76425)[0;0m INFO 01-27 15:32:36 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=76425)[0;0m INFO 01-27 15:32:36 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP1 pid=76425)[0;0m INFO 01-27 15:32:36 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=1
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:37 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:37 [default_loader.py:291] Loading weights took 0.25 seconds
[0;36m(Worker_TP1 pid=76425)[0;0m INFO 01-27 15:32:37 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:37 [gpu_model_runner.py:3921] Model loading took 66.91 GiB memory and 25.565395 seconds
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:47 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:32:47 [backends.py:704] Dynamo bytecode transform time: 8.64 s
[0;36m(Worker_TP1 pid=76425)[0;0m INFO 01-27 15:33:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.770 s
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.725 s
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:00 [monitor.py:34] torch.compile takes 17.36 s in total
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:02 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:02 [backends.py:704] Dynamo bytecode transform time: 1.81 s
[0;36m(Worker_TP1 pid=76425)[0;0m INFO 01-27 15:33:03 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.291 s
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:03 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.280 s
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:03 [monitor.py:34] torch.compile takes 19.45 s in total
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:05 [gpu_worker.py:355] Available KV cache memory: 2.5 GiB
[0;36m(EngineCore_DP0 pid=76256)[0;0m INFO 01-27 15:33:05 [kv_cache_utils.py:1307] GPU KV cache size: 14,864 tokens
[0;36m(EngineCore_DP0 pid=76256)[0;0m INFO 01-27 15:33:05 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.97x
[0;36m(Worker_TP0 pid=76424)[0;0m INFO 01-27 15:33:25 [gpu_model_runner.py:4880] Graph capturing finished in 21 secs, took 4.24 GiB
[0;36m(EngineCore_DP0 pid=76256)[0;0m INFO 01-27 15:33:25 [core.py:272] init engine (profile, create kv cache, warmup model) took 47.69 seconds
[0;36m(EngineCore_DP0 pid=76256)[0;0m INFO 01-27 15:33:28 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:28 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=76062)[0;0m WARNING 01-27 15:33:28 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:28 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:29 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:29 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [serving.py:221] Chat template warmup completed in 3112.3ms
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:32 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:52 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 15.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:33:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.50, Accepted throughput: 5.42 tokens/s, Drafted throughput: 6.89 tokens/s, Accepted: 132 tokens, Drafted: 168 tokens, Per-position acceptance rate: 0.875, 0.875, 0.833, 0.750, 0.750, 0.708, 0.708, Avg Draft acceptance rate: 78.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:02 [loggers.py:257] Engine 000: Avg prompt throughput: 88.7 tokens/s, Avg generation throughput: 82.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.47, Accepted throughput: 67.09 tokens/s, Drafted throughput: 104.99 tokens/s, Accepted: 671 tokens, Drafted: 1050 tokens, Per-position acceptance rate: 0.833, 0.760, 0.687, 0.607, 0.573, 0.533, 0.480, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:12 [loggers.py:257] Engine 000: Avg prompt throughput: 88.1 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.25, Accepted throughput: 64.10 tokens/s, Drafted throughput: 105.69 tokens/s, Accepted: 641 tokens, Drafted: 1057 tokens, Per-position acceptance rate: 0.848, 0.735, 0.636, 0.576, 0.563, 0.470, 0.417, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:22 [loggers.py:257] Engine 000: Avg prompt throughput: 109.3 tokens/s, Avg generation throughput: 76.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.05, Accepted throughput: 61.20 tokens/s, Drafted throughput: 105.69 tokens/s, Accepted: 612 tokens, Drafted: 1057 tokens, Per-position acceptance rate: 0.821, 0.715, 0.609, 0.550, 0.497, 0.437, 0.424, Avg Draft acceptance rate: 57.9%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:32 [loggers.py:257] Engine 000: Avg prompt throughput: 65.4 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.64, Accepted throughput: 70.99 tokens/s, Drafted throughput: 107.09 tokens/s, Accepted: 710 tokens, Drafted: 1071 tokens, Per-position acceptance rate: 0.882, 0.771, 0.680, 0.621, 0.588, 0.575, 0.523, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:42 [loggers.py:257] Engine 000: Avg prompt throughput: 99.3 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.07, Accepted throughput: 61.80 tokens/s, Drafted throughput: 106.40 tokens/s, Accepted: 618 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.796, 0.717, 0.612, 0.553, 0.500, 0.454, 0.434, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:52 [loggers.py:257] Engine 000: Avg prompt throughput: 100.5 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:34:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.86, Accepted throughput: 73.80 tokens/s, Drafted throughput: 106.40 tokens/s, Accepted: 738 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.895, 0.822, 0.704, 0.658, 0.638, 0.579, 0.559, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:02 [loggers.py:257] Engine 000: Avg prompt throughput: 77.2 tokens/s, Avg generation throughput: 74.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.89, Accepted throughput: 59.20 tokens/s, Drafted throughput: 106.40 tokens/s, Accepted: 592 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.809, 0.691, 0.618, 0.526, 0.467, 0.408, 0.375, Avg Draft acceptance rate: 55.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:12 [loggers.py:257] Engine 000: Avg prompt throughput: 124.5 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.24, Accepted throughput: 63.20 tokens/s, Drafted throughput: 104.30 tokens/s, Accepted: 632 tokens, Drafted: 1043 tokens, Per-position acceptance rate: 0.799, 0.691, 0.611, 0.570, 0.557, 0.517, 0.497, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:22 [loggers.py:257] Engine 000: Avg prompt throughput: 97.1 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.96, Accepted throughput: 74.90 tokens/s, Drafted throughput: 105.70 tokens/s, Accepted: 749 tokens, Drafted: 1057 tokens, Per-position acceptance rate: 0.881, 0.821, 0.735, 0.675, 0.649, 0.623, 0.576, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:32 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:42 [loggers.py:257] Engine 000: Avg prompt throughput: 152.7 tokens/s, Avg generation throughput: 132.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.58, Accepted throughput: 54.00 tokens/s, Drafted throughput: 82.60 tokens/s, Accepted: 1080 tokens, Drafted: 1652 tokens, Per-position acceptance rate: 0.839, 0.767, 0.695, 0.627, 0.602, 0.547, 0.500, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:52 [loggers.py:257] Engine 000: Avg prompt throughput: 167.6 tokens/s, Avg generation throughput: 141.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:35:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.16, Accepted throughput: 113.90 tokens/s, Drafted throughput: 191.80 tokens/s, Accepted: 1139 tokens, Drafted: 1918 tokens, Per-position acceptance rate: 0.839, 0.719, 0.617, 0.562, 0.522, 0.464, 0.434, Avg Draft acceptance rate: 59.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:02 [loggers.py:257] Engine 000: Avg prompt throughput: 130.5 tokens/s, Avg generation throughput: 145.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.20, Accepted throughput: 117.69 tokens/s, Drafted throughput: 195.98 tokens/s, Accepted: 1177 tokens, Drafted: 1960 tokens, Per-position acceptance rate: 0.804, 0.725, 0.625, 0.564, 0.525, 0.496, 0.464, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:12 [loggers.py:257] Engine 000: Avg prompt throughput: 177.7 tokens/s, Avg generation throughput: 150.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.51, Accepted throughput: 122.70 tokens/s, Drafted throughput: 190.39 tokens/s, Accepted: 1227 tokens, Drafted: 1904 tokens, Per-position acceptance rate: 0.853, 0.765, 0.673, 0.614, 0.581, 0.526, 0.500, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:22 [loggers.py:257] Engine 000: Avg prompt throughput: 197.7 tokens/s, Avg generation throughput: 149.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.44, Accepted throughput: 120.69 tokens/s, Drafted throughput: 190.38 tokens/s, Accepted: 1207 tokens, Drafted: 1904 tokens, Per-position acceptance rate: 0.820, 0.735, 0.658, 0.599, 0.574, 0.540, 0.511, Avg Draft acceptance rate: 63.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:32 [loggers.py:257] Engine 000: Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.53, Accepted throughput: 26.30 tokens/s, Drafted throughput: 40.60 tokens/s, Accepted: 263 tokens, Drafted: 406 tokens, Per-position acceptance rate: 0.879, 0.793, 0.672, 0.621, 0.586, 0.517, 0.466, Avg Draft acceptance rate: 64.8%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:42 [loggers.py:257] Engine 000: Avg prompt throughput: 220.2 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.47, Accepted throughput: 127.28 tokens/s, Drafted throughput: 199.47 tokens/s, Accepted: 1273 tokens, Drafted: 1995 tokens, Per-position acceptance rate: 0.835, 0.751, 0.681, 0.614, 0.589, 0.523, 0.474, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:52 [loggers.py:257] Engine 000: Avg prompt throughput: 294.8 tokens/s, Avg generation throughput: 269.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:36:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.08, Accepted throughput: 216.50 tokens/s, Drafted throughput: 371.00 tokens/s, Accepted: 2165 tokens, Drafted: 3710 tokens, Per-position acceptance rate: 0.811, 0.715, 0.609, 0.543, 0.500, 0.470, 0.436, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:02 [loggers.py:257] Engine 000: Avg prompt throughput: 331.9 tokens/s, Avg generation throughput: 279.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.38, Accepted throughput: 226.79 tokens/s, Drafted throughput: 362.58 tokens/s, Accepted: 2268 tokens, Drafted: 3626 tokens, Per-position acceptance rate: 0.815, 0.732, 0.653, 0.597, 0.568, 0.519, 0.496, Avg Draft acceptance rate: 62.5%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:12 [loggers.py:257] Engine 000: Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 50.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.63, Accepted throughput: 42.10 tokens/s, Drafted throughput: 63.70 tokens/s, Accepted: 421 tokens, Drafted: 637 tokens, Per-position acceptance rate: 0.879, 0.802, 0.692, 0.637, 0.593, 0.538, 0.484, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:22 [loggers.py:257] Engine 000: Avg prompt throughput: 372.2 tokens/s, Avg generation throughput: 269.0 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.32, Accepted throughput: 217.29 tokens/s, Drafted throughput: 352.08 tokens/s, Accepted: 2173 tokens, Drafted: 3521 tokens, Per-position acceptance rate: 0.823, 0.724, 0.644, 0.596, 0.561, 0.507, 0.465, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:32 [loggers.py:257] Engine 000: Avg prompt throughput: 602.4 tokens/s, Avg generation throughput: 498.3 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.40, Accepted throughput: 405.65 tokens/s, Drafted throughput: 645.32 tokens/s, Accepted: 4057 tokens, Drafted: 6454 tokens, Per-position acceptance rate: 0.834, 0.748, 0.648, 0.594, 0.566, 0.520, 0.490, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:42 [loggers.py:257] Engine 000: Avg prompt throughput: 419.1 tokens/s, Avg generation throughput: 397.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.51, Accepted throughput: 325.98 tokens/s, Drafted throughput: 506.06 tokens/s, Accepted: 3260 tokens, Drafted: 5061 tokens, Per-position acceptance rate: 0.855, 0.752, 0.679, 0.632, 0.577, 0.527, 0.487, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:52 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:37:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.74, Accepted throughput: 13.20 tokens/s, Drafted throughput: 16.10 tokens/s, Accepted: 132 tokens, Drafted: 161 tokens, Per-position acceptance rate: 0.913, 0.913, 0.870, 0.783, 0.783, 0.739, 0.739, Avg Draft acceptance rate: 82.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1008.1 tokens/s, Avg generation throughput: 735.1 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.36, Accepted throughput: 595.89 tokens/s, Drafted throughput: 957.58 tokens/s, Accepted: 5959 tokens, Drafted: 9576 tokens, Per-position acceptance rate: 0.831, 0.741, 0.651, 0.591, 0.561, 0.507, 0.474, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:12 [loggers.py:257] Engine 000: Avg prompt throughput: 954.4 tokens/s, Avg generation throughput: 795.7 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.44, Accepted throughput: 649.17 tokens/s, Drafted throughput: 1022.65 tokens/s, Accepted: 6492 tokens, Drafted: 10227 tokens, Per-position acceptance rate: 0.841, 0.739, 0.672, 0.613, 0.562, 0.522, 0.495, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:22 [loggers.py:257] Engine 000: Avg prompt throughput: 869.7 tokens/s, Avg generation throughput: 784.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.69, Accepted throughput: 648.40 tokens/s, Drafted throughput: 968.10 tokens/s, Accepted: 6484 tokens, Drafted: 9681 tokens, Per-position acceptance rate: 0.863, 0.776, 0.691, 0.644, 0.610, 0.572, 0.534, Avg Draft acceptance rate: 67.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:32 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.87, Accepted throughput: 18.50 tokens/s, Drafted throughput: 26.60 tokens/s, Accepted: 185 tokens, Drafted: 266 tokens, Per-position acceptance rate: 0.842, 0.737, 0.684, 0.684, 0.684, 0.658, 0.579, Avg Draft acceptance rate: 69.5%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1160.5 tokens/s, Avg generation throughput: 704.3 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.40, Accepted throughput: 571.45 tokens/s, Drafted throughput: 909.93 tokens/s, Accepted: 5715 tokens, Drafted: 9100 tokens, Per-position acceptance rate: 0.834, 0.744, 0.654, 0.597, 0.567, 0.518, 0.483, Avg Draft acceptance rate: 62.8%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1279.7 tokens/s, Avg generation throughput: 1073.8 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 55.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:38:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.45, Accepted throughput: 876.98 tokens/s, Drafted throughput: 1378.27 tokens/s, Accepted: 8770 tokens, Drafted: 13783 tokens, Per-position acceptance rate: 0.846, 0.746, 0.672, 0.615, 0.564, 0.520, 0.492, Avg Draft acceptance rate: 63.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1251.8 tokens/s, Avg generation throughput: 1078.9 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.47, Accepted throughput: 882.71 tokens/s, Drafted throughput: 1381.11 tokens/s, Accepted: 8827 tokens, Drafted: 13811 tokens, Per-position acceptance rate: 0.841, 0.752, 0.667, 0.614, 0.570, 0.532, 0.498, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1368.6 tokens/s, Avg generation throughput: 1057.6 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 53.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.51, Accepted throughput: 868.46 tokens/s, Drafted throughput: 1346.74 tokens/s, Accepted: 8685 tokens, Drafted: 13468 tokens, Per-position acceptance rate: 0.849, 0.754, 0.675, 0.622, 0.573, 0.533, 0.508, Avg Draft acceptance rate: 64.5%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:22 [loggers.py:257] Engine 000: Avg prompt throughput: 591.6 tokens/s, Avg generation throughput: 811.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.50, Accepted throughput: 668.18 tokens/s, Drafted throughput: 1040.17 tokens/s, Accepted: 6682 tokens, Drafted: 10402 tokens, Per-position acceptance rate: 0.842, 0.751, 0.674, 0.620, 0.575, 0.537, 0.498, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:32 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.50, Accepted throughput: 13.20 tokens/s, Drafted throughput: 16.80 tokens/s, Accepted: 132 tokens, Drafted: 168 tokens, Per-position acceptance rate: 0.875, 0.875, 0.833, 0.750, 0.750, 0.708, 0.708, Avg Draft acceptance rate: 78.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1581.0 tokens/s, Avg generation throughput: 1000.4 tokens/s, Running: 45 reqs, Waiting: 13 reqs, GPU KV cache usage: 87.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.37, Accepted throughput: 807.76 tokens/s, Drafted throughput: 1294.24 tokens/s, Accepted: 8078 tokens, Drafted: 12943 tokens, Per-position acceptance rate: 0.832, 0.740, 0.657, 0.598, 0.559, 0.509, 0.475, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1473.5 tokens/s, Avg generation throughput: 1079.8 tokens/s, Running: 49 reqs, Waiting: 9 reqs, GPU KV cache usage: 92.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:39:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.58, Accepted throughput: 886.95 tokens/s, Drafted throughput: 1356.53 tokens/s, Accepted: 8870 tokens, Drafted: 13566 tokens, Per-position acceptance rate: 0.853, 0.759, 0.680, 0.632, 0.588, 0.548, 0.519, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1316.1 tokens/s, Avg generation throughput: 1152.9 tokens/s, Running: 49 reqs, Waiting: 14 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.37, Accepted throughput: 937.70 tokens/s, Drafted throughput: 1500.80 tokens/s, Accepted: 9377 tokens, Drafted: 15008 tokens, Per-position acceptance rate: 0.840, 0.739, 0.658, 0.596, 0.551, 0.512, 0.479, Avg Draft acceptance rate: 62.5%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1328.0 tokens/s, Avg generation throughput: 1064.9 tokens/s, Running: 49 reqs, Waiting: 9 reqs, GPU KV cache usage: 92.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.37, Accepted throughput: 868.59 tokens/s, Drafted throughput: 1391.59 tokens/s, Accepted: 8686 tokens, Drafted: 13916 tokens, Per-position acceptance rate: 0.830, 0.734, 0.656, 0.602, 0.551, 0.515, 0.481, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1322.3 tokens/s, Avg generation throughput: 1105.9 tokens/s, Running: 55 reqs, Waiting: 5 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.24, Accepted throughput: 894.62 tokens/s, Drafted throughput: 1476.17 tokens/s, Accepted: 8947 tokens, Drafted: 14763 tokens, Per-position acceptance rate: 0.809, 0.708, 0.629, 0.581, 0.539, 0.505, 0.471, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1174.2 tokens/s, Avg generation throughput: 1014.8 tokens/s, Running: 54 reqs, Waiting: 8 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.79, Accepted throughput: 802.33 tokens/s, Drafted throughput: 1480.36 tokens/s, Accepted: 8024 tokens, Drafted: 14805 tokens, Per-position acceptance rate: 0.767, 0.659, 0.575, 0.509, 0.461, 0.425, 0.398, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1259.7 tokens/s, Avg generation throughput: 1055.3 tokens/s, Running: 50 reqs, Waiting: 7 reqs, GPU KV cache usage: 90.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.11, Accepted throughput: 846.41 tokens/s, Drafted throughput: 1443.25 tokens/s, Accepted: 8465 tokens, Drafted: 14434 tokens, Per-position acceptance rate: 0.793, 0.700, 0.618, 0.562, 0.518, 0.472, 0.442, Avg Draft acceptance rate: 58.6%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1238.2 tokens/s, Avg generation throughput: 1097.8 tokens/s, Running: 49 reqs, Waiting: 12 reqs, GPU KV cache usage: 93.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:40:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.12, Accepted throughput: 885.00 tokens/s, Drafted throughput: 1504.83 tokens/s, Accepted: 8851 tokens, Drafted: 15050 tokens, Per-position acceptance rate: 0.801, 0.694, 0.620, 0.565, 0.512, 0.479, 0.445, Avg Draft acceptance rate: 58.8%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:41:02 [loggers.py:257] Engine 000: Avg prompt throughput: 407.5 tokens/s, Avg generation throughput: 741.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:41:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.71, Accepted throughput: 589.85 tokens/s, Drafted throughput: 1113.60 tokens/s, Accepted: 5899 tokens, Drafted: 11137 tokens, Per-position acceptance rate: 0.722, 0.622, 0.557, 0.511, 0.474, 0.429, 0.393, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:41:12 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:41:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.55, Accepted throughput: 1.80 tokens/s, Drafted throughput: 23.10 tokens/s, Accepted: 18 tokens, Drafted: 231 tokens, Per-position acceptance rate: 0.273, 0.091, 0.091, 0.061, 0.030, 0.000, 0.000, Avg Draft acceptance rate: 7.8%
[0;36m(APIServer pid=76062)[0;0m INFO 01-27 15:41:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
