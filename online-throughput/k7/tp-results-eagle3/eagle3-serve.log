[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:08 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:08 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'eagle3', 'model': 'yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', 'num_speculative_tokens': 7}}
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:10 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:10 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:11 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=83126)[0;0m WARNING 01-27 15:42:11 [model.py:1883] Casting torch.float16 to torch.bfloat16.
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:11 [model.py:1559] Using max model len 2048
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:11 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:11 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:42:11 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(EngineCore_DP0 pid=83336)[0;0m INFO 01-27 15:42:18 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='eagle3', model='yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', num_spec_tokens=7), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=83336)[0;0m WARNING 01-27 15:42:18 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 15:42:24 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:51289 backend=nccl
INFO 01-27 15:42:24 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:51289 backend=nccl
INFO 01-27 15:42:24 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 15:42:24 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 15:42:24 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 15:42:24 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 15:42:24 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 15:42:24 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 15:42:24 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 15:42:25 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 15:42:25 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:25 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:26 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:49 [default_loader.py:291] Loading weights took 21.76 seconds
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:49 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=83495)[0;0m INFO 01-27 15:42:50 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:53 [default_loader.py:291] Loading weights took 3.01 seconds
[0;36m(Worker_TP1 pid=83495)[0;0m INFO 01-27 15:42:54 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:54 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP1 pid=83495)[0;0m INFO 01-27 15:42:55 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:55 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:42:55 [gpu_model_runner.py:3921] Model loading took 67.35 GiB memory and 29.993023 seconds
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:04 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:04 [backends.py:704] Dynamo bytecode transform time: 8.67 s
[0;36m(Worker_TP1 pid=83495)[0;0m INFO 01-27 15:43:18 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 8.588 s
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:18 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 8.411 s
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:18 [monitor.py:34] torch.compile takes 17.08 s in total
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:18 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:18 [backends.py:704] Dynamo bytecode transform time: 0.36 s
[0;36m(Worker_TP1 pid=83495)[0;0m INFO 01-27 15:43:18 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.051 s
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:18 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.040 s
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:18 [monitor.py:34] torch.compile takes 17.48 s in total
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:19 [gpu_worker.py:355] Available KV cache memory: 2.05 GiB
[0;36m(EngineCore_DP0 pid=83336)[0;0m INFO 01-27 15:43:19 [kv_cache_utils.py:1307] GPU KV cache size: 13,280 tokens
[0;36m(EngineCore_DP0 pid=83336)[0;0m INFO 01-27 15:43:19 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.65x
[0;36m(Worker_TP0 pid=83494)[0;0m INFO 01-27 15:43:37 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 4.12 GiB
[0;36m(EngineCore_DP0 pid=83336)[0;0m INFO 01-27 15:43:37 [core.py:272] init engine (profile, create kv cache, warmup model) took 41.18 seconds
[0;36m(EngineCore_DP0 pid=83336)[0;0m INFO 01-27 15:43:39 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:39 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=83126)[0;0m WARNING 01-27 15:43:39 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:39 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:39 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:39 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [serving.py:221] Chat template warmup completed in 2592.0ms
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:42 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:53 [loggers.py:257] Engine 000: Avg prompt throughput: 58.7 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:43:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.29, Accepted throughput: 37.09 tokens/s, Drafted throughput: 78.82 tokens/s, Accepted: 504 tokens, Drafted: 1071 tokens, Per-position acceptance rate: 0.765, 0.647, 0.595, 0.431, 0.314, 0.281, 0.261, Avg Draft acceptance rate: 47.1%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:03 [loggers.py:257] Engine 000: Avg prompt throughput: 80.6 tokens/s, Avg generation throughput: 67.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 49.30 tokens/s, Drafted throughput: 127.39 tokens/s, Accepted: 493 tokens, Drafted: 1274 tokens, Per-position acceptance rate: 0.802, 0.621, 0.467, 0.313, 0.231, 0.148, 0.126, Avg Draft acceptance rate: 38.7%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:13 [loggers.py:257] Engine 000: Avg prompt throughput: 59.9 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 37.50 tokens/s, Drafted throughput: 128.79 tokens/s, Accepted: 375 tokens, Drafted: 1288 tokens, Per-position acceptance rate: 0.641, 0.413, 0.310, 0.228, 0.196, 0.141, 0.109, Avg Draft acceptance rate: 29.1%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:23 [loggers.py:257] Engine 000: Avg prompt throughput: 86.6 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 46.90 tokens/s, Drafted throughput: 127.39 tokens/s, Accepted: 469 tokens, Drafted: 1274 tokens, Per-position acceptance rate: 0.703, 0.555, 0.401, 0.308, 0.269, 0.187, 0.154, Avg Draft acceptance rate: 36.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:33 [loggers.py:257] Engine 000: Avg prompt throughput: 50.1 tokens/s, Avg generation throughput: 65.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 46.70 tokens/s, Drafted throughput: 130.90 tokens/s, Accepted: 467 tokens, Drafted: 1309 tokens, Per-position acceptance rate: 0.658, 0.487, 0.369, 0.310, 0.246, 0.230, 0.198, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:43 [loggers.py:257] Engine 000: Avg prompt throughput: 80.6 tokens/s, Avg generation throughput: 75.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.20, Accepted throughput: 58.30 tokens/s, Drafted throughput: 127.40 tokens/s, Accepted: 583 tokens, Drafted: 1274 tokens, Per-position acceptance rate: 0.802, 0.632, 0.495, 0.418, 0.346, 0.269, 0.242, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:53 [loggers.py:257] Engine 000: Avg prompt throughput: 77.5 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:44:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 49.59 tokens/s, Drafted throughput: 128.78 tokens/s, Accepted: 496 tokens, Drafted: 1288 tokens, Per-position acceptance rate: 0.745, 0.592, 0.440, 0.326, 0.255, 0.196, 0.141, Avg Draft acceptance rate: 38.5%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:03 [loggers.py:257] Engine 000: Avg prompt throughput: 103.2 tokens/s, Avg generation throughput: 75.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.17, Accepted throughput: 57.70 tokens/s, Drafted throughput: 127.40 tokens/s, Accepted: 577 tokens, Drafted: 1274 tokens, Per-position acceptance rate: 0.758, 0.637, 0.484, 0.429, 0.363, 0.286, 0.214, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:13 [loggers.py:257] Engine 000: Avg prompt throughput: 31.0 tokens/s, Avg generation throughput: 47.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 28.70 tokens/s, Drafted throughput: 131.60 tokens/s, Accepted: 287 tokens, Drafted: 1316 tokens, Per-position acceptance rate: 0.590, 0.356, 0.223, 0.165, 0.096, 0.053, 0.043, Avg Draft acceptance rate: 21.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:23 [loggers.py:257] Engine 000: Avg prompt throughput: 124.5 tokens/s, Avg generation throughput: 74.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.20, Accepted throughput: 56.60 tokens/s, Drafted throughput: 123.91 tokens/s, Accepted: 566 tokens, Drafted: 1239 tokens, Per-position acceptance rate: 0.780, 0.667, 0.554, 0.418, 0.322, 0.254, 0.203, Avg Draft acceptance rate: 45.7%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:33 [loggers.py:257] Engine 000: Avg prompt throughput: 85.6 tokens/s, Avg generation throughput: 76.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.21, Accepted throughput: 58.10 tokens/s, Drafted throughput: 126.70 tokens/s, Accepted: 581 tokens, Drafted: 1267 tokens, Per-position acceptance rate: 0.884, 0.680, 0.508, 0.370, 0.315, 0.249, 0.204, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:43 [loggers.py:257] Engine 000: Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 8.30 tokens/s, Drafted throughput: 24.50 tokens/s, Accepted: 83 tokens, Drafted: 245 tokens, Per-position acceptance rate: 0.686, 0.514, 0.429, 0.314, 0.200, 0.143, 0.086, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:53 [loggers.py:257] Engine 000: Avg prompt throughput: 129.4 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:45:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.10, Accepted throughput: 66.00 tokens/s, Drafted throughput: 149.10 tokens/s, Accepted: 660 tokens, Drafted: 1491 tokens, Per-position acceptance rate: 0.789, 0.624, 0.563, 0.399, 0.296, 0.221, 0.207, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:03 [loggers.py:257] Engine 000: Avg prompt throughput: 103.7 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 82.20 tokens/s, Drafted throughput: 233.79 tokens/s, Accepted: 822 tokens, Drafted: 2338 tokens, Per-position acceptance rate: 0.707, 0.524, 0.389, 0.284, 0.234, 0.177, 0.147, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:13 [loggers.py:257] Engine 000: Avg prompt throughput: 139.1 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 82.29 tokens/s, Drafted throughput: 231.68 tokens/s, Accepted: 823 tokens, Drafted: 2317 tokens, Per-position acceptance rate: 0.689, 0.508, 0.372, 0.308, 0.254, 0.196, 0.160, Avg Draft acceptance rate: 35.5%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:23 [loggers.py:257] Engine 000: Avg prompt throughput: 142.8 tokens/s, Avg generation throughput: 122.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.68, Accepted throughput: 89.10 tokens/s, Drafted throughput: 232.39 tokens/s, Accepted: 891 tokens, Drafted: 2324 tokens, Per-position acceptance rate: 0.726, 0.560, 0.413, 0.328, 0.268, 0.211, 0.178, Avg Draft acceptance rate: 38.3%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:33 [loggers.py:257] Engine 000: Avg prompt throughput: 122.0 tokens/s, Avg generation throughput: 125.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 92.39 tokens/s, Drafted throughput: 233.07 tokens/s, Accepted: 924 tokens, Drafted: 2331 tokens, Per-position acceptance rate: 0.730, 0.574, 0.429, 0.363, 0.285, 0.222, 0.171, Avg Draft acceptance rate: 39.6%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:43 [loggers.py:257] Engine 000: Avg prompt throughput: 180.3 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 87.20 tokens/s, Drafted throughput: 226.79 tokens/s, Accepted: 872 tokens, Drafted: 2268 tokens, Per-position acceptance rate: 0.728, 0.571, 0.457, 0.336, 0.247, 0.198, 0.154, Avg Draft acceptance rate: 38.4%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:53 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 62.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:46:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 46.79 tokens/s, Drafted throughput: 112.69 tokens/s, Accepted: 468 tokens, Drafted: 1127 tokens, Per-position acceptance rate: 0.845, 0.627, 0.466, 0.329, 0.280, 0.199, 0.161, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:03 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.36, Accepted throughput: 49.70 tokens/s, Drafted throughput: 103.59 tokens/s, Accepted: 497 tokens, Drafted: 1036 tokens, Per-position acceptance rate: 0.791, 0.662, 0.608, 0.453, 0.318, 0.270, 0.257, Avg Draft acceptance rate: 48.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:13 [loggers.py:257] Engine 000: Avg prompt throughput: 265.9 tokens/s, Avg generation throughput: 218.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 154.78 tokens/s, Drafted throughput: 447.95 tokens/s, Accepted: 1548 tokens, Drafted: 4480 tokens, Per-position acceptance rate: 0.700, 0.520, 0.383, 0.281, 0.230, 0.166, 0.139, Avg Draft acceptance rate: 34.6%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:23 [loggers.py:257] Engine 000: Avg prompt throughput: 280.1 tokens/s, Avg generation throughput: 239.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 177.19 tokens/s, Drafted throughput: 442.37 tokens/s, Accepted: 1772 tokens, Drafted: 4424 tokens, Per-position acceptance rate: 0.728, 0.582, 0.446, 0.359, 0.286, 0.225, 0.177, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:33 [loggers.py:257] Engine 000: Avg prompt throughput: 233.8 tokens/s, Avg generation throughput: 217.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 159.49 tokens/s, Drafted throughput: 412.98 tokens/s, Accepted: 1595 tokens, Drafted: 4130 tokens, Per-position acceptance rate: 0.751, 0.573, 0.436, 0.331, 0.259, 0.197, 0.158, Avg Draft acceptance rate: 38.6%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 5.40 tokens/s, Drafted throughput: 15.40 tokens/s, Accepted: 54 tokens, Drafted: 154 tokens, Per-position acceptance rate: 0.727, 0.545, 0.455, 0.273, 0.182, 0.136, 0.136, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:53 [loggers.py:257] Engine 000: Avg prompt throughput: 408.9 tokens/s, Avg generation throughput: 279.4 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:47:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 203.69 tokens/s, Drafted throughput: 525.68 tokens/s, Accepted: 2037 tokens, Drafted: 5257 tokens, Per-position acceptance rate: 0.730, 0.562, 0.447, 0.336, 0.264, 0.201, 0.173, Avg Draft acceptance rate: 38.7%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:03 [loggers.py:257] Engine 000: Avg prompt throughput: 478.5 tokens/s, Avg generation throughput: 428.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 315.48 tokens/s, Drafted throughput: 797.96 tokens/s, Accepted: 3155 tokens, Drafted: 7980 tokens, Per-position acceptance rate: 0.736, 0.568, 0.437, 0.354, 0.282, 0.221, 0.171, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:13 [loggers.py:257] Engine 000: Avg prompt throughput: 506.4 tokens/s, Avg generation throughput: 417.8 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 308.78 tokens/s, Drafted throughput: 776.94 tokens/s, Accepted: 3088 tokens, Drafted: 7770 tokens, Per-position acceptance rate: 0.765, 0.594, 0.449, 0.333, 0.258, 0.214, 0.170, Avg Draft acceptance rate: 39.7%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 41.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 29.40 tokens/s, Drafted throughput: 86.80 tokens/s, Accepted: 294 tokens, Drafted: 868 tokens, Per-position acceptance rate: 0.766, 0.548, 0.403, 0.258, 0.185, 0.129, 0.081, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:33 [loggers.py:257] Engine 000: Avg prompt throughput: 532.8 tokens/s, Avg generation throughput: 326.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.73, Accepted throughput: 237.40 tokens/s, Drafted throughput: 608.99 tokens/s, Accepted: 2374 tokens, Drafted: 6090 tokens, Per-position acceptance rate: 0.722, 0.556, 0.447, 0.341, 0.272, 0.208, 0.182, Avg Draft acceptance rate: 39.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:43 [loggers.py:257] Engine 000: Avg prompt throughput: 839.2 tokens/s, Avg generation throughput: 690.5 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.82, Accepted throughput: 511.75 tokens/s, Drafted throughput: 1271.78 tokens/s, Accepted: 5118 tokens, Drafted: 12719 tokens, Per-position acceptance rate: 0.759, 0.589, 0.450, 0.349, 0.276, 0.223, 0.170, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:53 [loggers.py:257] Engine 000: Avg prompt throughput: 760.6 tokens/s, Avg generation throughput: 635.5 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:48:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 454.28 tokens/s, Drafted throughput: 1276.73 tokens/s, Accepted: 4543 tokens, Drafted: 12768 tokens, Per-position acceptance rate: 0.706, 0.539, 0.396, 0.302, 0.228, 0.179, 0.141, Avg Draft acceptance rate: 35.6%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:03 [loggers.py:257] Engine 000: Avg prompt throughput: 720.2 tokens/s, Avg generation throughput: 680.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.07, Accepted throughput: 515.87 tokens/s, Drafted throughput: 1176.63 tokens/s, Accepted: 5159 tokens, Drafted: 11767 tokens, Per-position acceptance rate: 0.767, 0.625, 0.496, 0.400, 0.319, 0.257, 0.206, Avg Draft acceptance rate: 43.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 13.30 tokens/s, Drafted throughput: 31.50 tokens/s, Accepted: 133 tokens, Drafted: 315 tokens, Per-position acceptance rate: 0.667, 0.578, 0.511, 0.400, 0.378, 0.244, 0.178, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:23 [loggers.py:257] Engine 000: Avg prompt throughput: 870.7 tokens/s, Avg generation throughput: 547.1 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 399.88 tokens/s, Drafted throughput: 1007.24 tokens/s, Accepted: 3999 tokens, Drafted: 10073 tokens, Per-position acceptance rate: 0.730, 0.571, 0.450, 0.353, 0.281, 0.218, 0.177, Avg Draft acceptance rate: 39.7%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1112.5 tokens/s, Avg generation throughput: 886.6 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 644.11 tokens/s, Drafted throughput: 1723.42 tokens/s, Accepted: 6441 tokens, Drafted: 17234 tokens, Per-position acceptance rate: 0.726, 0.560, 0.418, 0.318, 0.242, 0.197, 0.155, Avg Draft acceptance rate: 37.4%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1227.8 tokens/s, Avg generation throughput: 936.9 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.93, Accepted throughput: 700.70 tokens/s, Drafted throughput: 1673.01 tokens/s, Accepted: 7007 tokens, Drafted: 16730 tokens, Per-position acceptance rate: 0.754, 0.603, 0.469, 0.375, 0.298, 0.238, 0.194, Avg Draft acceptance rate: 41.9%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1112.5 tokens/s, Avg generation throughput: 933.0 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:49:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 682.06 tokens/s, Drafted throughput: 1772.29 tokens/s, Accepted: 6821 tokens, Drafted: 17724 tokens, Per-position acceptance rate: 0.723, 0.568, 0.439, 0.336, 0.266, 0.203, 0.159, Avg Draft acceptance rate: 38.5%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1145.0 tokens/s, Avg generation throughput: 985.3 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 737.64 tokens/s, Drafted throughput: 1766.65 tokens/s, Accepted: 7377 tokens, Drafted: 17668 tokens, Per-position acceptance rate: 0.748, 0.603, 0.478, 0.370, 0.302, 0.234, 0.188, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:13 [loggers.py:257] Engine 000: Avg prompt throughput: 183.7 tokens/s, Avg generation throughput: 436.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 314.97 tokens/s, Drafted throughput: 879.12 tokens/s, Accepted: 3150 tokens, Drafted: 8792 tokens, Per-position acceptance rate: 0.709, 0.524, 0.402, 0.311, 0.233, 0.187, 0.142, Avg Draft acceptance rate: 35.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:23 [loggers.py:257] Engine 000: Avg prompt throughput: 437.4 tokens/s, Avg generation throughput: 29.9 tokens/s, Running: 38 reqs, Waiting: 0 reqs, GPU KV cache usage: 55.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.42, Accepted throughput: 21.20 tokens/s, Drafted throughput: 43.40 tokens/s, Accepted: 212 tokens, Drafted: 434 tokens, Per-position acceptance rate: 0.790, 0.726, 0.645, 0.468, 0.290, 0.258, 0.242, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1234.2 tokens/s, Avg generation throughput: 920.0 tokens/s, Running: 49 reqs, Waiting: 14 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 677.25 tokens/s, Drafted throughput: 1669.13 tokens/s, Accepted: 6774 tokens, Drafted: 16695 tokens, Per-position acceptance rate: 0.752, 0.587, 0.452, 0.353, 0.285, 0.226, 0.185, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1059.8 tokens/s, Avg generation throughput: 896.2 tokens/s, Running: 42 reqs, Waiting: 18 reqs, GPU KV cache usage: 90.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 645.37 tokens/s, Drafted throughput: 1781.43 tokens/s, Accepted: 6454 tokens, Drafted: 17815 tokens, Per-position acceptance rate: 0.713, 0.545, 0.405, 0.312, 0.237, 0.184, 0.141, Avg Draft acceptance rate: 36.2%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1139.6 tokens/s, Avg generation throughput: 932.4 tokens/s, Running: 49 reqs, Waiting: 14 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:50:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.95, Accepted throughput: 695.78 tokens/s, Drafted throughput: 1653.35 tokens/s, Accepted: 6958 tokens, Drafted: 16534 tokens, Per-position acceptance rate: 0.750, 0.605, 0.480, 0.377, 0.299, 0.238, 0.197, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1038.3 tokens/s, Avg generation throughput: 947.8 tokens/s, Running: 41 reqs, Waiting: 19 reqs, GPU KV cache usage: 93.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 697.33 tokens/s, Drafted throughput: 1759.62 tokens/s, Accepted: 6974 tokens, Drafted: 17598 tokens, Per-position acceptance rate: 0.734, 0.587, 0.453, 0.350, 0.276, 0.211, 0.163, Avg Draft acceptance rate: 39.6%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1046.5 tokens/s, Avg generation throughput: 901.1 tokens/s, Running: 47 reqs, Waiting: 15 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.76, Accepted throughput: 663.28 tokens/s, Drafted throughput: 1679.94 tokens/s, Accepted: 6633 tokens, Drafted: 16800 tokens, Per-position acceptance rate: 0.737, 0.575, 0.449, 0.345, 0.273, 0.216, 0.170, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1085.0 tokens/s, Avg generation throughput: 952.1 tokens/s, Running: 47 reqs, Waiting: 14 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.73, Accepted throughput: 695.67 tokens/s, Drafted throughput: 1782.83 tokens/s, Accepted: 6957 tokens, Drafted: 17829 tokens, Per-position acceptance rate: 0.740, 0.571, 0.446, 0.342, 0.262, 0.205, 0.164, Avg Draft acceptance rate: 39.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1195.3 tokens/s, Avg generation throughput: 916.6 tokens/s, Running: 50 reqs, Waiting: 11 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.76, Accepted throughput: 672.33 tokens/s, Drafted throughput: 1707.81 tokens/s, Accepted: 6724 tokens, Drafted: 17080 tokens, Per-position acceptance rate: 0.730, 0.566, 0.447, 0.351, 0.276, 0.217, 0.169, Avg Draft acceptance rate: 39.4%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1138.9 tokens/s, Avg generation throughput: 927.4 tokens/s, Running: 46 reqs, Waiting: 14 reqs, GPU KV cache usage: 92.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 681.75 tokens/s, Drafted throughput: 1705.09 tokens/s, Accepted: 6818 tokens, Drafted: 17052 tokens, Per-position acceptance rate: 0.729, 0.573, 0.444, 0.357, 0.286, 0.229, 0.181, Avg Draft acceptance rate: 40.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:53 [loggers.py:257] Engine 000: Avg prompt throughput: 880.8 tokens/s, Avg generation throughput: 885.2 tokens/s, Running: 43 reqs, Waiting: 19 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:51:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 628.83 tokens/s, Drafted throughput: 1794.61 tokens/s, Accepted: 6289 tokens, Drafted: 17948 tokens, Per-position acceptance rate: 0.706, 0.520, 0.394, 0.298, 0.223, 0.172, 0.139, Avg Draft acceptance rate: 35.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:52:03 [loggers.py:257] Engine 000: Avg prompt throughput: 885.8 tokens/s, Avg generation throughput: 968.0 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 36.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:52:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 707.29 tokens/s, Drafted throughput: 1870.39 tokens/s, Accepted: 7073 tokens, Drafted: 18704 tokens, Per-position acceptance rate: 0.714, 0.549, 0.430, 0.329, 0.259, 0.205, 0.161, Avg Draft acceptance rate: 37.8%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:52:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:52:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 41.00 tokens/s, Drafted throughput: 132.29 tokens/s, Accepted: 410 tokens, Drafted: 1323 tokens, Per-position acceptance rate: 0.619, 0.503, 0.381, 0.270, 0.190, 0.122, 0.085, Avg Draft acceptance rate: 31.0%
[0;36m(APIServer pid=83126)[0;0m INFO 01-27 15:52:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
