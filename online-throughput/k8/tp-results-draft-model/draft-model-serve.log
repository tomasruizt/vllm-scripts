[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:01 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:01 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'draft_model', 'model': 'meta-llama/Llama-3.2-1B', 'num_speculative_tokens': 8, 'max_model_len': 5000}}
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:05 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:05 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:07 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:07 [model.py:1559] Using max model len 131072
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:07 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=92076)[0;0m WARNING 01-27 16:10:07 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:10:07 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=92292)[0;0m INFO 01-27 16:10:14 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='draft_model', model='meta-llama/Llama-3.2-1B', num_spec_tokens=8), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8448], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=92292)[0;0m WARNING 01-27 16:10:14 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 16:10:19 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:41845 backend=nccl
INFO 01-27 16:10:19 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:41845 backend=nccl
INFO 01-27 16:10:20 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 16:10:20 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 16:10:20 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 16:10:20 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 16:10:20 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 16:10:20 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 16:10:20 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 16:10:21 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 16:10:21 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:21 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:45 [default_loader.py:291] Loading weights took 21.79 seconds
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:45 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:45 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:45 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=0
[0;36m(Worker_TP1 pid=92472)[0;0m INFO 01-27 16:10:46 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP1 pid=92472)[0;0m INFO 01-27 16:10:46 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(Worker_TP1 pid=92472)[0;0m INFO 01-27 16:10:46 [draft_model.py:165] Starting to load draft model meta-llama/Llama-3.2-1B. TP=2, rank=1
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:46 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:46 [default_loader.py:291] Loading weights took 0.26 seconds
[0;36m(Worker_TP1 pid=92472)[0;0m INFO 01-27 16:10:46 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:47 [gpu_model_runner.py:3921] Model loading took 66.91 GiB memory and 25.378274 seconds
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:56 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:10:56 [backends.py:704] Dynamo bytecode transform time: 8.55 s
[0;36m(Worker_TP1 pid=92472)[0;0m INFO 01-27 16:11:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.767 s
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 8.719 s
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:09 [monitor.py:34] torch.compile takes 17.27 s in total
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:11 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/3dbc892f98/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:11 [backends.py:704] Dynamo bytecode transform time: 1.82 s
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:12 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.305 s
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:12 [monitor.py:34] torch.compile takes 19.39 s in total
[0;36m(Worker_TP1 pid=92472)[0;0m INFO 01-27 16:11:12 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8448) from the cache, took 0.278 s
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:14 [gpu_worker.py:355] Available KV cache memory: 2.5 GiB
[0;36m(EngineCore_DP0 pid=92292)[0;0m INFO 01-27 16:11:14 [kv_cache_utils.py:1307] GPU KV cache size: 14,864 tokens
[0;36m(EngineCore_DP0 pid=92292)[0;0m INFO 01-27 16:11:14 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.97x
[0;36m(Worker_TP0 pid=92471)[0;0m INFO 01-27 16:11:33 [gpu_model_runner.py:4880] Graph capturing finished in 19 secs, took 3.91 GiB
[0;36m(EngineCore_DP0 pid=92292)[0;0m INFO 01-27 16:11:33 [core.py:272] init engine (profile, create kv cache, warmup model) took 46.31 seconds
[0;36m(EngineCore_DP0 pid=92292)[0;0m INFO 01-27 16:11:36 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:36 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=92076)[0;0m WARNING 01-27 16:11:36 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:36 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:36 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:36 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [serving.py:221] Chat template warmup completed in 3832.2ms
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:11:40 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:11 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 74.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.17, Accepted throughput: 1.85 tokens/s, Drafted throughput: 2.87 tokens/s, Accepted: 621 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.850, 0.792, 0.717, 0.625, 0.592, 0.567, 0.533, 0.500, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:21 [loggers.py:257] Engine 000: Avg prompt throughput: 82.4 tokens/s, Avg generation throughput: 81.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.71, Accepted throughput: 67.30 tokens/s, Drafted throughput: 114.40 tokens/s, Accepted: 673 tokens, Drafted: 1144 tokens, Per-position acceptance rate: 0.818, 0.720, 0.657, 0.601, 0.559, 0.483, 0.441, 0.427, Avg Draft acceptance rate: 58.8%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:31 [loggers.py:257] Engine 000: Avg prompt throughput: 105.6 tokens/s, Avg generation throughput: 79.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.64, Accepted throughput: 65.40 tokens/s, Drafted throughput: 112.80 tokens/s, Accepted: 654 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.851, 0.745, 0.638, 0.589, 0.532, 0.461, 0.433, 0.390, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:41 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 82.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.74, Accepted throughput: 68.20 tokens/s, Drafted throughput: 115.19 tokens/s, Accepted: 682 tokens, Drafted: 1152 tokens, Per-position acceptance rate: 0.854, 0.701, 0.611, 0.583, 0.549, 0.500, 0.472, 0.465, Avg Draft acceptance rate: 59.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:51 [loggers.py:257] Engine 000: Avg prompt throughput: 80.6 tokens/s, Avg generation throughput: 76.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:17:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.32, Accepted throughput: 62.20 tokens/s, Drafted throughput: 115.20 tokens/s, Accepted: 622 tokens, Drafted: 1152 tokens, Per-position acceptance rate: 0.785, 0.694, 0.590, 0.507, 0.465, 0.451, 0.424, 0.403, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:01 [loggers.py:257] Engine 000: Avg prompt throughput: 95.3 tokens/s, Avg generation throughput: 83.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.79, Accepted throughput: 68.50 tokens/s, Drafted throughput: 114.40 tokens/s, Accepted: 685 tokens, Drafted: 1144 tokens, Per-position acceptance rate: 0.839, 0.797, 0.636, 0.573, 0.545, 0.483, 0.476, 0.441, Avg Draft acceptance rate: 59.9%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:11 [loggers.py:257] Engine 000: Avg prompt throughput: 104.2 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.04, Accepted throughput: 71.50 tokens/s, Drafted throughput: 113.59 tokens/s, Accepted: 715 tokens, Drafted: 1136 tokens, Per-position acceptance rate: 0.859, 0.746, 0.676, 0.634, 0.585, 0.535, 0.514, 0.486, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:21 [loggers.py:257] Engine 000: Avg prompt throughput: 114.1 tokens/s, Avg generation throughput: 74.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.28, Accepted throughput: 60.30 tokens/s, Drafted throughput: 112.80 tokens/s, Accepted: 603 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.787, 0.660, 0.582, 0.518, 0.475, 0.440, 0.418, 0.397, Avg Draft acceptance rate: 53.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:31 [loggers.py:257] Engine 000: Avg prompt throughput: 108.2 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.34, Accepted throughput: 75.30 tokens/s, Drafted throughput: 112.80 tokens/s, Accepted: 753 tokens, Drafted: 1128 tokens, Per-position acceptance rate: 0.844, 0.816, 0.716, 0.645, 0.624, 0.582, 0.574, 0.539, Avg Draft acceptance rate: 66.8%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:41 [loggers.py:257] Engine 000: Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.36, Accepted throughput: 15.70 tokens/s, Drafted throughput: 28.80 tokens/s, Accepted: 157 tokens, Drafted: 288 tokens, Per-position acceptance rate: 0.861, 0.778, 0.583, 0.556, 0.472, 0.444, 0.333, 0.333, Avg Draft acceptance rate: 54.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:51 [loggers.py:257] Engine 000: Avg prompt throughput: 79.7 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:18:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.88, Accepted throughput: 44.10 tokens/s, Drafted throughput: 59.99 tokens/s, Accepted: 441 tokens, Drafted: 600 tokens, Per-position acceptance rate: 0.880, 0.840, 0.813, 0.707, 0.707, 0.667, 0.653, 0.613, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:01 [loggers.py:257] Engine 000: Avg prompt throughput: 153.4 tokens/s, Avg generation throughput: 147.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.56, Accepted throughput: 121.19 tokens/s, Drafted throughput: 212.79 tokens/s, Accepted: 1212 tokens, Drafted: 2128 tokens, Per-position acceptance rate: 0.820, 0.726, 0.628, 0.571, 0.523, 0.466, 0.425, 0.398, Avg Draft acceptance rate: 57.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:11 [loggers.py:257] Engine 000: Avg prompt throughput: 162.2 tokens/s, Avg generation throughput: 146.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.50, Accepted throughput: 119.80 tokens/s, Drafted throughput: 212.80 tokens/s, Accepted: 1198 tokens, Drafted: 2128 tokens, Per-position acceptance rate: 0.820, 0.688, 0.598, 0.556, 0.519, 0.466, 0.440, 0.417, Avg Draft acceptance rate: 56.3%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:21 [loggers.py:257] Engine 000: Avg prompt throughput: 176.7 tokens/s, Avg generation throughput: 152.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.78, Accepted throughput: 126.19 tokens/s, Drafted throughput: 211.19 tokens/s, Accepted: 1262 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.830, 0.765, 0.640, 0.583, 0.542, 0.492, 0.477, 0.451, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:31 [loggers.py:257] Engine 000: Avg prompt throughput: 179.1 tokens/s, Avg generation throughput: 143.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.42, Accepted throughput: 116.70 tokens/s, Drafted throughput: 211.21 tokens/s, Accepted: 1167 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.795, 0.670, 0.595, 0.545, 0.500, 0.458, 0.439, 0.417, Avg Draft acceptance rate: 55.3%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:41 [loggers.py:257] Engine 000: Avg prompt throughput: 119.7 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.06, Accepted throughput: 87.50 tokens/s, Drafted throughput: 138.40 tokens/s, Accepted: 875 tokens, Drafted: 1384 tokens, Per-position acceptance rate: 0.838, 0.798, 0.676, 0.613, 0.578, 0.549, 0.520, 0.486, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:51 [loggers.py:257] Engine 000: Avg prompt throughput: 79.7 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:19:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 7.27, Accepted throughput: 23.20 tokens/s, Drafted throughput: 29.60 tokens/s, Accepted: 232 tokens, Drafted: 296 tokens, Per-position acceptance rate: 0.892, 0.892, 0.865, 0.757, 0.757, 0.703, 0.703, 0.703, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:01 [loggers.py:257] Engine 000: Avg prompt throughput: 277.2 tokens/s, Avg generation throughput: 279.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.64, Accepted throughput: 229.68 tokens/s, Drafted throughput: 395.96 tokens/s, Accepted: 2297 tokens, Drafted: 3960 tokens, Per-position acceptance rate: 0.826, 0.721, 0.642, 0.580, 0.535, 0.477, 0.442, 0.416, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:11 [loggers.py:257] Engine 000: Avg prompt throughput: 333.0 tokens/s, Avg generation throughput: 278.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.73, Accepted throughput: 229.89 tokens/s, Drafted throughput: 388.79 tokens/s, Accepted: 2299 tokens, Drafted: 3888 tokens, Per-position acceptance rate: 0.835, 0.741, 0.636, 0.576, 0.535, 0.494, 0.469, 0.444, Avg Draft acceptance rate: 59.1%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:21 [loggers.py:257] Engine 000: Avg prompt throughput: 180.9 tokens/s, Avg generation throughput: 163.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.72, Accepted throughput: 134.50 tokens/s, Drafted throughput: 228.00 tokens/s, Accepted: 1345 tokens, Drafted: 2280 tokens, Per-position acceptance rate: 0.814, 0.744, 0.639, 0.575, 0.537, 0.495, 0.470, 0.446, Avg Draft acceptance rate: 59.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:31 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 8.00, Accepted throughput: 11.20 tokens/s, Drafted throughput: 12.80 tokens/s, Accepted: 112 tokens, Drafted: 128 tokens, Per-position acceptance rate: 0.938, 0.938, 0.938, 0.875, 0.875, 0.812, 0.812, 0.812, Avg Draft acceptance rate: 87.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:41 [loggers.py:257] Engine 000: Avg prompt throughput: 597.5 tokens/s, Avg generation throughput: 451.6 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.65, Accepted throughput: 371.28 tokens/s, Drafted throughput: 638.36 tokens/s, Accepted: 3713 tokens, Drafted: 6384 tokens, Per-position acceptance rate: 0.822, 0.727, 0.630, 0.571, 0.534, 0.484, 0.455, 0.430, Avg Draft acceptance rate: 58.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:51 [loggers.py:257] Engine 000: Avg prompt throughput: 598.4 tokens/s, Avg generation throughput: 513.0 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:20:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.92, Accepted throughput: 425.46 tokens/s, Drafted throughput: 691.13 tokens/s, Accepted: 4255 tokens, Drafted: 6912 tokens, Per-position acceptance rate: 0.844, 0.751, 0.661, 0.609, 0.562, 0.528, 0.499, 0.471, Avg Draft acceptance rate: 61.6%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:01 [loggers.py:257] Engine 000: Avg prompt throughput: 177.1 tokens/s, Avg generation throughput: 190.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.71, Accepted throughput: 159.10 tokens/s, Drafted throughput: 270.40 tokens/s, Accepted: 1591 tokens, Drafted: 2704 tokens, Per-position acceptance rate: 0.828, 0.731, 0.639, 0.592, 0.541, 0.500, 0.456, 0.420, Avg Draft acceptance rate: 58.8%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:11 [loggers.py:257] Engine 000: Avg prompt throughput: 279.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.05, Accepted throughput: 86.90 tokens/s, Drafted throughput: 137.59 tokens/s, Accepted: 869 tokens, Drafted: 1376 tokens, Per-position acceptance rate: 0.843, 0.762, 0.686, 0.628, 0.605, 0.541, 0.512, 0.477, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:21 [loggers.py:257] Engine 000: Avg prompt throughput: 937.6 tokens/s, Avg generation throughput: 794.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.74, Accepted throughput: 654.74 tokens/s, Drafted throughput: 1103.90 tokens/s, Accepted: 6548 tokens, Drafted: 11040 tokens, Per-position acceptance rate: 0.830, 0.741, 0.643, 0.580, 0.539, 0.494, 0.470, 0.447, Avg Draft acceptance rate: 59.3%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:31 [loggers.py:257] Engine 000: Avg prompt throughput: 970.3 tokens/s, Avg generation throughput: 779.8 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.81, Accepted throughput: 647.64 tokens/s, Drafted throughput: 1076.70 tokens/s, Accepted: 6477 tokens, Drafted: 10768 tokens, Per-position acceptance rate: 0.829, 0.732, 0.662, 0.603, 0.556, 0.510, 0.475, 0.444, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:41 [loggers.py:257] Engine 000: Avg prompt throughput: 665.8 tokens/s, Avg generation throughput: 676.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.22, Accepted throughput: 571.25 tokens/s, Drafted throughput: 875.92 tokens/s, Accepted: 5713 tokens, Drafted: 8760 tokens, Per-position acceptance rate: 0.862, 0.770, 0.693, 0.651, 0.614, 0.574, 0.542, 0.511, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:51 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:21:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 8.27, Accepted throughput: 8.00 tokens/s, Drafted throughput: 8.80 tokens/s, Accepted: 80 tokens, Drafted: 88 tokens, Per-position acceptance rate: 0.909, 0.909, 0.909, 0.909, 0.909, 0.909, 0.909, 0.909, Avg Draft acceptance rate: 90.9%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:01 [loggers.py:257] Engine 000: Avg prompt throughput: 1335.7 tokens/s, Avg generation throughput: 852.5 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 53.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.79, Accepted throughput: 702.06 tokens/s, Drafted throughput: 1171.94 tokens/s, Accepted: 7021 tokens, Drafted: 11720 tokens, Per-position acceptance rate: 0.825, 0.740, 0.651, 0.592, 0.552, 0.502, 0.476, 0.455, Avg Draft acceptance rate: 59.9%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:11 [loggers.py:257] Engine 000: Avg prompt throughput: 1255.6 tokens/s, Avg generation throughput: 1060.1 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 57.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.86, Accepted throughput: 881.81 tokens/s, Drafted throughput: 1450.25 tokens/s, Accepted: 8819 tokens, Drafted: 14504 tokens, Per-position acceptance rate: 0.838, 0.739, 0.659, 0.607, 0.560, 0.519, 0.486, 0.457, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:21 [loggers.py:257] Engine 000: Avg prompt throughput: 1207.9 tokens/s, Avg generation throughput: 1034.6 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 58.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.81, Accepted throughput: 857.43 tokens/s, Drafted throughput: 1426.28 tokens/s, Accepted: 8575 tokens, Drafted: 14264 tokens, Per-position acceptance rate: 0.836, 0.738, 0.653, 0.597, 0.552, 0.510, 0.477, 0.447, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:31 [loggers.py:257] Engine 000: Avg prompt throughput: 1276.1 tokens/s, Avg generation throughput: 1025.1 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 55.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.92, Accepted throughput: 856.87 tokens/s, Drafted throughput: 1391.96 tokens/s, Accepted: 8569 tokens, Drafted: 13920 tokens, Per-position acceptance rate: 0.837, 0.749, 0.666, 0.616, 0.564, 0.525, 0.500, 0.468, Avg Draft acceptance rate: 61.6%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:41 [loggers.py:257] Engine 000: Avg prompt throughput: 556.0 tokens/s, Avg generation throughput: 744.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.95, Accepted throughput: 624.61 tokens/s, Drafted throughput: 1009.61 tokens/s, Accepted: 6246 tokens, Drafted: 10096 tokens, Per-position acceptance rate: 0.845, 0.759, 0.680, 0.609, 0.571, 0.533, 0.492, 0.459, Avg Draft acceptance rate: 61.9%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:51 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:22:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 9.00, Accepted throughput: 5.60 tokens/s, Drafted throughput: 5.60 tokens/s, Accepted: 56 tokens, Drafted: 56 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:01 [loggers.py:257] Engine 000: Avg prompt throughput: 1421.7 tokens/s, Avg generation throughput: 824.3 tokens/s, Running: 48 reqs, Waiting: 11 reqs, GPU KV cache usage: 94.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.81, Accepted throughput: 675.26 tokens/s, Drafted throughput: 1123.93 tokens/s, Accepted: 6753 tokens, Drafted: 11240 tokens, Per-position acceptance rate: 0.828, 0.740, 0.649, 0.596, 0.554, 0.505, 0.478, 0.456, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:11 [loggers.py:257] Engine 000: Avg prompt throughput: 1398.0 tokens/s, Avg generation throughput: 1040.2 tokens/s, Running: 46 reqs, Waiting: 14 reqs, GPU KV cache usage: 88.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.68, Accepted throughput: 859.56 tokens/s, Drafted throughput: 1470.33 tokens/s, Accepted: 8596 tokens, Drafted: 14704 tokens, Per-position acceptance rate: 0.819, 0.718, 0.637, 0.581, 0.536, 0.493, 0.458, 0.435, Avg Draft acceptance rate: 58.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:21 [loggers.py:257] Engine 000: Avg prompt throughput: 1242.2 tokens/s, Avg generation throughput: 1124.9 tokens/s, Running: 50 reqs, Waiting: 13 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.60, Accepted throughput: 923.55 tokens/s, Drafted throughput: 1607.29 tokens/s, Accepted: 9235 tokens, Drafted: 16072 tokens, Per-position acceptance rate: 0.798, 0.701, 0.621, 0.567, 0.529, 0.489, 0.458, 0.434, Avg Draft acceptance rate: 57.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:31 [loggers.py:257] Engine 000: Avg prompt throughput: 1279.9 tokens/s, Avg generation throughput: 1019.2 tokens/s, Running: 49 reqs, Waiting: 9 reqs, GPU KV cache usage: 92.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.53, Accepted throughput: 837.37 tokens/s, Drafted throughput: 1479.95 tokens/s, Accepted: 8374 tokens, Drafted: 14800 tokens, Per-position acceptance rate: 0.788, 0.694, 0.616, 0.564, 0.514, 0.481, 0.450, 0.422, Avg Draft acceptance rate: 56.6%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:41 [loggers.py:257] Engine 000: Avg prompt throughput: 1352.9 tokens/s, Avg generation throughput: 1048.9 tokens/s, Running: 54 reqs, Waiting: 7 reqs, GPU KV cache usage: 91.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:41 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.65, Accepted throughput: 865.83 tokens/s, Drafted throughput: 1488.06 tokens/s, Accepted: 8658 tokens, Drafted: 14880 tokens, Per-position acceptance rate: 0.800, 0.713, 0.633, 0.575, 0.532, 0.501, 0.463, 0.437, Avg Draft acceptance rate: 58.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:51 [loggers.py:257] Engine 000: Avg prompt throughput: 1053.2 tokens/s, Avg generation throughput: 1077.8 tokens/s, Running: 47 reqs, Waiting: 11 reqs, GPU KV cache usage: 91.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:23:51 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.12, Accepted throughput: 867.06 tokens/s, Drafted throughput: 1682.33 tokens/s, Accepted: 8671 tokens, Drafted: 16824 tokens, Per-position acceptance rate: 0.769, 0.655, 0.563, 0.511, 0.459, 0.421, 0.389, 0.355, Avg Draft acceptance rate: 51.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:01 [loggers.py:257] Engine 000: Avg prompt throughput: 1333.1 tokens/s, Avg generation throughput: 1019.0 tokens/s, Running: 55 reqs, Waiting: 4 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:01 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.42, Accepted throughput: 830.00 tokens/s, Drafted throughput: 1503.02 tokens/s, Accepted: 8301 tokens, Drafted: 15032 tokens, Per-position acceptance rate: 0.780, 0.676, 0.596, 0.538, 0.500, 0.468, 0.443, 0.417, Avg Draft acceptance rate: 55.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:11 [loggers.py:257] Engine 000: Avg prompt throughput: 1154.5 tokens/s, Avg generation throughput: 1094.8 tokens/s, Running: 45 reqs, Waiting: 15 reqs, GPU KV cache usage: 90.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:11 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.52, Accepted throughput: 899.12 tokens/s, Drafted throughput: 1591.06 tokens/s, Accepted: 8992 tokens, Drafted: 15912 tokens, Per-position acceptance rate: 0.811, 0.694, 0.627, 0.567, 0.513, 0.472, 0.434, 0.403, Avg Draft acceptance rate: 56.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:21 [loggers.py:257] Engine 000: Avg prompt throughput: 885.7 tokens/s, Avg generation throughput: 1067.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:21 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.60, Accepted throughput: 884.93 tokens/s, Drafted throughput: 1538.28 tokens/s, Accepted: 8850 tokens, Drafted: 15384 tokens, Per-position acceptance rate: 0.813, 0.700, 0.627, 0.574, 0.526, 0.488, 0.452, 0.423, Avg Draft acceptance rate: 57.5%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:31 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:31 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 7.60 tokens/s, Drafted throughput: 32.80 tokens/s, Accepted: 76 tokens, Drafted: 328 tokens, Per-position acceptance rate: 0.488, 0.268, 0.244, 0.195, 0.195, 0.195, 0.146, 0.122, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=92076)[0;0m INFO 01-27 16:24:41 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
