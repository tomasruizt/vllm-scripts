[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:31:58 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:31:58 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'eagle3', 'model': 'yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', 'num_speculative_tokens': 8}}
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:31:59 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:31:59 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:32:01 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=99956)[0;0m WARNING 01-27 16:32:01 [model.py:1883] Casting torch.float16 to torch.bfloat16.
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:32:01 [model.py:1559] Using max model len 2048
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:32:01 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:32:01 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:32:01 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(EngineCore_DP0 pid=100168)[0;0m INFO 01-27 16:32:08 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='eagle3', model='yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', num_spec_tokens=8), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=100168)[0;0m WARNING 01-27 16:32:08 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-27 16:32:14 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49293 backend=nccl
INFO 01-27 16:32:14 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49293 backend=nccl
INFO 01-27 16:32:14 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-27 16:32:14 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 16:32:14 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-27 16:32:14 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-27 16:32:14 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-27 16:32:14 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-27 16:32:14 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-27 16:32:14 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-27 16:32:14 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:15 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:16 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP1 pid=100330)[0;0m INFO 01-27 16:32:40 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:41 [default_loader.py:291] Loading weights took 22.28 seconds
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:41 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:45 [default_loader.py:291] Loading weights took 3.03 seconds
[0;36m(Worker_TP1 pid=100330)[0;0m INFO 01-27 16:32:45 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP1 pid=100330)[0;0m INFO 01-27 16:32:45 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:45 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:46 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:47 [gpu_model_runner.py:3921] Model loading took 67.35 GiB memory and 31.264783 seconds
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:56 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:32:56 [backends.py:704] Dynamo bytecode transform time: 8.77 s
[0;36m(Worker_TP1 pid=100330)[0;0m INFO 01-27 16:33:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 8.614 s
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 8.420 s
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:09 [monitor.py:34] torch.compile takes 17.19 s in total
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:09 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:09 [backends.py:704] Dynamo bytecode transform time: 0.36 s
[0;36m(Worker_TP1 pid=100330)[0;0m INFO 01-27 16:33:10 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.050 s
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:10 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.040 s
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:10 [monitor.py:34] torch.compile takes 17.59 s in total
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:11 [gpu_worker.py:355] Available KV cache memory: 2.05 GiB
[0;36m(EngineCore_DP0 pid=100168)[0;0m INFO 01-27 16:33:11 [kv_cache_utils.py:1307] GPU KV cache size: 13,280 tokens
[0;36m(EngineCore_DP0 pid=100168)[0;0m INFO 01-27 16:33:11 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.65x
[0;36m(Worker_TP0 pid=100329)[0;0m INFO 01-27 16:33:26 [gpu_model_runner.py:4880] Graph capturing finished in 15 secs, took 3.81 GiB
[0;36m(EngineCore_DP0 pid=100168)[0;0m INFO 01-27 16:33:26 [core.py:272] init engine (profile, create kv cache, warmup model) took 39.87 seconds
[0;36m(EngineCore_DP0 pid=100168)[0;0m INFO 01-27 16:33:29 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:29 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=99956)[0;0m WARNING 01-27 16:33:29 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:29 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:29 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:29 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [serving.py:221] Chat template warmup completed in 3443.5ms
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:33 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:43 [loggers.py:257] Engine 000: Avg prompt throughput: 62.8 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.53, Accepted throughput: 36.28 tokens/s, Drafted throughput: 82.21 tokens/s, Accepted: 526 tokens, Drafted: 1192 tokens, Per-position acceptance rate: 0.799, 0.651, 0.570, 0.423, 0.336, 0.295, 0.255, 0.201, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:53 [loggers.py:257] Engine 000: Avg prompt throughput: 69.3 tokens/s, Avg generation throughput: 67.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:33:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 49.50 tokens/s, Drafted throughput: 142.40 tokens/s, Accepted: 495 tokens, Drafted: 1424 tokens, Per-position acceptance rate: 0.815, 0.601, 0.449, 0.309, 0.225, 0.146, 0.135, 0.101, Avg Draft acceptance rate: 34.8%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:03 [loggers.py:257] Engine 000: Avg prompt throughput: 59.9 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 38.30 tokens/s, Drafted throughput: 142.38 tokens/s, Accepted: 383 tokens, Drafted: 1424 tokens, Per-position acceptance rate: 0.646, 0.421, 0.331, 0.242, 0.197, 0.140, 0.101, 0.073, Avg Draft acceptance rate: 26.9%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:13 [loggers.py:257] Engine 000: Avg prompt throughput: 86.6 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 46.70 tokens/s, Drafted throughput: 140.79 tokens/s, Accepted: 467 tokens, Drafted: 1408 tokens, Per-position acceptance rate: 0.688, 0.551, 0.403, 0.295, 0.261, 0.188, 0.159, 0.108, Avg Draft acceptance rate: 33.2%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:23 [loggers.py:257] Engine 000: Avg prompt throughput: 65.4 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 45.90 tokens/s, Drafted throughput: 142.40 tokens/s, Accepted: 459 tokens, Drafted: 1424 tokens, Per-position acceptance rate: 0.657, 0.478, 0.360, 0.281, 0.242, 0.225, 0.185, 0.152, Avg Draft acceptance rate: 32.2%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:33 [loggers.py:257] Engine 000: Avg prompt throughput: 65.3 tokens/s, Avg generation throughput: 75.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.27, Accepted throughput: 58.20 tokens/s, Drafted throughput: 142.39 tokens/s, Accepted: 582 tokens, Drafted: 1424 tokens, Per-position acceptance rate: 0.781, 0.618, 0.483, 0.388, 0.309, 0.270, 0.247, 0.174, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:43 [loggers.py:257] Engine 000: Avg prompt throughput: 77.5 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.74, Accepted throughput: 48.80 tokens/s, Drafted throughput: 142.40 tokens/s, Accepted: 488 tokens, Drafted: 1424 tokens, Per-position acceptance rate: 0.742, 0.590, 0.433, 0.309, 0.247, 0.180, 0.129, 0.112, Avg Draft acceptance rate: 34.3%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:53 [loggers.py:257] Engine 000: Avg prompt throughput: 114.6 tokens/s, Avg generation throughput: 79.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:34:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.62, Accepted throughput: 62.70 tokens/s, Drafted throughput: 138.39 tokens/s, Accepted: 627 tokens, Drafted: 1384 tokens, Per-position acceptance rate: 0.815, 0.699, 0.526, 0.457, 0.387, 0.295, 0.237, 0.208, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:03 [loggers.py:257] Engine 000: Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 26.80 tokens/s, Drafted throughput: 147.19 tokens/s, Accepted: 268 tokens, Drafted: 1472 tokens, Per-position acceptance rate: 0.582, 0.342, 0.207, 0.147, 0.076, 0.043, 0.033, 0.027, Avg Draft acceptance rate: 18.2%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:13 [loggers.py:257] Engine 000: Avg prompt throughput: 124.5 tokens/s, Avg generation throughput: 73.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.26, Accepted throughput: 55.79 tokens/s, Drafted throughput: 136.78 tokens/s, Accepted: 558 tokens, Drafted: 1368 tokens, Per-position acceptance rate: 0.784, 0.661, 0.532, 0.392, 0.298, 0.240, 0.205, 0.152, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:23 [loggers.py:257] Engine 000: Avg prompt throughput: 85.6 tokens/s, Avg generation throughput: 76.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.34, Accepted throughput: 58.50 tokens/s, Drafted throughput: 139.99 tokens/s, Accepted: 585 tokens, Drafted: 1400 tokens, Per-position acceptance rate: 0.874, 0.680, 0.497, 0.366, 0.320, 0.240, 0.200, 0.166, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:33 [loggers.py:257] Engine 000: Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 13.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.21, Accepted throughput: 9.30 tokens/s, Drafted throughput: 33.60 tokens/s, Accepted: 93 tokens, Drafted: 336 tokens, Per-position acceptance rate: 0.714, 0.429, 0.357, 0.262, 0.190, 0.143, 0.071, 0.048, Avg Draft acceptance rate: 27.7%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:43 [loggers.py:257] Engine 000: Avg prompt throughput: 109.4 tokens/s, Avg generation throughput: 83.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.41, Accepted throughput: 64.40 tokens/s, Drafted throughput: 151.20 tokens/s, Accepted: 644 tokens, Drafted: 1512 tokens, Per-position acceptance rate: 0.825, 0.651, 0.571, 0.407, 0.317, 0.243, 0.212, 0.180, Avg Draft acceptance rate: 42.6%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:53 [loggers.py:257] Engine 000: Avg prompt throughput: 123.7 tokens/s, Avg generation throughput: 118.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:35:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 85.10 tokens/s, Drafted throughput: 267.99 tokens/s, Accepted: 851 tokens, Drafted: 2680 tokens, Per-position acceptance rate: 0.710, 0.510, 0.391, 0.287, 0.230, 0.173, 0.140, 0.099, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:03 [loggers.py:257] Engine 000: Avg prompt throughput: 139.1 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 83.99 tokens/s, Drafted throughput: 264.78 tokens/s, Accepted: 840 tokens, Drafted: 2648 tokens, Per-position acceptance rate: 0.680, 0.505, 0.366, 0.278, 0.242, 0.190, 0.157, 0.121, Avg Draft acceptance rate: 31.7%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:13 [loggers.py:257] Engine 000: Avg prompt throughput: 142.8 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 96.29 tokens/s, Drafted throughput: 263.98 tokens/s, Accepted: 963 tokens, Drafted: 2640 tokens, Per-position acceptance rate: 0.736, 0.567, 0.421, 0.339, 0.288, 0.233, 0.182, 0.152, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:23 [loggers.py:257] Engine 000: Avg prompt throughput: 122.0 tokens/s, Avg generation throughput: 124.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 91.79 tokens/s, Drafted throughput: 264.78 tokens/s, Accepted: 918 tokens, Drafted: 2648 tokens, Per-position acceptance rate: 0.731, 0.559, 0.411, 0.332, 0.251, 0.196, 0.154, 0.139, Avg Draft acceptance rate: 34.7%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:33 [loggers.py:257] Engine 000: Avg prompt throughput: 193.0 tokens/s, Avg generation throughput: 131.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.09, Accepted throughput: 99.09 tokens/s, Drafted throughput: 256.77 tokens/s, Accepted: 991 tokens, Drafted: 2568 tokens, Per-position acceptance rate: 0.763, 0.620, 0.483, 0.374, 0.287, 0.231, 0.190, 0.140, Avg Draft acceptance rate: 38.6%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:43 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 33.10 tokens/s, Drafted throughput: 103.99 tokens/s, Accepted: 331 tokens, Drafted: 1040 tokens, Per-position acceptance rate: 0.800, 0.577, 0.415, 0.254, 0.208, 0.123, 0.100, 0.069, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:53 [loggers.py:257] Engine 000: Avg prompt throughput: 79.7 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:36:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.77, Accepted throughput: 19.60 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 196 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.808, 0.731, 0.635, 0.462, 0.327, 0.308, 0.269, 0.231, Avg Draft acceptance rate: 47.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:03 [loggers.py:257] Engine 000: Avg prompt throughput: 244.0 tokens/s, Avg generation throughput: 234.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 172.39 tokens/s, Drafted throughput: 492.78 tokens/s, Accepted: 1724 tokens, Drafted: 4928 tokens, Per-position acceptance rate: 0.737, 0.552, 0.433, 0.320, 0.265, 0.198, 0.169, 0.125, Avg Draft acceptance rate: 35.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:13 [loggers.py:257] Engine 000: Avg prompt throughput: 248.3 tokens/s, Avg generation throughput: 235.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 174.08 tokens/s, Drafted throughput: 496.75 tokens/s, Accepted: 1741 tokens, Drafted: 4968 tokens, Per-position acceptance rate: 0.733, 0.554, 0.409, 0.330, 0.266, 0.214, 0.164, 0.134, Avg Draft acceptance rate: 35.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:23 [loggers.py:257] Engine 000: Avg prompt throughput: 287.3 tokens/s, Avg generation throughput: 231.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.82, Accepted throughput: 170.79 tokens/s, Drafted throughput: 484.78 tokens/s, Accepted: 1708 tokens, Drafted: 4848 tokens, Per-position acceptance rate: 0.746, 0.571, 0.432, 0.332, 0.256, 0.195, 0.158, 0.129, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:33 [loggers.py:257] Engine 000: Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 16.70 tokens/s, Drafted throughput: 56.79 tokens/s, Accepted: 167 tokens, Drafted: 568 tokens, Per-position acceptance rate: 0.761, 0.577, 0.394, 0.225, 0.169, 0.113, 0.085, 0.028, Avg Draft acceptance rate: 29.4%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:43 [loggers.py:257] Engine 000: Avg prompt throughput: 197.5 tokens/s, Avg generation throughput: 125.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.27, Accepted throughput: 95.79 tokens/s, Drafted throughput: 234.38 tokens/s, Accepted: 958 tokens, Drafted: 2344 tokens, Per-position acceptance rate: 0.805, 0.635, 0.532, 0.392, 0.294, 0.242, 0.205, 0.164, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:53 [loggers.py:257] Engine 000: Avg prompt throughput: 474.9 tokens/s, Avg generation throughput: 403.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:37:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.72, Accepted throughput: 295.48 tokens/s, Drafted throughput: 869.55 tokens/s, Accepted: 2955 tokens, Drafted: 8696 tokens, Per-position acceptance rate: 0.714, 0.541, 0.407, 0.314, 0.258, 0.198, 0.162, 0.126, Avg Draft acceptance rate: 34.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:03 [loggers.py:257] Engine 000: Avg prompt throughput: 544.2 tokens/s, Avg generation throughput: 418.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 311.58 tokens/s, Drafted throughput: 857.54 tokens/s, Accepted: 3116 tokens, Drafted: 8576 tokens, Per-position acceptance rate: 0.751, 0.587, 0.451, 0.340, 0.263, 0.213, 0.167, 0.134, Avg Draft acceptance rate: 36.3%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:13 [loggers.py:257] Engine 000: Avg prompt throughput: 177.1 tokens/s, Avg generation throughput: 223.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 162.29 tokens/s, Drafted throughput: 495.18 tokens/s, Accepted: 1623 tokens, Drafted: 4952 tokens, Per-position acceptance rate: 0.746, 0.559, 0.404, 0.291, 0.218, 0.174, 0.129, 0.100, Avg Draft acceptance rate: 32.8%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:23 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.86, Accepted throughput: 3.40 tokens/s, Drafted throughput: 5.60 tokens/s, Accepted: 34 tokens, Drafted: 56 tokens, Per-position acceptance rate: 1.000, 1.000, 0.857, 0.571, 0.429, 0.429, 0.286, 0.286, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:33 [loggers.py:257] Engine 000: Avg prompt throughput: 737.9 tokens/s, Avg generation throughput: 529.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.83, Accepted throughput: 390.78 tokens/s, Drafted throughput: 1105.53 tokens/s, Accepted: 3908 tokens, Drafted: 11056 tokens, Per-position acceptance rate: 0.725, 0.553, 0.428, 0.334, 0.271, 0.211, 0.170, 0.135, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:43 [loggers.py:257] Engine 000: Avg prompt throughput: 792.8 tokens/s, Avg generation throughput: 667.5 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 494.49 tokens/s, Drafted throughput: 1410.38 tokens/s, Accepted: 4945 tokens, Drafted: 14104 tokens, Per-position acceptance rate: 0.741, 0.576, 0.438, 0.325, 0.247, 0.199, 0.155, 0.124, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:53 [loggers.py:257] Engine 000: Avg prompt throughput: 803.6 tokens/s, Avg generation throughput: 645.2 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:38:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 471.76 tokens/s, Drafted throughput: 1391.89 tokens/s, Accepted: 4718 tokens, Drafted: 13920 tokens, Per-position acceptance rate: 0.714, 0.555, 0.409, 0.318, 0.251, 0.195, 0.152, 0.117, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:03 [loggers.py:257] Engine 000: Avg prompt throughput: 497.8 tokens/s, Avg generation throughput: 504.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.19, Accepted throughput: 387.09 tokens/s, Drafted throughput: 970.37 tokens/s, Accepted: 3871 tokens, Drafted: 9704 tokens, Per-position acceptance rate: 0.759, 0.622, 0.495, 0.392, 0.322, 0.255, 0.202, 0.143, Avg Draft acceptance rate: 39.9%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:23 [loggers.py:257] Engine 000: Avg prompt throughput: 758.6 tokens/s, Avg generation throughput: 440.4 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 163.38 tokens/s, Drafted throughput: 441.96 tokens/s, Accepted: 3268 tokens, Drafted: 8840 tokens, Per-position acceptance rate: 0.730, 0.573, 0.456, 0.355, 0.285, 0.225, 0.187, 0.146, Avg Draft acceptance rate: 37.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1136.7 tokens/s, Avg generation throughput: 850.7 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.76, Accepted throughput: 627.24 tokens/s, Drafted throughput: 1816.64 tokens/s, Accepted: 6273 tokens, Drafted: 18168 tokens, Per-position acceptance rate: 0.737, 0.567, 0.423, 0.317, 0.246, 0.197, 0.153, 0.122, Avg Draft acceptance rate: 34.5%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1068.2 tokens/s, Avg generation throughput: 923.0 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.94, Accepted throughput: 690.15 tokens/s, Drafted throughput: 1879.86 tokens/s, Accepted: 6902 tokens, Drafted: 18800 tokens, Per-position acceptance rate: 0.734, 0.587, 0.451, 0.356, 0.280, 0.220, 0.177, 0.130, Avg Draft acceptance rate: 36.7%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1013.8 tokens/s, Avg generation throughput: 891.6 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:39:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 659.53 tokens/s, Drafted throughput: 1879.79 tokens/s, Accepted: 6596 tokens, Drafted: 18800 tokens, Per-position acceptance rate: 0.723, 0.563, 0.434, 0.341, 0.268, 0.209, 0.158, 0.111, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1147.7 tokens/s, Avg generation throughput: 910.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.04, Accepted throughput: 687.94 tokens/s, Drafted throughput: 1811.85 tokens/s, Accepted: 6880 tokens, Drafted: 18120 tokens, Per-position acceptance rate: 0.749, 0.607, 0.476, 0.372, 0.297, 0.231, 0.180, 0.125, Avg Draft acceptance rate: 38.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:13 [loggers.py:257] Engine 000: Avg prompt throughput: 526.9 tokens/s, Avg generation throughput: 709.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.68, Accepted throughput: 522.25 tokens/s, Drafted throughput: 1556.65 tokens/s, Accepted: 5223 tokens, Drafted: 15568 tokens, Per-position acceptance rate: 0.712, 0.542, 0.418, 0.317, 0.249, 0.189, 0.150, 0.109, Avg Draft acceptance rate: 33.5%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:23 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.89, Accepted throughput: 4.40 tokens/s, Drafted throughput: 7.20 tokens/s, Accepted: 44 tokens, Drafted: 72 tokens, Per-position acceptance rate: 1.000, 1.000, 0.778, 0.556, 0.444, 0.444, 0.333, 0.333, Avg Draft acceptance rate: 61.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1177.4 tokens/s, Avg generation throughput: 664.5 tokens/s, Running: 44 reqs, Waiting: 20 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.95, Accepted throughput: 491.87 tokens/s, Drafted throughput: 1332.44 tokens/s, Accepted: 4920 tokens, Drafted: 13328 tokens, Per-position acceptance rate: 0.742, 0.582, 0.455, 0.350, 0.281, 0.221, 0.181, 0.140, Avg Draft acceptance rate: 36.9%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1016.3 tokens/s, Avg generation throughput: 876.5 tokens/s, Running: 40 reqs, Waiting: 24 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 640.88 tokens/s, Drafted throughput: 1908.45 tokens/s, Accepted: 6410 tokens, Drafted: 19088 tokens, Per-position acceptance rate: 0.721, 0.550, 0.405, 0.312, 0.241, 0.189, 0.150, 0.119, Avg Draft acceptance rate: 33.6%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1106.6 tokens/s, Avg generation throughput: 878.0 tokens/s, Running: 41 reqs, Waiting: 17 reqs, GPU KV cache usage: 87.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:40:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.01, Accepted throughput: 660.18 tokens/s, Drafted throughput: 1752.74 tokens/s, Accepted: 6602 tokens, Drafted: 17528 tokens, Per-position acceptance rate: 0.747, 0.594, 0.466, 0.367, 0.293, 0.231, 0.183, 0.132, Avg Draft acceptance rate: 37.7%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1090.5 tokens/s, Avg generation throughput: 869.3 tokens/s, Running: 41 reqs, Waiting: 22 reqs, GPU KV cache usage: 96.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.73, Accepted throughput: 636.17 tokens/s, Drafted throughput: 1865.51 tokens/s, Accepted: 6362 tokens, Drafted: 18656 tokens, Per-position acceptance rate: 0.714, 0.561, 0.428, 0.327, 0.256, 0.192, 0.146, 0.102, Avg Draft acceptance rate: 34.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1084.9 tokens/s, Avg generation throughput: 915.7 tokens/s, Running: 44 reqs, Waiting: 20 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.05, Accepted throughput: 692.58 tokens/s, Drafted throughput: 1816.48 tokens/s, Accepted: 6927 tokens, Drafted: 18168 tokens, Per-position acceptance rate: 0.752, 0.605, 0.476, 0.375, 0.300, 0.234, 0.181, 0.128, Avg Draft acceptance rate: 38.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1020.9 tokens/s, Avg generation throughput: 856.1 tokens/s, Running: 47 reqs, Waiting: 13 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.85, Accepted throughput: 634.09 tokens/s, Drafted throughput: 1782.36 tokens/s, Accepted: 6341 tokens, Drafted: 17824 tokens, Per-position acceptance rate: 0.741, 0.566, 0.438, 0.339, 0.263, 0.206, 0.164, 0.128, Avg Draft acceptance rate: 35.6%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:33 [loggers.py:257] Engine 000: Avg prompt throughput: 961.2 tokens/s, Avg generation throughput: 900.8 tokens/s, Running: 45 reqs, Waiting: 16 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 664.68 tokens/s, Drafted throughput: 1890.35 tokens/s, Accepted: 6647 tokens, Drafted: 18904 tokens, Per-position acceptance rate: 0.722, 0.562, 0.441, 0.337, 0.262, 0.208, 0.162, 0.119, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1099.2 tokens/s, Avg generation throughput: 880.7 tokens/s, Running: 48 reqs, Waiting: 13 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 648.76 tokens/s, Drafted throughput: 1844.41 tokens/s, Accepted: 6489 tokens, Drafted: 18448 tokens, Per-position acceptance rate: 0.725, 0.550, 0.429, 0.333, 0.263, 0.212, 0.168, 0.134, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1128.9 tokens/s, Avg generation throughput: 851.9 tokens/s, Running: 45 reqs, Waiting: 17 reqs, GPU KV cache usage: 94.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:41:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 627.06 tokens/s, Drafted throughput: 1789.49 tokens/s, Accepted: 6271 tokens, Drafted: 17896 tokens, Per-position acceptance rate: 0.722, 0.559, 0.426, 0.330, 0.259, 0.211, 0.169, 0.127, Avg Draft acceptance rate: 35.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:42:03 [loggers.py:257] Engine 000: Avg prompt throughput: 855.3 tokens/s, Avg generation throughput: 835.8 tokens/s, Running: 40 reqs, Waiting: 21 reqs, GPU KV cache usage: 93.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:42:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 601.80 tokens/s, Drafted throughput: 1876.80 tokens/s, Accepted: 6018 tokens, Drafted: 18768 tokens, Per-position acceptance rate: 0.704, 0.525, 0.399, 0.303, 0.223, 0.173, 0.135, 0.103, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:42:13 [loggers.py:257] Engine 000: Avg prompt throughput: 579.4 tokens/s, Avg generation throughput: 799.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:42:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 587.36 tokens/s, Drafted throughput: 1771.86 tokens/s, Accepted: 5874 tokens, Drafted: 17720 tokens, Per-position acceptance rate: 0.696, 0.532, 0.412, 0.309, 0.241, 0.192, 0.153, 0.117, Avg Draft acceptance rate: 33.1%
[0;36m(APIServer pid=99956)[0;0m INFO 01-27 16:42:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
