(APIServer pid=31690) INFO 01-26 17:22:55 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
(APIServer pid=31690) INFO 01-26 17:22:55 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256, 'speculative_config': {'method': 'eagle3', 'model': 'yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', 'num_speculative_tokens': 4}}
(APIServer pid=31690) INFO 01-26 17:22:56 [model.py:541] Resolved architecture: LlamaForCausalLM
(APIServer pid=31690) INFO 01-26 17:22:56 [model.py:1559] Using max model len 5000
(APIServer pid=31690) INFO 01-26 17:22:58 [model.py:541] Resolved architecture: LlamaForCausalLM
(APIServer pid=31690) WARNING 01-26 17:22:58 [model.py:1883] Casting torch.float16 to torch.bfloat16.
(APIServer pid=31690) INFO 01-26 17:22:58 [model.py:1559] Using max model len 2048
(APIServer pid=31690) INFO 01-26 17:22:58 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
(APIServer pid=31690) INFO 01-26 17:22:58 [vllm.py:618] Asynchronous scheduling is enabled.
(APIServer pid=31690) INFO 01-26 17:22:58 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
(EngineCore_DP0 pid=31881) INFO 01-26 17:23:05 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=SpeculativeConfig(method='eagle3', model='yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', num_spec_tokens=4), tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
(EngineCore_DP0 pid=31881) WARNING 01-26 17:23:05 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-26 17:23:11 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49531 backend=nccl
INFO 01-26 17:23:11 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49531 backend=nccl
INFO 01-26 17:23:11 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-26 17:23:11 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-26 17:23:11 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-26 17:23:11 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-26 17:23:11 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-26 17:23:11 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-26 17:23:11 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
WARNING 01-26 17:23:11 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
WARNING 01-26 17:23:11 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
(Worker_TP0 pid=32054) INFO 01-26 17:23:11 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
(Worker_TP1 pid=32055) /home/shadeform/.venv/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
(Worker_TP1 pid=32055) We recommend installing via `pip install torch-c-dlpack-ext`
(Worker_TP1 pid=32055)   warnings.warn(
(Worker_TP0 pid=32054) /home/shadeform/.venv/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
(Worker_TP0 pid=32054) We recommend installing via `pip install torch-c-dlpack-ext`
(Worker_TP0 pid=32054)   warnings.warn(
(Worker_TP0 pid=32054) INFO 01-26 17:23:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:00<00:19,  1.46it/s]
Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:01<00:21,  1.33it/s]
Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:02<00:20,  1.30it/s]
Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:02<00:19,  1.36it/s]
Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:03<00:18,  1.33it/s]
Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:03<00:13,  1.77it/s]
Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:04<00:14,  1.63it/s]
Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:05<00:14,  1.48it/s]
Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:06<00:15,  1.40it/s]
Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:06<00:13,  1.53it/s]
Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:07<00:13,  1.45it/s]
Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:08<00:13,  1.38it/s]
Loading safetensors checkpoint shards:  43% Completed | 13/30 [00:09<00:12,  1.35it/s]
Loading safetensors checkpoint shards:  47% Completed | 14/30 [00:09<00:11,  1.34it/s]
Loading safetensors checkpoint shards:  50% Completed | 15/30 [00:10<00:11,  1.32it/s]
Loading safetensors checkpoint shards:  53% Completed | 16/30 [00:11<00:10,  1.29it/s]
Loading safetensors checkpoint shards:  57% Completed | 17/30 [00:12<00:10,  1.28it/s]
Loading safetensors checkpoint shards:  60% Completed | 18/30 [00:13<00:09,  1.27it/s]
Loading safetensors checkpoint shards:  63% Completed | 19/30 [00:13<00:08,  1.26it/s]
Loading safetensors checkpoint shards:  67% Completed | 20/30 [00:14<00:07,  1.26it/s]
Loading safetensors checkpoint shards:  70% Completed | 21/30 [00:15<00:06,  1.32it/s]
Loading safetensors checkpoint shards:  73% Completed | 22/30 [00:16<00:06,  1.30it/s]
Loading safetensors checkpoint shards:  77% Completed | 23/30 [00:16<00:05,  1.29it/s]
Loading safetensors checkpoint shards:  80% Completed | 24/30 [00:17<00:04,  1.28it/s]
Loading safetensors checkpoint shards:  83% Completed | 25/30 [00:18<00:03,  1.33it/s]
Loading safetensors checkpoint shards:  87% Completed | 26/30 [00:19<00:02,  1.36it/s]
Loading safetensors checkpoint shards:  90% Completed | 27/30 [00:19<00:02,  1.40it/s]
Loading safetensors checkpoint shards:  93% Completed | 28/30 [00:20<00:01,  1.35it/s]
Loading safetensors checkpoint shards:  97% Completed | 29/30 [00:21<00:00,  1.31it/s]
(Worker_TP1 pid=32055) INFO 01-26 17:23:37 [gpu_model_runner.py:3851] Loading drafter model...
Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:22<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:22<00:00,  1.35it/s]
(Worker_TP0 pid=32054) 
(Worker_TP0 pid=32054) INFO 01-26 17:23:37 [default_loader.py:291] Loading weights took 22.29 seconds
(Worker_TP0 pid=32054) INFO 01-26 17:23:37 [gpu_model_runner.py:3851] Loading drafter model...
(Worker_TP1 pid=32055) INFO 01-26 17:23:42 [weight_utils.py:510] Time spent downloading weights for yuhuili/EAGLE3-LLaMA3.3-Instruct-70B: 4.479866 seconds
Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.53s/it]
Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.53s/it]
(Worker_TP0 pid=32054) 
(Worker_TP0 pid=32054) INFO 01-26 17:23:45 [default_loader.py:291] Loading weights took 3.19 seconds
(Worker_TP1 pid=32055) INFO 01-26 17:23:46 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
(Worker_TP0 pid=32054) INFO 01-26 17:23:46 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
(Worker_TP1 pid=32055) INFO 01-26 17:23:46 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
(Worker_TP0 pid=32054) INFO 01-26 17:23:47 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
(Worker_TP0 pid=32054) INFO 01-26 17:23:47 [gpu_model_runner.py:3921] Model loading took 67.35 GiB memory and 34.803897 seconds
(Worker_TP0 pid=32054) INFO 01-26 17:23:56 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/backbone for vLLM's torch.compile
(Worker_TP0 pid=32054) INFO 01-26 17:23:56 [backends.py:704] Dynamo bytecode transform time: 8.67 s
(Worker_TP1 pid=32055) INFO 01-26 17:24:02 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
(Worker_TP0 pid=32054) INFO 01-26 17:24:02 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
(Worker_TP0 pid=32054) INFO 01-26 17:24:16 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 14.67 s
(Worker_TP0 pid=32054) INFO 01-26 17:24:16 [monitor.py:34] torch.compile takes 23.34 s in total
(Worker_TP0 pid=32054) INFO 01-26 17:24:16 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/d6d5e2bbba/rank_0_0/eagle_head for vLLM's torch.compile
(Worker_TP0 pid=32054) INFO 01-26 17:24:16 [backends.py:704] Dynamo bytecode transform time: 0.36 s
(Worker_TP0 pid=32054) INFO 01-26 17:24:20 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 4.12 s
(Worker_TP0 pid=32054) INFO 01-26 17:24:20 [monitor.py:34] torch.compile takes 27.82 s in total
(Worker_TP0 pid=32054) INFO 01-26 17:24:22 [gpu_worker.py:355] Available KV cache memory: 2.05 GiB
(EngineCore_DP0 pid=31881) INFO 01-26 17:24:22 [kv_cache_utils.py:1307] GPU KV cache size: 13,280 tokens
(EngineCore_DP0 pid=31881) INFO 01-26 17:24:22 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 2.65x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:08<00:00,  5.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.71it/s]
(Worker_TP0 pid=32054) INFO 01-26 17:24:39 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 4.10 GiB
(EngineCore_DP0 pid=31881) INFO 01-26 17:24:39 [core.py:272] init engine (profile, create kv cache, warmup model) took 51.79 seconds
(EngineCore_DP0 pid=31881) INFO 01-26 17:24:41 [vllm.py:618] Asynchronous scheduling is enabled.
(APIServer pid=31690) INFO 01-26 17:24:42 [api_server.py:663] Supported tasks: ['generate']
(APIServer pid=31690) WARNING 01-26 17:24:42 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
(APIServer pid=31690) INFO 01-26 17:24:42 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
(APIServer pid=31690) INFO 01-26 17:24:42 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
(APIServer pid=31690) INFO 01-26 17:24:42 [serving.py:185] Warming up chat template processing...
(APIServer pid=31690) INFO 01-26 17:24:45 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=31690) INFO 01-26 17:24:45 [serving.py:221] Chat template warmup completed in 3031.1ms
(APIServer pid=31690) INFO 01-26 17:24:45 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
(APIServer pid=31690) INFO 01-26 17:24:45 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
(APIServer pid=31690) INFO 01-26 17:24:45 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:38] Available routes are:
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /docs, Methods: HEAD, GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=31690) INFO 01-26 17:24:45 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=31690) INFO:     Started server process [31690]
(APIServer pid=31690) INFO:     Waiting for application startup.
(APIServer pid=31690) INFO:     Application startup complete.
(APIServer pid=31690) INFO 01-26 17:25:46 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:25:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 2.53 tokens/s, Drafted throughput: 4.18 tokens/s, Accepted: 162 tokens, Drafted: 268 tokens, Per-position acceptance rate: 0.776, 0.657, 0.552, 0.433, Avg Draft acceptance rate: 60.4%
(APIServer pid=31690) INFO 01-26 17:25:56 [loggers.py:257] Engine 000: Avg prompt throughput: 68.0 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:25:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 47.20 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 472 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.778, 0.631, 0.556, 0.419, Avg Draft acceptance rate: 59.6%
(APIServer pid=31690) INFO 01-26 17:26:06 [loggers.py:257] Engine 000: Avg prompt throughput: 64.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:26:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 39.10 tokens/s, Drafted throughput: 80.00 tokens/s, Accepted: 391 tokens, Drafted: 800 tokens, Per-position acceptance rate: 0.745, 0.535, 0.390, 0.285, Avg Draft acceptance rate: 48.9%
(APIServer pid=31690) INFO 01-26 17:26:16 [loggers.py:257] Engine 000: Avg prompt throughput: 73.2 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:26:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 40.50 tokens/s, Drafted throughput: 78.80 tokens/s, Accepted: 405 tokens, Drafted: 788 tokens, Per-position acceptance rate: 0.711, 0.594, 0.431, 0.320, Avg Draft acceptance rate: 51.4%
(APIServer pid=31690) INFO 01-26 17:26:26 [loggers.py:257] Engine 000: Avg prompt throughput: 77.1 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:26:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 41.90 tokens/s, Drafted throughput: 79.20 tokens/s, Accepted: 419 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.707, 0.581, 0.465, 0.364, Avg Draft acceptance rate: 52.9%
(APIServer pid=31690) INFO 01-26 17:26:36 [loggers.py:257] Engine 000: Avg prompt throughput: 48.5 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:26:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 35.60 tokens/s, Drafted throughput: 80.80 tokens/s, Accepted: 356 tokens, Drafted: 808 tokens, Per-position acceptance rate: 0.663, 0.475, 0.356, 0.267, Avg Draft acceptance rate: 44.1%
(APIServer pid=31690) INFO 01-26 17:26:46 [loggers.py:257] Engine 000: Avg prompt throughput: 65.3 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:26:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 49.49 tokens/s, Drafted throughput: 79.99 tokens/s, Accepted: 495 tokens, Drafted: 800 tokens, Per-position acceptance rate: 0.800, 0.645, 0.545, 0.485, Avg Draft acceptance rate: 61.9%
(APIServer pid=31690) INFO 01-26 17:26:56 [loggers.py:257] Engine 000: Avg prompt throughput: 77.5 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:26:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 39.40 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 394 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.707, 0.545, 0.429, 0.308, Avg Draft acceptance rate: 49.7%
(APIServer pid=31690) INFO 01-26 17:27:06 [loggers.py:257] Engine 000: Avg prompt throughput: 69.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:27:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 43.70 tokens/s, Drafted throughput: 79.59 tokens/s, Accepted: 437 tokens, Drafted: 796 tokens, Per-position acceptance rate: 0.739, 0.613, 0.457, 0.387, Avg Draft acceptance rate: 54.9%
(APIServer pid=31690) INFO 01-26 17:27:16 [loggers.py:257] Engine 000: Avg prompt throughput: 53.0 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:27:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 35.10 tokens/s, Drafted throughput: 80.40 tokens/s, Accepted: 351 tokens, Drafted: 804 tokens, Per-position acceptance rate: 0.672, 0.453, 0.353, 0.269, Avg Draft acceptance rate: 43.7%
(APIServer pid=31690) INFO 01-26 17:27:26 [loggers.py:257] Engine 000: Avg prompt throughput: 94.7 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:27:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 45.39 tokens/s, Drafted throughput: 78.79 tokens/s, Accepted: 454 tokens, Drafted: 788 tokens, Per-position acceptance rate: 0.736, 0.629, 0.528, 0.411, Avg Draft acceptance rate: 57.6%
(APIServer pid=31690) INFO 01-26 17:27:36 [loggers.py:257] Engine 000: Avg prompt throughput: 85.6 tokens/s, Avg generation throughput: 67.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:27:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 48.10 tokens/s, Drafted throughput: 77.99 tokens/s, Accepted: 481 tokens, Drafted: 780 tokens, Per-position acceptance rate: 0.841, 0.662, 0.523, 0.441, Avg Draft acceptance rate: 61.7%
(APIServer pid=31690) INFO 01-26 17:27:46 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:27:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 29.40 tokens/s, Drafted throughput: 52.00 tokens/s, Accepted: 294 tokens, Drafted: 520 tokens, Per-position acceptance rate: 0.815, 0.631, 0.485, 0.331, Avg Draft acceptance rate: 56.5%
(APIServer pid=31690) INFO 01-26 17:27:56 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 15.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:27:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 10.80 tokens/s, Drafted throughput: 19.60 tokens/s, Accepted: 108 tokens, Drafted: 196 tokens, Per-position acceptance rate: 0.714, 0.592, 0.510, 0.388, Avg Draft acceptance rate: 55.1%
(APIServer pid=31690) INFO 01-26 17:28:06 [loggers.py:257] Engine 000: Avg prompt throughput: 152.7 tokens/s, Avg generation throughput: 116.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:28:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 82.20 tokens/s, Drafted throughput: 135.20 tokens/s, Accepted: 822 tokens, Drafted: 1352 tokens, Per-position acceptance rate: 0.811, 0.669, 0.544, 0.408, Avg Draft acceptance rate: 60.8%
(APIServer pid=31690) INFO 01-26 17:28:16 [loggers.py:257] Engine 000: Avg prompt throughput: 105.6 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:28:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 68.59 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 686 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.678, 0.507, 0.388, 0.287, Avg Draft acceptance rate: 46.5%
(APIServer pid=31690) INFO 01-26 17:28:26 [loggers.py:257] Engine 000: Avg prompt throughput: 129.9 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:28:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 78.49 tokens/s, Drafted throughput: 146.78 tokens/s, Accepted: 785 tokens, Drafted: 1468 tokens, Per-position acceptance rate: 0.722, 0.578, 0.460, 0.379, Avg Draft acceptance rate: 53.5%
(APIServer pid=31690) INFO 01-26 17:28:36 [loggers.py:257] Engine 000: Avg prompt throughput: 123.9 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:28:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 77.79 tokens/s, Drafted throughput: 146.79 tokens/s, Accepted: 778 tokens, Drafted: 1468 tokens, Per-position acceptance rate: 0.730, 0.561, 0.458, 0.371, Avg Draft acceptance rate: 53.0%
(APIServer pid=31690) INFO 01-26 17:28:46 [loggers.py:257] Engine 000: Avg prompt throughput: 116.4 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:28:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 71.20 tokens/s, Drafted throughput: 146.80 tokens/s, Accepted: 712 tokens, Drafted: 1468 tokens, Per-position acceptance rate: 0.695, 0.529, 0.401, 0.316, Avg Draft acceptance rate: 48.5%
(APIServer pid=31690) INFO 01-26 17:28:56 [loggers.py:257] Engine 000: Avg prompt throughput: 168.1 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:28:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 84.40 tokens/s, Drafted throughput: 142.80 tokens/s, Accepted: 844 tokens, Drafted: 1428 tokens, Per-position acceptance rate: 0.787, 0.636, 0.518, 0.423, Avg Draft acceptance rate: 59.1%
(APIServer pid=31690) INFO 01-26 17:29:06 [loggers.py:257] Engine 000: Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:29:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 36.60 tokens/s, Drafted throughput: 64.39 tokens/s, Accepted: 366 tokens, Drafted: 644 tokens, Per-position acceptance rate: 0.814, 0.627, 0.491, 0.342, Avg Draft acceptance rate: 56.8%
(APIServer pid=31690) INFO 01-26 17:29:16 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 44.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.1%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:29:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 32.00 tokens/s, Drafted throughput: 50.00 tokens/s, Accepted: 320 tokens, Drafted: 500 tokens, Per-position acceptance rate: 0.784, 0.688, 0.608, 0.480, Avg Draft acceptance rate: 64.0%
(APIServer pid=31690) INFO 01-26 17:29:26 [loggers.py:257] Engine 000: Avg prompt throughput: 232.7 tokens/s, Avg generation throughput: 212.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.5%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:29:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 142.89 tokens/s, Drafted throughput: 275.99 tokens/s, Accepted: 1429 tokens, Drafted: 2760 tokens, Per-position acceptance rate: 0.728, 0.568, 0.443, 0.332, Avg Draft acceptance rate: 51.8%
(APIServer pid=31690) INFO 01-26 17:29:36 [loggers.py:257] Engine 000: Avg prompt throughput: 228.0 tokens/s, Avg generation throughput: 216.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.7%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:29:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 147.28 tokens/s, Drafted throughput: 276.36 tokens/s, Accepted: 1473 tokens, Drafted: 2764 tokens, Per-position acceptance rate: 0.735, 0.567, 0.456, 0.373, Avg Draft acceptance rate: 53.3%
(APIServer pid=31690) INFO 01-26 17:29:46 [loggers.py:257] Engine 000: Avg prompt throughput: 295.2 tokens/s, Avg generation throughput: 211.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:29:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 143.69 tokens/s, Drafted throughput: 271.59 tokens/s, Accepted: 1437 tokens, Drafted: 2716 tokens, Per-position acceptance rate: 0.730, 0.577, 0.451, 0.358, Avg Draft acceptance rate: 52.9%
(APIServer pid=31690) INFO 01-26 17:29:56 [loggers.py:257] Engine 000: Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:29:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 45.10 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 451 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.837, 0.642, 0.516, 0.379, Avg Draft acceptance rate: 59.3%
(APIServer pid=31690) INFO 01-26 17:30:06 [loggers.py:257] Engine 000: Avg prompt throughput: 160.3 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:30:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 70.09 tokens/s, Drafted throughput: 113.99 tokens/s, Accepted: 701 tokens, Drafted: 1140 tokens, Per-position acceptance rate: 0.793, 0.677, 0.554, 0.435, Avg Draft acceptance rate: 61.5%
(APIServer pid=31690) INFO 01-26 17:30:16 [loggers.py:257] Engine 000: Avg prompt throughput: 457.9 tokens/s, Avg generation throughput: 383.8 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.6%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:30:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 259.60 tokens/s, Drafted throughput: 497.60 tokens/s, Accepted: 2596 tokens, Drafted: 4976 tokens, Per-position acceptance rate: 0.724, 0.556, 0.450, 0.356, Avg Draft acceptance rate: 52.2%
(APIServer pid=31690) INFO 01-26 17:30:26 [loggers.py:257] Engine 000: Avg prompt throughput: 484.7 tokens/s, Avg generation throughput: 390.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:30:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 265.79 tokens/s, Drafted throughput: 496.38 tokens/s, Accepted: 2658 tokens, Drafted: 4964 tokens, Per-position acceptance rate: 0.750, 0.587, 0.454, 0.351, Avg Draft acceptance rate: 53.5%
(APIServer pid=31690) INFO 01-26 17:30:36 [loggers.py:257] Engine 000: Avg prompt throughput: 290.9 tokens/s, Avg generation throughput: 293.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:30:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 203.98 tokens/s, Drafted throughput: 361.97 tokens/s, Accepted: 2040 tokens, Drafted: 3620 tokens, Per-position acceptance rate: 0.789, 0.619, 0.476, 0.370, Avg Draft acceptance rate: 56.4%
(APIServer pid=31690) INFO 01-26 17:30:46 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:30:56 [loggers.py:257] Engine 000: Avg prompt throughput: 672.4 tokens/s, Avg generation throughput: 484.0 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.3%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:30:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 164.74 tokens/s, Drafted throughput: 306.78 tokens/s, Accepted: 3295 tokens, Drafted: 6136 tokens, Per-position acceptance rate: 0.739, 0.579, 0.465, 0.365, Avg Draft acceptance rate: 53.7%
(APIServer pid=31690) INFO 01-26 17:31:06 [loggers.py:257] Engine 000: Avg prompt throughput: 828.0 tokens/s, Avg generation throughput: 666.3 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.1%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:31:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 458.05 tokens/s, Drafted throughput: 835.92 tokens/s, Accepted: 4581 tokens, Drafted: 8360 tokens, Per-position acceptance rate: 0.762, 0.599, 0.467, 0.365, Avg Draft acceptance rate: 54.8%
(APIServer pid=31690) INFO 01-26 17:31:16 [loggers.py:257] Engine 000: Avg prompt throughput: 793.4 tokens/s, Avg generation throughput: 657.5 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.9%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:31:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 445.76 tokens/s, Drafted throughput: 845.13 tokens/s, Accepted: 4458 tokens, Drafted: 8452 tokens, Per-position acceptance rate: 0.742, 0.569, 0.441, 0.359, Avg Draft acceptance rate: 52.7%
(APIServer pid=31690) INFO 01-26 17:31:26 [loggers.py:257] Engine 000: Avg prompt throughput: 559.0 tokens/s, Avg generation throughput: 541.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:31:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 380.07 tokens/s, Drafted throughput: 651.15 tokens/s, Accepted: 3801 tokens, Drafted: 6512 tokens, Per-position acceptance rate: 0.783, 0.639, 0.508, 0.404, Avg Draft acceptance rate: 58.4%
(APIServer pid=31690) INFO 01-26 17:31:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:31:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1028.7 tokens/s, Avg generation throughput: 634.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.7%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:31:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 216.19 tokens/s, Drafted throughput: 397.57 tokens/s, Accepted: 4324 tokens, Drafted: 7952 tokens, Per-position acceptance rate: 0.744, 0.590, 0.470, 0.371, Avg Draft acceptance rate: 54.4%
(APIServer pid=31690) INFO 01-26 17:31:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1186.0 tokens/s, Avg generation throughput: 963.4 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 73.6%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:31:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 652.58 tokens/s, Drafted throughput: 1241.16 tokens/s, Accepted: 6526 tokens, Drafted: 12412 tokens, Per-position acceptance rate: 0.741, 0.573, 0.442, 0.347, Avg Draft acceptance rate: 52.6%
(APIServer pid=31690) INFO 01-26 17:32:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1267.4 tokens/s, Avg generation throughput: 1019.0 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.4%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:32:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 712.00 tokens/s, Drafted throughput: 1230.62 tokens/s, Accepted: 7121 tokens, Drafted: 12308 tokens, Per-position acceptance rate: 0.775, 0.630, 0.504, 0.405, Avg Draft acceptance rate: 57.9%
(APIServer pid=31690) INFO 01-26 17:32:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1087.9 tokens/s, Avg generation throughput: 1011.6 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.8%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:32:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 691.80 tokens/s, Drafted throughput: 1281.20 tokens/s, Accepted: 6918 tokens, Drafted: 12812 tokens, Per-position acceptance rate: 0.746, 0.592, 0.461, 0.362, Avg Draft acceptance rate: 54.0%
(APIServer pid=31690) INFO 01-26 17:32:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1082.1 tokens/s, Avg generation throughput: 973.7 tokens/s, Running: 17 reqs, Waiting: 0 reqs, GPU KV cache usage: 44.4%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:32:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 665.28 tokens/s, Drafted throughput: 1243.17 tokens/s, Accepted: 6653 tokens, Drafted: 12432 tokens, Per-position acceptance rate: 0.747, 0.586, 0.455, 0.353, Avg Draft acceptance rate: 53.5%
(APIServer pid=31690) INFO 01-26 17:32:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:32:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 83.39 tokens/s, Drafted throughput: 147.19 tokens/s, Accepted: 834 tokens, Drafted: 1472 tokens, Per-position acceptance rate: 0.753, 0.603, 0.495, 0.416, Avg Draft acceptance rate: 56.7%
(APIServer pid=31690) INFO 01-26 17:32:46 [loggers.py:257] Engine 000: Avg prompt throughput: 551.6 tokens/s, Avg generation throughput: 37.2 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 89.7%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:32:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 24.50 tokens/s, Drafted throughput: 38.40 tokens/s, Accepted: 245 tokens, Drafted: 384 tokens, Per-position acceptance rate: 0.792, 0.719, 0.594, 0.448, Avg Draft acceptance rate: 63.8%
(APIServer pid=31690) INFO 01-26 17:32:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1325.7 tokens/s, Avg generation throughput: 1133.4 tokens/s, Running: 47 reqs, Waiting: 13 reqs, GPU KV cache usage: 97.1%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:32:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 774.03 tokens/s, Drafted throughput: 1424.68 tokens/s, Accepted: 7741 tokens, Drafted: 14248 tokens, Per-position acceptance rate: 0.748, 0.592, 0.465, 0.368, Avg Draft acceptance rate: 54.3%
(APIServer pid=31690) INFO 01-26 17:33:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1247.3 tokens/s, Avg generation throughput: 998.3 tokens/s, Running: 46 reqs, Waiting: 16 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:33:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 685.46 tokens/s, Drafted throughput: 1245.52 tokens/s, Accepted: 6855 tokens, Drafted: 12456 tokens, Per-position acceptance rate: 0.755, 0.601, 0.469, 0.376, Avg Draft acceptance rate: 55.0%
(APIServer pid=31690) INFO 01-26 17:33:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1214.5 tokens/s, Avg generation throughput: 1033.8 tokens/s, Running: 45 reqs, Waiting: 18 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:33:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 704.72 tokens/s, Drafted throughput: 1309.67 tokens/s, Accepted: 7049 tokens, Drafted: 13100 tokens, Per-position acceptance rate: 0.746, 0.590, 0.457, 0.359, Avg Draft acceptance rate: 53.8%
(APIServer pid=31690) INFO 01-26 17:33:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1157.6 tokens/s, Avg generation throughput: 1045.9 tokens/s, Running: 43 reqs, Waiting: 19 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:33:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 725.59 tokens/s, Drafted throughput: 1279.19 tokens/s, Accepted: 7256 tokens, Drafted: 12792 tokens, Per-position acceptance rate: 0.769, 0.619, 0.493, 0.388, Avg Draft acceptance rate: 56.7%
(APIServer pid=31690) INFO 01-26 17:33:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1285.0 tokens/s, Avg generation throughput: 982.9 tokens/s, Running: 50 reqs, Waiting: 12 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:33:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 670.77 tokens/s, Drafted throughput: 1237.95 tokens/s, Accepted: 6708 tokens, Drafted: 12380 tokens, Per-position acceptance rate: 0.756, 0.590, 0.461, 0.360, Avg Draft acceptance rate: 54.2%
(APIServer pid=31690) INFO 01-26 17:33:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1109.4 tokens/s, Avg generation throughput: 1061.7 tokens/s, Running: 43 reqs, Waiting: 18 reqs, GPU KV cache usage: 94.0%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:33:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 724.35 tokens/s, Drafted throughput: 1341.91 tokens/s, Accepted: 7244 tokens, Drafted: 13420 tokens, Per-position acceptance rate: 0.747, 0.581, 0.465, 0.366, Avg Draft acceptance rate: 54.0%
(APIServer pid=31690) INFO 01-26 17:33:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1234.4 tokens/s, Avg generation throughput: 1027.4 tokens/s, Running: 45 reqs, Waiting: 13 reqs, GPU KV cache usage: 91.3%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:33:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 706.98 tokens/s, Drafted throughput: 1273.96 tokens/s, Accepted: 7070 tokens, Drafted: 12740 tokens, Per-position acceptance rate: 0.750, 0.603, 0.483, 0.383, Avg Draft acceptance rate: 55.5%
(APIServer pid=31690) INFO 01-26 17:34:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1197.1 tokens/s, Avg generation throughput: 999.5 tokens/s, Running: 45 reqs, Waiting: 17 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:34:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 666.88 tokens/s, Drafted throughput: 1324.96 tokens/s, Accepted: 6670 tokens, Drafted: 13252 tokens, Per-position acceptance rate: 0.720, 0.546, 0.421, 0.327, Avg Draft acceptance rate: 50.3%
(APIServer pid=31690) INFO 01-26 17:34:16 [loggers.py:257] Engine 000: Avg prompt throughput: 818.8 tokens/s, Avg generation throughput: 1000.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.8%, Prefix cache hit rate: 0.0%
(APIServer pid=31690) INFO 01-26 17:34:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 679.99 tokens/s, Drafted throughput: 1295.40 tokens/s, Accepted: 6801 tokens, Drafted: 12956 tokens, Per-position acceptance rate: 0.730, 0.570, 0.452, 0.348, Avg Draft acceptance rate: 52.5%