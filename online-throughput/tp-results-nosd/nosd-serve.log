[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:04 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:04 [utils.py:267] non-default args: {'model_tag': 'meta-llama/Llama-3.3-70B-Instruct', 'disable_uvicorn_access_log': True, 'model': 'meta-llama/Llama-3.3-70B-Instruct', 'max_model_len': 5000, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'max_num_seqs': 256}
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:06 [model.py:541] Resolved architecture: LlamaForCausalLM
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:06 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:07 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:07 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:35:07 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[0;36m(EngineCore_DP0 pid=40574)[0;0m INFO 01-26 17:35:14 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=meta-llama/Llama-3.3-70B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=40574)[0;0m WARNING 01-26 17:35:14 [multiproc_executor.py:897] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 01-26 17:35:20 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34799 backend=nccl
INFO 01-26 17:35:20 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34799 backend=nccl
INFO 01-26 17:35:20 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 01-26 17:35:20 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-26 17:35:20 [symm_mem.py:99] SymmMemCommunicator: symmetric memory initialization failed: CUDA driver error: invalid device ordinal Communicator is not available. To suppress this warning set VLLM_ALLREDUCE_USE_SYMM_MEM=0
WARNING 01-26 17:35:20 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-26 17:35:20 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-26 17:35:20 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 01-26 17:35:20 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:35:21 [gpu_model_runner.py:3824] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:35:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:35:46 [default_loader.py:291] Loading weights took 22.07 seconds
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:35:46 [gpu_model_runner.py:3921] Model loading took 65.74 GiB memory and 24.757135 seconds
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:35:55 [backends.py:644] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/08bcf73771/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:35:55 [backends.py:704] Dynamo bytecode transform time: 8.78 s
[0;36m(Worker_TP1 pid=40773)[0;0m INFO 01-26 17:36:00 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:36:00 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:36:11 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 10.99 s
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:36:11 [monitor.py:34] torch.compile takes 19.76 s in total
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:36:12 [gpu_worker.py:355] Available KV cache memory: 4.03 GiB
[0;36m(EngineCore_DP0 pid=40574)[0;0m INFO 01-26 17:36:12 [kv_cache_utils.py:1307] GPU KV cache size: 26,432 tokens
[0;36m(EngineCore_DP0 pid=40574)[0;0m INFO 01-26 17:36:12 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 5.28x
[0;36m(Worker_TP0 pid=40772)[0;0m INFO 01-26 17:36:27 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took 3.46 GiB
[0;36m(EngineCore_DP0 pid=40574)[0;0m INFO 01-26 17:36:27 [core.py:272] init engine (profile, create kv cache, warmup model) took 40.47 seconds
[0;36m(EngineCore_DP0 pid=40574)[0;0m INFO 01-26 17:36:29 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:29 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=40358)[0;0m WARNING 01-26 17:36:30 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:30 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:30 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:30 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [serving.py:221] Chat template warmup completed in 3086.0ms
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:33 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:36:54 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:37:04 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 21.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:37:14 [loggers.py:257] Engine 000: Avg prompt throughput: 20.1 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:37:24 [loggers.py:257] Engine 000: Avg prompt throughput: 29.5 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:37:34 [loggers.py:257] Engine 000: Avg prompt throughput: 18.4 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:37:44 [loggers.py:257] Engine 000: Avg prompt throughput: 20.0 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:37:54 [loggers.py:257] Engine 000: Avg prompt throughput: 30.9 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:38:04 [loggers.py:257] Engine 000: Avg prompt throughput: 13.1 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:38:14 [loggers.py:257] Engine 000: Avg prompt throughput: 46.8 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:38:24 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:38:34 [loggers.py:257] Engine 000: Avg prompt throughput: 34.5 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:38:44 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:38:54 [loggers.py:257] Engine 000: Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:39:04 [loggers.py:257] Engine 000: Avg prompt throughput: 17.3 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:39:14 [loggers.py:257] Engine 000: Avg prompt throughput: 31.2 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:39:24 [loggers.py:257] Engine 000: Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:39:34 [loggers.py:257] Engine 000: Avg prompt throughput: 13.6 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:39:44 [loggers.py:257] Engine 000: Avg prompt throughput: 28.6 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:39:54 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:40:04 [loggers.py:257] Engine 000: Avg prompt throughput: 26.6 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:40:14 [loggers.py:257] Engine 000: Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:40:24 [loggers.py:257] Engine 000: Avg prompt throughput: 17.8 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:40:34 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:40:44 [loggers.py:257] Engine 000: Avg prompt throughput: 46.2 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:40:54 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:41:04 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:41:14 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:41:24 [loggers.py:257] Engine 000: Avg prompt throughput: 47.6 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:41:34 [loggers.py:257] Engine 000: Avg prompt throughput: 31.1 tokens/s, Avg generation throughput: 22.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:41:44 [loggers.py:257] Engine 000: Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:41:54 [loggers.py:257] Engine 000: Avg prompt throughput: 43.6 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:42:04 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:42:14 [loggers.py:257] Engine 000: Avg prompt throughput: 29.3 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:42:24 [loggers.py:257] Engine 000: Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:42:34 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:42:44 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 32.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:42:54 [loggers.py:257] Engine 000: Avg prompt throughput: 47.9 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:43:04 [loggers.py:257] Engine 000: Avg prompt throughput: 50.9 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:43:14 [loggers.py:257] Engine 000: Avg prompt throughput: 24.2 tokens/s, Avg generation throughput: 45.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:43:24 [loggers.py:257] Engine 000: Avg prompt throughput: 83.1 tokens/s, Avg generation throughput: 44.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:43:34 [loggers.py:257] Engine 000: Avg prompt throughput: 56.1 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:43:44 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 45.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:43:54 [loggers.py:257] Engine 000: Avg prompt throughput: 38.4 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:44:04 [loggers.py:257] Engine 000: Avg prompt throughput: 42.2 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:44:14 [loggers.py:257] Engine 000: Avg prompt throughput: 60.6 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:44:24 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:44:34 [loggers.py:257] Engine 000: Avg prompt throughput: 66.5 tokens/s, Avg generation throughput: 44.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:44:44 [loggers.py:257] Engine 000: Avg prompt throughput: 31.0 tokens/s, Avg generation throughput: 44.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:44:54 [loggers.py:257] Engine 000: Avg prompt throughput: 82.5 tokens/s, Avg generation throughput: 44.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:45:04 [loggers.py:257] Engine 000: Avg prompt throughput: 63.1 tokens/s, Avg generation throughput: 44.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:45:14 [loggers.py:257] Engine 000: Avg prompt throughput: 35.2 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:45:24 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 40.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:45:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:45:44 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 28.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:45:54 [loggers.py:257] Engine 000: Avg prompt throughput: 69.3 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:46:04 [loggers.py:257] Engine 000: Avg prompt throughput: 118.7 tokens/s, Avg generation throughput: 86.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:46:14 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 87.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:46:24 [loggers.py:257] Engine 000: Avg prompt throughput: 80.6 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:46:34 [loggers.py:257] Engine 000: Avg prompt throughput: 95.3 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:46:44 [loggers.py:257] Engine 000: Avg prompt throughput: 104.2 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:46:54 [loggers.py:257] Engine 000: Avg prompt throughput: 121.7 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:47:04 [loggers.py:257] Engine 000: Avg prompt throughput: 112.1 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:47:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:47:24 [loggers.py:257] Engine 000: Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:47:34 [loggers.py:257] Engine 000: Avg prompt throughput: 152.7 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:47:44 [loggers.py:257] Engine 000: Avg prompt throughput: 183.5 tokens/s, Avg generation throughput: 164.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:47:54 [loggers.py:257] Engine 000: Avg prompt throughput: 158.1 tokens/s, Avg generation throughput: 165.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:48:04 [loggers.py:257] Engine 000: Avg prompt throughput: 216.7 tokens/s, Avg generation throughput: 164.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:48:14 [loggers.py:257] Engine 000: Avg prompt throughput: 217.6 tokens/s, Avg generation throughput: 163.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:48:24 [loggers.py:257] Engine 000: Avg prompt throughput: 201.6 tokens/s, Avg generation throughput: 164.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:48:34 [loggers.py:257] Engine 000: Avg prompt throughput: 205.6 tokens/s, Avg generation throughput: 164.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:48:44 [loggers.py:257] Engine 000: Avg prompt throughput: 37.3 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:48:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:49:04 [loggers.py:257] Engine 000: Avg prompt throughput: 279.0 tokens/s, Avg generation throughput: 78.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:49:14 [loggers.py:257] Engine 000: Avg prompt throughput: 305.0 tokens/s, Avg generation throughput: 300.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:49:24 [loggers.py:257] Engine 000: Avg prompt throughput: 365.3 tokens/s, Avg generation throughput: 296.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:49:34 [loggers.py:257] Engine 000: Avg prompt throughput: 363.5 tokens/s, Avg generation throughput: 296.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:49:44 [loggers.py:257] Engine 000: Avg prompt throughput: 326.3 tokens/s, Avg generation throughput: 298.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:49:54 [loggers.py:257] Engine 000: Avg prompt throughput: 333.5 tokens/s, Avg generation throughput: 295.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:50:04 [loggers.py:257] Engine 000: Avg prompt throughput: 360.7 tokens/s, Avg generation throughput: 296.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:50:14 [loggers.py:257] Engine 000: Avg prompt throughput: 448.5 tokens/s, Avg generation throughput: 291.1 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:50:24 [loggers.py:257] Engine 000: Avg prompt throughput: 71.2 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:50:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:50:44 [loggers.py:257] Engine 000: Avg prompt throughput: 233.1 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 19 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:50:54 [loggers.py:257] Engine 000: Avg prompt throughput: 654.2 tokens/s, Avg generation throughput: 539.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:51:04 [loggers.py:257] Engine 000: Avg prompt throughput: 693.9 tokens/s, Avg generation throughput: 542.4 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:51:14 [loggers.py:257] Engine 000: Avg prompt throughput: 699.2 tokens/s, Avg generation throughput: 546.2 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:51:24 [loggers.py:257] Engine 000: Avg prompt throughput: 703.0 tokens/s, Avg generation throughput: 535.9 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:51:34 [loggers.py:257] Engine 000: Avg prompt throughput: 578.4 tokens/s, Avg generation throughput: 562.7 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:51:44 [loggers.py:257] Engine 000: Avg prompt throughput: 777.6 tokens/s, Avg generation throughput: 535.6 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:51:54 [loggers.py:257] Engine 000: Avg prompt throughput: 591.6 tokens/s, Avg generation throughput: 556.3 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:52:04 [loggers.py:257] Engine 000: Avg prompt throughput: 652.5 tokens/s, Avg generation throughput: 543.1 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:52:14 [loggers.py:257] Engine 000: Avg prompt throughput: 68.7 tokens/s, Avg generation throughput: 346.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:52:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:52:34 [loggers.py:257] Engine 000: Avg prompt throughput: 437.5 tokens/s, Avg generation throughput: 22.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:52:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1234.5 tokens/s, Avg generation throughput: 902.2 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:52:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1155.6 tokens/s, Avg generation throughput: 858.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:53:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1043.5 tokens/s, Avg generation throughput: 915.3 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 65.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:53:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1038.4 tokens/s, Avg generation throughput: 916.3 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:53:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1143.6 tokens/s, Avg generation throughput: 902.2 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:53:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1074.0 tokens/s, Avg generation throughput: 914.7 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 60.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:53:44 [loggers.py:257] Engine 000: Avg prompt throughput: 966.9 tokens/s, Avg generation throughput: 936.4 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 70.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:53:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1189.8 tokens/s, Avg generation throughput: 900.9 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 60.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:54:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1039.4 tokens/s, Avg generation throughput: 916.9 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:54:14 [loggers.py:257] Engine 000: Avg prompt throughput: 818.8 tokens/s, Avg generation throughput: 902.8 tokens/s, Running: 39 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:54:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 246.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=40358)[0;0m INFO 01-26 17:54:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
