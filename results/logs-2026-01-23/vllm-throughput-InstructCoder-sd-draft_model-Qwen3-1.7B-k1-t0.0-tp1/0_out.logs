Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k1-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k1-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 169127
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:46 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:46 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15005, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 1, 'max_model_len': 5000}}
[0;36m(APIServer pid=169127)[0;0m WARNING 01-22 15:57:46 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:48 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:48 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:49 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:49 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:49 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=169127)[0;0m WARNING 01-22 15:57:49 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 15:57:49 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:58:02 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=1), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:58:03 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.49:48493 backend=nccl
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:58:03 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=169287)[0;0m WARNING 01-22 15:58:04 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:58:04 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:58:05 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe785d82fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c24ec930-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 15:58:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:58:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:58:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:58:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:58:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:58:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:58:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:11 [default_loader.py:291] Loading weights took 63.75 seconds
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:11 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:11 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:11 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-22 15:59:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:19 [default_loader.py:291] Loading weights took 7.10 seconds
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:20 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 74.703402 seconds
WARNING 01-22 15:59:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:42 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 15:59:42 [backends.py:704] Dynamo bytecode transform time: 21.72 s
WARNING 01-22 15:59:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 15:59:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:02 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 6.558 s
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:02 [monitor.py:34] torch.compile takes 28.28 s in total
WARNING 01-22 16:00:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:07 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:07 [backends.py:704] Dynamo bytecode transform time: 4.89 s
WARNING 01-22 16:00:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.756 s
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:13 [monitor.py:34] torch.compile takes 34.93 s in total
WARNING 01-22 16:00:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:17 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:17 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:17 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-22 16:00:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 16:00:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-22 16:00:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:31 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took 0.01 GiB
[0;36m(EngineCore_DP0 pid=169287)[0;0m INFO 01-22 16:00:32 [core.py:272] init engine (profile, create kv cache, warmup model) took 71.50 seconds
WARNING 01-22 16:00:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:35 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=169127)[0;0m WARNING 01-22 16:00:35 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:35 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:35 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:35 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [serving.py:221] Chat template warmup completed in 2558.2ms
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15005
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:38 [launcher.py:46] Route: /pooling, Methods: POST
WARNING 01-22 16:00:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:49 [loggers.py:257] Engine 000: Avg prompt throughput: 12.2 tokens/s, Avg generation throughput: 13.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 5.39 tokens/s, Drafted throughput: 7.85 tokens/s, Accepted: 81 tokens, Drafted: 118 tokens, Per-position acceptance rate: 0.686, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:59 [loggers.py:257] Engine 000: Avg prompt throughput: 35.6 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:00:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 16.20 tokens/s, Drafted throughput: 23.60 tokens/s, Accepted: 162 tokens, Drafted: 236 tokens, Per-position acceptance rate: 0.686, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:09 [loggers.py:257] Engine 000: Avg prompt throughput: 39.6 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 18.50 tokens/s, Drafted throughput: 25.30 tokens/s, Accepted: 185 tokens, Drafted: 253 tokens, Per-position acceptance rate: 0.731, Avg Draft acceptance rate: 73.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:19 [loggers.py:257] Engine 000: Avg prompt throughput: 29.9 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 19.50 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 195 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:29 [loggers.py:257] Engine 000: Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 44.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 19.00 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 190 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.748, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:39 [loggers.py:257] Engine 000: Avg prompt throughput: 18.6 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 19.00 tokens/s, Drafted throughput: 25.50 tokens/s, Accepted: 190 tokens, Drafted: 255 tokens, Per-position acceptance rate: 0.745, Avg Draft acceptance rate: 74.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:49 [loggers.py:257] Engine 000: Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 44.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 19.00 tokens/s, Drafted throughput: 25.30 tokens/s, Accepted: 190 tokens, Drafted: 253 tokens, Per-position acceptance rate: 0.751, Avg Draft acceptance rate: 75.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:59 [loggers.py:257] Engine 000: Avg prompt throughput: 26.8 tokens/s, Avg generation throughput: 43.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:01:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 17.40 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 174 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.685, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:09 [loggers.py:257] Engine 000: Avg prompt throughput: 39.7 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 17.90 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 179 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.705, Avg Draft acceptance rate: 70.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:19 [loggers.py:257] Engine 000: Avg prompt throughput: 28.8 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 18.60 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 186 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.732, Avg Draft acceptance rate: 73.2%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:29 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 19.70 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 197 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.776, Avg Draft acceptance rate: 77.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:39 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 18.20 tokens/s, Drafted throughput: 25.30 tokens/s, Accepted: 182 tokens, Drafted: 253 tokens, Per-position acceptance rate: 0.719, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:49 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 42.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 16.90 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 169 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.665, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:59 [loggers.py:257] Engine 000: Avg prompt throughput: 38.6 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:02:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 18.00 tokens/s, Drafted throughput: 25.30 tokens/s, Accepted: 180 tokens, Drafted: 253 tokens, Per-position acceptance rate: 0.711, Avg Draft acceptance rate: 71.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:09 [loggers.py:257] Engine 000: Avg prompt throughput: 31.1 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 19.20 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 192 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:19 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 44.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 18.90 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 189 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.744, Avg Draft acceptance rate: 74.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:29 [loggers.py:257] Engine 000: Avg prompt throughput: 40.0 tokens/s, Avg generation throughput: 41.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 16.50 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 165 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.650, Avg Draft acceptance rate: 65.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:39 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 44.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 18.60 tokens/s, Drafted throughput: 25.30 tokens/s, Accepted: 186 tokens, Drafted: 253 tokens, Per-position acceptance rate: 0.735, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:49 [loggers.py:257] Engine 000: Avg prompt throughput: 42.7 tokens/s, Avg generation throughput: 44.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 18.60 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 186 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.732, Avg Draft acceptance rate: 73.2%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:59 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:03:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 18.00 tokens/s, Drafted throughput: 25.40 tokens/s, Accepted: 180 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.709, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:09 [loggers.py:257] Engine 000: Avg prompt throughput: 17.4 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 17.60 tokens/s, Drafted throughput: 25.20 tokens/s, Accepted: 176 tokens, Drafted: 252 tokens, Per-position acceptance rate: 0.698, Avg Draft acceptance rate: 69.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:19 [loggers.py:257] Engine 000: Avg prompt throughput: 38.5 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 18.40 tokens/s, Drafted throughput: 25.30 tokens/s, Accepted: 184 tokens, Drafted: 253 tokens, Per-position acceptance rate: 0.727, Avg Draft acceptance rate: 72.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:29 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 18.30 tokens/s, Drafted throughput: 25.10 tokens/s, Accepted: 183 tokens, Drafted: 251 tokens, Per-position acceptance rate: 0.729, Avg Draft acceptance rate: 72.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  229.08    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         43.65     
Peak output token throughput (tok/s):    26.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          75.20     
---------------Time to First Token----------------
Mean TTFT (ms):                          54.23     
Median TTFT (ms):                        53.44     
P99 TTFT (ms):                           78.22     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.67     
Median TPOT (ms):                        22.68     
P99 TPOT (ms):                           23.99     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.97     
Median ITL (ms):                         38.94     
P99 ITL (ms):                            39.43     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.19     
Acceptance length:                       1.72      
Drafts:                                  5789      
Draft tokens:                            5789      
Accepted tokens:                         4179      
Per-position acceptance (%):
  Position 0:                            72.19     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:39 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 38.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 15.90 tokens/s, Drafted throughput: 22.90 tokens/s, Accepted: 159 tokens, Drafted: 229 tokens, Per-position acceptance rate: 0.694, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:04:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f33bf182fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d0ab8ae9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:05:59 [loggers.py:257] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 37.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:05:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 1.96 tokens/s, Drafted throughput: 2.62 tokens/s, Accepted: 157 tokens, Drafted: 210 tokens, Per-position acceptance rate: 0.748, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:09 [loggers.py:257] Engine 000: Avg prompt throughput: 57.0 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 36.80 tokens/s, Drafted throughput: 50.20 tokens/s, Accepted: 368 tokens, Drafted: 502 tokens, Per-position acceptance rate: 0.733, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:19 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 37.70 tokens/s, Drafted throughput: 50.19 tokens/s, Accepted: 377 tokens, Drafted: 502 tokens, Per-position acceptance rate: 0.751, Avg Draft acceptance rate: 75.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:29 [loggers.py:257] Engine 000: Avg prompt throughput: 69.5 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 37.00 tokens/s, Drafted throughput: 49.59 tokens/s, Accepted: 370 tokens, Drafted: 496 tokens, Per-position acceptance rate: 0.746, Avg Draft acceptance rate: 74.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:39 [loggers.py:257] Engine 000: Avg prompt throughput: 64.0 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 34.70 tokens/s, Drafted throughput: 49.99 tokens/s, Accepted: 347 tokens, Drafted: 500 tokens, Per-position acceptance rate: 0.694, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:49 [loggers.py:257] Engine 000: Avg prompt throughput: 68.5 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 37.40 tokens/s, Drafted throughput: 49.80 tokens/s, Accepted: 374 tokens, Drafted: 498 tokens, Per-position acceptance rate: 0.751, Avg Draft acceptance rate: 75.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:59 [loggers.py:257] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 84.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:06:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 33.90 tokens/s, Drafted throughput: 50.20 tokens/s, Accepted: 339 tokens, Drafted: 502 tokens, Per-position acceptance rate: 0.675, Avg Draft acceptance rate: 67.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:09 [loggers.py:257] Engine 000: Avg prompt throughput: 64.7 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 37.60 tokens/s, Drafted throughput: 49.99 tokens/s, Accepted: 376 tokens, Drafted: 500 tokens, Per-position acceptance rate: 0.752, Avg Draft acceptance rate: 75.2%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:19 [loggers.py:257] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 34.60 tokens/s, Drafted throughput: 50.20 tokens/s, Accepted: 346 tokens, Drafted: 502 tokens, Per-position acceptance rate: 0.689, Avg Draft acceptance rate: 68.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:29 [loggers.py:257] Engine 000: Avg prompt throughput: 82.4 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 37.30 tokens/s, Drafted throughput: 49.80 tokens/s, Accepted: 373 tokens, Drafted: 498 tokens, Per-position acceptance rate: 0.749, Avg Draft acceptance rate: 74.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:39 [loggers.py:257] Engine 000: Avg prompt throughput: 54.2 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 34.89 tokens/s, Drafted throughput: 50.19 tokens/s, Accepted: 349 tokens, Drafted: 502 tokens, Per-position acceptance rate: 0.695, Avg Draft acceptance rate: 69.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:49 [loggers.py:257] Engine 000: Avg prompt throughput: 63.3 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 36.49 tokens/s, Drafted throughput: 49.99 tokens/s, Accepted: 365 tokens, Drafted: 500 tokens, Per-position acceptance rate: 0.730, Avg Draft acceptance rate: 73.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  117.44    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.43      
Output token throughput (tok/s):         85.15     
Peak output token throughput (tok/s):    52.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          146.69    
---------------Time to First Token----------------
Mean TTFT (ms):                          88.78     
Median TTFT (ms):                        82.66     
P99 TTFT (ms):                           244.81    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.84     
Median TPOT (ms):                        22.83     
P99 TPOT (ms):                           24.32     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.32     
Median ITL (ms):                         39.19     
P99 ITL (ms):                            41.14     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.49     
Acceptance length:                       1.72      
Drafts:                                  5780      
Draft tokens:                            5780      
Accepted tokens:                         4190      
Per-position acceptance (%):
  Position 0:                            72.49     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:07:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 13.00 tokens/s, Drafted throughput: 18.60 tokens/s, Accepted: 130 tokens, Drafted: 186 tokens, Per-position acceptance rate: 0.699, Avg Draft acceptance rate: 69.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:08:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f44e8e9efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-83826f3a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:09 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 2.79 tokens/s, Drafted throughput: 3.63 tokens/s, Accepted: 195 tokens, Drafted: 254 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:19 [loggers.py:257] Engine 000: Avg prompt throughput: 107.9 tokens/s, Avg generation throughput: 173.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 73.70 tokens/s, Drafted throughput: 99.19 tokens/s, Accepted: 737 tokens, Drafted: 992 tokens, Per-position acceptance rate: 0.743, Avg Draft acceptance rate: 74.3%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:29 [loggers.py:257] Engine 000: Avg prompt throughput: 116.3 tokens/s, Avg generation throughput: 170.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 71.39 tokens/s, Drafted throughput: 98.79 tokens/s, Accepted: 714 tokens, Drafted: 988 tokens, Per-position acceptance rate: 0.723, Avg Draft acceptance rate: 72.3%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:39 [loggers.py:257] Engine 000: Avg prompt throughput: 120.4 tokens/s, Avg generation throughput: 172.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 72.39 tokens/s, Drafted throughput: 99.19 tokens/s, Accepted: 724 tokens, Drafted: 992 tokens, Per-position acceptance rate: 0.730, Avg Draft acceptance rate: 73.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:49 [loggers.py:257] Engine 000: Avg prompt throughput: 127.9 tokens/s, Avg generation throughput: 171.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 71.69 tokens/s, Drafted throughput: 98.59 tokens/s, Accepted: 717 tokens, Drafted: 986 tokens, Per-position acceptance rate: 0.727, Avg Draft acceptance rate: 72.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:59 [loggers.py:257] Engine 000: Avg prompt throughput: 146.9 tokens/s, Avg generation throughput: 169.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:09:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 69.89 tokens/s, Drafted throughput: 98.59 tokens/s, Accepted: 699 tokens, Drafted: 986 tokens, Per-position acceptance rate: 0.709, Avg Draft acceptance rate: 70.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  61.17     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.82      
Output token throughput (tok/s):         163.49    
Peak output token throughput (tok/s):    104.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          281.65    
---------------Time to First Token----------------
Mean TTFT (ms):                          83.34     
Median TTFT (ms):                        83.52     
P99 TTFT (ms):                           95.57     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.01     
Median TPOT (ms):                        23.06     
P99 TPOT (ms):                           24.17     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.72     
Median ITL (ms):                         39.58     
P99 ITL (ms):                            45.11     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.87     
Acceptance length:                       1.73      
Drafts:                                  5764      
Draft tokens:                            5764      
Accepted tokens:                         4200      
Per-position acceptance (%):
  Position 0:                            72.87     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:10:09 [loggers.py:257] Engine 000: Avg prompt throughput: 43.7 tokens/s, Avg generation throughput: 117.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:10:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 49.50 tokens/s, Drafted throughput: 68.39 tokens/s, Accepted: 495 tokens, Drafted: 684 tokens, Per-position acceptance rate: 0.724, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:10:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efdc5f76fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-fe3f5ae4-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:19 [loggers.py:257] Engine 000: Avg prompt throughput: 136.8 tokens/s, Avg generation throughput: 62.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 3.87 tokens/s, Drafted throughput: 4.93 tokens/s, Accepted: 271 tokens, Drafted: 345 tokens, Per-position acceptance rate: 0.786, Avg Draft acceptance rate: 78.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:29 [loggers.py:257] Engine 000: Avg prompt throughput: 220.7 tokens/s, Avg generation throughput: 341.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 144.19 tokens/s, Drafted throughput: 195.98 tokens/s, Accepted: 1442 tokens, Drafted: 1960 tokens, Per-position acceptance rate: 0.736, Avg Draft acceptance rate: 73.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:39 [loggers.py:257] Engine 000: Avg prompt throughput: 257.0 tokens/s, Avg generation throughput: 336.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 140.68 tokens/s, Drafted throughput: 195.17 tokens/s, Accepted: 1407 tokens, Drafted: 1952 tokens, Per-position acceptance rate: 0.721, Avg Draft acceptance rate: 72.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:49 [loggers.py:257] Engine 000: Avg prompt throughput: 215.7 tokens/s, Avg generation throughput: 338.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 141.87 tokens/s, Drafted throughput: 195.97 tokens/s, Accepted: 1419 tokens, Drafted: 1960 tokens, Per-position acceptance rate: 0.724, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:59 [loggers.py:257] Engine 000: Avg prompt throughput: 218.7 tokens/s, Avg generation throughput: 340.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:11:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 144.18 tokens/s, Drafted throughput: 195.97 tokens/s, Accepted: 1442 tokens, Drafted: 1960 tokens, Per-position acceptance rate: 0.736, Avg Draft acceptance rate: 73.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  48.66     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.64      
Output token throughput (tok/s):         328.81    
Peak output token throughput (tok/s):    208.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          569.04    
---------------Time to First Token----------------
Mean TTFT (ms):                          83.78     
Median TTFT (ms):                        84.57     
P99 TTFT (ms):                           112.36    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.27     
Median TPOT (ms):                        23.23     
P99 TPOT (ms):                           24.46     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.15     
Median ITL (ms):                         39.81     
P99 ITL (ms):                            47.07     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.92     
Acceptance length:                       1.73      
Drafts:                                  9227      
Draft tokens:                            9227      
Accepted tokens:                         6728      
Per-position acceptance (%):
  Position 0:                            72.92     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:12:09 [loggers.py:257] Engine 000: Avg prompt throughput: 138.4 tokens/s, Avg generation throughput: 199.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:12:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 82.79 tokens/s, Drafted throughput: 116.78 tokens/s, Accepted: 828 tokens, Drafted: 1168 tokens, Per-position acceptance rate: 0.709, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:12:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f895e1a6fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d98c17f9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:12:59 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 277.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:12:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 23.36 tokens/s, Drafted throughput: 31.74 tokens/s, Accepted: 1168 tokens, Drafted: 1587 tokens, Per-position acceptance rate: 0.736, Avg Draft acceptance rate: 73.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:09 [loggers.py:257] Engine 000: Avg prompt throughput: 488.8 tokens/s, Avg generation throughput: 643.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 269.05 tokens/s, Drafted throughput: 372.73 tokens/s, Accepted: 2691 tokens, Drafted: 3728 tokens, Per-position acceptance rate: 0.722, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:19 [loggers.py:257] Engine 000: Avg prompt throughput: 464.7 tokens/s, Avg generation throughput: 644.6 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 269.87 tokens/s, Drafted throughput: 372.75 tokens/s, Accepted: 2699 tokens, Drafted: 3728 tokens, Per-position acceptance rate: 0.724, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:29 [loggers.py:257] Engine 000: Avg prompt throughput: 505.1 tokens/s, Avg generation throughput: 637.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 264.99 tokens/s, Drafted throughput: 370.98 tokens/s, Accepted: 2650 tokens, Drafted: 3710 tokens, Per-position acceptance rate: 0.714, Avg Draft acceptance rate: 71.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:39 [loggers.py:257] Engine 000: Avg prompt throughput: 507.7 tokens/s, Avg generation throughput: 650.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 275.86 tokens/s, Drafted throughput: 372.55 tokens/s, Accepted: 2759 tokens, Drafted: 3726 tokens, Per-position acceptance rate: 0.740, Avg Draft acceptance rate: 74.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  51.31     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.12      
Output token throughput (tok/s):         623.42    
Peak output token throughput (tok/s):    400.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          1098.16   
---------------Time to First Token----------------
Mean TTFT (ms):                          91.36     
Median TTFT (ms):                        89.48     
P99 TTFT (ms):                           162.64    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.50     
Median TPOT (ms):                        24.55     
P99 TPOT (ms):                           26.06     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.20     
Median ITL (ms):                         41.47     
P99 ITL (ms):                            53.44     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.57     
Acceptance length:                       1.73      
Drafts:                                  18483     
Draft tokens:                            18483     
Accepted tokens:                         13414     
Per-position acceptance (%):
  Position 0:                            72.57     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:49 [loggers.py:257] Engine 000: Avg prompt throughput: 254.2 tokens/s, Avg generation throughput: 365.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 152.78 tokens/s, Drafted throughput: 212.18 tokens/s, Accepted: 1528 tokens, Drafted: 2122 tokens, Per-position acceptance rate: 0.720, Avg Draft acceptance rate: 72.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:13:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f358e4eafc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c8b57b30-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:14:59 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 5.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:14:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 0.34 tokens/s, Drafted throughput: 0.47 tokens/s, Accepted: 24 tokens, Drafted: 33 tokens, Per-position acceptance rate: 0.727, Avg Draft acceptance rate: 72.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:09 [loggers.py:257] Engine 000: Avg prompt throughput: 921.9 tokens/s, Avg generation throughput: 702.5 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 295.49 tokens/s, Drafted throughput: 401.98 tokens/s, Accepted: 2955 tokens, Drafted: 4020 tokens, Per-position acceptance rate: 0.735, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:19 [loggers.py:257] Engine 000: Avg prompt throughput: 703.2 tokens/s, Avg generation throughput: 1227.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 515.11 tokens/s, Drafted throughput: 710.67 tokens/s, Accepted: 5152 tokens, Drafted: 7108 tokens, Per-position acceptance rate: 0.725, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:29 [loggers.py:257] Engine 000: Avg prompt throughput: 828.5 tokens/s, Avg generation throughput: 1207.2 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 507.94 tokens/s, Drafted throughput: 696.72 tokens/s, Accepted: 5080 tokens, Drafted: 6968 tokens, Per-position acceptance rate: 0.729, Avg Draft acceptance rate: 72.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:39 [loggers.py:257] Engine 000: Avg prompt throughput: 899.9 tokens/s, Avg generation throughput: 1208.9 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 508.04 tokens/s, Drafted throughput: 697.72 tokens/s, Accepted: 5081 tokens, Drafted: 6978 tokens, Per-position acceptance rate: 0.728, Avg Draft acceptance rate: 72.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:49 [loggers.py:257] Engine 000: Avg prompt throughput: 980.1 tokens/s, Avg generation throughput: 1212.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 508.23 tokens/s, Drafted throughput: 700.57 tokens/s, Accepted: 5084 tokens, Drafted: 7008 tokens, Per-position acceptance rate: 0.725, Avg Draft acceptance rate: 72.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  54.84     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              5.84      
Output token throughput (tok/s):         1166.89   
Peak output token throughput (tok/s):    768.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          2047.04   
---------------Time to First Token----------------
Mean TTFT (ms):                          119.87    
Median TTFT (ms):                        102.54    
P99 TTFT (ms):                           420.11    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          26.10     
Median TPOT (ms):                        26.06     
P99 TPOT (ms):                           28.57     
---------------Inter-token Latency----------------
Mean ITL (ms):                           44.93     
Median ITL (ms):                         42.90     
P99 ITL (ms):                            80.52     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.50     
Acceptance length:                       1.73      
Drafts:                                  36993     
Draft tokens:                            36993     
Accepted tokens:                         26820     
Per-position acceptance (%):
  Position 0:                            72.50     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:59 [loggers.py:257] Engine 000: Avg prompt throughput: 492.2 tokens/s, Avg generation throughput: 852.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:15:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 352.48 tokens/s, Drafted throughput: 499.57 tokens/s, Accepted: 3525 tokens, Drafted: 4996 tokens, Per-position acceptance rate: 0.706, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd3f7212fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-297e2a0d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:19 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 0.70 tokens/s, Drafted throughput: 0.95 tokens/s, Accepted: 14 tokens, Drafted: 19 tokens, Per-position acceptance rate: 0.737, Avg Draft acceptance rate: 73.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:29 [loggers.py:257] Engine 000: Avg prompt throughput: 930.2 tokens/s, Avg generation throughput: 1171.7 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 62.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 489.87 tokens/s, Drafted throughput: 675.56 tokens/s, Accepted: 4899 tokens, Drafted: 6756 tokens, Per-position acceptance rate: 0.725, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1956.9 tokens/s, Avg generation throughput: 1916.9 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 46.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 808.96 tokens/s, Drafted throughput: 1100.53 tokens/s, Accepted: 8093 tokens, Drafted: 11010 tokens, Per-position acceptance rate: 0.735, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1279.3 tokens/s, Avg generation throughput: 2022.7 tokens/s, Running: 57 reqs, Waiting: 0 reqs, GPU KV cache usage: 50.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 838.20 tokens/s, Drafted throughput: 1179.56 tokens/s, Accepted: 8383 tokens, Drafted: 11797 tokens, Per-position acceptance rate: 0.711, Avg Draft acceptance rate: 71.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1557.3 tokens/s, Avg generation throughput: 2038.7 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 51.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:16:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 857.47 tokens/s, Drafted throughput: 1175.05 tokens/s, Accepted: 8578 tokens, Drafted: 11755 tokens, Per-position acceptance rate: 0.730, Avg Draft acceptance rate: 73.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1813.6 tokens/s, Avg generation throughput: 1987.0 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 826.94 tokens/s, Drafted throughput: 1152.61 tokens/s, Accepted: 8270 tokens, Drafted: 11527 tokens, Per-position acceptance rate: 0.717, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1030.1 tokens/s, Avg generation throughput: 2118.3 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 61.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 880.67 tokens/s, Drafted throughput: 1233.84 tokens/s, Accepted: 8810 tokens, Drafted: 12343 tokens, Per-position acceptance rate: 0.714, Avg Draft acceptance rate: 71.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  64.71     
Total input tokens:                      94775     
Total generated tokens:                  127983    
Request throughput (req/s):              9.89      
Output token throughput (tok/s):         1977.71   
Peak output token throughput (tok/s):    1408.00   
Peak concurrent requests:                104.00    
Total token throughput (tok/s):          3442.27   
---------------Time to First Token----------------
Mean TTFT (ms):                          173.83    
Median TTFT (ms):                        155.02    
P99 TTFT (ms):                           524.77    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          30.79     
Median TPOT (ms):                        30.82     
P99 TPOT (ms):                           34.41     
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.88     
Median ITL (ms):                         47.75     
P99 ITL (ms):                            140.30    
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.10     
Acceptance length:                       1.72      
Drafts:                                  74145     
Draft tokens:                            74145     
Accepted tokens:                         53456     
Per-position acceptance (%):
  Position 0:                            72.10     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:29 [loggers.py:257] Engine 000: Avg prompt throughput: 907.6 tokens/s, Avg generation throughput: 1556.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 648.97 tokens/s, Drafted throughput: 905.56 tokens/s, Accepted: 6490 tokens, Drafted: 9056 tokens, Per-position acceptance rate: 0.717, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:17:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7792d12fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d42b51be-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1942.5 tokens/s, Avg generation throughput: 954.6 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 91.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 69.14 tokens/s, Drafted throughput: 87.90 tokens/s, Accepted: 4149 tokens, Drafted: 5275 tokens, Per-position acceptance rate: 0.787, Avg Draft acceptance rate: 78.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1835.5 tokens/s, Avg generation throughput: 2563.1 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 95.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1063.86 tokens/s, Drafted throughput: 1489.42 tokens/s, Accepted: 10647 tokens, Drafted: 14906 tokens, Per-position acceptance rate: 0.714, Avg Draft acceptance rate: 71.4%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1667.3 tokens/s, Avg generation throughput: 2727.9 tokens/s, Running: 115 reqs, Waiting: 12 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1137.22 tokens/s, Drafted throughput: 1581.53 tokens/s, Accepted: 11377 tokens, Drafted: 15822 tokens, Per-position acceptance rate: 0.719, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1751.9 tokens/s, Avg generation throughput: 2729.3 tokens/s, Running: 105 reqs, Waiting: 23 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:18:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1129.60 tokens/s, Drafted throughput: 1589.92 tokens/s, Accepted: 11303 tokens, Drafted: 15909 tokens, Per-position acceptance rate: 0.710, Avg Draft acceptance rate: 71.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1760.2 tokens/s, Avg generation throughput: 2594.0 tokens/s, Running: 97 reqs, Waiting: 31 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1072.06 tokens/s, Drafted throughput: 1511.49 tokens/s, Accepted: 10727 tokens, Drafted: 15124 tokens, Per-position acceptance rate: 0.709, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1641.2 tokens/s, Avg generation throughput: 2512.4 tokens/s, Running: 94 reqs, Waiting: 23 reqs, GPU KV cache usage: 92.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1043.24 tokens/s, Drafted throughput: 1458.58 tokens/s, Accepted: 10434 tokens, Drafted: 14588 tokens, Per-position acceptance rate: 0.715, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:29 [loggers.py:257] Engine 000: Avg prompt throughput: 2530.7 tokens/s, Avg generation throughput: 2476.8 tokens/s, Running: 119 reqs, Waiting: 0 reqs, GPU KV cache usage: 91.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1030.91 tokens/s, Drafted throughput: 1431.22 tokens/s, Accepted: 10309 tokens, Drafted: 14312 tokens, Per-position acceptance rate: 0.720, Avg Draft acceptance rate: 72.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:39 [loggers.py:257] Engine 000: Avg prompt throughput: 2352.6 tokens/s, Avg generation throughput: 2462.2 tokens/s, Running: 121 reqs, Waiting: 0 reqs, GPU KV cache usage: 74.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1023.73 tokens/s, Drafted throughput: 1425.87 tokens/s, Accepted: 10239 tokens, Drafted: 14261 tokens, Per-position acceptance rate: 0.718, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1962.3 tokens/s, Avg generation throughput: 2753.9 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 82.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1152.87 tokens/s, Drafted throughput: 1590.66 tokens/s, Accepted: 11529 tokens, Drafted: 15907 tokens, Per-position acceptance rate: 0.725, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1476.1 tokens/s, Avg generation throughput: 2754.2 tokens/s, Running: 103 reqs, Waiting: 0 reqs, GPU KV cache usage: 79.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:19:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1145.51 tokens/s, Drafted throughput: 1601.02 tokens/s, Accepted: 11460 tokens, Drafted: 16017 tokens, Per-position acceptance rate: 0.715, Avg Draft acceptance rate: 71.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  98.44     
Total input tokens:                      189093    
Total generated tokens:                  255970    
Request throughput (req/s):              13.00     
Output token throughput (tok/s):         2600.25   
Peak output token throughput (tok/s):    2304.00   
Peak concurrent requests:                172.00    
Total token throughput (tok/s):          4521.13   
---------------Time to First Token----------------
Mean TTFT (ms):                          695.55    
Median TTFT (ms):                        429.46    
P99 TTFT (ms):                           3565.87   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          44.87     
Median TPOT (ms):                        44.39     
P99 TPOT (ms):                           61.89     
---------------Inter-token Latency----------------
Mean ITL (ms):                           76.82     
Median ITL (ms):                         55.61     
P99 ITL (ms):                            311.59    
---------------Speculative Decoding---------------
Acceptance rate (%):                     71.69     
Acceptance length:                       1.72      
Drafts:                                  148487    
Draft tokens:                            148487    
Accepted tokens:                         106444    
Per-position acceptance (%):
  Position 0:                            71.69     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1078.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 435.06 tokens/s, Drafted throughput: 648.35 tokens/s, Accepted: 4351 tokens, Drafted: 6484 tokens, Per-position acceptance rate: 0.671, Avg Draft acceptance rate: 67.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fcff4376fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-df304889-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:29 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1.60 tokens/s, Drafted throughput: 2.25 tokens/s, Accepted: 32 tokens, Drafted: 45 tokens, Per-position acceptance rate: 0.711, Avg Draft acceptance rate: 71.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:39 [loggers.py:257] Engine 000: Avg prompt throughput: 3020.3 tokens/s, Avg generation throughput: 1675.8 tokens/s, Running: 122 reqs, Waiting: 134 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 717.73 tokens/s, Drafted throughput: 938.31 tokens/s, Accepted: 7178 tokens, Drafted: 9384 tokens, Per-position acceptance rate: 0.765, Avg Draft acceptance rate: 76.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:49 [loggers.py:257] Engine 000: Avg prompt throughput: 625.9 tokens/s, Avg generation throughput: 2251.8 tokens/s, Running: 117 reqs, Waiting: 139 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 926.71 tokens/s, Drafted throughput: 1312.28 tokens/s, Accepted: 9268 tokens, Drafted: 13124 tokens, Per-position acceptance rate: 0.706, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:59 [loggers.py:257] Engine 000: Avg prompt throughput: 2292.0 tokens/s, Avg generation throughput: 2678.9 tokens/s, Running: 110 reqs, Waiting: 145 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:20:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1113.29 tokens/s, Drafted throughput: 1550.87 tokens/s, Accepted: 11136 tokens, Drafted: 15513 tokens, Per-position acceptance rate: 0.718, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1775.8 tokens/s, Avg generation throughput: 2586.7 tokens/s, Running: 109 reqs, Waiting: 146 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1066.49 tokens/s, Drafted throughput: 1506.85 tokens/s, Accepted: 10666 tokens, Drafted: 15070 tokens, Per-position acceptance rate: 0.708, Avg Draft acceptance rate: 70.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1901.0 tokens/s, Avg generation throughput: 2559.0 tokens/s, Running: 106 reqs, Waiting: 149 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1061.99 tokens/s, Drafted throughput: 1483.35 tokens/s, Accepted: 10626 tokens, Drafted: 14842 tokens, Per-position acceptance rate: 0.716, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1843.8 tokens/s, Avg generation throughput: 2548.8 tokens/s, Running: 109 reqs, Waiting: 146 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1057.93 tokens/s, Drafted throughput: 1475.84 tokens/s, Accepted: 10579 tokens, Drafted: 14758 tokens, Per-position acceptance rate: 0.717, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1861.6 tokens/s, Avg generation throughput: 2509.5 tokens/s, Running: 110 reqs, Waiting: 146 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1043.96 tokens/s, Drafted throughput: 1451.80 tokens/s, Accepted: 10446 tokens, Drafted: 14527 tokens, Per-position acceptance rate: 0.719, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1840.2 tokens/s, Avg generation throughput: 2429.0 tokens/s, Running: 102 reqs, Waiting: 152 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1002.21 tokens/s, Drafted throughput: 1413.78 tokens/s, Accepted: 10023 tokens, Drafted: 14139 tokens, Per-position acceptance rate: 0.709, Avg Draft acceptance rate: 70.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1804.1 tokens/s, Avg generation throughput: 2469.6 tokens/s, Running: 109 reqs, Waiting: 143 reqs, GPU KV cache usage: 98.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:21:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1024.48 tokens/s, Drafted throughput: 1429.84 tokens/s, Accepted: 10246 tokens, Drafted: 14300 tokens, Per-position acceptance rate: 0.717, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1798.1 tokens/s, Avg generation throughput: 2544.0 tokens/s, Running: 102 reqs, Waiting: 152 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1058.81 tokens/s, Drafted throughput: 1470.91 tokens/s, Accepted: 10588 tokens, Drafted: 14709 tokens, Per-position acceptance rate: 0.720, Avg Draft acceptance rate: 72.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1848.7 tokens/s, Avg generation throughput: 2479.6 tokens/s, Running: 110 reqs, Waiting: 145 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1028.78 tokens/s, Drafted throughput: 1435.80 tokens/s, Accepted: 10295 tokens, Drafted: 14368 tokens, Per-position acceptance rate: 0.717, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1837.2 tokens/s, Avg generation throughput: 2502.4 tokens/s, Running: 102 reqs, Waiting: 153 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1038.29 tokens/s, Drafted throughput: 1450.04 tokens/s, Accepted: 10384 tokens, Drafted: 14502 tokens, Per-position acceptance rate: 0.716, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1880.6 tokens/s, Avg generation throughput: 2521.8 tokens/s, Running: 105 reqs, Waiting: 150 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1045.37 tokens/s, Drafted throughput: 1462.16 tokens/s, Accepted: 10454 tokens, Drafted: 14622 tokens, Per-position acceptance rate: 0.715, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1889.7 tokens/s, Avg generation throughput: 2465.8 tokens/s, Running: 108 reqs, Waiting: 146 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1024.98 tokens/s, Drafted throughput: 1424.59 tokens/s, Accepted: 10252 tokens, Drafted: 14249 tokens, Per-position acceptance rate: 0.719, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1808.7 tokens/s, Avg generation throughput: 2530.6 tokens/s, Running: 105 reqs, Waiting: 149 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:22:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1051.83 tokens/s, Drafted throughput: 1464.31 tokens/s, Accepted: 10519 tokens, Drafted: 14644 tokens, Per-position acceptance rate: 0.718, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1740.4 tokens/s, Avg generation throughput: 2539.1 tokens/s, Running: 104 reqs, Waiting: 150 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1051.04 tokens/s, Drafted throughput: 1475.07 tokens/s, Accepted: 10512 tokens, Drafted: 14753 tokens, Per-position acceptance rate: 0.713, Avg Draft acceptance rate: 71.3%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1965.6 tokens/s, Avg generation throughput: 2548.0 tokens/s, Running: 107 reqs, Waiting: 147 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1062.08 tokens/s, Drafted throughput: 1470.64 tokens/s, Accepted: 10622 tokens, Drafted: 14708 tokens, Per-position acceptance rate: 0.722, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1717.6 tokens/s, Avg generation throughput: 2507.1 tokens/s, Running: 105 reqs, Waiting: 150 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1045.20 tokens/s, Drafted throughput: 1447.73 tokens/s, Accepted: 10459 tokens, Drafted: 14487 tokens, Per-position acceptance rate: 0.722, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1739.4 tokens/s, Avg generation throughput: 2571.0 tokens/s, Running: 107 reqs, Waiting: 149 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1061.83 tokens/s, Drafted throughput: 1494.10 tokens/s, Accepted: 10619 tokens, Drafted: 14942 tokens, Per-position acceptance rate: 0.711, Avg Draft acceptance rate: 71.1%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1930.9 tokens/s, Avg generation throughput: 2574.2 tokens/s, Running: 109 reqs, Waiting: 72 reqs, GPU KV cache usage: 98.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 1071.13 tokens/s, Drafted throughput: 1487.80 tokens/s, Accepted: 10712 tokens, Drafted: 14879 tokens, Per-position acceptance rate: 0.720, Avg Draft acceptance rate: 72.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  206.14    
Total input tokens:                      373233    
Total generated tokens:                  511919    
Request throughput (req/s):              12.42     
Output token throughput (tok/s):         2483.37   
Peak output token throughput (tok/s):    2729.00   
Peak concurrent requests:                292.00    
Total token throughput (tok/s):          4293.96   
---------------Time to First Token----------------
Mean TTFT (ms):                          8170.99   
Median TTFT (ms):                        9586.08   
P99 TTFT (ms):                           12570.83  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          58.67     
Median TPOT (ms):                        53.63     
P99 TPOT (ms):                           89.90     
---------------Inter-token Latency----------------
Mean ITL (ms):                           100.33    
Median ITL (ms):                         60.09     
P99 ITL (ms):                            435.41    
---------------Speculative Decoding---------------
Acceptance rate (%):                     71.68     
Acceptance length:                       1.72      
Drafts:                                  296470    
Draft tokens:                            296470    
Accepted tokens:                         212507    
Per-position acceptance (%):
  Position 0:                            71.68     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k1-t0.0-tp1...
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:23:59 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:24:02 [loggers.py:257] Engine 000: Avg prompt throughput: 147.2 tokens/s, Avg generation throughput: 1295.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=169127)[0;0m INFO 01-22 16:24:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 531.34 tokens/s, Drafted throughput: 763.86 tokens/s, Accepted: 6972 tokens, Drafted: 10023 tokens, Per-position acceptance rate: 0.696, Avg Draft acceptance rate: 69.6%
