Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k2-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k2-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 179141
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:26 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:26 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15006, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 2, 'max_model_len': 5000}}
[0;36m(APIServer pid=179141)[0;0m WARNING 01-22 16:27:26 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:27 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:27 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:28 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:28 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:28 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=179141)[0;0m WARNING 01-22 16:27:28 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:27:28 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f416fc86fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e94fb5c5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 16:27:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:27:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:27:39 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=2), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:27:41 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.49:48817 backend=nccl
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:27:41 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=179408)[0;0m WARNING 01-22 16:27:42 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:27:42 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-22 16:27:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:27:43 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 16:27:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:27:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:27:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:38 [default_loader.py:291] Loading weights took 53.04 seconds
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:38 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:38 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:38 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-22 16:28:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:41 [default_loader.py:291] Loading weights took 2.18 seconds
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:42 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 58.604093 seconds
WARNING 01-22 16:28:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:28:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:55 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:28:55 [backends.py:704] Dynamo bytecode transform time: 13.27 s
WARNING 01-22 16:28:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:29:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:29:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.645 s
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:13 [monitor.py:34] torch.compile takes 17.92 s in total
WARNING 01-22 16:29:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:18 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:18 [backends.py:704] Dynamo bytecode transform time: 4.84 s
WARNING 01-22 16:29:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:29:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:24 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.725 s
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:24 [monitor.py:34] torch.compile takes 24.48 s in total
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:26 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:26 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:26 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-22 16:29:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:29:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
WARNING 01-22 16:29:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:40 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took -0.03 GiB
[0;36m(EngineCore_DP0 pid=179408)[0;0m INFO 01-22 16:29:41 [core.py:272] init engine (profile, create kv cache, warmup model) took 59.06 seconds
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:42 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=179141)[0;0m WARNING 01-22 16:29:43 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:43 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:43 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:43 [serving.py:185] Warming up chat template processing...
WARNING 01-22 16:29:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15006)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15006 ssl:default [Connect call failed (\'127.0.0.1\', 15006)]\n''
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [serving.py:221] Chat template warmup completed in 1779.0ms
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15006
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:45 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:55 [loggers.py:257] Engine 000: Avg prompt throughput: 27.9 tokens/s, Avg generation throughput: 23.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:29:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 12.73 tokens/s, Drafted throughput: 21.81 tokens/s, Accepted: 167 tokens, Drafted: 286 tokens, Per-position acceptance rate: 0.720, 0.448, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:05 [loggers.py:257] Engine 000: Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 27.90 tokens/s, Drafted throughput: 47.00 tokens/s, Accepted: 279 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.702, 0.485, Avg Draft acceptance rate: 59.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:15 [loggers.py:257] Engine 000: Avg prompt throughput: 33.0 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 29.50 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 295 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.763, 0.487, Avg Draft acceptance rate: 62.5%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:25 [loggers.py:257] Engine 000: Avg prompt throughput: 36.2 tokens/s, Avg generation throughput: 53.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 29.90 tokens/s, Drafted throughput: 47.00 tokens/s, Accepted: 299 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.740, 0.532, Avg Draft acceptance rate: 63.6%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.6 tokens/s, Avg generation throughput: 50.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 27.10 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 271 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.695, 0.453, Avg Draft acceptance rate: 57.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:45 [loggers.py:257] Engine 000: Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 31.99 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 320 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.784, 0.572, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:55 [loggers.py:257] Engine 000: Avg prompt throughput: 52.2 tokens/s, Avg generation throughput: 50.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:30:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 27.20 tokens/s, Drafted throughput: 46.80 tokens/s, Accepted: 272 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.684, 0.479, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:05 [loggers.py:257] Engine 000: Avg prompt throughput: 43.1 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 31.20 tokens/s, Drafted throughput: 47.00 tokens/s, Accepted: 312 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.770, 0.557, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:15 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 31.70 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 317 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.767, 0.576, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:25 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 50.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 26.30 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 263 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.678, 0.436, Avg Draft acceptance rate: 55.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:35 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 51.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 27.40 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 274 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.678, 0.483, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:45 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 31.00 tokens/s, Drafted throughput: 46.80 tokens/s, Accepted: 310 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.765, 0.560, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:55 [loggers.py:257] Engine 000: Avg prompt throughput: 44.1 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:31:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 28.10 tokens/s, Drafted throughput: 46.99 tokens/s, Accepted: 281 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.702, 0.494, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:05 [loggers.py:257] Engine 000: Avg prompt throughput: 40.0 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 28.40 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 284 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.708, 0.496, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:15 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 28.40 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 284 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.708, 0.496, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:25 [loggers.py:257] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 52.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 29.00 tokens/s, Drafted throughput: 46.80 tokens/s, Accepted: 290 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.726, 0.513, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:35 [loggers.py:257] Engine 000: Avg prompt throughput: 22.8 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 31.40 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 314 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.772, 0.553, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:45 [loggers.py:257] Engine 000: Avg prompt throughput: 51.1 tokens/s, Avg generation throughput: 53.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 30.10 tokens/s, Drafted throughput: 46.79 tokens/s, Accepted: 301 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.731, 0.556, Avg Draft acceptance rate: 64.3%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:55 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:32:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 30.80 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 308 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.750, 0.555, Avg Draft acceptance rate: 65.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  189.99    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.26      
Output token throughput (tok/s):         52.63     
Peak output token throughput (tok/s):    24.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          90.67     
---------------Time to First Token----------------
Mean TTFT (ms):                          55.94     
Median TTFT (ms):                        55.50     
P99 TTFT (ms):                           64.28     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.73     
Median TPOT (ms):                        18.74     
P99 TPOT (ms):                           20.41     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.85     
Median ITL (ms):                         41.85     
P99 ITL (ms):                            42.29     
---------------Speculative Decoding---------------
Acceptance rate (%):                     62.02     
Acceptance length:                       2.24      
Drafts:                                  4454      
Draft tokens:                            8908      
Accepted tokens:                         5525      
Per-position acceptance (%):
  Position 0:                            72.77     
  Position 1:                            51.28     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:05 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 35.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 19.20 tokens/s, Drafted throughput: 33.40 tokens/s, Accepted: 192 tokens, Drafted: 334 tokens, Per-position acceptance rate: 0.695, 0.455, Avg Draft acceptance rate: 57.5%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f60475eafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-5ec6d31e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:25 [loggers.py:257] Engine 000: Avg prompt throughput: 36.6 tokens/s, Avg generation throughput: 23.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 6.30 tokens/s, Drafted throughput: 10.40 tokens/s, Accepted: 126 tokens, Drafted: 208 tokens, Per-position acceptance rate: 0.740, 0.471, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:35 [loggers.py:257] Engine 000: Avg prompt throughput: 74.3 tokens/s, Avg generation throughput: 102.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 56.10 tokens/s, Drafted throughput: 91.59 tokens/s, Accepted: 561 tokens, Drafted: 916 tokens, Per-position acceptance rate: 0.725, 0.500, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:45 [loggers.py:257] Engine 000: Avg prompt throughput: 74.9 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 57.20 tokens/s, Drafted throughput: 92.00 tokens/s, Accepted: 572 tokens, Drafted: 920 tokens, Per-position acceptance rate: 0.733, 0.511, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:55 [loggers.py:257] Engine 000: Avg prompt throughput: 47.8 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:33:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 58.49 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 585 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.734, 0.521, Avg Draft acceptance rate: 62.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:05 [loggers.py:257] Engine 000: Avg prompt throughput: 102.5 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 60.49 tokens/s, Drafted throughput: 91.98 tokens/s, Accepted: 605 tokens, Drafted: 920 tokens, Per-position acceptance rate: 0.759, 0.557, Avg Draft acceptance rate: 65.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:15 [loggers.py:257] Engine 000: Avg prompt throughput: 62.6 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 53.90 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 539 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.683, 0.478, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:25 [loggers.py:257] Engine 000: Avg prompt throughput: 88.5 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 59.79 tokens/s, Drafted throughput: 92.39 tokens/s, Accepted: 598 tokens, Drafted: 924 tokens, Per-position acceptance rate: 0.747, 0.548, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:35 [loggers.py:257] Engine 000: Avg prompt throughput: 63.2 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 55.20 tokens/s, Drafted throughput: 92.39 tokens/s, Accepted: 552 tokens, Drafted: 924 tokens, Per-position acceptance rate: 0.701, 0.494, Avg Draft acceptance rate: 59.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:45 [loggers.py:257] Engine 000: Avg prompt throughput: 95.8 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 58.20 tokens/s, Drafted throughput: 92.39 tokens/s, Accepted: 582 tokens, Drafted: 924 tokens, Per-position acceptance rate: 0.725, 0.535, Avg Draft acceptance rate: 63.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:55 [loggers.py:257] Engine 000: Avg prompt throughput: 76.3 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:34:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 57.50 tokens/s, Drafted throughput: 92.40 tokens/s, Accepted: 575 tokens, Drafted: 924 tokens, Per-position acceptance rate: 0.716, 0.528, Avg Draft acceptance rate: 62.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  98.32     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.51      
Output token throughput (tok/s):         101.71    
Peak output token throughput (tok/s):    48.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          175.21    
---------------Time to First Token----------------
Mean TTFT (ms):                          89.35     
Median TTFT (ms):                        88.28     
P99 TTFT (ms):                           127.42    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.91     
Median TPOT (ms):                        19.10     
P99 TPOT (ms):                           20.29     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.26     
Median ITL (ms):                         42.19     
P99 ITL (ms):                            45.51     
---------------Speculative Decoding---------------
Acceptance rate (%):                     62.12     
Acceptance length:                       2.24      
Drafts:                                  4452      
Draft tokens:                            8904      
Accepted tokens:                         5531      
Per-position acceptance (%):
  Position 0:                            72.48     
  Position 1:                            51.75     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:05 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 34.40 tokens/s, Drafted throughput: 56.99 tokens/s, Accepted: 344 tokens, Drafted: 570 tokens, Per-position acceptance rate: 0.719, 0.488, Avg Draft acceptance rate: 60.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f887112efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2dc08fbe-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:25 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 31.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 8.85 tokens/s, Drafted throughput: 13.90 tokens/s, Accepted: 177 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.763, 0.511, Avg Draft acceptance rate: 63.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:35 [loggers.py:257] Engine 000: Avg prompt throughput: 107.9 tokens/s, Avg generation throughput: 203.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 111.69 tokens/s, Drafted throughput: 183.18 tokens/s, Accepted: 1117 tokens, Drafted: 1832 tokens, Per-position acceptance rate: 0.717, 0.502, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:45 [loggers.py:257] Engine 000: Avg prompt throughput: 171.7 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 111.89 tokens/s, Drafted throughput: 180.79 tokens/s, Accepted: 1119 tokens, Drafted: 1808 tokens, Per-position acceptance rate: 0.731, 0.507, Avg Draft acceptance rate: 61.9%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:55 [loggers.py:257] Engine 000: Avg prompt throughput: 138.9 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:35:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 112.99 tokens/s, Drafted throughput: 181.99 tokens/s, Accepted: 1130 tokens, Drafted: 1820 tokens, Per-position acceptance rate: 0.714, 0.527, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:05 [loggers.py:257] Engine 000: Avg prompt throughput: 162.4 tokens/s, Avg generation throughput: 205.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 114.59 tokens/s, Drafted throughput: 181.18 tokens/s, Accepted: 1146 tokens, Drafted: 1812 tokens, Per-position acceptance rate: 0.736, 0.529, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:15 [loggers.py:257] Engine 000: Avg prompt throughput: 82.2 tokens/s, Avg generation throughput: 170.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 94.79 tokens/s, Drafted throughput: 152.78 tokens/s, Accepted: 948 tokens, Drafted: 1528 tokens, Per-position acceptance rate: 0.721, 0.520, Avg Draft acceptance rate: 62.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  51.58     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.97      
Output token throughput (tok/s):         193.88    
Peak output token throughput (tok/s):    96.00     
Peak concurrent requests:                8.00      
Total token throughput (tok/s):          334.00    
---------------Time to First Token----------------
Mean TTFT (ms):                          88.95     
Median TTFT (ms):                        89.02     
P99 TTFT (ms):                           96.92     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.27     
Median TPOT (ms):                        19.45     
P99 TPOT (ms):                           21.09     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.03     
Median ITL (ms):                         42.89     
P99 ITL (ms):                            47.93     
---------------Speculative Decoding---------------
Acceptance rate (%):                     62.17     
Acceptance length:                       2.24      
Drafts:                                  4455      
Draft tokens:                            8910      
Accepted tokens:                         5539      
Per-position acceptance (%):
  Position 0:                            72.55     
  Position 1:                            51.78     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 1.00 tokens/s, Drafted throughput: 1.80 tokens/s, Accepted: 10 tokens, Drafted: 18 tokens, Per-position acceptance rate: 0.778, 0.333, Avg Draft acceptance rate: 55.6%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe5cd422fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e17a0759-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 5.30 tokens/s, Drafted throughput: 9.00 tokens/s, Accepted: 53 tokens, Drafted: 90 tokens, Per-position acceptance rate: 0.711, 0.467, Avg Draft acceptance rate: 58.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:45 [loggers.py:257] Engine 000: Avg prompt throughput: 215.3 tokens/s, Avg generation throughput: 306.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 168.08 tokens/s, Drafted throughput: 276.37 tokens/s, Accepted: 1681 tokens, Drafted: 2764 tokens, Per-position acceptance rate: 0.713, 0.503, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:55 [loggers.py:257] Engine 000: Avg prompt throughput: 352.9 tokens/s, Avg generation throughput: 393.7 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:36:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 217.19 tokens/s, Drafted throughput: 352.19 tokens/s, Accepted: 2172 tokens, Drafted: 3522 tokens, Per-position acceptance rate: 0.722, 0.512, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:05 [loggers.py:257] Engine 000: Avg prompt throughput: 243.7 tokens/s, Avg generation throughput: 403.0 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 223.18 tokens/s, Drafted throughput: 358.17 tokens/s, Accepted: 2232 tokens, Drafted: 3582 tokens, Per-position acceptance rate: 0.726, 0.520, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:15 [loggers.py:257] Engine 000: Avg prompt throughput: 357.0 tokens/s, Avg generation throughput: 396.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 218.17 tokens/s, Drafted throughput: 353.55 tokens/s, Accepted: 2182 tokens, Drafted: 3536 tokens, Per-position acceptance rate: 0.719, 0.515, Avg Draft acceptance rate: 61.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  41.65     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.92      
Output token throughput (tok/s):         384.15    
Peak output token throughput (tok/s):    184.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          664.81    
---------------Time to First Token----------------
Mean TTFT (ms):                          92.08     
Median TTFT (ms):                        92.20     
P99 TTFT (ms):                           100.66    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.79     
Median TPOT (ms):                        19.86     
P99 TPOT (ms):                           21.79     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.92     
Median ITL (ms):                         43.55     
P99 ITL (ms):                            52.06     
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.32     
Acceptance length:                       2.23      
Drafts:                                  7173      
Draft tokens:                            14346     
Accepted tokens:                         8797      
Per-position acceptance (%):
  Position 0:                            71.74     
  Position 1:                            50.90     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.13, Accepted throughput: 58.49 tokens/s, Drafted throughput: 103.79 tokens/s, Accepted: 585 tokens, Drafted: 1038 tokens, Per-position acceptance rate: 0.676, 0.451, Avg Draft acceptance rate: 56.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb1cc66efc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-56b51935-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:45 [loggers.py:257] Engine 000: Avg prompt throughput: 357.4 tokens/s, Avg generation throughput: 344.4 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 94.19 tokens/s, Drafted throughput: 154.48 tokens/s, Accepted: 1884 tokens, Drafted: 3090 tokens, Per-position acceptance rate: 0.717, 0.503, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:55 [loggers.py:257] Engine 000: Avg prompt throughput: 591.0 tokens/s, Avg generation throughput: 757.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:37:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 420.95 tokens/s, Drafted throughput: 672.11 tokens/s, Accepted: 4210 tokens, Drafted: 6722 tokens, Per-position acceptance rate: 0.729, 0.523, Avg Draft acceptance rate: 62.6%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:05 [loggers.py:257] Engine 000: Avg prompt throughput: 470.2 tokens/s, Avg generation throughput: 748.9 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 409.16 tokens/s, Drafted throughput: 677.93 tokens/s, Accepted: 4092 tokens, Drafted: 6780 tokens, Per-position acceptance rate: 0.706, 0.501, Avg Draft acceptance rate: 60.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:15 [loggers.py:257] Engine 000: Avg prompt throughput: 631.0 tokens/s, Avg generation throughput: 749.7 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 416.15 tokens/s, Drafted throughput: 665.32 tokens/s, Accepted: 4162 tokens, Drafted: 6654 tokens, Per-position acceptance rate: 0.730, 0.521, Avg Draft acceptance rate: 62.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  44.10     
Total input tokens:                      24361     
Total generated tokens:                  31989     
Request throughput (req/s):              3.63      
Output token throughput (tok/s):         725.42    
Peak output token throughput (tok/s):    352.00    
Peak concurrent requests:                29.00     
Total token throughput (tok/s):          1277.85   
---------------Time to First Token----------------
Mean TTFT (ms):                          99.92     
Median TTFT (ms):                        98.64     
P99 TTFT (ms):                           145.97    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.82     
Median TPOT (ms):                        20.86     
P99 TPOT (ms):                           23.03     
---------------Inter-token Latency----------------
Mean ITL (ms):                           46.44     
Median ITL (ms):                         45.58     
P99 ITL (ms):                            57.52     
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.96     
Acceptance length:                       2.24      
Drafts:                                  14269     
Draft tokens:                            28538     
Accepted tokens:                         17683     
Per-position acceptance (%):
  Position 0:                            72.18     
  Position 1:                            51.74     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:25 [loggers.py:257] Engine 000: Avg prompt throughput: 404.3 tokens/s, Avg generation throughput: 617.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 344.25 tokens/s, Drafted throughput: 547.73 tokens/s, Accepted: 3443 tokens, Drafted: 5478 tokens, Per-position acceptance rate: 0.725, 0.532, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f89ef676fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2b2ace7a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:45 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 3.90 tokens/s, Drafted throughput: 6.60 tokens/s, Accepted: 78 tokens, Drafted: 132 tokens, Per-position acceptance rate: 0.712, 0.470, Avg Draft acceptance rate: 59.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:55 [loggers.py:257] Engine 000: Avg prompt throughput: 943.8 tokens/s, Avg generation throughput: 1154.4 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:38:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 641.03 tokens/s, Drafted throughput: 1017.73 tokens/s, Accepted: 6412 tokens, Drafted: 10180 tokens, Per-position acceptance rate: 0.732, 0.528, Avg Draft acceptance rate: 63.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:05 [loggers.py:257] Engine 000: Avg prompt throughput: 1077.3 tokens/s, Avg generation throughput: 1367.2 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 753.07 tokens/s, Drafted throughput: 1225.98 tokens/s, Accepted: 7532 tokens, Drafted: 12262 tokens, Per-position acceptance rate: 0.722, 0.507, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:15 [loggers.py:257] Engine 000: Avg prompt throughput: 1169.1 tokens/s, Avg generation throughput: 1355.0 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 751.31 tokens/s, Drafted throughput: 1204.09 tokens/s, Accepted: 7515 tokens, Drafted: 12044 tokens, Per-position acceptance rate: 0.727, 0.521, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1113.6 tokens/s, Avg generation throughput: 1372.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 757.23 tokens/s, Drafted throughput: 1224.93 tokens/s, Accepted: 7574 tokens, Drafted: 12252 tokens, Per-position acceptance rate: 0.723, 0.513, Avg Draft acceptance rate: 61.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  48.54     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.59      
Output token throughput (tok/s):         1318.31   
Peak output token throughput (tok/s):    672.00    
Peak concurrent requests:                54.00     
Total token throughput (tok/s):          2312.66   
---------------Time to First Token----------------
Mean TTFT (ms):                          115.18    
Median TTFT (ms):                        108.55    
P99 TTFT (ms):                           182.88    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.80     
Median TPOT (ms):                        22.78     
P99 TPOT (ms):                           25.86     
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.73     
Median ITL (ms):                         48.58     
P99 ITL (ms):                            97.26     
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.67     
Acceptance length:                       2.23      
Drafts:                                  28614     
Draft tokens:                            57228     
Accepted tokens:                         35290     
Per-position acceptance (%):
  Position 0:                            72.01     
  Position 1:                            51.32     
==================================================
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:36 [loggers.py:257] Engine 000: Avg prompt throughput: 521.5 tokens/s, Avg generation throughput: 1154.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 628.65 tokens/s, Drafted throughput: 1054.32 tokens/s, Accepted: 6287 tokens, Drafted: 10544 tokens, Per-position acceptance rate: 0.695, 0.497, Avg Draft acceptance rate: 59.6%
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:46 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8b836eefc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-291f73bb-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:56 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:39:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 3.20 tokens/s, Drafted throughput: 5.50 tokens/s, Accepted: 64 tokens, Drafted: 110 tokens, Per-position acceptance rate: 0.709, 0.455, Avg Draft acceptance rate: 58.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1924.8 tokens/s, Avg generation throughput: 1702.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 950.98 tokens/s, Drafted throughput: 1487.90 tokens/s, Accepted: 9513 tokens, Drafted: 14884 tokens, Per-position acceptance rate: 0.741, 0.538, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1616.6 tokens/s, Avg generation throughput: 2223.5 tokens/s, Running: 58 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1219.64 tokens/s, Drafted throughput: 2003.70 tokens/s, Accepted: 12197 tokens, Drafted: 20038 tokens, Per-position acceptance rate: 0.714, 0.504, Avg Draft acceptance rate: 60.9%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1423.6 tokens/s, Avg generation throughput: 2356.3 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 61.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1298.01 tokens/s, Drafted throughput: 2113.05 tokens/s, Accepted: 12986 tokens, Drafted: 21140 tokens, Per-position acceptance rate: 0.717, 0.511, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1701.5 tokens/s, Avg generation throughput: 2282.6 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 56.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 1259.39 tokens/s, Drafted throughput: 2037.98 tokens/s, Accepted: 12599 tokens, Drafted: 20388 tokens, Per-position acceptance rate: 0.723, 0.513, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1886.0 tokens/s, Avg generation throughput: 2244.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 48.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1236.06 tokens/s, Drafted throughput: 2009.74 tokens/s, Accepted: 12361 tokens, Drafted: 20098 tokens, Per-position acceptance rate: 0.718, 0.512, Avg Draft acceptance rate: 61.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  58.23     
Total input tokens:                      94775     
Total generated tokens:                  127930    
Request throughput (req/s):              10.99     
Output token throughput (tok/s):         2196.84   
Peak output token throughput (tok/s):    1216.00   
Peak concurrent requests:                106.00    
Total token throughput (tok/s):          3824.33   
---------------Time to First Token----------------
Mean TTFT (ms):                          180.76    
Median TTFT (ms):                        158.58    
P99 TTFT (ms):                           536.91    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          27.33     
Median TPOT (ms):                        27.30     
P99 TPOT (ms):                           32.19     
---------------Inter-token Latency----------------
Mean ITL (ms):                           60.76     
Median ITL (ms):                         53.55     
P99 ITL (ms):                            136.93    
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.57     
Acceptance length:                       2.23      
Drafts:                                  57251     
Draft tokens:                            114502    
Accepted tokens:                         70497     
Per-position acceptance (%):
  Position 0:                            71.99     
  Position 1:                            51.14     
==================================================
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:56 [loggers.py:257] Engine 000: Avg prompt throughput: 922.5 tokens/s, Avg generation throughput: 1988.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:40:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1088.40 tokens/s, Drafted throughput: 1802.83 tokens/s, Accepted: 10885 tokens, Drafted: 18030 tokens, Per-position acceptance rate: 0.711, 0.497, Avg Draft acceptance rate: 60.4%
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fed7e4befc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-bbec738b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:16 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 2.55 tokens/s, Drafted throughput: 4.10 tokens/s, Accepted: 51 tokens, Drafted: 82 tokens, Per-position acceptance rate: 0.756, 0.488, Avg Draft acceptance rate: 62.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1925.4 tokens/s, Avg generation throughput: 2192.9 tokens/s, Running: 90 reqs, Waiting: 32 reqs, GPU KV cache usage: 93.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1210.84 tokens/s, Drafted throughput: 1940.71 tokens/s, Accepted: 12109 tokens, Drafted: 19408 tokens, Per-position acceptance rate: 0.726, 0.522, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1886.0 tokens/s, Avg generation throughput: 2414.0 tokens/s, Running: 93 reqs, Waiting: 23 reqs, GPU KV cache usage: 94.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1329.42 tokens/s, Drafted throughput: 2153.11 tokens/s, Accepted: 13296 tokens, Drafted: 21534 tokens, Per-position acceptance rate: 0.723, 0.512, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:46 [loggers.py:257] Engine 000: Avg prompt throughput: 2295.9 tokens/s, Avg generation throughput: 2467.8 tokens/s, Running: 106 reqs, Waiting: 0 reqs, GPU KV cache usage: 78.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1350.61 tokens/s, Drafted throughput: 2220.85 tokens/s, Accepted: 13507 tokens, Drafted: 22210 tokens, Per-position acceptance rate: 0.710, 0.506, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:56 [loggers.py:257] Engine 000: Avg prompt throughput: 2532.7 tokens/s, Avg generation throughput: 2667.5 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 80.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:41:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1469.24 tokens/s, Drafted throughput: 2381.40 tokens/s, Accepted: 14706 tokens, Drafted: 23836 tokens, Per-position acceptance rate: 0.720, 0.514, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1858.4 tokens/s, Avg generation throughput: 2929.4 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1627.44 tokens/s, Drafted throughput: 2593.39 tokens/s, Accepted: 16287 tokens, Drafted: 25954 tokens, Per-position acceptance rate: 0.728, 0.527, Avg Draft acceptance rate: 62.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1726.1 tokens/s, Avg generation throughput: 2859.8 tokens/s, Running: 122 reqs, Waiting: 5 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1573.11 tokens/s, Drafted throughput: 2562.45 tokens/s, Accepted: 15732 tokens, Drafted: 25626 tokens, Per-position acceptance rate: 0.716, 0.512, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1784.1 tokens/s, Avg generation throughput: 2763.8 tokens/s, Running: 104 reqs, Waiting: 23 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 1499.66 tokens/s, Drafted throughput: 2518.77 tokens/s, Accepted: 14998 tokens, Drafted: 25190 tokens, Per-position acceptance rate: 0.701, 0.490, Avg Draft acceptance rate: 59.5%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1769.2 tokens/s, Avg generation throughput: 2691.8 tokens/s, Running: 89 reqs, Waiting: 32 reqs, GPU KV cache usage: 93.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1471.51 tokens/s, Drafted throughput: 2428.89 tokens/s, Accepted: 14717 tokens, Drafted: 24292 tokens, Per-position acceptance rate: 0.708, 0.503, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:46 [loggers.py:257] Engine 000: Avg prompt throughput: 2149.7 tokens/s, Avg generation throughput: 2447.3 tokens/s, Running: 106 reqs, Waiting: 16 reqs, GPU KV cache usage: 95.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1333.87 tokens/s, Drafted throughput: 2211.22 tokens/s, Accepted: 13341 tokens, Drafted: 22116 tokens, Per-position acceptance rate: 0.708, 0.499, Avg Draft acceptance rate: 60.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  95.48     
Total input tokens:                      189093    
Total generated tokens:                  255968    
Request throughput (req/s):              13.41     
Output token throughput (tok/s):         2680.86   
Peak output token throughput (tok/s):    1899.00   
Peak concurrent requests:                168.00    
Total token throughput (tok/s):          4661.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          680.53    
Median TTFT (ms):                        390.08    
P99 TTFT (ms):                           3256.20   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          43.46     
Median TPOT (ms):                        43.09     
P99 TPOT (ms):                           59.99     
---------------Inter-token Latency----------------
Mean ITL (ms):                           96.17     
Median ITL (ms):                         68.01     
P99 ITL (ms):                            326.64    
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.20     
Acceptance length:                       2.22      
Drafts:                                  114822    
Draft tokens:                            229644    
Accepted tokens:                         140549    
Per-position acceptance (%):
  Position 0:                            71.50     
  Position 1:                            50.90     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:56 [loggers.py:257] Engine 000: Avg prompt throughput: 976.2 tokens/s, Avg generation throughput: 2165.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:42:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1191.27 tokens/s, Drafted throughput: 1958.14 tokens/s, Accepted: 11913 tokens, Drafted: 19582 tokens, Per-position acceptance rate: 0.711, 0.505, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f174eb92fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15006, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-19ada8a5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:16 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 3.90 tokens/s, Drafted throughput: 6.70 tokens/s, Accepted: 78 tokens, Drafted: 134 tokens, Per-position acceptance rate: 0.701, 0.463, Avg Draft acceptance rate: 58.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:26 [loggers.py:257] Engine 000: Avg prompt throughput: 2968.9 tokens/s, Avg generation throughput: 2334.1 tokens/s, Running: 98 reqs, Waiting: 157 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1293.22 tokens/s, Drafted throughput: 2043.32 tokens/s, Accepted: 12934 tokens, Drafted: 20436 tokens, Per-position acceptance rate: 0.734, 0.532, Avg Draft acceptance rate: 63.3%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1023.9 tokens/s, Avg generation throughput: 2153.3 tokens/s, Running: 112 reqs, Waiting: 141 reqs, GPU KV cache usage: 97.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1178.25 tokens/s, Drafted throughput: 1931.15 tokens/s, Accepted: 11784 tokens, Drafted: 19314 tokens, Per-position acceptance rate: 0.716, 0.505, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1871.6 tokens/s, Avg generation throughput: 2637.8 tokens/s, Running: 101 reqs, Waiting: 147 reqs, GPU KV cache usage: 92.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1446.97 tokens/s, Drafted throughput: 2365.38 tokens/s, Accepted: 14471 tokens, Drafted: 23656 tokens, Per-position acceptance rate: 0.712, 0.512, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1907.2 tokens/s, Avg generation throughput: 2570.9 tokens/s, Running: 111 reqs, Waiting: 132 reqs, GPU KV cache usage: 93.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:43:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 1399.09 tokens/s, Drafted throughput: 2322.25 tokens/s, Accepted: 13993 tokens, Drafted: 23226 tokens, Per-position acceptance rate: 0.704, 0.501, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:06 [loggers.py:257] Engine 000: Avg prompt throughput: 2086.0 tokens/s, Avg generation throughput: 2541.2 tokens/s, Running: 116 reqs, Waiting: 132 reqs, GPU KV cache usage: 94.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1392.13 tokens/s, Drafted throughput: 2276.49 tokens/s, Accepted: 13922 tokens, Drafted: 22766 tokens, Per-position acceptance rate: 0.712, 0.511, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:16 [loggers.py:257] Engine 000: Avg prompt throughput: 2019.1 tokens/s, Avg generation throughput: 2546.7 tokens/s, Running: 125 reqs, Waiting: 117 reqs, GPU KV cache usage: 88.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1393.24 tokens/s, Drafted throughput: 2284.10 tokens/s, Accepted: 13933 tokens, Drafted: 22842 tokens, Per-position acceptance rate: 0.714, 0.506, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:26 [loggers.py:257] Engine 000: Avg prompt throughput: 2217.9 tokens/s, Avg generation throughput: 2546.8 tokens/s, Running: 136 reqs, Waiting: 113 reqs, GPU KV cache usage: 95.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1394.37 tokens/s, Drafted throughput: 2285.23 tokens/s, Accepted: 13946 tokens, Drafted: 22856 tokens, Per-position acceptance rate: 0.712, 0.508, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:36 [loggers.py:257] Engine 000: Avg prompt throughput: 2217.3 tokens/s, Avg generation throughput: 2437.0 tokens/s, Running: 149 reqs, Waiting: 96 reqs, GPU KV cache usage: 91.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1337.01 tokens/s, Drafted throughput: 2183.66 tokens/s, Accepted: 13371 tokens, Drafted: 21838 tokens, Per-position acceptance rate: 0.717, 0.507, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:46 [loggers.py:257] Engine 000: Avg prompt throughput: 2082.5 tokens/s, Avg generation throughput: 2560.0 tokens/s, Running: 158 reqs, Waiting: 93 reqs, GPU KV cache usage: 97.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1402.28 tokens/s, Drafted throughput: 2295.60 tokens/s, Accepted: 14024 tokens, Drafted: 22958 tokens, Per-position acceptance rate: 0.713, 0.509, Avg Draft acceptance rate: 61.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1957.2 tokens/s, Avg generation throughput: 2687.8 tokens/s, Running: 161 reqs, Waiting: 94 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:44:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1488.62 tokens/s, Drafted throughput: 2383.83 tokens/s, Accepted: 14891 tokens, Drafted: 23846 tokens, Per-position acceptance rate: 0.725, 0.524, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1816.0 tokens/s, Avg generation throughput: 2623.5 tokens/s, Running: 151 reqs, Waiting: 105 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1437.88 tokens/s, Drafted throughput: 2351.97 tokens/s, Accepted: 14379 tokens, Drafted: 23520 tokens, Per-position acceptance rate: 0.714, 0.509, Avg Draft acceptance rate: 61.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1835.1 tokens/s, Avg generation throughput: 2664.6 tokens/s, Running: 142 reqs, Waiting: 113 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1464.33 tokens/s, Drafted throughput: 2381.92 tokens/s, Accepted: 14645 tokens, Drafted: 23822 tokens, Per-position acceptance rate: 0.718, 0.511, Avg Draft acceptance rate: 61.5%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1757.7 tokens/s, Avg generation throughput: 2690.5 tokens/s, Running: 133 reqs, Waiting: 122 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1475.80 tokens/s, Drafted throughput: 2413.00 tokens/s, Accepted: 14758 tokens, Drafted: 24130 tokens, Per-position acceptance rate: 0.717, 0.507, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1737.2 tokens/s, Avg generation throughput: 2666.7 tokens/s, Running: 126 reqs, Waiting: 130 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1456.43 tokens/s, Drafted throughput: 2402.27 tokens/s, Accepted: 14576 tokens, Drafted: 24042 tokens, Per-position acceptance rate: 0.713, 0.500, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1992.1 tokens/s, Avg generation throughput: 2644.5 tokens/s, Running: 123 reqs, Waiting: 133 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1444.47 tokens/s, Drafted throughput: 2378.78 tokens/s, Accepted: 14446 tokens, Drafted: 23790 tokens, Per-position acceptance rate: 0.711, 0.503, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1635.5 tokens/s, Avg generation throughput: 2637.1 tokens/s, Running: 118 reqs, Waiting: 138 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:45:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1445.73 tokens/s, Drafted throughput: 2364.33 tokens/s, Accepted: 14459 tokens, Drafted: 23646 tokens, Per-position acceptance rate: 0.716, 0.507, Avg Draft acceptance rate: 61.1%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1743.7 tokens/s, Avg generation throughput: 2578.4 tokens/s, Running: 112 reqs, Waiting: 143 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1418.31 tokens/s, Drafted throughput: 2303.64 tokens/s, Accepted: 14189 tokens, Drafted: 23046 tokens, Per-position acceptance rate: 0.720, 0.512, Avg Draft acceptance rate: 61.6%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1935.4 tokens/s, Avg generation throughput: 2693.1 tokens/s, Running: 112 reqs, Waiting: 143 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1470.17 tokens/s, Drafted throughput: 2428.90 tokens/s, Accepted: 14712 tokens, Drafted: 24306 tokens, Per-position acceptance rate: 0.711, 0.500, Avg Draft acceptance rate: 60.5%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1732.2 tokens/s, Avg generation throughput: 2606.9 tokens/s, Running: 108 reqs, Waiting: 103 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1421.92 tokens/s, Drafted throughput: 2351.75 tokens/s, Accepted: 14228 tokens, Drafted: 23532 tokens, Per-position acceptance rate: 0.712, 0.497, Avg Draft acceptance rate: 60.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  198.31    
Total input tokens:                      373233    
Total generated tokens:                  511919    
Request throughput (req/s):              12.91     
Output token throughput (tok/s):         2581.35   
Peak output token throughput (tok/s):    2044.00   
Peak concurrent requests:                287.00    
Total token throughput (tok/s):          4463.37   
---------------Time to First Token----------------
Mean TTFT (ms):                          8408.65   
Median TTFT (ms):                        9433.56   
P99 TTFT (ms):                           12329.31  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          54.39     
Median TPOT (ms):                        51.34     
P99 TPOT (ms):                           87.19     
---------------Inter-token Latency----------------
Mean ITL (ms):                           120.08    
Median ITL (ms):                         75.11     
P99 ITL (ms):                            425.80    
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.11     
Acceptance length:                       2.22      
Drafts:                                  229546    
Draft tokens:                            459092    
Accepted tokens:                         280559    
Per-position acceptance (%):
  Position 0:                            71.45     
  Position 1:                            50.77     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k2-t0.0-tp1...
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:36 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:37 [loggers.py:257] Engine 000: Avg prompt throughput: 694.2 tokens/s, Avg generation throughput: 2109.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=179141)[0;0m INFO 01-22 16:46:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 1153.37 tokens/s, Drafted throughput: 1924.54 tokens/s, Accepted: 12928 tokens, Drafted: 21572 tokens, Per-position acceptance rate: 0.702, 0.496, Avg Draft acceptance rate: 59.9%
