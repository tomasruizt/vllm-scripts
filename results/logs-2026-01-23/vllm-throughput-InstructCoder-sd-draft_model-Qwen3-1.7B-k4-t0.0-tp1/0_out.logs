Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k4-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k4-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 186493
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:49:59 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:49:59 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15008, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 4, 'max_model_len': 5000}}
[0;36m(APIServer pid=186493)[0;0m WARNING 01-22 16:49:59 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:50:01 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:50:01 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:50:02 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:50:02 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:50:02 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=186493)[0;0m WARNING 01-22 16:50:02 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:50:02 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f16d04befc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d26e134e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 16:50:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:12 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:50:13 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=4), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:50:14 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.49:52941 backend=nccl
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:50:14 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=186651)[0;0m WARNING 01-22 16:50:15 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:50:16 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-22 16:50:17 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:50:17 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 16:50:22 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:27 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:32 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:37 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:42 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:47 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:52 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:50:57 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:01 [default_loader.py:291] Loading weights took 42.41 seconds
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:01 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:01 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:01 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-22 16:51:02 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:04 [default_loader.py:291] Loading weights took 2.19 seconds
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:05 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 48.213674 seconds
WARNING 01-22 16:51:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:51:12 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:51:17 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:19 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:19 [backends.py:704] Dynamo bytecode transform time: 13.46 s
WARNING 01-22 16:51:22 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:51:27 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:51:32 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:36 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.821 s
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:36 [monitor.py:34] torch.compile takes 18.28 s in total
WARNING 01-22 16:51:37 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:41 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:41 [backends.py:704] Dynamo bytecode transform time: 4.84 s
WARNING 01-22 16:51:42 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:51:47 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:47 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.685 s
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:47 [monitor.py:34] torch.compile takes 24.80 s in total
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:50 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:50 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:51:50 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-22 16:51:52 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:51:57 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
WARNING 01-22 16:52:02 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:52:07 [gpu_model_runner.py:4880] Graph capturing finished in 16 secs, took -0.05 GiB
WARNING 01-22 16:52:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
[0;36m(EngineCore_DP0 pid=186651)[0;0m INFO 01-22 16:52:07 [core.py:272] init engine (profile, create kv cache, warmup model) took 62.04 seconds
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:09 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=186493)[0;0m WARNING 01-22 16:52:09 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:09 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:09 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:09 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [serving.py:221] Chat template warmup completed in 1697.3ms
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15008
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:11 [launcher.py:46] Route: /pooling, Methods: POST
WARNING 01-22 16:52:12 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15008)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15008 ssl:default [Connect call failed (\'127.0.0.1\', 15008)]\n''
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:22 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 11.41 tokens/s, Drafted throughput: 24.51 tokens/s, Accepted: 149 tokens, Drafted: 320 tokens, Per-position acceptance rate: 0.725, 0.500, 0.388, 0.250, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:32 [loggers.py:257] Engine 000: Avg prompt throughput: 32.8 tokens/s, Avg generation throughput: 52.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 31.70 tokens/s, Drafted throughput: 82.40 tokens/s, Accepted: 317 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.617, 0.403, 0.291, 0.228, Avg Draft acceptance rate: 38.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:42 [loggers.py:257] Engine 000: Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 37.70 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 377 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.765, 0.480, 0.348, 0.255, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:52 [loggers.py:257] Engine 000: Avg prompt throughput: 36.2 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:52:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 40.59 tokens/s, Drafted throughput: 81.99 tokens/s, Accepted: 406 tokens, Drafted: 820 tokens, Per-position acceptance rate: 0.776, 0.537, 0.405, 0.263, Avg Draft acceptance rate: 49.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:02 [loggers.py:257] Engine 000: Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 38.40 tokens/s, Drafted throughput: 81.99 tokens/s, Accepted: 384 tokens, Drafted: 820 tokens, Per-position acceptance rate: 0.776, 0.483, 0.376, 0.239, Avg Draft acceptance rate: 46.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:12 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 39.00 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 390 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.681, 0.520, 0.397, 0.314, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:22 [loggers.py:257] Engine 000: Avg prompt throughput: 48.3 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 39.40 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 394 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.750, 0.529, 0.373, 0.279, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:32 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 40.69 tokens/s, Drafted throughput: 81.99 tokens/s, Accepted: 407 tokens, Drafted: 820 tokens, Per-position acceptance rate: 0.732, 0.537, 0.415, 0.302, Avg Draft acceptance rate: 49.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:42 [loggers.py:257] Engine 000: Avg prompt throughput: 42.8 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 36.40 tokens/s, Drafted throughput: 81.60 tokens/s, Accepted: 364 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.681, 0.446, 0.358, 0.299, Avg Draft acceptance rate: 44.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:52 [loggers.py:257] Engine 000: Avg prompt throughput: 41.2 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:53:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 37.10 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 371 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.735, 0.485, 0.373, 0.225, Avg Draft acceptance rate: 45.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:02 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 41.10 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 411 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.755, 0.529, 0.412, 0.319, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:12 [loggers.py:257] Engine 000: Avg prompt throughput: 44.1 tokens/s, Avg generation throughput: 57.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 36.80 tokens/s, Drafted throughput: 81.60 tokens/s, Accepted: 368 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.711, 0.471, 0.358, 0.265, Avg Draft acceptance rate: 45.1%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:22 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.69, Accepted throughput: 34.40 tokens/s, Drafted throughput: 81.60 tokens/s, Accepted: 344 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.691, 0.456, 0.309, 0.230, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:32 [loggers.py:257] Engine 000: Avg prompt throughput: 30.8 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 35.80 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 358 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.675, 0.471, 0.335, 0.257, Avg Draft acceptance rate: 43.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:42 [loggers.py:257] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 41.50 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 415 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.755, 0.529, 0.426, 0.324, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:52 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:54:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.63, Accepted throughput: 33.29 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 333 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.657, 0.451, 0.328, 0.196, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:02 [loggers.py:257] Engine 000: Avg prompt throughput: 48.4 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 40.00 tokens/s, Drafted throughput: 81.60 tokens/s, Accepted: 400 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.730, 0.554, 0.377, 0.299, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:12 [loggers.py:257] Engine 000: Avg prompt throughput: 33.8 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 38.40 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 384 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.716, 0.520, 0.373, 0.275, Avg Draft acceptance rate: 47.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  172.92    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.29      
Output token throughput (tok/s):         57.83     
Peak output token throughput (tok/s):    21.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          99.62     
---------------Time to First Token----------------
Mean TTFT (ms):                          63.92     
Median TTFT (ms):                        61.66     
P99 TTFT (ms):                           94.46     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.98     
Median TPOT (ms):                        17.11     
P99 TPOT (ms):                           19.33     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.02     
Median ITL (ms):                         48.02     
P99 ITL (ms):                            48.49     
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.14     
Acceptance length:                       2.85      
Drafts:                                  3518      
Draft tokens:                            14072     
Accepted tokens:                         6493      
Per-position acceptance (%):
  Position 0:                            71.75     
  Position 1:                            49.32     
  Position 2:                            36.73     
  Position 3:                            26.78     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 4.80 tokens/s, Drafted throughput: 14.40 tokens/s, Accepted: 48 tokens, Drafted: 144 tokens, Per-position acceptance rate: 0.639, 0.333, 0.222, 0.139, Avg Draft acceptance rate: 33.3%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fde8f5aafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ddd880e6-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:32 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 3.70 tokens/s, Drafted throughput: 6.40 tokens/s, Accepted: 37 tokens, Drafted: 64 tokens, Per-position acceptance rate: 0.875, 0.688, 0.500, 0.250, Avg Draft acceptance rate: 57.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:42 [loggers.py:257] Engine 000: Avg prompt throughput: 59.6 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.69, Accepted throughput: 54.09 tokens/s, Drafted throughput: 127.98 tokens/s, Accepted: 541 tokens, Drafted: 1280 tokens, Per-position acceptance rate: 0.684, 0.453, 0.316, 0.237, Avg Draft acceptance rate: 42.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:52 [loggers.py:257] Engine 000: Avg prompt throughput: 77.6 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:55:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.90, Accepted throughput: 76.30 tokens/s, Drafted throughput: 160.79 tokens/s, Accepted: 763 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.754, 0.505, 0.378, 0.261, Avg Draft acceptance rate: 47.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:02 [loggers.py:257] Engine 000: Avg prompt throughput: 78.1 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 75.00 tokens/s, Drafted throughput: 159.19 tokens/s, Accepted: 750 tokens, Drafted: 1592 tokens, Per-position acceptance rate: 0.729, 0.505, 0.369, 0.281, Avg Draft acceptance rate: 47.1%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:12 [loggers.py:257] Engine 000: Avg prompt throughput: 102.5 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 79.39 tokens/s, Drafted throughput: 159.99 tokens/s, Accepted: 794 tokens, Drafted: 1600 tokens, Per-position acceptance rate: 0.735, 0.540, 0.407, 0.302, Avg Draft acceptance rate: 49.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:22 [loggers.py:257] Engine 000: Avg prompt throughput: 86.4 tokens/s, Avg generation throughput: 111.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 71.10 tokens/s, Drafted throughput: 160.80 tokens/s, Accepted: 711 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.719, 0.460, 0.346, 0.244, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:32 [loggers.py:257] Engine 000: Avg prompt throughput: 73.9 tokens/s, Avg generation throughput: 115.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 74.99 tokens/s, Drafted throughput: 159.98 tokens/s, Accepted: 750 tokens, Drafted: 1600 tokens, Per-position acceptance rate: 0.723, 0.500, 0.385, 0.268, Avg Draft acceptance rate: 46.9%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:42 [loggers.py:257] Engine 000: Avg prompt throughput: 90.1 tokens/s, Avg generation throughput: 111.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 71.70 tokens/s, Drafted throughput: 160.80 tokens/s, Accepted: 717 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.687, 0.495, 0.338, 0.264, Avg Draft acceptance rate: 44.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:52 [loggers.py:257] Engine 000: Avg prompt throughput: 91.1 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:56:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 80.19 tokens/s, Drafted throughput: 159.17 tokens/s, Accepted: 802 tokens, Drafted: 1592 tokens, Per-position acceptance rate: 0.734, 0.550, 0.427, 0.304, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:02 [loggers.py:257] Engine 000: Avg prompt throughput: 63.4 tokens/s, Avg generation throughput: 111.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 72.79 tokens/s, Drafted throughput: 154.38 tokens/s, Accepted: 728 tokens, Drafted: 1544 tokens, Per-position acceptance rate: 0.715, 0.536, 0.363, 0.272, Avg Draft acceptance rate: 47.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  89.41     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.56      
Output token throughput (tok/s):         111.84    
Peak output token throughput (tok/s):    42.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          192.67    
---------------Time to First Token----------------
Mean TTFT (ms):                          102.57    
Median TTFT (ms):                        100.77    
P99 TTFT (ms):                           132.39    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.02     
Median TPOT (ms):                        17.26     
P99 TPOT (ms):                           19.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.45     
Median ITL (ms):                         48.34     
P99 ITL (ms):                            52.90     
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.65     
Acceptance length:                       2.87      
Drafts:                                  3495      
Draft tokens:                            13980     
Accepted tokens:                         6521      
Per-position acceptance (%):
  Position 0:                            72.07     
  Position 1:                            50.50     
  Position 2:                            37.02     
  Position 3:                            26.98     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:12 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 5.50 tokens/s, Drafted throughput: 17.60 tokens/s, Accepted: 55 tokens, Drafted: 176 tokens, Per-position acceptance rate: 0.636, 0.295, 0.205, 0.114, Avg Draft acceptance rate: 31.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f43dffd2fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d6b8b715-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:22 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.14, Accepted throughput: 2.20 tokens/s, Drafted throughput: 2.80 tokens/s, Accepted: 22 tokens, Drafted: 28 tokens, Per-position acceptance rate: 1.000, 0.857, 0.857, 0.429, Avg Draft acceptance rate: 78.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:32 [loggers.py:257] Engine 000: Avg prompt throughput: 118.5 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 101.68 tokens/s, Drafted throughput: 221.16 tokens/s, Accepted: 1017 tokens, Drafted: 2212 tokens, Per-position acceptance rate: 0.727, 0.488, 0.354, 0.269, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:42 [loggers.py:257] Engine 000: Avg prompt throughput: 165.3 tokens/s, Avg generation throughput: 227.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 148.28 tokens/s, Drafted throughput: 315.16 tokens/s, Accepted: 1483 tokens, Drafted: 3152 tokens, Per-position acceptance rate: 0.732, 0.506, 0.373, 0.270, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:52 [loggers.py:257] Engine 000: Avg prompt throughput: 185.1 tokens/s, Avg generation throughput: 224.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:57:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 145.38 tokens/s, Drafted throughput: 315.16 tokens/s, Accepted: 1454 tokens, Drafted: 3152 tokens, Per-position acceptance rate: 0.711, 0.490, 0.369, 0.275, Avg Draft acceptance rate: 46.1%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:02 [loggers.py:257] Engine 000: Avg prompt throughput: 159.0 tokens/s, Avg generation throughput: 224.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 146.29 tokens/s, Drafted throughput: 314.37 tokens/s, Accepted: 1463 tokens, Drafted: 3144 tokens, Per-position acceptance rate: 0.728, 0.491, 0.369, 0.274, Avg Draft acceptance rate: 46.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:12 [loggers.py:257] Engine 000: Avg prompt throughput: 94.8 tokens/s, Avg generation throughput: 184.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 118.69 tokens/s, Drafted throughput: 263.99 tokens/s, Accepted: 1187 tokens, Drafted: 2640 tokens, Per-position acceptance rate: 0.717, 0.488, 0.345, 0.248, Avg Draft acceptance rate: 45.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  46.87     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.07      
Output token throughput (tok/s):         213.35    
Peak output token throughput (tok/s):    84.00     
Peak concurrent requests:                8.00      
Total token throughput (tok/s):          367.54    
---------------Time to First Token----------------
Mean TTFT (ms):                          102.94    
Median TTFT (ms):                        102.45    
P99 TTFT (ms):                           133.13    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.37     
Median TPOT (ms):                        17.32     
P99 TPOT (ms):                           20.54     
---------------Inter-token Latency----------------
Mean ITL (ms):                           49.24     
Median ITL (ms):                         48.98     
P99 ITL (ms):                            55.34     
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.32     
Acceptance length:                       2.85      
Drafts:                                  3510      
Draft tokens:                            14040     
Accepted tokens:                         6503      
Per-position acceptance (%):
  Position 0:                            72.39     
  Position 1:                            49.46     
  Position 2:                            36.50     
  Position 3:                            26.92     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.00, Accepted throughput: 0.40 tokens/s, Drafted throughput: 0.40 tokens/s, Accepted: 4 tokens, Drafted: 4 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f04ca272fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c78ff7e9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:32 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 5.20 tokens/s, Drafted throughput: 9.20 tokens/s, Accepted: 52 tokens, Drafted: 92 tokens, Per-position acceptance rate: 0.826, 0.652, 0.478, 0.304, Avg Draft acceptance rate: 56.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:42 [loggers.py:257] Engine 000: Avg prompt throughput: 215.3 tokens/s, Avg generation throughput: 311.8 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 201.38 tokens/s, Drafted throughput: 443.15 tokens/s, Accepted: 2014 tokens, Drafted: 4432 tokens, Per-position acceptance rate: 0.714, 0.486, 0.358, 0.260, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:52 [loggers.py:257] Engine 000: Avg prompt throughput: 380.8 tokens/s, Avg generation throughput: 430.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:58:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 281.45 tokens/s, Drafted throughput: 595.10 tokens/s, Accepted: 2815 tokens, Drafted: 5952 tokens, Per-position acceptance rate: 0.708, 0.503, 0.390, 0.290, Avg Draft acceptance rate: 47.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:02 [loggers.py:257] Engine 000: Avg prompt throughput: 325.8 tokens/s, Avg generation throughput: 429.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 279.78 tokens/s, Drafted throughput: 599.15 tokens/s, Accepted: 2798 tokens, Drafted: 5992 tokens, Per-position acceptance rate: 0.720, 0.505, 0.374, 0.270, Avg Draft acceptance rate: 46.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:12 [loggers.py:257] Engine 000: Avg prompt throughput: 247.0 tokens/s, Avg generation throughput: 420.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 271.77 tokens/s, Drafted throughput: 593.53 tokens/s, Accepted: 2718 tokens, Drafted: 5936 tokens, Per-position acceptance rate: 0.708, 0.489, 0.365, 0.270, Avg Draft acceptance rate: 45.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  38.80     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              2.06      
Output token throughput (tok/s):         412.41    
Peak output token throughput (tok/s):    160.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          713.73    
---------------Time to First Token----------------
Mean TTFT (ms):                          106.58    
Median TTFT (ms):                        106.71    
P99 TTFT (ms):                           136.11    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.23     
Median TPOT (ms):                        18.19     
P99 TPOT (ms):                           21.68     
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.78     
Median ITL (ms):                         51.33     
P99 ITL (ms):                            62.68     
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.41     
Acceptance length:                       2.86      
Drafts:                                  5605      
Draft tokens:                            22420     
Accepted tokens:                         10405     
Per-position acceptance (%):
  Position 0:                            71.29     
  Position 1:                            49.72     
  Position 2:                            37.31     
  Position 3:                            27.31     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 13.50 tokens/s, Drafted throughput: 30.80 tokens/s, Accepted: 135 tokens, Drafted: 308 tokens, Per-position acceptance rate: 0.727, 0.494, 0.325, 0.208, Avg Draft acceptance rate: 43.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe8fc0befc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-18aaca0e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:32 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 5.30 tokens/s, Drafted throughput: 10.40 tokens/s, Accepted: 53 tokens, Drafted: 104 tokens, Per-position acceptance rate: 0.769, 0.577, 0.423, 0.269, Avg Draft acceptance rate: 51.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:42 [loggers.py:257] Engine 000: Avg prompt throughput: 468.8 tokens/s, Avg generation throughput: 581.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 377.03 tokens/s, Drafted throughput: 814.24 tokens/s, Accepted: 3771 tokens, Drafted: 8144 tokens, Per-position acceptance rate: 0.720, 0.493, 0.366, 0.274, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:52 [loggers.py:257] Engine 000: Avg prompt throughput: 680.0 tokens/s, Avg generation throughput: 780.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 16:59:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 506.30 tokens/s, Drafted throughput: 1101.38 tokens/s, Accepted: 5064 tokens, Drafted: 11016 tokens, Per-position acceptance rate: 0.711, 0.499, 0.361, 0.268, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:02 [loggers.py:257] Engine 000: Avg prompt throughput: 517.0 tokens/s, Avg generation throughput: 784.1 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 505.96 tokens/s, Drafted throughput: 1119.11 tokens/s, Accepted: 5060 tokens, Drafted: 11192 tokens, Per-position acceptance rate: 0.710, 0.480, 0.353, 0.266, Avg Draft acceptance rate: 45.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:12 [loggers.py:257] Engine 000: Avg prompt throughput: 707.5 tokens/s, Avg generation throughput: 807.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 531.75 tokens/s, Drafted throughput: 1103.89 tokens/s, Accepted: 5318 tokens, Drafted: 11040 tokens, Per-position acceptance rate: 0.723, 0.513, 0.392, 0.299, Avg Draft acceptance rate: 48.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  42.76     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.74      
Output token throughput (tok/s):         748.10    
Peak output token throughput (tok/s):    304.00    
Peak concurrent requests:                30.00     
Total token throughput (tok/s):          1317.77   
---------------Time to First Token----------------
Mean TTFT (ms):                          117.78    
Median TTFT (ms):                        114.86    
P99 TTFT (ms):                           177.02    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.69     
Median TPOT (ms):                        19.74     
P99 TPOT (ms):                           23.93     
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.80     
Median ITL (ms):                         54.82     
P99 ITL (ms):                            86.47     
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.32     
Acceptance length:                       2.85      
Drafts:                                  11232     
Draft tokens:                            44928     
Accepted tokens:                         20809     
Per-position acceptance (%):
  Position 0:                            71.39     
  Position 1:                            49.54     
  Position 2:                            36.71     
  Position 3:                            27.63     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:22 [loggers.py:257] Engine 000: Avg prompt throughput: 62.3 tokens/s, Avg generation throughput: 257.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 166.99 tokens/s, Drafted throughput: 372.37 tokens/s, Accepted: 1670 tokens, Drafted: 3724 tokens, Per-position acceptance rate: 0.692, 0.480, 0.354, 0.267, Avg Draft acceptance rate: 44.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:32 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff109262fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6ed64a65-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:42 [loggers.py:257] Engine 000: Avg prompt throughput: 487.2 tokens/s, Avg generation throughput: 420.2 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 137.89 tokens/s, Drafted throughput: 282.39 tokens/s, Accepted: 2758 tokens, Drafted: 5648 tokens, Per-position acceptance rate: 0.727, 0.527, 0.401, 0.298, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:52 [loggers.py:257] Engine 000: Avg prompt throughput: 977.6 tokens/s, Avg generation throughput: 1422.9 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:00:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 923.53 tokens/s, Drafted throughput: 2007.84 tokens/s, Accepted: 9236 tokens, Drafted: 20080 tokens, Per-position acceptance rate: 0.709, 0.498, 0.366, 0.267, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1297.3 tokens/s, Avg generation throughput: 1424.6 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.90, Accepted throughput: 934.41 tokens/s, Drafted throughput: 1968.00 tokens/s, Accepted: 9346 tokens, Drafted: 19684 tokens, Per-position acceptance rate: 0.718, 0.512, 0.383, 0.286, Avg Draft acceptance rate: 47.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1076.0 tokens/s, Avg generation throughput: 1431.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 933.05 tokens/s, Drafted throughput: 2002.47 tokens/s, Accepted: 9332 tokens, Drafted: 20028 tokens, Per-position acceptance rate: 0.725, 0.502, 0.363, 0.273, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1005.9 tokens/s, Avg generation throughput: 1432.7 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 928.20 tokens/s, Drafted throughput: 2029.96 tokens/s, Accepted: 9284 tokens, Drafted: 20304 tokens, Per-position acceptance rate: 0.701, 0.497, 0.365, 0.267, Avg Draft acceptance rate: 45.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  47.20     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.78      
Output token throughput (tok/s):         1355.64   
Peak output token throughput (tok/s):    544.00    
Peak concurrent requests:                49.00     
Total token throughput (tok/s):          2378.16   
---------------Time to First Token----------------
Mean TTFT (ms):                          138.93    
Median TTFT (ms):                        129.47    
P99 TTFT (ms):                           218.38    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.71     
Median TPOT (ms):                        21.71     
P99 TPOT (ms):                           26.98     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.64     
Median ITL (ms):                         58.47     
P99 ITL (ms):                            108.51    
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.43     
Acceptance length:                       2.86      
Drafts:                                  22430     
Draft tokens:                            89720     
Accepted tokens:                         41661     
Per-position acceptance (%):
  Position 0:                            71.25     
  Position 1:                            50.25     
  Position 2:                            36.92     
  Position 3:                            27.33     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:32 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 285.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 183.19 tokens/s, Drafted throughput: 426.78 tokens/s, Accepted: 1832 tokens, Drafted: 4268 tokens, Per-position acceptance rate: 0.681, 0.472, 0.326, 0.237, Avg Draft acceptance rate: 42.9%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:42 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9831db6fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c90e83fb-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:52 [loggers.py:257] Engine 000: Avg prompt throughput: 948.5 tokens/s, Avg generation throughput: 912.3 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 58.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:01:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 299.22 tokens/s, Drafted throughput: 614.93 tokens/s, Accepted: 5985 tokens, Drafted: 12300 tokens, Per-position acceptance rate: 0.736, 0.526, 0.394, 0.290, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1938.6 tokens/s, Avg generation throughput: 2028.3 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 42.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 1331.52 tokens/s, Drafted throughput: 2816.63 tokens/s, Accepted: 13316 tokens, Drafted: 28168 tokens, Per-position acceptance rate: 0.722, 0.506, 0.380, 0.283, Avg Draft acceptance rate: 47.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1350.4 tokens/s, Avg generation throughput: 2178.6 tokens/s, Running: 56 reqs, Waiting: 0 reqs, GPU KV cache usage: 48.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 1406.89 tokens/s, Drafted throughput: 3100.97 tokens/s, Accepted: 14070 tokens, Drafted: 31012 tokens, Per-position acceptance rate: 0.703, 0.491, 0.356, 0.265, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1548.0 tokens/s, Avg generation throughput: 2164.4 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1401.17 tokens/s, Drafted throughput: 3065.71 tokens/s, Accepted: 14013 tokens, Drafted: 30660 tokens, Per-position acceptance rate: 0.707, 0.494, 0.361, 0.267, Avg Draft acceptance rate: 45.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1705.3 tokens/s, Avg generation throughput: 2071.4 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 46.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 1330.31 tokens/s, Drafted throughput: 2976.95 tokens/s, Accepted: 13306 tokens, Drafted: 29776 tokens, Per-position acceptance rate: 0.699, 0.482, 0.349, 0.258, Avg Draft acceptance rate: 44.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1481.2 tokens/s, Avg generation throughput: 2149.7 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1389.72 tokens/s, Drafted throughput: 3044.02 tokens/s, Accepted: 13899 tokens, Drafted: 30444 tokens, Per-position acceptance rate: 0.702, 0.494, 0.360, 0.270, Avg Draft acceptance rate: 45.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  61.98     
Total input tokens:                      94775     
Total generated tokens:                  127930    
Request throughput (req/s):              10.33     
Output token throughput (tok/s):         2063.91   
Peak output token throughput (tok/s):    943.00    
Peak concurrent requests:                94.00     
Total token throughput (tok/s):          3592.92   
---------------Time to First Token----------------
Mean TTFT (ms):                          205.94    
Median TTFT (ms):                        189.88    
P99 TTFT (ms):                           502.79    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          28.91     
Median TPOT (ms):                        28.94     
P99 TPOT (ms):                           36.08     
---------------Inter-token Latency----------------
Mean ITL (ms):                           81.25     
Median ITL (ms):                         72.07     
P99 ITL (ms):                            166.59    
---------------Speculative Decoding---------------
Acceptance rate (%):                     45.71     
Acceptance length:                       2.83      
Drafts:                                  45297     
Draft tokens:                            181188    
Accepted tokens:                         82815     
Per-position acceptance (%):
  Position 0:                            70.66     
  Position 1:                            49.33     
  Position 2:                            36.05     
  Position 3:                            26.79     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:52 [loggers.py:257] Engine 000: Avg prompt throughput: 522.4 tokens/s, Avg generation throughput: 1306.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:02:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 835.22 tokens/s, Drafted throughput: 1911.82 tokens/s, Accepted: 8353 tokens, Drafted: 19120 tokens, Per-position acceptance rate: 0.691, 0.473, 0.334, 0.250, Avg Draft acceptance rate: 43.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:02 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc871c06fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7e728d20-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:12 [loggers.py:257] Engine 000: Avg prompt throughput: 302.1 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 20 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.70, Accepted throughput: 6.90 tokens/s, Drafted throughput: 16.20 tokens/s, Accepted: 138 tokens, Drafted: 324 tokens, Per-position acceptance rate: 0.728, 0.432, 0.321, 0.222, Avg Draft acceptance rate: 42.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1940.4 tokens/s, Avg generation throughput: 2301.7 tokens/s, Running: 102 reqs, Waiting: 14 reqs, GPU KV cache usage: 93.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 1492.49 tokens/s, Drafted throughput: 3197.98 tokens/s, Accepted: 14925 tokens, Drafted: 31980 tokens, Per-position acceptance rate: 0.718, 0.500, 0.373, 0.276, Avg Draft acceptance rate: 46.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1909.2 tokens/s, Avg generation throughput: 2363.3 tokens/s, Running: 110 reqs, Waiting: 14 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 1541.27 tokens/s, Drafted throughput: 3293.33 tokens/s, Accepted: 15414 tokens, Drafted: 32936 tokens, Per-position acceptance rate: 0.715, 0.508, 0.370, 0.280, Avg Draft acceptance rate: 46.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1821.2 tokens/s, Avg generation throughput: 2354.3 tokens/s, Running: 113 reqs, Waiting: 4 reqs, GPU KV cache usage: 94.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1520.14 tokens/s, Drafted throughput: 3332.82 tokens/s, Accepted: 15205 tokens, Drafted: 33336 tokens, Per-position acceptance rate: 0.704, 0.497, 0.360, 0.263, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1760.1 tokens/s, Avg generation throughput: 2322.9 tokens/s, Running: 119 reqs, Waiting: 0 reqs, GPU KV cache usage: 95.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:03:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 1488.00 tokens/s, Drafted throughput: 3342.17 tokens/s, Accepted: 14881 tokens, Drafted: 33424 tokens, Per-position acceptance rate: 0.697, 0.482, 0.347, 0.256, Avg Draft acceptance rate: 44.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1936.9 tokens/s, Avg generation throughput: 2384.3 tokens/s, Running: 120 reqs, Waiting: 0 reqs, GPU KV cache usage: 92.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1542.69 tokens/s, Drafted throughput: 3370.31 tokens/s, Accepted: 15442 tokens, Drafted: 33736 tokens, Per-position acceptance rate: 0.706, 0.494, 0.361, 0.271, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1767.8 tokens/s, Avg generation throughput: 2421.0 tokens/s, Running: 121 reqs, Waiting: 0 reqs, GPU KV cache usage: 87.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1566.25 tokens/s, Drafted throughput: 3424.88 tokens/s, Accepted: 15664 tokens, Drafted: 34252 tokens, Per-position acceptance rate: 0.708, 0.491, 0.359, 0.271, Avg Draft acceptance rate: 45.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1792.0 tokens/s, Avg generation throughput: 2458.3 tokens/s, Running: 118 reqs, Waiting: 0 reqs, GPU KV cache usage: 91.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.90, Accepted throughput: 1612.34 tokens/s, Drafted throughput: 3395.07 tokens/s, Accepted: 16124 tokens, Drafted: 33952 tokens, Per-position acceptance rate: 0.722, 0.515, 0.378, 0.285, Avg Draft acceptance rate: 47.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1985.2 tokens/s, Avg generation throughput: 2317.6 tokens/s, Running: 122 reqs, Waiting: 0 reqs, GPU KV cache usage: 89.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 1487.24 tokens/s, Drafted throughput: 3326.05 tokens/s, Accepted: 14874 tokens, Drafted: 33264 tokens, Per-position acceptance rate: 0.704, 0.482, 0.346, 0.256, Avg Draft acceptance rate: 44.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1768.9 tokens/s, Avg generation throughput: 2388.3 tokens/s, Running: 123 reqs, Waiting: 0 reqs, GPU KV cache usage: 82.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 1550.66 tokens/s, Drafted throughput: 3357.12 tokens/s, Accepted: 15507 tokens, Drafted: 33572 tokens, Per-position acceptance rate: 0.708, 0.501, 0.363, 0.275, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1838.2 tokens/s, Avg generation throughput: 2419.3 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 85.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:04:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 1568.96 tokens/s, Drafted throughput: 3409.29 tokens/s, Accepted: 15691 tokens, Drafted: 34096 tokens, Per-position acceptance rate: 0.715, 0.498, 0.360, 0.267, Avg Draft acceptance rate: 46.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  108.08    
Total input tokens:                      189093    
Total generated tokens:                  255921    
Request throughput (req/s):              11.84     
Output token throughput (tok/s):         2367.79   
Peak output token throughput (tok/s):    1280.00   
Peak concurrent requests:                163.00    
Total token throughput (tok/s):          4117.29   
---------------Time to First Token----------------
Mean TTFT (ms):                          781.85    
Median TTFT (ms):                        441.74    
P99 TTFT (ms):                           3564.16   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          48.97     
Median TPOT (ms):                        48.91     
P99 TPOT (ms):                           69.81     
---------------Inter-token Latency----------------
Mean ITL (ms):                           137.49    
Median ITL (ms):                         109.00    
P99 ITL (ms):                            334.32    
---------------Speculative Decoding---------------
Acceptance rate (%):                     45.79     
Acceptance length:                       2.83      
Drafts:                                  90438     
Draft tokens:                            361752    
Accepted tokens:                         165633    
Per-position acceptance (%):
  Position 0:                            70.85     
  Position 1:                            49.56     
  Position 2:                            35.95     
  Position 3:                            26.79     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:02 [loggers.py:257] Engine 000: Avg prompt throughput: 101.9 tokens/s, Avg generation throughput: 1852.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 1189.36 tokens/s, Drafted throughput: 2716.87 tokens/s, Accepted: 11895 tokens, Drafted: 27172 tokens, Per-position acceptance rate: 0.693, 0.482, 0.334, 0.242, Avg Draft acceptance rate: 43.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:12 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f46021a6fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15008, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-74ebea8f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:22 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 5.00 tokens/s, Drafted throughput: 11.00 tokens/s, Accepted: 100 tokens, Drafted: 220 tokens, Per-position acceptance rate: 0.691, 0.509, 0.382, 0.236, Avg Draft acceptance rate: 45.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:32 [loggers.py:257] Engine 000: Avg prompt throughput: 2898.9 tokens/s, Avg generation throughput: 2060.6 tokens/s, Running: 104 reqs, Waiting: 152 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 1344.81 tokens/s, Drafted throughput: 2787.36 tokens/s, Accepted: 13457 tokens, Drafted: 27892 tokens, Per-position acceptance rate: 0.733, 0.518, 0.388, 0.291, Avg Draft acceptance rate: 48.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:42 [loggers.py:257] Engine 000: Avg prompt throughput: 802.8 tokens/s, Avg generation throughput: 1989.8 tokens/s, Running: 108 reqs, Waiting: 142 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 1282.08 tokens/s, Drafted throughput: 2826.94 tokens/s, Accepted: 12822 tokens, Drafted: 28272 tokens, Per-position acceptance rate: 0.696, 0.489, 0.358, 0.270, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1881.0 tokens/s, Avg generation throughput: 2420.8 tokens/s, Running: 109 reqs, Waiting: 147 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:05:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1564.97 tokens/s, Drafted throughput: 3418.15 tokens/s, Accepted: 15660 tokens, Drafted: 34204 tokens, Per-position acceptance rate: 0.710, 0.502, 0.357, 0.263, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1887.3 tokens/s, Avg generation throughput: 2302.8 tokens/s, Running: 116 reqs, Waiting: 140 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1486.12 tokens/s, Drafted throughput: 3264.23 tokens/s, Accepted: 14862 tokens, Drafted: 32644 tokens, Per-position acceptance rate: 0.706, 0.491, 0.358, 0.266, Avg Draft acceptance rate: 45.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1528.5 tokens/s, Avg generation throughput: 2281.0 tokens/s, Running: 112 reqs, Waiting: 142 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1459.38 tokens/s, Drafted throughput: 3299.72 tokens/s, Accepted: 14595 tokens, Drafted: 33000 tokens, Per-position acceptance rate: 0.692, 0.477, 0.345, 0.256, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1992.0 tokens/s, Avg generation throughput: 2363.0 tokens/s, Running: 122 reqs, Waiting: 134 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 1531.06 tokens/s, Drafted throughput: 3314.72 tokens/s, Accepted: 15311 tokens, Drafted: 33148 tokens, Per-position acceptance rate: 0.711, 0.498, 0.361, 0.277, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1511.8 tokens/s, Avg generation throughput: 2312.1 tokens/s, Running: 123 reqs, Waiting: 132 reqs, GPU KV cache usage: 98.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 1495.97 tokens/s, Drafted throughput: 3259.31 tokens/s, Accepted: 14961 tokens, Drafted: 32596 tokens, Per-position acceptance rate: 0.709, 0.496, 0.361, 0.271, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1982.2 tokens/s, Avg generation throughput: 2299.5 tokens/s, Running: 123 reqs, Waiting: 133 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 1480.87 tokens/s, Drafted throughput: 3264.88 tokens/s, Accepted: 14821 tokens, Drafted: 32676 tokens, Per-position acceptance rate: 0.710, 0.488, 0.353, 0.263, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1662.5 tokens/s, Avg generation throughput: 2306.0 tokens/s, Running: 125 reqs, Waiting: 131 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:06:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1488.31 tokens/s, Drafted throughput: 3263.23 tokens/s, Accepted: 14883 tokens, Drafted: 32632 tokens, Per-position acceptance rate: 0.701, 0.494, 0.357, 0.272, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1868.5 tokens/s, Avg generation throughput: 2334.3 tokens/s, Running: 128 reqs, Waiting: 128 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1506.10 tokens/s, Drafted throughput: 3300.37 tokens/s, Accepted: 15063 tokens, Drafted: 33008 tokens, Per-position acceptance rate: 0.709, 0.496, 0.357, 0.264, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1632.0 tokens/s, Avg generation throughput: 2284.0 tokens/s, Running: 129 reqs, Waiting: 125 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 1470.65 tokens/s, Drafted throughput: 3261.89 tokens/s, Accepted: 14707 tokens, Drafted: 32620 tokens, Per-position acceptance rate: 0.700, 0.488, 0.352, 0.263, Avg Draft acceptance rate: 45.1%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1938.8 tokens/s, Avg generation throughput: 2284.5 tokens/s, Running: 136 reqs, Waiting: 120 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1475.34 tokens/s, Drafted throughput: 3243.82 tokens/s, Accepted: 14756 tokens, Drafted: 32444 tokens, Per-position acceptance rate: 0.706, 0.495, 0.354, 0.265, Avg Draft acceptance rate: 45.5%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1699.5 tokens/s, Avg generation throughput: 2308.9 tokens/s, Running: 134 reqs, Waiting: 121 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 1486.89 tokens/s, Drafted throughput: 3277.55 tokens/s, Accepted: 14880 tokens, Drafted: 32800 tokens, Per-position acceptance rate: 0.705, 0.489, 0.357, 0.263, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1731.2 tokens/s, Avg generation throughput: 2220.1 tokens/s, Running: 144 reqs, Waiting: 111 reqs, GPU KV cache usage: 98.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 1426.07 tokens/s, Drafted throughput: 3174.68 tokens/s, Accepted: 14263 tokens, Drafted: 31752 tokens, Per-position acceptance rate: 0.699, 0.488, 0.349, 0.261, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1715.2 tokens/s, Avg generation throughput: 2372.5 tokens/s, Running: 146 reqs, Waiting: 107 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:07:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1530.11 tokens/s, Drafted throughput: 3355.77 tokens/s, Accepted: 15304 tokens, Drafted: 33564 tokens, Per-position acceptance rate: 0.708, 0.489, 0.358, 0.268, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:02 [loggers.py:257] Engine 000: Avg prompt throughput: 1759.6 tokens/s, Avg generation throughput: 2373.1 tokens/s, Running: 148 reqs, Waiting: 107 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 1537.27 tokens/s, Drafted throughput: 3339.99 tokens/s, Accepted: 15382 tokens, Drafted: 33420 tokens, Per-position acceptance rate: 0.712, 0.493, 0.366, 0.270, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:12 [loggers.py:257] Engine 000: Avg prompt throughput: 1775.1 tokens/s, Avg generation throughput: 2232.6 tokens/s, Running: 160 reqs, Waiting: 90 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 1428.05 tokens/s, Drafted throughput: 3203.26 tokens/s, Accepted: 14282 tokens, Drafted: 32036 tokens, Per-position acceptance rate: 0.699, 0.483, 0.345, 0.256, Avg Draft acceptance rate: 44.6%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:22 [loggers.py:257] Engine 000: Avg prompt throughput: 1463.8 tokens/s, Avg generation throughput: 2289.9 tokens/s, Running: 146 reqs, Waiting: 101 reqs, GPU KV cache usage: 95.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1461.73 tokens/s, Drafted throughput: 3301.45 tokens/s, Accepted: 14618 tokens, Drafted: 33016 tokens, Per-position acceptance rate: 0.700, 0.479, 0.344, 0.249, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:32 [loggers.py:257] Engine 000: Avg prompt throughput: 1701.4 tokens/s, Avg generation throughput: 2406.3 tokens/s, Running: 148 reqs, Waiting: 101 reqs, GPU KV cache usage: 94.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 1561.98 tokens/s, Drafted throughput: 3375.56 tokens/s, Accepted: 15620 tokens, Drafted: 33756 tokens, Per-position acceptance rate: 0.712, 0.499, 0.365, 0.275, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:42 [loggers.py:257] Engine 000: Avg prompt throughput: 1470.8 tokens/s, Avg generation throughput: 2402.3 tokens/s, Running: 135 reqs, Waiting: 111 reqs, GPU KV cache usage: 91.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 1541.81 tokens/s, Drafted throughput: 3446.20 tokens/s, Accepted: 15419 tokens, Drafted: 34464 tokens, Per-position acceptance rate: 0.700, 0.485, 0.346, 0.258, Avg Draft acceptance rate: 44.7%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:52 [loggers.py:257] Engine 000: Avg prompt throughput: 1678.6 tokens/s, Avg generation throughput: 2282.2 tokens/s, Running: 146 reqs, Waiting: 54 reqs, GPU KV cache usage: 94.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:08:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1456.33 tokens/s, Drafted throughput: 3296.64 tokens/s, Accepted: 14564 tokens, Drafted: 32968 tokens, Per-position acceptance rate: 0.691, 0.477, 0.342, 0.257, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:09:02 [loggers.py:257] Engine 000: Avg prompt throughput: 731.3 tokens/s, Avg generation throughput: 2673.4 tokens/s, Running: 43 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:09:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1736.67 tokens/s, Drafted throughput: 3793.31 tokens/s, Accepted: 17368 tokens, Drafted: 37936 tokens, Per-position acceptance rate: 0.702, 0.493, 0.362, 0.275, Avg Draft acceptance rate: 45.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  222.59    
Total input tokens:                      373233    
Total generated tokens:                  511908    
Request throughput (req/s):              11.50     
Output token throughput (tok/s):         2299.78   
Peak output token throughput (tok/s):    1332.00   
Peak concurrent requests:                283.00    
Total token throughput (tok/s):          3976.56   
---------------Time to First Token----------------
Mean TTFT (ms):                          10040.64  
Median TTFT (ms):                        10630.02  
P99 TTFT (ms):                           15200.08  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          57.31     
Median TPOT (ms):                        54.26     
P99 TPOT (ms):                           98.39     
---------------Inter-token Latency----------------
Mean ITL (ms):                           159.88    
Median ITL (ms):                         114.08    
P99 ITL (ms):                            417.33    
---------------Speculative Decoding---------------
Acceptance rate (%):                     45.42     
Acceptance length:                       2.82      
Drafts:                                  181647    
Draft tokens:                            726588    
Accepted tokens:                         330009    
Per-position acceptance (%):
  Position 0:                            70.46     
  Position 1:                            49.09     
  Position 2:                            35.57     
  Position 3:                            26.55     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k4-t0.0-tp1...
[0;36m(APIServer pid=186493)[0;0m INFO 01-22 17:09:06 [launcher.py:110] Shutting down FastAPI HTTP server.
