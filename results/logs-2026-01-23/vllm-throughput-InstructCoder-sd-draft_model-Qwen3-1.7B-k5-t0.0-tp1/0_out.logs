Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k5-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k5-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 193204
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:17 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:17 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15009, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=193204)[0;0m WARNING 01-22 17:10:17 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:18 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:18 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:19 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:19 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:19 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=193204)[0;0m WARNING 01-22 17:10:19 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:10:19 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0349baefc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c5972f1a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 17:10:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:10:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:10:30 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=5), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:10:32 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.49:33747 backend=nccl
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:10:32 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=193314)[0;0m WARNING 01-22 17:10:32 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:10:33 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:10:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 17:10:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:10:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:10:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:10:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:10:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:10:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:11:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:11:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:11:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:16 [default_loader.py:291] Loading weights took 41.26 seconds
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:16 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:16 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:16 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-22 17:11:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:19 [default_loader.py:291] Loading weights took 2.16 seconds
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:20 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 46.441685 seconds
WARNING 01-22 17:11:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:11:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:32 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:32 [backends.py:704] Dynamo bytecode transform time: 11.99 s
WARNING 01-22 17:11:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:11:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:11:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:48 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.444 s
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:48 [monitor.py:34] torch.compile takes 14.43 s in total
WARNING 01-22 17:11:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:52 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:52 [backends.py:704] Dynamo bytecode transform time: 4.82 s
WARNING 01-22 17:11:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:58 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 0.722 s
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:11:58 [monitor.py:34] torch.compile takes 19.98 s in total
WARNING 01-22 17:11:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:12:00 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:12:00 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:12:00 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-22 17:12:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:12:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
WARNING 01-22 17:12:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:12:18 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took -0.06 GiB
[0;36m(EngineCore_DP0 pid=193314)[0;0m INFO 01-22 17:12:18 [core.py:272] init engine (profile, create kv cache, warmup model) took 58.02 seconds
WARNING 01-22 17:12:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15009)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15009 ssl:default [Connect call failed (\'127.0.0.1\', 15009)]\n''
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:20 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=193204)[0;0m WARNING 01-22 17:12:20 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:20 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:20 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:20 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [serving.py:221] Chat template warmup completed in 1737.8ms
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15009
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:22 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:33 [loggers.py:257] Engine 000: Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 32.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 21.69 tokens/s, Drafted throughput: 55.76 tokens/s, Accepted: 282 tokens, Drafted: 725 tokens, Per-position acceptance rate: 0.724, 0.469, 0.345, 0.262, 0.145, Avg Draft acceptance rate: 38.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:43 [loggers.py:257] Engine 000: Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 33.70 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 337 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.653, 0.425, 0.290, 0.212, 0.166, Avg Draft acceptance rate: 34.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:53 [loggers.py:257] Engine 000: Avg prompt throughput: 45.5 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:12:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 42.40 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 424 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.777, 0.497, 0.383, 0.295, 0.244, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:03 [loggers.py:257] Engine 000: Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 37.10 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 371 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.755, 0.490, 0.339, 0.198, 0.151, Avg Draft acceptance rate: 38.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:13 [loggers.py:257] Engine 000: Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 41.00 tokens/s, Drafted throughput: 96.50 tokens/s, Accepted: 410 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.746, 0.492, 0.394, 0.290, 0.202, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:23 [loggers.py:257] Engine 000: Avg prompt throughput: 37.5 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 41.60 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 416 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.719, 0.526, 0.411, 0.307, 0.203, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:33 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 42.10 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 421 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.745, 0.542, 0.385, 0.286, 0.234, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:43 [loggers.py:257] Engine 000: Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 45.40 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 454 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.767, 0.560, 0.440, 0.342, 0.244, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:53 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:13:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 34.20 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 342 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.693, 0.422, 0.339, 0.188, 0.141, Avg Draft acceptance rate: 35.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:03 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 38.39 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 384 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.703, 0.505, 0.349, 0.260, 0.182, Avg Draft acceptance rate: 40.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:13 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 42.80 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 428 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.715, 0.518, 0.415, 0.326, 0.244, Avg Draft acceptance rate: 44.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:23 [loggers.py:257] Engine 000: Avg prompt throughput: 59.4 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 40.00 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 400 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.708, 0.510, 0.375, 0.271, 0.219, Avg Draft acceptance rate: 41.7%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:33 [loggers.py:257] Engine 000: Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 55.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 36.10 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 361 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.684, 0.440, 0.332, 0.249, 0.166, Avg Draft acceptance rate: 37.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:43 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 41.19 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 412 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.682, 0.495, 0.391, 0.333, 0.245, Avg Draft acceptance rate: 42.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:53 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 58.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:14:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 38.90 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 389 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.760, 0.510, 0.365, 0.229, 0.161, Avg Draft acceptance rate: 40.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:03 [loggers.py:257] Engine 000: Avg prompt throughput: 51.1 tokens/s, Avg generation throughput: 58.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 39.50 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 395 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.745, 0.547, 0.359, 0.229, 0.177, Avg Draft acceptance rate: 41.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:13 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 42.00 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 420 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.734, 0.552, 0.391, 0.302, 0.208, Avg Draft acceptance rate: 43.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  171.18    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.29      
Output token throughput (tok/s):         58.42     
Peak output token throughput (tok/s):    20.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          100.64    
---------------Time to First Token----------------
Mean TTFT (ms):                          64.82     
Median TTFT (ms):                        64.41     
P99 TTFT (ms):                           72.85     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.80     
Median TPOT (ms):                        16.91     
P99 TPOT (ms):                           19.35     
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.98     
Median ITL (ms):                         50.98     
P99 ITL (ms):                            51.50     
---------------Speculative Decoding---------------
Acceptance rate (%):                     41.13     
Acceptance length:                       3.06      
Drafts:                                  3279      
Draft tokens:                            16395     
Accepted tokens:                         6744      
Per-position acceptance (%):
  Position 0:                            72.28     
  Position 1:                            49.86     
  Position 2:                            36.96     
  Position 3:                            26.87     
  Position 4:                            19.70     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:23 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 35.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 22.90 tokens/s, Drafted throughput: 62.99 tokens/s, Accepted: 229 tokens, Drafted: 630 tokens, Per-position acceptance rate: 0.683, 0.429, 0.310, 0.230, 0.167, Avg Draft acceptance rate: 36.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb58d9eafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0aa6903a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:43 [loggers.py:257] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 38.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 13.05 tokens/s, Drafted throughput: 31.00 tokens/s, Accepted: 261 tokens, Drafted: 620 tokens, Per-position acceptance rate: 0.742, 0.516, 0.395, 0.290, 0.161, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:53 [loggers.py:257] Engine 000: Avg prompt throughput: 82.9 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:15:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 70.50 tokens/s, Drafted throughput: 188.00 tokens/s, Accepted: 705 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.705, 0.452, 0.309, 0.226, 0.184, Avg Draft acceptance rate: 37.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:03 [loggers.py:257] Engine 000: Avg prompt throughput: 59.3 tokens/s, Avg generation throughput: 115.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 77.49 tokens/s, Drafted throughput: 188.98 tokens/s, Accepted: 775 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.746, 0.497, 0.376, 0.241, 0.190, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:13 [loggers.py:257] Engine 000: Avg prompt throughput: 92.9 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 77.40 tokens/s, Drafted throughput: 187.99 tokens/s, Accepted: 774 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.713, 0.505, 0.364, 0.282, 0.194, Avg Draft acceptance rate: 41.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:23 [loggers.py:257] Engine 000: Avg prompt throughput: 81.0 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 79.89 tokens/s, Drafted throughput: 187.99 tokens/s, Accepted: 799 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.729, 0.516, 0.380, 0.290, 0.210, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:33 [loggers.py:257] Engine 000: Avg prompt throughput: 98.4 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 79.59 tokens/s, Drafted throughput: 187.97 tokens/s, Accepted: 796 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.731, 0.527, 0.402, 0.263, 0.194, Avg Draft acceptance rate: 42.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:43 [loggers.py:257] Engine 000: Avg prompt throughput: 82.0 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 73.09 tokens/s, Drafted throughput: 187.97 tokens/s, Accepted: 731 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.705, 0.473, 0.335, 0.245, 0.186, Avg Draft acceptance rate: 38.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:53 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:16:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 78.50 tokens/s, Drafted throughput: 188.99 tokens/s, Accepted: 785 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.712, 0.500, 0.370, 0.278, 0.217, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:03 [loggers.py:257] Engine 000: Avg prompt throughput: 65.8 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 77.59 tokens/s, Drafted throughput: 188.99 tokens/s, Accepted: 776 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.722, 0.516, 0.362, 0.257, 0.196, Avg Draft acceptance rate: 41.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  89.45     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.56      
Output token throughput (tok/s):         111.80    
Peak output token throughput (tok/s):    40.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          192.59    
---------------Time to First Token----------------
Mean TTFT (ms):                          105.76    
Median TTFT (ms):                        106.33    
P99 TTFT (ms):                           113.74    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.06     
Median TPOT (ms):                        17.34     
P99 TPOT (ms):                           19.82     
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.51     
Median ITL (ms):                         51.42     
P99 ITL (ms):                            55.98     
---------------Speculative Decoding---------------
Acceptance rate (%):                     40.76     
Acceptance length:                       3.04      
Drafts:                                  3295      
Draft tokens:                            16475     
Accepted tokens:                         6716      
Per-position acceptance (%):
  Position 0:                            72.05     
  Position 1:                            49.92     
  Position 2:                            36.27     
  Position 3:                            26.13     
  Position 4:                            19.45     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:13 [loggers.py:257] Engine 000: Avg prompt throughput: 33.8 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 44.50 tokens/s, Drafted throughput: 113.50 tokens/s, Accepted: 445 tokens, Drafted: 1135 tokens, Per-position acceptance rate: 0.709, 0.489, 0.339, 0.256, 0.167, Avg Draft acceptance rate: 39.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fa162ff6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-79c2f7c0-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:33 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 50.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 17.30 tokens/s, Drafted throughput: 38.74 tokens/s, Accepted: 346 tokens, Drafted: 775 tokens, Per-position acceptance rate: 0.729, 0.542, 0.432, 0.310, 0.219, Avg Draft acceptance rate: 44.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:43 [loggers.py:257] Engine 000: Avg prompt throughput: 155.7 tokens/s, Avg generation throughput: 220.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 146.57 tokens/s, Drafted throughput: 371.94 tokens/s, Accepted: 1466 tokens, Drafted: 3720 tokens, Per-position acceptance rate: 0.731, 0.472, 0.348, 0.241, 0.179, Avg Draft acceptance rate: 39.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:53 [loggers.py:257] Engine 000: Avg prompt throughput: 148.1 tokens/s, Avg generation throughput: 232.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:17:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 157.88 tokens/s, Drafted throughput: 371.95 tokens/s, Accepted: 1579 tokens, Drafted: 3720 tokens, Per-position acceptance rate: 0.718, 0.507, 0.398, 0.293, 0.207, Avg Draft acceptance rate: 42.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:03 [loggers.py:257] Engine 000: Avg prompt throughput: 168.7 tokens/s, Avg generation throughput: 226.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 151.90 tokens/s, Drafted throughput: 372.99 tokens/s, Accepted: 1519 tokens, Drafted: 3730 tokens, Per-position acceptance rate: 0.709, 0.508, 0.358, 0.261, 0.200, Avg Draft acceptance rate: 40.7%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:13 [loggers.py:257] Engine 000: Avg prompt throughput: 181.8 tokens/s, Avg generation throughput: 232.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 159.39 tokens/s, Drafted throughput: 371.97 tokens/s, Accepted: 1594 tokens, Drafted: 3720 tokens, Per-position acceptance rate: 0.734, 0.523, 0.383, 0.286, 0.216, Avg Draft acceptance rate: 42.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  46.14     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.08      
Output token throughput (tok/s):         216.73    
Peak output token throughput (tok/s):    80.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          373.36    
---------------Time to First Token----------------
Mean TTFT (ms):                          106.00    
Median TTFT (ms):                        107.41    
P99 TTFT (ms):                           115.25    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.10     
Median TPOT (ms):                        17.52     
P99 TPOT (ms):                           19.78     
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.06     
Median ITL (ms):                         51.84     
P99 ITL (ms):                            57.98     
---------------Speculative Decoding---------------
Acceptance rate (%):                     41.35     
Acceptance length:                       3.07      
Drafts:                                  3268      
Draft tokens:                            16340     
Accepted tokens:                         6756      
Per-position acceptance (%):
  Position 0:                            72.12     
  Position 1:                            50.34     
  Position 2:                            37.24     
  Position 3:                            27.02     
  Position 4:                            20.01     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:23 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 38.30 tokens/s, Drafted throughput: 102.50 tokens/s, Accepted: 383 tokens, Drafted: 1025 tokens, Per-position acceptance rate: 0.688, 0.473, 0.322, 0.229, 0.156, Avg Draft acceptance rate: 37.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f193e236fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b8d3f951-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:43 [loggers.py:257] Engine 000: Avg prompt throughput: 165.7 tokens/s, Avg generation throughput: 177.3 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 59.09 tokens/s, Drafted throughput: 146.73 tokens/s, Accepted: 1182 tokens, Drafted: 2935 tokens, Per-position acceptance rate: 0.707, 0.484, 0.358, 0.269, 0.196, Avg Draft acceptance rate: 40.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:53 [loggers.py:257] Engine 000: Avg prompt throughput: 321.5 tokens/s, Avg generation throughput: 438.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:18:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 296.27 tokens/s, Drafted throughput: 715.42 tokens/s, Accepted: 2963 tokens, Drafted: 7155 tokens, Per-position acceptance rate: 0.718, 0.504, 0.375, 0.270, 0.203, Avg Draft acceptance rate: 41.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:03 [loggers.py:257] Engine 000: Avg prompt throughput: 318.6 tokens/s, Avg generation throughput: 442.6 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 301.68 tokens/s, Drafted throughput: 712.96 tokens/s, Accepted: 3017 tokens, Drafted: 7130 tokens, Per-position acceptance rate: 0.725, 0.522, 0.379, 0.281, 0.208, Avg Draft acceptance rate: 42.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:13 [loggers.py:257] Engine 000: Avg prompt throughput: 331.8 tokens/s, Avg generation throughput: 427.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 285.78 tokens/s, Drafted throughput: 713.95 tokens/s, Accepted: 2858 tokens, Drafted: 7140 tokens, Per-position acceptance rate: 0.709, 0.487, 0.350, 0.259, 0.197, Avg Draft acceptance rate: 40.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  38.43     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              2.08      
Output token throughput (tok/s):         416.39    
Peak output token throughput (tok/s):    152.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          720.62    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.27    
Median TTFT (ms):                        111.40    
P99 TTFT (ms):                           124.53    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.96     
Median TPOT (ms):                        17.97     
P99 TPOT (ms):                           21.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.27     
Median ITL (ms):                         53.81     
P99 ITL (ms):                            63.31     
---------------Speculative Decoding---------------
Acceptance rate (%):                     40.90     
Acceptance length:                       3.04      
Drafts:                                  5268      
Draft tokens:                            26340     
Accepted tokens:                         10773     
Per-position acceptance (%):
  Position 0:                            71.36     
  Position 1:                            49.96     
  Position 2:                            36.47     
  Position 3:                            26.77     
  Position 4:                            19.95     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:23 [loggers.py:257] Engine 000: Avg prompt throughput: 49.7 tokens/s, Avg generation throughput: 134.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.90, Accepted throughput: 88.40 tokens/s, Drafted throughput: 233.00 tokens/s, Accepted: 884 tokens, Drafted: 2330 tokens, Per-position acceptance rate: 0.687, 0.472, 0.337, 0.238, 0.163, Avg Draft acceptance rate: 37.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3125d86fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7d5bb9d5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:43 [loggers.py:257] Engine 000: Avg prompt throughput: 370.0 tokens/s, Avg generation throughput: 360.9 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 121.54 tokens/s, Drafted throughput: 292.48 tokens/s, Accepted: 2431 tokens, Drafted: 5850 tokens, Per-position acceptance rate: 0.732, 0.496, 0.373, 0.275, 0.202, Avg Draft acceptance rate: 41.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:53 [loggers.py:257] Engine 000: Avg prompt throughput: 578.5 tokens/s, Avg generation throughput: 823.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:19:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 557.70 tokens/s, Drafted throughput: 1337.25 tokens/s, Accepted: 5578 tokens, Drafted: 13375 tokens, Per-position acceptance rate: 0.719, 0.511, 0.371, 0.278, 0.207, Avg Draft acceptance rate: 41.7%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:03 [loggers.py:257] Engine 000: Avg prompt throughput: 653.4 tokens/s, Avg generation throughput: 799.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 535.46 tokens/s, Drafted throughput: 1332.66 tokens/s, Accepted: 5356 tokens, Drafted: 13330 tokens, Per-position acceptance rate: 0.709, 0.485, 0.358, 0.265, 0.193, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:13 [loggers.py:257] Engine 000: Avg prompt throughput: 597.9 tokens/s, Avg generation throughput: 823.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 558.10 tokens/s, Drafted throughput: 1336.76 tokens/s, Accepted: 5582 tokens, Drafted: 13370 tokens, Per-position acceptance rate: 0.718, 0.503, 0.371, 0.280, 0.215, Avg Draft acceptance rate: 41.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  41.08     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.89      
Output token throughput (tok/s):         778.77    
Peak output token throughput (tok/s):    288.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          1371.81   
---------------Time to First Token----------------
Mean TTFT (ms):                          121.76    
Median TTFT (ms):                        120.08    
P99 TTFT (ms):                           168.21    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.09     
Median TPOT (ms):                        19.00     
P99 TPOT (ms):                           23.83     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.97     
Median ITL (ms):                         57.08     
P99 ITL (ms):                            72.80     
---------------Speculative Decoding---------------
Acceptance rate (%):                     41.23     
Acceptance length:                       3.06      
Drafts:                                  10481     
Draft tokens:                            52405     
Accepted tokens:                         21605     
Per-position acceptance (%):
  Position 0:                            71.65     
  Position 1:                            49.96     
  Position 2:                            36.69     
  Position 3:                            27.34     
  Position 4:                            20.50     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:23 [loggers.py:257] Engine 000: Avg prompt throughput: 254.2 tokens/s, Avg generation throughput: 411.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 278.87 tokens/s, Drafted throughput: 682.93 tokens/s, Accepted: 2789 tokens, Drafted: 6830 tokens, Per-position acceptance rate: 0.710, 0.502, 0.361, 0.265, 0.204, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f272eba2fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ee7e80bc-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:43 [loggers.py:257] Engine 000: Avg prompt throughput: 487.1 tokens/s, Avg generation throughput: 305.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 104.99 tokens/s, Drafted throughput: 230.22 tokens/s, Accepted: 2100 tokens, Drafted: 4605 tokens, Per-position acceptance rate: 0.769, 0.540, 0.414, 0.315, 0.243, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:53 [loggers.py:257] Engine 000: Avg prompt throughput: 915.5 tokens/s, Avg generation throughput: 1421.1 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:20:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 951.65 tokens/s, Drafted throughput: 2367.87 tokens/s, Accepted: 9517 tokens, Drafted: 23680 tokens, Per-position acceptance rate: 0.710, 0.492, 0.354, 0.262, 0.193, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1128.4 tokens/s, Avg generation throughput: 1415.1 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 950.39 tokens/s, Drafted throughput: 2345.23 tokens/s, Accepted: 9505 tokens, Drafted: 23455 tokens, Per-position acceptance rate: 0.705, 0.492, 0.358, 0.270, 0.202, Avg Draft acceptance rate: 40.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1082.5 tokens/s, Avg generation throughput: 1398.8 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 929.16 tokens/s, Drafted throughput: 2373.40 tokens/s, Accepted: 9292 tokens, Drafted: 23735 tokens, Per-position acceptance rate: 0.701, 0.469, 0.335, 0.256, 0.196, Avg Draft acceptance rate: 39.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1137.6 tokens/s, Avg generation throughput: 1429.3 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 963.15 tokens/s, Drafted throughput: 2342.39 tokens/s, Accepted: 9632 tokens, Drafted: 23425 tokens, Per-position acceptance rate: 0.704, 0.502, 0.372, 0.274, 0.203, Avg Draft acceptance rate: 41.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  47.62     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.72      
Output token throughput (tok/s):         1343.87   
Peak output token throughput (tok/s):    544.00    
Peak concurrent requests:                51.00     
Total token throughput (tok/s):          2357.49   
---------------Time to First Token----------------
Mean TTFT (ms):                          150.27    
Median TTFT (ms):                        142.77    
P99 TTFT (ms):                           227.06    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.87     
Median TPOT (ms):                        21.82     
P99 TPOT (ms):                           27.43     
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.22     
Median ITL (ms):                         60.92     
P99 ITL (ms):                            117.34    
---------------Speculative Decoding---------------
Acceptance rate (%):                     40.08     
Acceptance length:                       3.00      
Drafts:                                  21356     
Draft tokens:                            106780    
Accepted tokens:                         42795     
Per-position acceptance (%):
  Position 0:                            70.43     
  Position 1:                            48.70     
  Position 2:                            35.24     
  Position 3:                            26.33     
  Position 4:                            19.69     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:33 [loggers.py:257] Engine 000: Avg prompt throughput: 93.2 tokens/s, Avg generation throughput: 449.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 287.99 tokens/s, Drafted throughput: 822.97 tokens/s, Accepted: 2880 tokens, Drafted: 8230 tokens, Per-position acceptance rate: 0.659, 0.438, 0.293, 0.208, 0.151, Avg Draft acceptance rate: 35.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f12b2ae6fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3d5eb613-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:53 [loggers.py:257] Engine 000: Avg prompt throughput: 948.2 tokens/s, Avg generation throughput: 618.5 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 50.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:21:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 214.75 tokens/s, Drafted throughput: 457.14 tokens/s, Accepted: 4296 tokens, Drafted: 9145 tokens, Per-position acceptance rate: 0.784, 0.560, 0.424, 0.327, 0.254, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1505.6 tokens/s, Avg generation throughput: 2014.6 tokens/s, Running: 60 reqs, Waiting: 0 reqs, GPU KV cache usage: 51.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 1340.21 tokens/s, Drafted throughput: 3391.77 tokens/s, Accepted: 13403 tokens, Drafted: 33920 tokens, Per-position acceptance rate: 0.699, 0.481, 0.351, 0.255, 0.190, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1441.2 tokens/s, Avg generation throughput: 2127.1 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 56.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1440.56 tokens/s, Drafted throughput: 3452.17 tokens/s, Accepted: 14407 tokens, Drafted: 34525 tokens, Per-position acceptance rate: 0.712, 0.503, 0.373, 0.284, 0.216, Avg Draft acceptance rate: 41.7%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1738.8 tokens/s, Avg generation throughput: 2022.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 46.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 1362.72 tokens/s, Drafted throughput: 3338.33 tokens/s, Accepted: 13632 tokens, Drafted: 33395 tokens, Per-position acceptance rate: 0.711, 0.497, 0.361, 0.269, 0.203, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1344.8 tokens/s, Avg generation throughput: 2101.4 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 1402.88 tokens/s, Drafted throughput: 3523.20 tokens/s, Accepted: 14030 tokens, Drafted: 35235 tokens, Per-position acceptance rate: 0.702, 0.490, 0.349, 0.258, 0.192, Avg Draft acceptance rate: 39.8%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1562.1 tokens/s, Avg generation throughput: 2074.9 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 1396.99 tokens/s, Drafted throughput: 3413.97 tokens/s, Accepted: 13970 tokens, Drafted: 34140 tokens, Per-position acceptance rate: 0.712, 0.500, 0.363, 0.270, 0.201, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:53 [loggers.py:257] Engine 000: Avg prompt throughput: 953.2 tokens/s, Avg generation throughput: 1851.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:22:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 1230.82 tokens/s, Drafted throughput: 3169.03 tokens/s, Accepted: 12310 tokens, Drafted: 31695 tokens, Per-position acceptance rate: 0.695, 0.475, 0.339, 0.250, 0.183, Avg Draft acceptance rate: 38.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  63.78     
Total input tokens:                      94775     
Total generated tokens:                  127983    
Request throughput (req/s):              10.03     
Output token throughput (tok/s):         2006.65   
Peak output token throughput (tok/s):    896.00    
Peak concurrent requests:                89.00     
Total token throughput (tok/s):          3492.64   
---------------Time to First Token----------------
Mean TTFT (ms):                          223.32    
Median TTFT (ms):                        205.60    
P99 TTFT (ms):                           528.59    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          29.61     
Median TPOT (ms):                        29.52     
P99 TPOT (ms):                           38.66     
---------------Inter-token Latency----------------
Mean ITL (ms):                           89.03     
Median ITL (ms):                         77.02     
P99 ITL (ms):                            167.70    
---------------Speculative Decoding---------------
Acceptance rate (%):                     40.58     
Acceptance length:                       3.03      
Drafts:                                  42360     
Draft tokens:                            211800    
Accepted tokens:                         85955     
Per-position acceptance (%):
  Position 0:                            70.87     
  Position 1:                            49.41     
  Position 2:                            35.90     
  Position 3:                            26.70     
  Position 4:                            20.03     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 3.80 tokens/s, Drafted throughput: 9.50 tokens/s, Accepted: 38 tokens, Drafted: 95 tokens, Per-position acceptance rate: 0.579, 0.526, 0.474, 0.263, 0.158, Avg Draft acceptance rate: 40.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f39fd72afc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-671e4f4e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:13 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 4.10 tokens/s, Drafted throughput: 9.00 tokens/s, Accepted: 41 tokens, Drafted: 90 tokens, Per-position acceptance rate: 0.833, 0.667, 0.389, 0.278, 0.111, Avg Draft acceptance rate: 45.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1925.4 tokens/s, Avg generation throughput: 1655.5 tokens/s, Running: 110 reqs, Waiting: 18 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 1123.56 tokens/s, Drafted throughput: 2597.41 tokens/s, Accepted: 11236 tokens, Drafted: 25975 tokens, Per-position acceptance rate: 0.735, 0.524, 0.388, 0.293, 0.223, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1825.0 tokens/s, Avg generation throughput: 2056.4 tokens/s, Running: 119 reqs, Waiting: 8 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 1383.64 tokens/s, Drafted throughput: 3379.12 tokens/s, Accepted: 13838 tokens, Drafted: 33795 tokens, Per-position acceptance rate: 0.712, 0.491, 0.363, 0.274, 0.206, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1684.7 tokens/s, Avg generation throughput: 2139.8 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 96.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 1442.18 tokens/s, Drafted throughput: 3509.47 tokens/s, Accepted: 14424 tokens, Drafted: 35100 tokens, Per-position acceptance rate: 0.714, 0.502, 0.362, 0.272, 0.206, Avg Draft acceptance rate: 41.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1730.7 tokens/s, Avg generation throughput: 2220.7 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 93.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:23:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 1485.30 tokens/s, Drafted throughput: 3699.01 tokens/s, Accepted: 14865 tokens, Drafted: 37020 tokens, Per-position acceptance rate: 0.706, 0.490, 0.354, 0.261, 0.198, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1503.7 tokens/s, Avg generation throughput: 2194.1 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 1445.11 tokens/s, Drafted throughput: 3763.15 tokens/s, Accepted: 14462 tokens, Drafted: 37660 tokens, Per-position acceptance rate: 0.686, 0.469, 0.335, 0.248, 0.183, Avg Draft acceptance rate: 38.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1680.6 tokens/s, Avg generation throughput: 2205.1 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 91.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 1475.21 tokens/s, Drafted throughput: 3672.78 tokens/s, Accepted: 14753 tokens, Drafted: 36730 tokens, Per-position acceptance rate: 0.706, 0.491, 0.351, 0.263, 0.198, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1618.8 tokens/s, Avg generation throughput: 2311.9 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 89.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 1544.63 tokens/s, Drafted throughput: 3851.34 tokens/s, Accepted: 15459 tokens, Drafted: 38545 tokens, Per-position acceptance rate: 0.699, 0.487, 0.354, 0.267, 0.199, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1727.5 tokens/s, Avg generation throughput: 2201.7 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 1469.12 tokens/s, Drafted throughput: 3676.29 tokens/s, Accepted: 14692 tokens, Drafted: 36765 tokens, Per-position acceptance rate: 0.699, 0.488, 0.355, 0.262, 0.194, Avg Draft acceptance rate: 40.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1646.1 tokens/s, Avg generation throughput: 2119.8 tokens/s, Running: 123 reqs, Waiting: 0 reqs, GPU KV cache usage: 88.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 1396.46 tokens/s, Drafted throughput: 3630.13 tokens/s, Accepted: 13966 tokens, Drafted: 36305 tokens, Per-position acceptance rate: 0.692, 0.471, 0.333, 0.245, 0.183, Avg Draft acceptance rate: 38.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1592.8 tokens/s, Avg generation throughput: 2273.7 tokens/s, Running: 122 reqs, Waiting: 0 reqs, GPU KV cache usage: 84.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:24:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 1516.61 tokens/s, Drafted throughput: 3792.77 tokens/s, Accepted: 15167 tokens, Drafted: 37930 tokens, Per-position acceptance rate: 0.704, 0.491, 0.349, 0.259, 0.196, Avg Draft acceptance rate: 40.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1720.1 tokens/s, Avg generation throughput: 2222.4 tokens/s, Running: 123 reqs, Waiting: 0 reqs, GPU KV cache usage: 87.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 1481.63 tokens/s, Drafted throughput: 3726.05 tokens/s, Accepted: 14828 tokens, Drafted: 37290 tokens, Per-position acceptance rate: 0.703, 0.491, 0.350, 0.254, 0.191, Avg Draft acceptance rate: 39.8%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:13 [loggers.py:257] Engine 000: Avg prompt throughput: 247.1 tokens/s, Avg generation throughput: 1999.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 1320.08 tokens/s, Drafted throughput: 3491.94 tokens/s, Accepted: 13201 tokens, Drafted: 34920 tokens, Per-position acceptance rate: 0.684, 0.467, 0.324, 0.240, 0.176, Avg Draft acceptance rate: 37.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  117.63    
Total input tokens:                      189093    
Total generated tokens:                  255968    
Request throughput (req/s):              10.88     
Output token throughput (tok/s):         2176.09   
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                155.00    
Total token throughput (tok/s):          3783.65   
---------------Time to First Token----------------
Mean TTFT (ms):                          863.84    
Median TTFT (ms):                        517.64    
P99 TTFT (ms):                           3710.30   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          52.76     
Median TPOT (ms):                        51.94     
P99 TPOT (ms):                           75.95     
---------------Inter-token Latency----------------
Mean ITL (ms):                           156.62    
Median ITL (ms):                         136.14    
P99 ITL (ms):                            307.53    
---------------Speculative Decoding---------------
Acceptance rate (%):                     39.92     
Acceptance length:                       3.00      
Drafts:                                  85563     
Draft tokens:                            427815    
Accepted tokens:                         170801    
Per-position acceptance (%):
  Position 0:                            70.23     
  Position 1:                            48.76     
  Position 2:                            35.05     
  Position 3:                            26.05     
  Position 4:                            19.54     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.00, Accepted throughput: 0.00 tokens/s, Drafted throughput: 4.00 tokens/s, Accepted: 0 tokens, Drafted: 40 tokens, Per-position acceptance rate: 0.000, 0.000, 0.000, 0.000, 0.000, Avg Draft acceptance rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd0b6bdefc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15009, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-799a6c2c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:33 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 0.60 tokens/s, Drafted throughput: 1.00 tokens/s, Accepted: 6 tokens, Drafted: 10 tokens, Per-position acceptance rate: 1.000, 0.500, 0.500, 0.500, 0.500, Avg Draft acceptance rate: 60.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:43 [loggers.py:257] Engine 000: Avg prompt throughput: 2855.1 tokens/s, Avg generation throughput: 1183.7 tokens/s, Running: 135 reqs, Waiting: 121 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 826.81 tokens/s, Drafted throughput: 1692.31 tokens/s, Accepted: 8269 tokens, Drafted: 16925 tokens, Per-position acceptance rate: 0.786, 0.567, 0.446, 0.358, 0.286, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:53 [loggers.py:257] Engine 000: Avg prompt throughput: 648.0 tokens/s, Avg generation throughput: 1964.5 tokens/s, Running: 127 reqs, Waiting: 125 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:25:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 1282.68 tokens/s, Drafted throughput: 3415.96 tokens/s, Accepted: 12827 tokens, Drafted: 34160 tokens, Per-position acceptance rate: 0.682, 0.464, 0.328, 0.235, 0.168, Avg Draft acceptance rate: 37.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1872.2 tokens/s, Avg generation throughput: 2153.4 tokens/s, Running: 136 reqs, Waiting: 110 reqs, GPU KV cache usage: 95.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 1448.32 tokens/s, Drafted throughput: 3542.79 tokens/s, Accepted: 14484 tokens, Drafted: 35430 tokens, Per-position acceptance rate: 0.707, 0.497, 0.363, 0.273, 0.205, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1571.7 tokens/s, Avg generation throughput: 2208.4 tokens/s, Running: 138 reqs, Waiting: 109 reqs, GPU KV cache usage: 94.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 1468.25 tokens/s, Drafted throughput: 3706.87 tokens/s, Accepted: 14685 tokens, Drafted: 37075 tokens, Per-position acceptance rate: 0.704, 0.490, 0.345, 0.251, 0.191, Avg Draft acceptance rate: 39.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1363.6 tokens/s, Avg generation throughput: 2141.6 tokens/s, Running: 131 reqs, Waiting: 114 reqs, GPU KV cache usage: 94.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 1406.08 tokens/s, Drafted throughput: 3685.92 tokens/s, Accepted: 14063 tokens, Drafted: 36865 tokens, Per-position acceptance rate: 0.680, 0.468, 0.332, 0.243, 0.184, Avg Draft acceptance rate: 38.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1530.1 tokens/s, Avg generation throughput: 2228.3 tokens/s, Running: 128 reqs, Waiting: 120 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 1493.90 tokens/s, Drafted throughput: 3691.51 tokens/s, Accepted: 14941 tokens, Drafted: 36920 tokens, Per-position acceptance rate: 0.712, 0.496, 0.355, 0.264, 0.196, Avg Draft acceptance rate: 40.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:43 [loggers.py:257] Engine 000: Avg prompt throughput: 975.8 tokens/s, Avg generation throughput: 2301.0 tokens/s, Running: 94 reqs, Waiting: 148 reqs, GPU KV cache usage: 89.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 1541.20 tokens/s, Drafted throughput: 3821.74 tokens/s, Accepted: 15413 tokens, Drafted: 38220 tokens, Per-position acceptance rate: 0.705, 0.491, 0.354, 0.267, 0.200, Avg Draft acceptance rate: 40.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1608.8 tokens/s, Avg generation throughput: 2268.3 tokens/s, Running: 97 reqs, Waiting: 155 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:26:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 1517.98 tokens/s, Drafted throughput: 3743.20 tokens/s, Accepted: 15181 tokens, Drafted: 37435 tokens, Per-position acceptance rate: 0.697, 0.495, 0.363, 0.271, 0.201, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1907.1 tokens/s, Avg generation throughput: 2130.4 tokens/s, Running: 100 reqs, Waiting: 154 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 1413.07 tokens/s, Drafted throughput: 3594.38 tokens/s, Accepted: 14139 tokens, Drafted: 35965 tokens, Per-position acceptance rate: 0.696, 0.475, 0.345, 0.257, 0.192, Avg Draft acceptance rate: 39.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1552.6 tokens/s, Avg generation throughput: 2008.0 tokens/s, Running: 110 reqs, Waiting: 146 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 1331.14 tokens/s, Drafted throughput: 3377.84 tokens/s, Accepted: 13312 tokens, Drafted: 33780 tokens, Per-position acceptance rate: 0.694, 0.485, 0.342, 0.256, 0.193, Avg Draft acceptance rate: 39.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1982.9 tokens/s, Avg generation throughput: 2109.3 tokens/s, Running: 121 reqs, Waiting: 135 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 1412.32 tokens/s, Drafted throughput: 3476.06 tokens/s, Accepted: 14125 tokens, Drafted: 34765 tokens, Per-position acceptance rate: 0.715, 0.496, 0.357, 0.263, 0.201, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1628.6 tokens/s, Avg generation throughput: 2133.3 tokens/s, Running: 125 reqs, Waiting: 130 reqs, GPU KV cache usage: 98.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 1418.94 tokens/s, Drafted throughput: 3597.84 tokens/s, Accepted: 14194 tokens, Drafted: 35990 tokens, Per-position acceptance rate: 0.697, 0.480, 0.343, 0.256, 0.196, Avg Draft acceptance rate: 39.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1983.2 tokens/s, Avg generation throughput: 2004.3 tokens/s, Running: 143 reqs, Waiting: 111 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 1330.09 tokens/s, Drafted throughput: 3380.21 tokens/s, Accepted: 13302 tokens, Drafted: 33805 tokens, Per-position acceptance rate: 0.702, 0.483, 0.344, 0.253, 0.186, Avg Draft acceptance rate: 39.3%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1392.1 tokens/s, Avg generation throughput: 2058.9 tokens/s, Running: 144 reqs, Waiting: 107 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:27:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 1352.88 tokens/s, Drafted throughput: 3540.18 tokens/s, Accepted: 13530 tokens, Drafted: 35405 tokens, Per-position acceptance rate: 0.691, 0.467, 0.333, 0.243, 0.176, Avg Draft acceptance rate: 38.2%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1459.0 tokens/s, Avg generation throughput: 2182.4 tokens/s, Running: 134 reqs, Waiting: 113 reqs, GPU KV cache usage: 93.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 1444.82 tokens/s, Drafted throughput: 3698.28 tokens/s, Accepted: 14449 tokens, Drafted: 36985 tokens, Per-position acceptance rate: 0.695, 0.477, 0.339, 0.251, 0.190, Avg Draft acceptance rate: 39.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1294.8 tokens/s, Avg generation throughput: 2147.3 tokens/s, Running: 126 reqs, Waiting: 120 reqs, GPU KV cache usage: 93.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 1414.89 tokens/s, Drafted throughput: 3660.21 tokens/s, Accepted: 14150 tokens, Drafted: 36605 tokens, Per-position acceptance rate: 0.692, 0.477, 0.336, 0.246, 0.182, Avg Draft acceptance rate: 38.7%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1454.6 tokens/s, Avg generation throughput: 2271.4 tokens/s, Running: 119 reqs, Waiting: 123 reqs, GPU KV cache usage: 94.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 1512.17 tokens/s, Drafted throughput: 3796.17 tokens/s, Accepted: 15133 tokens, Drafted: 37990 tokens, Per-position acceptance rate: 0.698, 0.481, 0.354, 0.263, 0.196, Avg Draft acceptance rate: 39.8%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1114.2 tokens/s, Avg generation throughput: 2238.6 tokens/s, Running: 102 reqs, Waiting: 149 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 1486.73 tokens/s, Drafted throughput: 3760.32 tokens/s, Accepted: 14870 tokens, Drafted: 37610 tokens, Per-position acceptance rate: 0.699, 0.486, 0.347, 0.255, 0.190, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1738.1 tokens/s, Avg generation throughput: 2238.2 tokens/s, Running: 100 reqs, Waiting: 150 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 1492.34 tokens/s, Drafted throughput: 3721.11 tokens/s, Accepted: 14925 tokens, Drafted: 37215 tokens, Per-position acceptance rate: 0.706, 0.488, 0.353, 0.264, 0.194, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1692.2 tokens/s, Avg generation throughput: 2218.3 tokens/s, Running: 105 reqs, Waiting: 149 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:28:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 1485.06 tokens/s, Drafted throughput: 3662.15 tokens/s, Accepted: 14852 tokens, Drafted: 36625 tokens, Per-position acceptance rate: 0.707, 0.490, 0.359, 0.267, 0.203, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1740.7 tokens/s, Avg generation throughput: 2154.3 tokens/s, Running: 109 reqs, Waiting: 145 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 1433.67 tokens/s, Drafted throughput: 3611.92 tokens/s, Accepted: 14343 tokens, Drafted: 36135 tokens, Per-position acceptance rate: 0.702, 0.485, 0.347, 0.258, 0.192, Avg Draft acceptance rate: 39.7%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1683.0 tokens/s, Avg generation throughput: 2136.2 tokens/s, Running: 121 reqs, Waiting: 135 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 1427.04 tokens/s, Drafted throughput: 3558.86 tokens/s, Accepted: 14281 tokens, Drafted: 35615 tokens, Per-position acceptance rate: 0.706, 0.486, 0.351, 0.263, 0.199, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1664.7 tokens/s, Avg generation throughput: 2147.5 tokens/s, Running: 133 reqs, Waiting: 60 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 1437.42 tokens/s, Drafted throughput: 3554.57 tokens/s, Accepted: 14376 tokens, Drafted: 35550 tokens, Per-position acceptance rate: 0.704, 0.498, 0.358, 0.267, 0.196, Avg Draft acceptance rate: 40.4%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:33 [loggers.py:257] Engine 000: Avg prompt throughput: 601.6 tokens/s, Avg generation throughput: 2312.2 tokens/s, Running: 58 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 1536.26 tokens/s, Drafted throughput: 3936.89 tokens/s, Accepted: 15363 tokens, Drafted: 39370 tokens, Per-position acceptance rate: 0.691, 0.480, 0.341, 0.251, 0.187, Avg Draft acceptance rate: 39.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  239.40    
Total input tokens:                      373233    
Total generated tokens:                  511918    
Request throughput (req/s):              10.69     
Output token throughput (tok/s):         2138.36   
Peak output token throughput (tok/s):    1103.00   
Peak concurrent requests:                284.00    
Total token throughput (tok/s):          3697.42   
---------------Time to First Token----------------
Mean TTFT (ms):                          11064.61  
Median TTFT (ms):                        11513.42  
P99 TTFT (ms):                           17183.28  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          60.38     
Median TPOT (ms):                        57.77     
P99 TPOT (ms):                           102.44    
---------------Inter-token Latency----------------
Mean ITL (ms):                           178.47    
Median ITL (ms):                         141.06    
P99 ITL (ms):                            421.66    
---------------Speculative Decoding---------------
Acceptance rate (%):                     39.76     
Acceptance length:                       2.99      
Drafts:                                  171421    
Draft tokens:                            857105    
Accepted tokens:                         340749    
Per-position acceptance (%):
  Position 0:                            70.05     
  Position 1:                            48.55     
  Position 2:                            34.91     
  Position 3:                            25.90     
  Position 4:                            19.36     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k5-t0.0-tp1...
[0;36m(APIServer pid=193204)[0;0m INFO 01-22 17:29:36 [launcher.py:110] Shutting down FastAPI HTTP server.
