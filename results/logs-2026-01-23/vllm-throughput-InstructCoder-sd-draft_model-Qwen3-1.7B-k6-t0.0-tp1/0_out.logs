Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k6-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k6-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 240291
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:15 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:15 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15011, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=240291)[0;0m WARNING 01-23 19:01:15 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:16 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:16 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:18 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:18 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:18 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=240291)[0;0m WARNING 01-23 19:01:18 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:01:18 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f97876c6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8326c6ec-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 19:01:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:01:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:01:29 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=6), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:01:30 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.68:54669 backend=nccl
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:01:30 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=240656)[0;0m WARNING 01-23 19:01:31 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:01:31 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:01:33 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 19:01:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:01:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:01:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:01:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:01:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:01:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:25 [default_loader.py:291] Loading weights took 50.71 seconds
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:25 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:25 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:25 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-23 19:02:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:32 [default_loader.py:291] Loading weights took 6.73 seconds
WARNING 01-23 19:02:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:33 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 60.673900 seconds
WARNING 01-23 19:02:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:48 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:02:48 [backends.py:704] Dynamo bytecode transform time: 14.05 s
WARNING 01-23 19:02:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:02:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:03:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:05 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.462 s
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:05 [monitor.py:34] torch.compile takes 18.52 s in total
WARNING 01-23 19:03:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:10 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:10 [backends.py:704] Dynamo bytecode transform time: 4.85 s
WARNING 01-23 19:03:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:16 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.650 s
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:16 [monitor.py:34] torch.compile takes 25.02 s in total
WARNING 01-23 19:03:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:18 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:18 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:18 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-23 19:03:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:03:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 19:03:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:38 [gpu_model_runner.py:4880] Graph capturing finished in 18 secs, took -0.06 GiB
WARNING 01-23 19:03:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=240656)[0;0m INFO 01-23 19:03:38 [core.py:272] init engine (profile, create kv cache, warmup model) took 64.80 seconds
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:40 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=240291)[0;0m WARNING 01-23 19:03:40 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:40 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:40 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:40 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [serving.py:221] Chat template warmup completed in 1748.5ms
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15011
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:42 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:53 [loggers.py:257] Engine 000: Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 37.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:03:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 25.63 tokens/s, Drafted throughput: 74.61 tokens/s, Accepted: 336 tokens, Drafted: 978 tokens, Per-position acceptance rate: 0.706, 0.460, 0.362, 0.270, 0.160, 0.104, Avg Draft acceptance rate: 34.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:03 [loggers.py:257] Engine 000: Avg prompt throughput: 39.6 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 34.70 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 347 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.696, 0.414, 0.287, 0.227, 0.193, 0.099, Avg Draft acceptance rate: 32.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:13 [loggers.py:257] Engine 000: Avg prompt throughput: 43.3 tokens/s, Avg generation throughput: 62.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 44.30 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 443 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.802, 0.538, 0.401, 0.297, 0.225, 0.170, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:23 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 37.10 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 371 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.762, 0.497, 0.354, 0.193, 0.144, 0.099, Avg Draft acceptance rate: 34.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:33 [loggers.py:257] Engine 000: Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 44.20 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 442 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.753, 0.522, 0.412, 0.324, 0.225, 0.192, Avg Draft acceptance rate: 40.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:43 [loggers.py:257] Engine 000: Avg prompt throughput: 26.8 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 36.00 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 360 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.656, 0.464, 0.344, 0.230, 0.164, 0.109, Avg Draft acceptance rate: 32.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:53 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 61.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:04:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 44.10 tokens/s, Drafted throughput: 108.60 tokens/s, Accepted: 441 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.757, 0.547, 0.398, 0.309, 0.249, 0.177, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:03 [loggers.py:257] Engine 000: Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 46.70 tokens/s, Drafted throughput: 108.00 tokens/s, Accepted: 467 tokens, Drafted: 1080 tokens, Per-position acceptance rate: 0.761, 0.567, 0.467, 0.367, 0.256, 0.178, Avg Draft acceptance rate: 43.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:13 [loggers.py:257] Engine 000: Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 51.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 32.80 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 328 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.683, 0.383, 0.284, 0.180, 0.142, 0.120, Avg Draft acceptance rate: 29.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:23 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 55.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 37.00 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 370 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.692, 0.500, 0.313, 0.225, 0.170, 0.132, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:33 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 60.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 43.19 tokens/s, Drafted throughput: 108.58 tokens/s, Accepted: 432 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.735, 0.525, 0.403, 0.309, 0.221, 0.193, Avg Draft acceptance rate: 39.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:43 [loggers.py:257] Engine 000: Avg prompt throughput: 59.4 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 38.80 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 388 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.735, 0.481, 0.337, 0.249, 0.199, 0.144, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:53 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 53.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:05:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 35.70 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 357 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.672, 0.443, 0.322, 0.246, 0.164, 0.104, Avg Draft acceptance rate: 32.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:03 [loggers.py:257] Engine 000: Avg prompt throughput: 73.5 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 43.69 tokens/s, Drafted throughput: 108.58 tokens/s, Accepted: 437 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.713, 0.519, 0.392, 0.331, 0.254, 0.204, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:13 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 36.50 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 365 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.721, 0.470, 0.339, 0.219, 0.148, 0.098, Avg Draft acceptance rate: 33.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:23 [loggers.py:257] Engine 000: Avg prompt throughput: 36.2 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 36.40 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 364 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.669, 0.470, 0.354, 0.227, 0.160, 0.133, Avg Draft acceptance rate: 33.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:33 [loggers.py:257] Engine 000: Avg prompt throughput: 44.9 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 40.60 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 406 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.731, 0.544, 0.368, 0.258, 0.187, 0.143, Avg Draft acceptance rate: 37.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  175.07    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.29      
Output token throughput (tok/s):         57.12     
Peak output token throughput (tok/s):    19.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          98.40     
---------------Time to First Token----------------
Mean TTFT (ms):                          67.44     
Median TTFT (ms):                        66.97     
P99 TTFT (ms):                           78.43     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.18     
Median TPOT (ms):                        17.11     
P99 TPOT (ms):                           20.76     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.97     
Median ITL (ms):                         53.97     
P99 ITL (ms):                            54.49     
---------------Speculative Decoding---------------
Acceptance rate (%):                     36.14     
Acceptance length:                       3.17      
Drafts:                                  3167      
Draft tokens:                            19002     
Accepted tokens:                         6867      
Per-position acceptance (%):
  Position 0:                            71.93     
  Position 1:                            49.07     
  Position 2:                            36.09     
  Position 3:                            26.18     
  Position 4:                            19.29     
  Position 5:                            14.27     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:43 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 34.60 tokens/s, Drafted throughput: 99.59 tokens/s, Accepted: 346 tokens, Drafted: 996 tokens, Per-position acceptance rate: 0.693, 0.464, 0.349, 0.247, 0.187, 0.145, Avg Draft acceptance rate: 34.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:06:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f5afc642fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-731542d9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:03 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 6.65 tokens/s, Drafted throughput: 20.70 tokens/s, Accepted: 133 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.696, 0.435, 0.333, 0.246, 0.130, 0.087, Avg Draft acceptance rate: 32.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:13 [loggers.py:257] Engine 000: Avg prompt throughput: 92.6 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 69.69 tokens/s, Drafted throughput: 196.77 tokens/s, Accepted: 697 tokens, Drafted: 1968 tokens, Per-position acceptance rate: 0.732, 0.470, 0.345, 0.262, 0.198, 0.119, Avg Draft acceptance rate: 35.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:23 [loggers.py:257] Engine 000: Avg prompt throughput: 74.9 tokens/s, Avg generation throughput: 116.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 80.59 tokens/s, Drafted throughput: 213.58 tokens/s, Accepted: 806 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.772, 0.497, 0.388, 0.250, 0.197, 0.160, Avg Draft acceptance rate: 37.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:33 [loggers.py:257] Engine 000: Avg prompt throughput: 87.5 tokens/s, Avg generation throughput: 118.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 83.69 tokens/s, Drafted throughput: 212.37 tokens/s, Accepted: 837 tokens, Drafted: 2124 tokens, Per-position acceptance rate: 0.743, 0.542, 0.410, 0.299, 0.212, 0.158, Avg Draft acceptance rate: 39.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:43 [loggers.py:257] Engine 000: Avg prompt throughput: 84.2 tokens/s, Avg generation throughput: 121.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 85.69 tokens/s, Drafted throughput: 212.38 tokens/s, Accepted: 857 tokens, Drafted: 2124 tokens, Per-position acceptance rate: 0.749, 0.523, 0.407, 0.322, 0.240, 0.181, Avg Draft acceptance rate: 40.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:53 [loggers.py:257] Engine 000: Avg prompt throughput: 79.8 tokens/s, Avg generation throughput: 102.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:07:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 66.19 tokens/s, Drafted throughput: 214.77 tokens/s, Accepted: 662 tokens, Drafted: 2148 tokens, Per-position acceptance rate: 0.684, 0.450, 0.296, 0.190, 0.131, 0.098, Avg Draft acceptance rate: 30.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:03 [loggers.py:257] Engine 000: Avg prompt throughput: 99.1 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 80.59 tokens/s, Drafted throughput: 211.18 tokens/s, Accepted: 806 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.733, 0.540, 0.384, 0.264, 0.205, 0.165, Avg Draft acceptance rate: 38.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:13 [loggers.py:257] Engine 000: Avg prompt throughput: 87.0 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 76.99 tokens/s, Drafted throughput: 213.58 tokens/s, Accepted: 770 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.688, 0.489, 0.357, 0.278, 0.197, 0.154, Avg Draft acceptance rate: 36.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:23 [loggers.py:257] Engine 000: Avg prompt throughput: 54.2 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 72.79 tokens/s, Drafted throughput: 214.78 tokens/s, Accepted: 728 tokens, Drafted: 2148 tokens, Per-position acceptance rate: 0.704, 0.486, 0.335, 0.223, 0.168, 0.117, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:33 [loggers.py:257] Engine 000: Avg prompt throughput: 63.4 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 70.29 tokens/s, Drafted throughput: 194.38 tokens/s, Accepted: 703 tokens, Drafted: 1944 tokens, Per-position acceptance rate: 0.713, 0.503, 0.358, 0.253, 0.191, 0.151, Avg Draft acceptance rate: 36.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  90.51     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.55      
Output token throughput (tok/s):         110.49    
Peak output token throughput (tok/s):    38.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          190.34    
---------------Time to First Token----------------
Mean TTFT (ms):                          113.48    
Median TTFT (ms):                        112.18    
P99 TTFT (ms):                           143.84    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.25     
Median TPOT (ms):                        17.39     
P99 TPOT (ms):                           20.39     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.50     
Median ITL (ms):                         54.40     
P99 ITL (ms):                            58.75     
---------------Speculative Decoding---------------
Acceptance rate (%):                     36.42     
Acceptance length:                       3.19      
Drafts:                                  3150      
Draft tokens:                            18900     
Accepted tokens:                         6884      
Per-position acceptance (%):
  Position 0:                            72.41     
  Position 1:                            49.94     
  Position 2:                            36.41     
  Position 3:                            26.00     
  Position 4:                            19.30     
  Position 5:                            14.48     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 1.80 tokens/s, Drafted throughput: 6.00 tokens/s, Accepted: 18 tokens, Drafted: 60 tokens, Per-position acceptance rate: 0.700, 0.300, 0.300, 0.200, 0.200, 0.100, Avg Draft acceptance rate: 30.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fef02bf2fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-23ccf8ef-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:53 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:08:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 7.10 tokens/s, Drafted throughput: 21.00 tokens/s, Accepted: 71 tokens, Drafted: 210 tokens, Per-position acceptance rate: 0.714, 0.486, 0.400, 0.257, 0.086, 0.086, Avg Draft acceptance rate: 33.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:03 [loggers.py:257] Engine 000: Avg prompt throughput: 147.4 tokens/s, Avg generation throughput: 177.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.21, Accepted throughput: 122.39 tokens/s, Drafted throughput: 332.96 tokens/s, Accepted: 1224 tokens, Drafted: 3330 tokens, Per-position acceptance rate: 0.715, 0.488, 0.364, 0.276, 0.211, 0.151, Avg Draft acceptance rate: 36.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:13 [loggers.py:257] Engine 000: Avg prompt throughput: 149.0 tokens/s, Avg generation throughput: 218.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 149.29 tokens/s, Drafted throughput: 419.98 tokens/s, Accepted: 1493 tokens, Drafted: 4200 tokens, Per-position acceptance rate: 0.703, 0.491, 0.357, 0.257, 0.181, 0.143, Avg Draft acceptance rate: 35.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:23 [loggers.py:257] Engine 000: Avg prompt throughput: 172.5 tokens/s, Avg generation throughput: 216.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 146.98 tokens/s, Drafted throughput: 416.35 tokens/s, Accepted: 1470 tokens, Drafted: 4164 tokens, Per-position acceptance rate: 0.707, 0.486, 0.352, 0.246, 0.184, 0.143, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:33 [loggers.py:257] Engine 000: Avg prompt throughput: 159.0 tokens/s, Avg generation throughput: 225.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 156.28 tokens/s, Drafted throughput: 418.75 tokens/s, Accepted: 1563 tokens, Drafted: 4188 tokens, Per-position acceptance rate: 0.728, 0.504, 0.371, 0.279, 0.206, 0.150, Avg Draft acceptance rate: 37.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  46.67     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.07      
Output token throughput (tok/s):         214.25    
Peak output token throughput (tok/s):    72.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          369.10    
---------------Time to First Token----------------
Mean TTFT (ms):                          114.24    
Median TTFT (ms):                        114.17    
P99 TTFT (ms):                           144.84    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.63     
Median TPOT (ms):                        17.59     
P99 TPOT (ms):                           21.47     
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.59     
Median ITL (ms):                         55.32     
P99 ITL (ms):                            61.71     
---------------Speculative Decoding---------------
Acceptance rate (%):                     36.29     
Acceptance length:                       3.18      
Drafts:                                  3156      
Draft tokens:                            18936     
Accepted tokens:                         6871      
Per-position acceptance (%):
  Position 0:                            71.45     
  Position 1:                            49.68     
  Position 2:                            36.09     
  Position 3:                            26.27     
  Position 4:                            19.42     
  Position 5:                            14.80     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:43 [loggers.py:257] Engine 000: Avg prompt throughput: 94.8 tokens/s, Avg generation throughput: 171.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 118.29 tokens/s, Drafted throughput: 325.78 tokens/s, Accepted: 1183 tokens, Drafted: 3258 tokens, Per-position acceptance rate: 0.718, 0.510, 0.355, 0.254, 0.190, 0.151, Avg Draft acceptance rate: 36.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:09:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f6b69d22fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-fae1c217-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:03 [loggers.py:257] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 22.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 7.30 tokens/s, Drafted throughput: 22.20 tokens/s, Accepted: 146 tokens, Drafted: 444 tokens, Per-position acceptance rate: 0.716, 0.432, 0.338, 0.257, 0.135, 0.095, Avg Draft acceptance rate: 32.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:13 [loggers.py:257] Engine 000: Avg prompt throughput: 303.6 tokens/s, Avg generation throughput: 408.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 277.09 tokens/s, Drafted throughput: 790.18 tokens/s, Accepted: 2771 tokens, Drafted: 7902 tokens, Per-position acceptance rate: 0.724, 0.482, 0.340, 0.240, 0.178, 0.139, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:23 [loggers.py:257] Engine 000: Avg prompt throughput: 349.6 tokens/s, Avg generation throughput: 432.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 299.17 tokens/s, Drafted throughput: 802.71 tokens/s, Accepted: 2992 tokens, Drafted: 8028 tokens, Per-position acceptance rate: 0.722, 0.504, 0.371, 0.274, 0.206, 0.159, Avg Draft acceptance rate: 37.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:33 [loggers.py:257] Engine 000: Avg prompt throughput: 255.2 tokens/s, Avg generation throughput: 410.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 277.67 tokens/s, Drafted throughput: 808.72 tokens/s, Accepted: 2777 tokens, Drafted: 8088 tokens, Per-position acceptance rate: 0.700, 0.473, 0.332, 0.234, 0.180, 0.141, Avg Draft acceptance rate: 34.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  39.44     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              2.03      
Output token throughput (tok/s):         405.63    
Peak output token throughput (tok/s):    144.00    
Peak concurrent requests:                13.00     
Total token throughput (tok/s):          702.00    
---------------Time to First Token----------------
Mean TTFT (ms):                          119.49    
Median TTFT (ms):                        118.39    
P99 TTFT (ms):                           156.66    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.54     
Median TPOT (ms):                        18.52     
P99 TPOT (ms):                           23.42     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.60     
Median ITL (ms):                         56.94     
P99 ITL (ms):                            70.62     
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.53     
Acceptance length:                       3.13      
Drafts:                                  5123      
Draft tokens:                            30738     
Accepted tokens:                         10922     
Per-position acceptance (%):
  Position 0:                            71.33     
  Position 1:                            48.84     
  Position 2:                            34.86     
  Position 3:                            24.93     
  Position 4:                            18.72     
  Position 5:                            14.52     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:43 [loggers.py:257] Engine 000: Avg prompt throughput: 224.9 tokens/s, Avg generation throughput: 346.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 236.88 tokens/s, Drafted throughput: 668.94 tokens/s, Accepted: 2369 tokens, Drafted: 6690 tokens, Per-position acceptance rate: 0.705, 0.496, 0.352, 0.248, 0.183, 0.141, Avg Draft acceptance rate: 35.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:10:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1276ab2fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8f23ffd1-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:03 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 6.30 tokens/s, Drafted throughput: 19.80 tokens/s, Accepted: 126 tokens, Drafted: 396 tokens, Per-position acceptance rate: 0.697, 0.439, 0.333, 0.242, 0.121, 0.076, Avg Draft acceptance rate: 31.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:13 [loggers.py:257] Engine 000: Avg prompt throughput: 688.8 tokens/s, Avg generation throughput: 745.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 516.04 tokens/s, Drafted throughput: 1380.43 tokens/s, Accepted: 5161 tokens, Drafted: 13806 tokens, Per-position acceptance rate: 0.731, 0.500, 0.372, 0.273, 0.208, 0.159, Avg Draft acceptance rate: 37.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:23 [loggers.py:257] Engine 000: Avg prompt throughput: 520.4 tokens/s, Avg generation throughput: 800.3 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 551.79 tokens/s, Drafted throughput: 1511.99 tokens/s, Accepted: 5518 tokens, Drafted: 15120 tokens, Per-position acceptance rate: 0.710, 0.494, 0.361, 0.270, 0.198, 0.156, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:33 [loggers.py:257] Engine 000: Avg prompt throughput: 683.1 tokens/s, Avg generation throughput: 774.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 530.33 tokens/s, Drafted throughput: 1484.19 tokens/s, Accepted: 5304 tokens, Drafted: 14844 tokens, Per-position acceptance rate: 0.703, 0.477, 0.355, 0.266, 0.198, 0.146, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:43 [loggers.py:257] Engine 000: Avg prompt throughput: 543.5 tokens/s, Avg generation throughput: 811.6 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 565.32 tokens/s, Drafted throughput: 1489.00 tokens/s, Accepted: 5654 tokens, Drafted: 14892 tokens, Per-position acceptance rate: 0.715, 0.509, 0.380, 0.283, 0.218, 0.173, Avg Draft acceptance rate: 38.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  43.07     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.71      
Output token throughput (tok/s):         742.78    
Peak output token throughput (tok/s):    272.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          1308.40   
---------------Time to First Token----------------
Mean TTFT (ms):                          129.57    
Median TTFT (ms):                        128.49    
P99 TTFT (ms):                           185.02    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.43     
Median TPOT (ms):                        19.46     
P99 TPOT (ms):                           23.77     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.72     
Median ITL (ms):                         60.47     
P99 ITL (ms):                            89.46     
---------------Speculative Decoding---------------
Acceptance rate (%):                     36.71     
Acceptance length:                       3.20      
Drafts:                                  10021     
Draft tokens:                            60126     
Accepted tokens:                         22075     
Per-position acceptance (%):
  Position 0:                            71.23     
  Position 1:                            49.36     
  Position 2:                            36.49     
  Position 3:                            27.10     
  Position 4:                            20.39     
  Position 5:                            15.72     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:11:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 44.50 tokens/s, Drafted throughput: 148.19 tokens/s, Accepted: 445 tokens, Drafted: 1482 tokens, Per-position acceptance rate: 0.636, 0.429, 0.287, 0.198, 0.142, 0.109, Avg Draft acceptance rate: 30.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efad9896fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8d803391-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:13 [loggers.py:257] Engine 000: Avg prompt throughput: 722.2 tokens/s, Avg generation throughput: 675.4 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 231.00 tokens/s, Drafted throughput: 634.06 tokens/s, Accepted: 4621 tokens, Drafted: 12684 tokens, Per-position acceptance rate: 0.719, 0.491, 0.360, 0.265, 0.198, 0.153, Avg Draft acceptance rate: 36.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1011.6 tokens/s, Avg generation throughput: 1357.9 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 931.14 tokens/s, Drafted throughput: 2586.77 tokens/s, Accepted: 9313 tokens, Drafted: 25872 tokens, Per-position acceptance rate: 0.711, 0.490, 0.354, 0.261, 0.194, 0.150, Avg Draft acceptance rate: 36.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1119.2 tokens/s, Avg generation throughput: 1378.6 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 961.25 tokens/s, Drafted throughput: 2544.28 tokens/s, Accepted: 9616 tokens, Drafted: 25452 tokens, Per-position acceptance rate: 0.720, 0.507, 0.378, 0.280, 0.215, 0.167, Avg Draft acceptance rate: 37.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:43 [loggers.py:257] Engine 000: Avg prompt throughput: 999.8 tokens/s, Avg generation throughput: 1368.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 931.64 tokens/s, Drafted throughput: 2645.27 tokens/s, Accepted: 9319 tokens, Drafted: 26460 tokens, Per-position acceptance rate: 0.702, 0.476, 0.344, 0.256, 0.190, 0.146, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:53 [loggers.py:257] Engine 000: Avg prompt throughput: 974.0 tokens/s, Avg generation throughput: 1341.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:12:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 910.53 tokens/s, Drafted throughput: 2612.20 tokens/s, Accepted: 9106 tokens, Drafted: 26124 tokens, Per-position acceptance rate: 0.695, 0.486, 0.343, 0.246, 0.183, 0.139, Avg Draft acceptance rate: 34.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  51.07     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.27      
Output token throughput (tok/s):         1252.99   
Peak output token throughput (tok/s):    480.00    
Peak concurrent requests:                51.00     
Total token throughput (tok/s):          2198.08   
---------------Time to First Token----------------
Mean TTFT (ms):                          156.44    
Median TTFT (ms):                        150.95    
P99 TTFT (ms):                           237.24    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.82     
Median TPOT (ms):                        22.81     
P99 TPOT (ms):                           28.92     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.25     
Median ITL (ms):                         66.95     
P99 ITL (ms):                            117.26    
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.81     
Acceptance length:                       3.15      
Drafts:                                  20392     
Draft tokens:                            122352    
Accepted tokens:                         43811     
Per-position acceptance (%):
  Position 0:                            70.62     
  Position 1:                            48.75     
  Position 2:                            35.29     
  Position 3:                            25.89     
  Position 4:                            19.37     
  Position 5:                            14.93     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:03 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 295.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 196.88 tokens/s, Drafted throughput: 617.35 tokens/s, Accepted: 1969 tokens, Drafted: 6174 tokens, Per-position acceptance rate: 0.664, 0.446, 0.312, 0.217, 0.155, 0.121, Avg Draft acceptance rate: 31.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fbd17a6efc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d52d0ae9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:23 [loggers.py:257] Engine 000: Avg prompt throughput: 948.5 tokens/s, Avg generation throughput: 452.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 162.80 tokens/s, Drafted throughput: 360.59 tokens/s, Accepted: 3256 tokens, Drafted: 7212 tokens, Per-position acceptance rate: 0.806, 0.580, 0.455, 0.355, 0.279, 0.234, Avg Draft acceptance rate: 45.1%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1062.9 tokens/s, Avg generation throughput: 1949.9 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 61.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 1324.89 tokens/s, Drafted throughput: 3793.50 tokens/s, Accepted: 13250 tokens, Drafted: 37938 tokens, Per-position acceptance rate: 0.697, 0.479, 0.344, 0.251, 0.186, 0.139, Avg Draft acceptance rate: 34.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1782.0 tokens/s, Avg generation throughput: 1813.3 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 45.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 1257.64 tokens/s, Drafted throughput: 3392.83 tokens/s, Accepted: 12577 tokens, Drafted: 33930 tokens, Per-position acceptance rate: 0.708, 0.493, 0.366, 0.280, 0.210, 0.166, Avg Draft acceptance rate: 37.1%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1191.0 tokens/s, Avg generation throughput: 1937.6 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 57.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:13:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 1317.14 tokens/s, Drafted throughput: 3745.95 tokens/s, Accepted: 13173 tokens, Drafted: 37464 tokens, Per-position acceptance rate: 0.698, 0.492, 0.346, 0.248, 0.185, 0.140, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1485.3 tokens/s, Avg generation throughput: 1882.1 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 47.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 1296.64 tokens/s, Drafted throughput: 3558.77 tokens/s, Accepted: 12968 tokens, Drafted: 35592 tokens, Per-position acceptance rate: 0.708, 0.490, 0.363, 0.272, 0.199, 0.155, Avg Draft acceptance rate: 36.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1263.0 tokens/s, Avg generation throughput: 1924.3 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 1306.82 tokens/s, Drafted throughput: 3733.64 tokens/s, Accepted: 13073 tokens, Drafted: 37350 tokens, Per-position acceptance rate: 0.695, 0.477, 0.345, 0.253, 0.187, 0.143, Avg Draft acceptance rate: 35.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1602.4 tokens/s, Avg generation throughput: 1844.8 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 50.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 1262.60 tokens/s, Drafted throughput: 3523.51 tokens/s, Accepted: 12627 tokens, Drafted: 35238 tokens, Per-position acceptance rate: 0.708, 0.491, 0.348, 0.259, 0.195, 0.149, Avg Draft acceptance rate: 35.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  69.54     
Total input tokens:                      94775     
Total generated tokens:                  127979    
Request throughput (req/s):              9.20      
Output token throughput (tok/s):         1840.48   
Peak output token throughput (tok/s):    768.00    
Peak concurrent requests:                90.00     
Total token throughput (tok/s):          3203.44   
---------------Time to First Token----------------
Mean TTFT (ms):                          251.22    
Median TTFT (ms):                        228.39    
P99 TTFT (ms):                           626.74    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          32.27     
Median TPOT (ms):                        32.36     
P99 TPOT (ms):                           42.07     
---------------Inter-token Latency----------------
Mean ITL (ms):                           100.75    
Median ITL (ms):                         90.06     
P99 ITL (ms):                            177.98    
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.79     
Acceptance length:                       3.15      
Drafts:                                  40791     
Draft tokens:                            244746    
Accepted tokens:                         87601     
Per-position acceptance (%):
  Position 0:                            70.37     
  Position 1:                            48.82     
  Position 2:                            35.22     
  Position 3:                            26.05     
  Position 4:                            19.41     
  Position 5:                            14.89     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:33 [loggers.py:257] Engine 000: Avg prompt throughput: 159.3 tokens/s, Avg generation throughput: 1012.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 680.98 tokens/s, Drafted throughput: 2043.55 tokens/s, Accepted: 6810 tokens, Drafted: 20436 tokens, Per-position acceptance rate: 0.684, 0.468, 0.323, 0.230, 0.170, 0.124, Avg Draft acceptance rate: 33.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f13149a6fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-812d9d3a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:53 [loggers.py:257] Engine 000: Avg prompt throughput: 722.5 tokens/s, Avg generation throughput: 33.2 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:14:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 9.40 tokens/s, Drafted throughput: 29.40 tokens/s, Accepted: 188 tokens, Drafted: 588 tokens, Per-position acceptance rate: 0.786, 0.367, 0.286, 0.224, 0.143, 0.112, Avg Draft acceptance rate: 32.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1221.1 tokens/s, Avg generation throughput: 2202.8 tokens/s, Running: 94 reqs, Waiting: 29 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 1504.68 tokens/s, Drafted throughput: 4162.79 tokens/s, Accepted: 15049 tokens, Drafted: 41634 tokens, Per-position acceptance rate: 0.709, 0.494, 0.358, 0.262, 0.195, 0.150, Avg Draft acceptance rate: 36.1%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1814.9 tokens/s, Avg generation throughput: 2068.1 tokens/s, Running: 102 reqs, Waiting: 24 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 1435.19 tokens/s, Drafted throughput: 3815.98 tokens/s, Accepted: 14352 tokens, Drafted: 38160 tokens, Per-position acceptance rate: 0.716, 0.504, 0.372, 0.282, 0.214, 0.169, Avg Draft acceptance rate: 37.6%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1634.1 tokens/s, Avg generation throughput: 1986.7 tokens/s, Running: 108 reqs, Waiting: 19 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 1360.52 tokens/s, Drafted throughput: 3794.40 tokens/s, Accepted: 13616 tokens, Drafted: 37974 tokens, Per-position acceptance rate: 0.704, 0.494, 0.354, 0.258, 0.193, 0.149, Avg Draft acceptance rate: 35.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1757.3 tokens/s, Avg generation throughput: 2039.1 tokens/s, Running: 124 reqs, Waiting: 3 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 1399.87 tokens/s, Drafted throughput: 3861.99 tokens/s, Accepted: 14006 tokens, Drafted: 38640 tokens, Per-position acceptance rate: 0.709, 0.489, 0.357, 0.264, 0.199, 0.157, Avg Draft acceptance rate: 36.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1538.6 tokens/s, Avg generation throughput: 1985.8 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 1340.92 tokens/s, Drafted throughput: 3910.25 tokens/s, Accepted: 13409 tokens, Drafted: 39102 tokens, Per-position acceptance rate: 0.681, 0.465, 0.337, 0.252, 0.187, 0.136, Avg Draft acceptance rate: 34.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1586.1 tokens/s, Avg generation throughput: 2020.7 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 89.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:15:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1380.96 tokens/s, Drafted throughput: 3873.50 tokens/s, Accepted: 13810 tokens, Drafted: 38736 tokens, Per-position acceptance rate: 0.707, 0.487, 0.345, 0.257, 0.194, 0.149, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1471.3 tokens/s, Avg generation throughput: 2068.9 tokens/s, Running: 120 reqs, Waiting: 0 reqs, GPU KV cache usage: 81.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1402.60 tokens/s, Drafted throughput: 4032.32 tokens/s, Accepted: 14027 tokens, Drafted: 40326 tokens, Per-position acceptance rate: 0.692, 0.474, 0.340, 0.252, 0.189, 0.141, Avg Draft acceptance rate: 34.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1498.1 tokens/s, Avg generation throughput: 2063.0 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 94.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 1401.38 tokens/s, Drafted throughput: 3977.05 tokens/s, Accepted: 14015 tokens, Drafted: 39774 tokens, Per-position acceptance rate: 0.695, 0.483, 0.347, 0.255, 0.189, 0.145, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1287.2 tokens/s, Avg generation throughput: 2015.3 tokens/s, Running: 115 reqs, Waiting: 5 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 1349.41 tokens/s, Drafted throughput: 4026.93 tokens/s, Accepted: 13495 tokens, Drafted: 40272 tokens, Per-position acceptance rate: 0.685, 0.460, 0.322, 0.239, 0.172, 0.132, Avg Draft acceptance rate: 33.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1286.5 tokens/s, Avg generation throughput: 2072.2 tokens/s, Running: 109 reqs, Waiting: 10 reqs, GPU KV cache usage: 94.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1403.48 tokens/s, Drafted throughput: 4036.45 tokens/s, Accepted: 14036 tokens, Drafted: 40368 tokens, Per-position acceptance rate: 0.688, 0.474, 0.338, 0.249, 0.190, 0.147, Avg Draft acceptance rate: 34.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1351.6 tokens/s, Avg generation throughput: 2184.0 tokens/s, Running: 100 reqs, Waiting: 25 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1486.76 tokens/s, Drafted throughput: 4216.47 tokens/s, Accepted: 14873 tokens, Drafted: 42180 tokens, Per-position acceptance rate: 0.697, 0.486, 0.346, 0.253, 0.189, 0.145, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1726.6 tokens/s, Avg generation throughput: 2069.5 tokens/s, Running: 102 reqs, Waiting: 21 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:16:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1413.96 tokens/s, Drafted throughput: 3962.16 tokens/s, Accepted: 14149 tokens, Drafted: 39648 tokens, Per-position acceptance rate: 0.702, 0.483, 0.343, 0.259, 0.199, 0.155, Avg Draft acceptance rate: 35.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  127.63    
Total input tokens:                      189093    
Total generated tokens:                  255968    
Request throughput (req/s):              10.03     
Output token throughput (tok/s):         2005.54   
Peak output token throughput (tok/s):    896.00    
Peak concurrent requests:                151.00    
Total token throughput (tok/s):          3487.11   
---------------Time to First Token----------------
Mean TTFT (ms):                          948.43    
Median TTFT (ms):                        524.06    
P99 TTFT (ms):                           4381.67   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          57.31     
Median TPOT (ms):                        56.34     
P99 TPOT (ms):                           87.72     
---------------Inter-token Latency----------------
Mean ITL (ms):                           176.68    
Median ITL (ms):                         160.38    
P99 ITL (ms):                            334.99    
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.25     
Acceptance length:                       3.11      
Drafts:                                  82364     
Draft tokens:                            494184    
Accepted tokens:                         174199    
Per-position acceptance (%):
  Position 0:                            69.74     
  Position 1:                            48.12     
  Position 2:                            34.45     
  Position 3:                            25.50     
  Position 4:                            19.06     
  Position 5:                            14.63     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:03 [loggers.py:257] Engine 000: Avg prompt throughput: 26.7 tokens/s, Avg generation throughput: 800.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 530.65 tokens/s, Drafted throughput: 1719.43 tokens/s, Accepted: 5307 tokens, Drafted: 17196 tokens, Per-position acceptance rate: 0.665, 0.443, 0.289, 0.204, 0.147, 0.103, Avg Draft acceptance rate: 30.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fcdb6b7efc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-5e681897-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:23 [loggers.py:257] Engine 000: Avg prompt throughput: 196.1 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 6.70 tokens/s, Drafted throughput: 21.00 tokens/s, Accepted: 134 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.700, 0.429, 0.329, 0.243, 0.129, 0.086, Avg Draft acceptance rate: 31.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:33 [loggers.py:257] Engine 000: Avg prompt throughput: 2651.9 tokens/s, Avg generation throughput: 2015.0 tokens/s, Running: 103 reqs, Waiting: 153 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 1383.69 tokens/s, Drafted throughput: 3685.18 tokens/s, Accepted: 13846 tokens, Drafted: 36876 tokens, Per-position acceptance rate: 0.723, 0.505, 0.371, 0.277, 0.210, 0.166, Avg Draft acceptance rate: 37.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:43 [loggers.py:257] Engine 000: Avg prompt throughput: 866.1 tokens/s, Avg generation throughput: 1735.3 tokens/s, Running: 116 reqs, Waiting: 139 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1186.65 tokens/s, Drafted throughput: 3324.47 tokens/s, Accepted: 11867 tokens, Drafted: 33246 tokens, Per-position acceptance rate: 0.693, 0.480, 0.353, 0.262, 0.199, 0.154, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1818.3 tokens/s, Avg generation throughput: 1998.4 tokens/s, Running: 132 reqs, Waiting: 123 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:17:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 1373.22 tokens/s, Drafted throughput: 3777.37 tokens/s, Accepted: 13733 tokens, Drafted: 37776 tokens, Per-position acceptance rate: 0.703, 0.497, 0.360, 0.266, 0.199, 0.156, Avg Draft acceptance rate: 36.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1549.2 tokens/s, Avg generation throughput: 1995.6 tokens/s, Running: 134 reqs, Waiting: 116 reqs, GPU KV cache usage: 94.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1362.22 tokens/s, Drafted throughput: 3823.58 tokens/s, Accepted: 13623 tokens, Drafted: 38238 tokens, Per-position acceptance rate: 0.704, 0.485, 0.349, 0.256, 0.194, 0.151, Avg Draft acceptance rate: 35.6%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1458.6 tokens/s, Avg generation throughput: 1926.1 tokens/s, Running: 137 reqs, Waiting: 112 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 1293.21 tokens/s, Drafted throughput: 3829.54 tokens/s, Accepted: 12933 tokens, Drafted: 38298 tokens, Per-position acceptance rate: 0.680, 0.466, 0.332, 0.243, 0.178, 0.128, Avg Draft acceptance rate: 33.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1290.0 tokens/s, Avg generation throughput: 2085.3 tokens/s, Running: 127 reqs, Waiting: 119 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 1413.51 tokens/s, Drafted throughput: 4068.05 tokens/s, Accepted: 14137 tokens, Drafted: 40686 tokens, Per-position acceptance rate: 0.694, 0.474, 0.337, 0.250, 0.188, 0.142, Avg Draft acceptance rate: 34.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1020.6 tokens/s, Avg generation throughput: 2087.3 tokens/s, Running: 112 reqs, Waiting: 135 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 1404.88 tokens/s, Drafted throughput: 4119.54 tokens/s, Accepted: 14049 tokens, Drafted: 41196 tokens, Per-position acceptance rate: 0.684, 0.469, 0.331, 0.245, 0.181, 0.137, Avg Draft acceptance rate: 34.1%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1404.6 tokens/s, Avg generation throughput: 2189.9 tokens/s, Running: 100 reqs, Waiting: 152 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.21, Accepted throughput: 1509.47 tokens/s, Drafted throughput: 4092.25 tokens/s, Accepted: 15096 tokens, Drafted: 40926 tokens, Per-position acceptance rate: 0.716, 0.502, 0.365, 0.272, 0.202, 0.157, Avg Draft acceptance rate: 36.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1773.7 tokens/s, Avg generation throughput: 1914.5 tokens/s, Running: 111 reqs, Waiting: 145 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:18:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1296.94 tokens/s, Drafted throughput: 3725.82 tokens/s, Accepted: 12970 tokens, Drafted: 37260 tokens, Per-position acceptance rate: 0.693, 0.471, 0.339, 0.254, 0.187, 0.145, Avg Draft acceptance rate: 34.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1651.3 tokens/s, Avg generation throughput: 1908.7 tokens/s, Running: 122 reqs, Waiting: 133 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 1289.95 tokens/s, Drafted throughput: 3732.17 tokens/s, Accepted: 12901 tokens, Drafted: 37326 tokens, Per-position acceptance rate: 0.689, 0.468, 0.334, 0.249, 0.188, 0.146, Avg Draft acceptance rate: 34.6%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1806.1 tokens/s, Avg generation throughput: 1960.4 tokens/s, Running: 134 reqs, Waiting: 119 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1333.86 tokens/s, Drafted throughput: 3768.20 tokens/s, Accepted: 13340 tokens, Drafted: 37686 tokens, Per-position acceptance rate: 0.694, 0.478, 0.346, 0.259, 0.195, 0.151, Avg Draft acceptance rate: 35.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1512.3 tokens/s, Avg generation throughput: 1859.2 tokens/s, Running: 141 reqs, Waiting: 108 reqs, GPU KV cache usage: 97.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 1244.32 tokens/s, Drafted throughput: 3721.26 tokens/s, Accepted: 12443 tokens, Drafted: 37212 tokens, Per-position acceptance rate: 0.685, 0.466, 0.321, 0.230, 0.171, 0.133, Avg Draft acceptance rate: 33.4%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1073.0 tokens/s, Avg generation throughput: 2068.0 tokens/s, Running: 124 reqs, Waiting: 121 reqs, GPU KV cache usage: 92.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 1396.77 tokens/s, Drafted throughput: 4044.22 tokens/s, Accepted: 13969 tokens, Drafted: 40446 tokens, Per-position acceptance rate: 0.693, 0.475, 0.331, 0.247, 0.186, 0.139, Avg Draft acceptance rate: 34.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:43 [loggers.py:257] Engine 000: Avg prompt throughput: 912.7 tokens/s, Avg generation throughput: 2163.6 tokens/s, Running: 99 reqs, Waiting: 151 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1473.30 tokens/s, Drafted throughput: 4179.02 tokens/s, Accepted: 14735 tokens, Drafted: 41796 tokens, Per-position acceptance rate: 0.698, 0.488, 0.347, 0.252, 0.188, 0.142, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1713.4 tokens/s, Avg generation throughput: 2057.2 tokens/s, Running: 98 reqs, Waiting: 157 reqs, GPU KV cache usage: 98.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:19:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1397.56 tokens/s, Drafted throughput: 3958.08 tokens/s, Accepted: 13976 tokens, Drafted: 39582 tokens, Per-position acceptance rate: 0.699, 0.477, 0.342, 0.258, 0.196, 0.147, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1778.3 tokens/s, Avg generation throughput: 1963.6 tokens/s, Running: 109 reqs, Waiting: 146 reqs, GPU KV cache usage: 98.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 1337.65 tokens/s, Drafted throughput: 3767.57 tokens/s, Accepted: 13378 tokens, Drafted: 37680 tokens, Per-position acceptance rate: 0.701, 0.477, 0.344, 0.256, 0.197, 0.155, Avg Draft acceptance rate: 35.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1682.5 tokens/s, Avg generation throughput: 1976.4 tokens/s, Running: 128 reqs, Waiting: 128 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 1350.75 tokens/s, Drafted throughput: 3758.79 tokens/s, Accepted: 13519 tokens, Drafted: 37620 tokens, Per-position acceptance rate: 0.704, 0.478, 0.352, 0.268, 0.200, 0.155, Avg Draft acceptance rate: 35.9%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1759.1 tokens/s, Avg generation throughput: 1941.8 tokens/s, Running: 141 reqs, Waiting: 113 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 1322.11 tokens/s, Drafted throughput: 3723.04 tokens/s, Accepted: 13221 tokens, Drafted: 37230 tokens, Per-position acceptance rate: 0.706, 0.480, 0.350, 0.257, 0.191, 0.146, Avg Draft acceptance rate: 35.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1409.7 tokens/s, Avg generation throughput: 1903.8 tokens/s, Running: 143 reqs, Waiting: 104 reqs, GPU KV cache usage: 96.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 1263.43 tokens/s, Drafted throughput: 3854.78 tokens/s, Accepted: 12635 tokens, Drafted: 38550 tokens, Per-position acceptance rate: 0.680, 0.454, 0.316, 0.225, 0.165, 0.127, Avg Draft acceptance rate: 32.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:43 [loggers.py:257] Engine 000: Avg prompt throughput: 915.6 tokens/s, Avg generation throughput: 2071.4 tokens/s, Running: 120 reqs, Waiting: 123 reqs, GPU KV cache usage: 92.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1403.02 tokens/s, Drafted throughput: 4036.87 tokens/s, Accepted: 14032 tokens, Drafted: 40374 tokens, Per-position acceptance rate: 0.698, 0.479, 0.341, 0.250, 0.180, 0.137, Avg Draft acceptance rate: 34.8%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:53 [loggers.py:257] Engine 000: Avg prompt throughput: 983.5 tokens/s, Avg generation throughput: 2184.4 tokens/s, Running: 98 reqs, Waiting: 153 reqs, GPU KV cache usage: 95.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:20:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 1490.64 tokens/s, Drafted throughput: 4192.36 tokens/s, Accepted: 14908 tokens, Drafted: 41928 tokens, Per-position acceptance rate: 0.700, 0.486, 0.351, 0.260, 0.192, 0.145, Avg Draft acceptance rate: 35.6%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1747.6 tokens/s, Avg generation throughput: 2108.0 tokens/s, Running: 100 reqs, Waiting: 154 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1438.78 tokens/s, Drafted throughput: 4024.93 tokens/s, Accepted: 14398 tokens, Drafted: 40278 tokens, Per-position acceptance rate: 0.712, 0.489, 0.349, 0.261, 0.189, 0.144, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1668.7 tokens/s, Avg generation throughput: 1980.3 tokens/s, Running: 112 reqs, Waiting: 143 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1346.35 tokens/s, Drafted throughput: 3814.65 tokens/s, Accepted: 13464 tokens, Drafted: 38148 tokens, Per-position acceptance rate: 0.694, 0.476, 0.344, 0.257, 0.196, 0.150, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1722.4 tokens/s, Avg generation throughput: 1972.5 tokens/s, Running: 130 reqs, Waiting: 88 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 1358.18 tokens/s, Drafted throughput: 3719.67 tokens/s, Accepted: 13583 tokens, Drafted: 37200 tokens, Per-position acceptance rate: 0.710, 0.504, 0.358, 0.269, 0.198, 0.152, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:33 [loggers.py:257] Engine 000: Avg prompt throughput: 969.1 tokens/s, Avg generation throughput: 2113.6 tokens/s, Running: 101 reqs, Waiting: 0 reqs, GPU KV cache usage: 78.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1435.18 tokens/s, Drafted throughput: 4112.35 tokens/s, Accepted: 14352 tokens, Drafted: 41124 tokens, Per-position acceptance rate: 0.688, 0.476, 0.342, 0.255, 0.189, 0.144, Avg Draft acceptance rate: 34.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  256.40    
Total input tokens:                      373233    
Total generated tokens:                  511918    
Request throughput (req/s):              9.98      
Output token throughput (tok/s):         1996.57   
Peak output token throughput (tok/s):    994.00    
Peak concurrent requests:                281.00    
Total token throughput (tok/s):          3452.25   
---------------Time to First Token----------------
Mean TTFT (ms):                          12084.60  
Median TTFT (ms):                        12447.33  
P99 TTFT (ms):                           17853.34  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          63.71     
Median TPOT (ms):                        61.20     
P99 TPOT (ms):                           110.10    
---------------Inter-token Latency----------------
Mean ITL (ms):                           195.77    
Median ITL (ms):                         159.21    
P99 ITL (ms):                            412.83    
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.15     
Acceptance length:                       3.11      
Drafts:                                  164929    
Draft tokens:                            989574    
Accepted tokens:                         347799    
Per-position acceptance (%):
  Position 0:                            69.70     
  Position 1:                            47.93     
  Position 2:                            34.34     
  Position 3:                            25.43     
  Position 4:                            18.96     
  Position 5:                            14.52     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-1.7B-k6-t0.0-tp1...
[0;36m(APIServer pid=240291)[0;0m INFO 01-23 19:21:40 [launcher.py:110] Shutting down FastAPI HTTP server.
