Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k1-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k1-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 199636
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:45 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:45 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15015, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 1, 'max_model_len': 5000}}
[0;36m(APIServer pid=199636)[0;0m WARNING 01-22 17:30:45 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:47 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:47 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:48 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:48 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:48 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=199636)[0;0m WARNING 01-22 17:30:48 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:30:48 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efe40922fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c3a8bcd5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 17:30:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:30:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:30:59 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=1), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:00 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.49:43927 backend=nccl
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:00 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=200002)[0;0m WARNING 01-22 17:31:01 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:01 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:02 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 17:31:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:44 [default_loader.py:291] Loading weights took 41.61 seconds
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:44 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:44 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:31:44 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-22 17:31:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:31:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:01 [default_loader.py:291] Loading weights took 15.90 seconds
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:02 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 60.318348 seconds
WARNING 01-22 17:32:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:32:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:32:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:14 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:14 [backends.py:704] Dynamo bytecode transform time: 11.98 s
WARNING 01-22 17:32:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:32:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:32:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:32 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.911 s
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:32 [monitor.py:34] torch.compile takes 16.89 s in total
WARNING 01-22 17:32:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:32:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:38 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:38 [backends.py:704] Dynamo bytecode transform time: 6.10 s
WARNING 01-22 17:32:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:47 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.272 s
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:47 [monitor.py:34] torch.compile takes 25.26 s in total
WARNING 01-22 17:32:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:49 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:49 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:32:49 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-22 17:32:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
WARNING 01-22 17:32:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:33:03 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took 0.10 GiB
WARNING 01-22 17:33:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15015)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15015 ssl:default [Connect call failed (\'127.0.0.1\', 15015)]\n''
[0;36m(EngineCore_DP0 pid=200002)[0;0m INFO 01-22 17:33:03 [core.py:272] init engine (profile, create kv cache, warmup model) took 60.61 seconds
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:05 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=199636)[0;0m WARNING 01-22 17:33:05 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:05 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:05 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:05 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [serving.py:221] Chat template warmup completed in 1732.7ms
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15015
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:07 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:18 [loggers.py:257] Engine 000: Avg prompt throughput: 28.1 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 12.34 tokens/s, Drafted throughput: 16.33 tokens/s, Accepted: 161 tokens, Drafted: 213 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:28 [loggers.py:257] Engine 000: Avg prompt throughput: 32.8 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.50 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 185 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.781, Avg Draft acceptance rate: 78.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:38 [loggers.py:257] Engine 000: Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 41.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 18.00 tokens/s, Drafted throughput: 23.60 tokens/s, Accepted: 180 tokens, Drafted: 236 tokens, Per-position acceptance rate: 0.763, Avg Draft acceptance rate: 76.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:48 [loggers.py:257] Engine 000: Avg prompt throughput: 25.9 tokens/s, Avg generation throughput: 42.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.50 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 185 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.781, Avg Draft acceptance rate: 78.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:58 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 42.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:33:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.50 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 185 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.777, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:08 [loggers.py:257] Engine 000: Avg prompt throughput: 30.3 tokens/s, Avg generation throughput: 41.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 17.40 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 174 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.734, Avg Draft acceptance rate: 73.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:18 [loggers.py:257] Engine 000: Avg prompt throughput: 21.0 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 19.60 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 196 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.824, Avg Draft acceptance rate: 82.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:28 [loggers.py:257] Engine 000: Avg prompt throughput: 26.8 tokens/s, Avg generation throughput: 41.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 17.70 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 177 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.747, Avg Draft acceptance rate: 74.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:38 [loggers.py:257] Engine 000: Avg prompt throughput: 39.7 tokens/s, Avg generation throughput: 41.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 17.20 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 172 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.723, Avg Draft acceptance rate: 72.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:48 [loggers.py:257] Engine 000: Avg prompt throughput: 28.8 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 19.10 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 191 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.806, Avg Draft acceptance rate: 80.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:58 [loggers.py:257] Engine 000: Avg prompt throughput: 44.8 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:34:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 19.90 tokens/s, Drafted throughput: 23.60 tokens/s, Accepted: 199 tokens, Drafted: 236 tokens, Per-position acceptance rate: 0.843, Avg Draft acceptance rate: 84.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:08 [loggers.py:257] Engine 000: Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 40.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 17.00 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 170 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.717, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:18 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 41.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 17.40 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 174 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.734, Avg Draft acceptance rate: 73.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:28 [loggers.py:257] Engine 000: Avg prompt throughput: 38.6 tokens/s, Avg generation throughput: 41.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 17.20 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 172 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.726, Avg Draft acceptance rate: 72.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:38 [loggers.py:257] Engine 000: Avg prompt throughput: 31.1 tokens/s, Avg generation throughput: 43.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 19.10 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 191 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.803, Avg Draft acceptance rate: 80.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:48 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 42.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.60 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 186 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.785, Avg Draft acceptance rate: 78.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:58 [loggers.py:257] Engine 000: Avg prompt throughput: 40.0 tokens/s, Avg generation throughput: 42.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:35:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 19.00 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 190 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.802, Avg Draft acceptance rate: 80.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:08 [loggers.py:257] Engine 000: Avg prompt throughput: 14.0 tokens/s, Avg generation throughput: 41.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 17.90 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 179 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.752, Avg Draft acceptance rate: 75.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:18 [loggers.py:257] Engine 000: Avg prompt throughput: 36.1 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 19.40 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 194 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.819, Avg Draft acceptance rate: 81.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:28 [loggers.py:257] Engine 000: Avg prompt throughput: 36.9 tokens/s, Avg generation throughput: 40.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 17.10 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 171 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.722, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:38 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.50 tokens/s, Drafted throughput: 23.60 tokens/s, Accepted: 185 tokens, Drafted: 236 tokens, Per-position acceptance rate: 0.784, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:48 [loggers.py:257] Engine 000: Avg prompt throughput: 38.5 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.60 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 186 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.782, Avg Draft acceptance rate: 78.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:58 [loggers.py:257] Engine 000: Avg prompt throughput: 25.2 tokens/s, Avg generation throughput: 42.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:36:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 18.30 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 183 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.772, Avg Draft acceptance rate: 77.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:08 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 18.50 tokens/s, Drafted throughput: 23.70 tokens/s, Accepted: 185 tokens, Drafted: 237 tokens, Per-position acceptance rate: 0.781, Avg Draft acceptance rate: 78.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  238.07    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.21      
Output token throughput (tok/s):         42.01     
Peak output token throughput (tok/s):    25.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          72.36     
---------------Time to First Token----------------
Mean TTFT (ms):                          56.17     
Median TTFT (ms):                        56.00     
P99 TTFT (ms):                           65.98     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.57     
Median TPOT (ms):                        23.55     
P99 TPOT (ms):                           24.75     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.66     
Median ITL (ms):                         41.66     
P99 ITL (ms):                            42.14     
---------------Speculative Decoding---------------
Acceptance rate (%):                     77.13     
Acceptance length:                       1.77      
Drafts:                                  5628      
Draft tokens:                            5628      
Accepted tokens:                         4341      
Per-position acceptance (%):
  Position 0:                            77.13     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 5.60 tokens/s, Drafted throughput: 7.40 tokens/s, Accepted: 56 tokens, Drafted: 74 tokens, Per-position acceptance rate: 0.757, Avg Draft acceptance rate: 75.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0357e0afc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-4850cb9a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:38 [loggers.py:257] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 11.95 tokens/s, Drafted throughput: 15.00 tokens/s, Accepted: 239 tokens, Drafted: 300 tokens, Per-position acceptance rate: 0.797, Avg Draft acceptance rate: 79.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:48 [loggers.py:257] Engine 000: Avg prompt throughput: 57.0 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 35.00 tokens/s, Drafted throughput: 46.79 tokens/s, Accepted: 350 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.748, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:58 [loggers.py:257] Engine 000: Avg prompt throughput: 54.8 tokens/s, Avg generation throughput: 82.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:37:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 36.00 tokens/s, Drafted throughput: 46.60 tokens/s, Accepted: 360 tokens, Drafted: 466 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:08 [loggers.py:257] Engine 000: Avg prompt throughput: 67.9 tokens/s, Avg generation throughput: 83.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 36.50 tokens/s, Drafted throughput: 46.80 tokens/s, Accepted: 365 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.780, Avg Draft acceptance rate: 78.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:18 [loggers.py:257] Engine 000: Avg prompt throughput: 68.5 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 35.20 tokens/s, Drafted throughput: 47.00 tokens/s, Accepted: 352 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.749, Avg Draft acceptance rate: 74.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:28 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 38.79 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 388 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.822, Avg Draft acceptance rate: 82.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:38 [loggers.py:257] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 81.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 34.20 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 342 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.725, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:48 [loggers.py:257] Engine 000: Avg prompt throughput: 64.7 tokens/s, Avg generation throughput: 84.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 37.00 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 370 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.784, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:58 [loggers.py:257] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 84.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:38:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 37.10 tokens/s, Drafted throughput: 46.99 tokens/s, Accepted: 371 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.789, Avg Draft acceptance rate: 78.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:08 [loggers.py:257] Engine 000: Avg prompt throughput: 73.5 tokens/s, Avg generation throughput: 83.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 36.20 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 362 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.767, Avg Draft acceptance rate: 76.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:18 [loggers.py:257] Engine 000: Avg prompt throughput: 44.3 tokens/s, Avg generation throughput: 83.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 35.90 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 359 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.761, Avg Draft acceptance rate: 76.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:28 [loggers.py:257] Engine 000: Avg prompt throughput: 63.7 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 36.50 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 365 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  121.08    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.41      
Output token throughput (tok/s):         82.59     
Peak output token throughput (tok/s):    50.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          142.28    
---------------Time to First Token----------------
Mean TTFT (ms):                          88.35     
Median TTFT (ms):                        88.41     
P99 TTFT (ms):                           97.38     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.61     
Median TPOT (ms):                        23.65     
P99 TPOT (ms):                           24.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.74     
Median ITL (ms):                         41.62     
P99 ITL (ms):                            43.57     
---------------Speculative Decoding---------------
Acceptance rate (%):                     77.19     
Acceptance length:                       1.77      
Drafts:                                  5628      
Draft tokens:                            5628      
Accepted tokens:                         4344      
Per-position acceptance (%):
  Position 0:                            77.19     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:38 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 47.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 20.70 tokens/s, Drafted throughput: 26.70 tokens/s, Accepted: 207 tokens, Drafted: 267 tokens, Per-position acceptance rate: 0.775, Avg Draft acceptance rate: 77.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f73ffc0efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f0527c28-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:58 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:39:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 6.85 tokens/s, Drafted throughput: 8.55 tokens/s, Accepted: 137 tokens, Drafted: 171 tokens, Per-position acceptance rate: 0.801, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:08 [loggers.py:257] Engine 000: Avg prompt throughput: 107.9 tokens/s, Avg generation throughput: 166.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 72.40 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 724 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.777, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:18 [loggers.py:257] Engine 000: Avg prompt throughput: 116.3 tokens/s, Avg generation throughput: 166.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 72.30 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 723 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.776, Avg Draft acceptance rate: 77.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:28 [loggers.py:257] Engine 000: Avg prompt throughput: 120.4 tokens/s, Avg generation throughput: 167.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 73.09 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 731 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.784, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:38 [loggers.py:257] Engine 000: Avg prompt throughput: 118.5 tokens/s, Avg generation throughput: 165.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 72.09 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 721 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.777, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:48 [loggers.py:257] Engine 000: Avg prompt throughput: 117.8 tokens/s, Avg generation throughput: 164.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 71.20 tokens/s, Drafted throughput: 93.20 tokens/s, Accepted: 712 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.764, Avg Draft acceptance rate: 76.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:58 [loggers.py:257] Engine 000: Avg prompt throughput: 82.2 tokens/s, Avg generation throughput: 149.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:40:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 64.89 tokens/s, Drafted throughput: 83.99 tokens/s, Accepted: 649 tokens, Drafted: 840 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  62.71     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.80      
Output token throughput (tok/s):         159.46    
Peak output token throughput (tok/s):    96.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          274.71    
---------------Time to First Token----------------
Mean TTFT (ms):                          88.94     
Median TTFT (ms):                        88.96     
P99 TTFT (ms):                           98.14     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.79     
Median TPOT (ms):                        23.76     
P99 TPOT (ms):                           25.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.17     
Median ITL (ms):                         42.02     
P99 ITL (ms):                            48.49     
---------------Speculative Decoding---------------
Acceptance rate (%):                     77.61     
Acceptance length:                       1.78      
Drafts:                                  5613      
Draft tokens:                            5613      
Accepted tokens:                         4356      
Per-position acceptance (%):
  Position 0:                            77.61     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 4.50 tokens/s, Drafted throughput: 5.90 tokens/s, Accepted: 45 tokens, Drafted: 59 tokens, Per-position acceptance rate: 0.763, Avg Draft acceptance rate: 76.3%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe16888afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a6342280-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:18 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.90, Accepted throughput: 2.60 tokens/s, Drafted throughput: 2.90 tokens/s, Accepted: 26 tokens, Drafted: 29 tokens, Per-position acceptance rate: 0.897, Avg Draft acceptance rate: 89.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:28 [loggers.py:257] Engine 000: Avg prompt throughput: 215.3 tokens/s, Avg generation throughput: 210.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 91.99 tokens/s, Drafted throughput: 116.99 tokens/s, Accepted: 920 tokens, Drafted: 1170 tokens, Per-position acceptance rate: 0.786, Avg Draft acceptance rate: 78.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:38 [loggers.py:257] Engine 000: Avg prompt throughput: 253.6 tokens/s, Avg generation throughput: 324.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 140.88 tokens/s, Drafted throughput: 182.38 tokens/s, Accepted: 1409 tokens, Drafted: 1824 tokens, Per-position acceptance rate: 0.772, Avg Draft acceptance rate: 77.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:48 [loggers.py:257] Engine 000: Avg prompt throughput: 235.3 tokens/s, Avg generation throughput: 324.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 142.39 tokens/s, Drafted throughput: 181.59 tokens/s, Accepted: 1424 tokens, Drafted: 1816 tokens, Per-position acceptance rate: 0.784, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:58 [loggers.py:257] Engine 000: Avg prompt throughput: 226.1 tokens/s, Avg generation throughput: 326.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:41:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 142.78 tokens/s, Drafted throughput: 183.17 tokens/s, Accepted: 1428 tokens, Drafted: 1832 tokens, Per-position acceptance rate: 0.779, Avg Draft acceptance rate: 77.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:08 [loggers.py:257] Engine 000: Avg prompt throughput: 238.7 tokens/s, Avg generation throughput: 324.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 140.99 tokens/s, Drafted throughput: 182.38 tokens/s, Accepted: 1410 tokens, Drafted: 1824 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  50.61     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.58      
Output token throughput (tok/s):         316.13    
Peak output token throughput (tok/s):    192.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          547.10    
---------------Time to First Token----------------
Mean TTFT (ms):                          94.22     
Median TTFT (ms):                        93.06     
P99 TTFT (ms):                           125.27    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.27     
Median TPOT (ms):                        24.25     
P99 TPOT (ms):                           25.61     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.98     
Median ITL (ms):                         42.62     
P99 ITL (ms):                            51.00     
---------------Speculative Decoding---------------
Acceptance rate (%):                     77.53     
Acceptance length:                       1.78      
Drafts:                                  8988      
Draft tokens:                            8988      
Accepted tokens:                         6968      
Per-position acceptance (%):
  Position 0:                            77.53     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 43.70 tokens/s, Drafted throughput: 60.60 tokens/s, Accepted: 437 tokens, Drafted: 606 tokens, Per-position acceptance rate: 0.721, Avg Draft acceptance rate: 72.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb676642fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-707ed30f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:38 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 205.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 45.15 tokens/s, Drafted throughput: 56.70 tokens/s, Accepted: 903 tokens, Drafted: 1134 tokens, Per-position acceptance rate: 0.796, Avg Draft acceptance rate: 79.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:48 [loggers.py:257] Engine 000: Avg prompt throughput: 488.8 tokens/s, Avg generation throughput: 614.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 266.85 tokens/s, Drafted throughput: 345.54 tokens/s, Accepted: 2669 tokens, Drafted: 3456 tokens, Per-position acceptance rate: 0.772, Avg Draft acceptance rate: 77.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:58 [loggers.py:257] Engine 000: Avg prompt throughput: 464.7 tokens/s, Avg generation throughput: 618.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:42:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 269.56 tokens/s, Drafted throughput: 347.15 tokens/s, Accepted: 2696 tokens, Drafted: 3472 tokens, Per-position acceptance rate: 0.776, Avg Draft acceptance rate: 77.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:08 [loggers.py:257] Engine 000: Avg prompt throughput: 497.1 tokens/s, Avg generation throughput: 610.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 262.86 tokens/s, Drafted throughput: 345.55 tokens/s, Accepted: 2629 tokens, Drafted: 3456 tokens, Per-position acceptance rate: 0.761, Avg Draft acceptance rate: 76.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:18 [loggers.py:257] Engine 000: Avg prompt throughput: 492.5 tokens/s, Avg generation throughput: 619.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 273.35 tokens/s, Drafted throughput: 343.94 tokens/s, Accepted: 2734 tokens, Drafted: 3440 tokens, Per-position acceptance rate: 0.795, Avg Draft acceptance rate: 79.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  53.44     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              2.99      
Output token throughput (tok/s):         598.61    
Peak output token throughput (tok/s):    368.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          1054.44   
---------------Time to First Token----------------
Mean TTFT (ms):                          102.03    
Median TTFT (ms):                        99.24     
P99 TTFT (ms):                           151.12    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          25.54     
Median TPOT (ms):                        25.58     
P99 TPOT (ms):                           27.09     
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.24     
Median ITL (ms):                         44.32     
P99 ITL (ms):                            59.33     
---------------Speculative Decoding---------------
Acceptance rate (%):                     77.55     
Acceptance length:                       1.78      
Drafts:                                  17970     
Draft tokens:                            17970     
Accepted tokens:                         13936     
Per-position acceptance (%):
  Position 0:                            77.55     
==================================================
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:28 [loggers.py:257] Engine 000: Avg prompt throughput: 277.3 tokens/s, Avg generation throughput: 551.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 239.09 tokens/s, Drafted throughput: 312.49 tokens/s, Accepted: 2391 tokens, Drafted: 3125 tokens, Per-position acceptance rate: 0.765, Avg Draft acceptance rate: 76.5%
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd875606fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-21f9ce5f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:48 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 2.30 tokens/s, Drafted throughput: 2.95 tokens/s, Accepted: 46 tokens, Drafted: 59 tokens, Per-position acceptance rate: 0.780, Avg Draft acceptance rate: 78.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:58 [loggers.py:257] Engine 000: Avg prompt throughput: 930.0 tokens/s, Avg generation throughput: 823.5 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:43:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 360.40 tokens/s, Drafted throughput: 457.48 tokens/s, Accepted: 3605 tokens, Drafted: 4576 tokens, Per-position acceptance rate: 0.788, Avg Draft acceptance rate: 78.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:08 [loggers.py:257] Engine 000: Avg prompt throughput: 851.7 tokens/s, Avg generation throughput: 1146.9 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 36.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 495.93 tokens/s, Drafted throughput: 647.61 tokens/s, Accepted: 4960 tokens, Drafted: 6477 tokens, Per-position acceptance rate: 0.766, Avg Draft acceptance rate: 76.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:18 [loggers.py:257] Engine 000: Avg prompt throughput: 683.6 tokens/s, Avg generation throughput: 1178.1 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 55.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 514.64 tokens/s, Drafted throughput: 660.62 tokens/s, Accepted: 5147 tokens, Drafted: 6607 tokens, Per-position acceptance rate: 0.779, Avg Draft acceptance rate: 77.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:28 [loggers.py:257] Engine 000: Avg prompt throughput: 887.8 tokens/s, Avg generation throughput: 1151.5 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 50.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 501.67 tokens/s, Drafted throughput: 646.63 tokens/s, Accepted: 5018 tokens, Drafted: 6468 tokens, Per-position acceptance rate: 0.776, Avg Draft acceptance rate: 77.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:38 [loggers.py:257] Engine 000: Avg prompt throughput: 980.1 tokens/s, Avg generation throughput: 1136.3 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 44.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 493.87 tokens/s, Drafted throughput: 639.83 tokens/s, Accepted: 4940 tokens, Drafted: 6400 tokens, Per-position acceptance rate: 0.772, Avg Draft acceptance rate: 77.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  57.48     
Total input tokens:                      48266     
Total generated tokens:                  63974     
Request throughput (req/s):              5.57      
Output token throughput (tok/s):         1113.05   
Peak output token throughput (tok/s):    704.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          1952.80   
---------------Time to First Token----------------
Mean TTFT (ms):                          118.27    
Median TTFT (ms):                        116.64    
P99 TTFT (ms):                           166.15    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          27.29     
Median TPOT (ms):                        27.28     
P99 TPOT (ms):                           29.56     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.21     
Median ITL (ms):                         46.09     
P99 ITL (ms):                            87.02     
---------------Speculative Decoding---------------
Acceptance rate (%):                     77.05     
Acceptance length:                       1.77      
Drafts:                                  36028     
Draft tokens:                            36028     
Accepted tokens:                         27760     
Per-position acceptance (%):
  Position 0:                            77.05     
==================================================
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:48 [loggers.py:257] Engine 000: Avg prompt throughput: 492.2 tokens/s, Avg generation throughput: 969.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 412.94 tokens/s, Drafted throughput: 555.32 tokens/s, Accepted: 4130 tokens, Drafted: 5554 tokens, Per-position acceptance rate: 0.744, Avg Draft acceptance rate: 74.4%
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:44:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3912d32fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a8f85cd9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:08 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 2.30 tokens/s, Drafted throughput: 3.00 tokens/s, Accepted: 46 tokens, Drafted: 60 tokens, Per-position acceptance rate: 0.767, Avg Draft acceptance rate: 76.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1359.2 tokens/s, Avg generation throughput: 1272.9 tokens/s, Running: 53 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 552.01 tokens/s, Drafted throughput: 711.58 tokens/s, Accepted: 5521 tokens, Drafted: 7117 tokens, Per-position acceptance rate: 0.776, Avg Draft acceptance rate: 77.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1475.2 tokens/s, Avg generation throughput: 1803.5 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 87.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 790.67 tokens/s, Drafted throughput: 1006.48 tokens/s, Accepted: 7910 tokens, Drafted: 10069 tokens, Per-position acceptance rate: 0.786, Avg Draft acceptance rate: 78.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:38 [loggers.py:257] Engine 000: Avg prompt throughput: 984.9 tokens/s, Avg generation throughput: 1836.9 tokens/s, Running: 53 reqs, Waiting: 7 reqs, GPU KV cache usage: 94.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 798.56 tokens/s, Drafted throughput: 1032.64 tokens/s, Accepted: 7986 tokens, Drafted: 10327 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1764.9 tokens/s, Avg generation throughput: 1779.8 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 76.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 773.77 tokens/s, Drafted throughput: 997.88 tokens/s, Accepted: 7741 tokens, Drafted: 9983 tokens, Per-position acceptance rate: 0.775, Avg Draft acceptance rate: 77.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:58 [loggers.py:257] Engine 000: Avg prompt throughput: 952.6 tokens/s, Avg generation throughput: 1907.1 tokens/s, Running: 54 reqs, Waiting: 8 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:45:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 819.09 tokens/s, Drafted throughput: 1083.89 tokens/s, Accepted: 8191 tokens, Drafted: 10839 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1731.6 tokens/s, Avg generation throughput: 1750.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 72.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 757.59 tokens/s, Drafted throughput: 983.60 tokens/s, Accepted: 7579 tokens, Drafted: 9840 tokens, Per-position acceptance rate: 0.770, Avg Draft acceptance rate: 77.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1025.7 tokens/s, Avg generation throughput: 1880.4 tokens/s, Running: 56 reqs, Waiting: 8 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 807.12 tokens/s, Drafted throughput: 1069.10 tokens/s, Accepted: 8072 tokens, Drafted: 10692 tokens, Per-position acceptance rate: 0.755, Avg Draft acceptance rate: 75.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  74.66     
Total input tokens:                      94775     
Total generated tokens:                  127954    
Request throughput (req/s):              8.57      
Output token throughput (tok/s):         1713.90   
Peak output token throughput (tok/s):    1280.00   
Peak concurrent requests:                103.00    
Total token throughput (tok/s):          2983.38   
---------------Time to First Token----------------
Mean TTFT (ms):                          269.32    
Median TTFT (ms):                        229.68    
P99 TTFT (ms):                           942.32    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          34.00     
Median TPOT (ms):                        33.80     
P99 TPOT (ms):                           42.40     
---------------Inter-token Latency----------------
Mean ITL (ms):                           59.96     
Median ITL (ms):                         51.19     
P99 ITL (ms):                            163.21    
---------------Speculative Decoding---------------
Acceptance rate (%):                     76.85     
Acceptance length:                       1.77      
Drafts:                                  72093     
Draft tokens:                            72093     
Accepted tokens:                         55400     
Per-position acceptance (%):
  Position 0:                            76.85     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:28 [loggers.py:257] Engine 000: Avg prompt throughput: 180.5 tokens/s, Avg generation throughput: 571.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 244.01 tokens/s, Drafted throughput: 327.91 tokens/s, Accepted: 2440 tokens, Drafted: 3279 tokens, Per-position acceptance rate: 0.744, Avg Draft acceptance rate: 74.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1a9226efc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-674035ee-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:48 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 4.25 tokens/s, Drafted throughput: 5.60 tokens/s, Accepted: 85 tokens, Drafted: 112 tokens, Per-position acceptance rate: 0.759, Avg Draft acceptance rate: 75.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1932.1 tokens/s, Avg generation throughput: 1576.0 tokens/s, Running: 77 reqs, Waiting: 47 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:46:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 684.71 tokens/s, Drafted throughput: 874.58 tokens/s, Accepted: 6848 tokens, Drafted: 8747 tokens, Per-position acceptance rate: 0.783, Avg Draft acceptance rate: 78.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1421.4 tokens/s, Avg generation throughput: 2010.9 tokens/s, Running: 65 reqs, Waiting: 62 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 876.85 tokens/s, Drafted throughput: 1124.53 tokens/s, Accepted: 8769 tokens, Drafted: 11246 tokens, Per-position acceptance rate: 0.780, Avg Draft acceptance rate: 78.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1007.6 tokens/s, Avg generation throughput: 1820.9 tokens/s, Running: 56 reqs, Waiting: 72 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 782.32 tokens/s, Drafted throughput: 1029.84 tokens/s, Accepted: 7826 tokens, Drafted: 10302 tokens, Per-position acceptance rate: 0.760, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1801.8 tokens/s, Avg generation throughput: 1663.7 tokens/s, Running: 82 reqs, Waiting: 40 reqs, GPU KV cache usage: 90.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 717.17 tokens/s, Drafted throughput: 933.86 tokens/s, Accepted: 7172 tokens, Drafted: 9339 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1515.2 tokens/s, Avg generation throughput: 1991.8 tokens/s, Running: 78 reqs, Waiting: 50 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 864.77 tokens/s, Drafted throughput: 1115.15 tokens/s, Accepted: 8652 tokens, Drafted: 11157 tokens, Per-position acceptance rate: 0.775, Avg Draft acceptance rate: 77.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:48 [loggers.py:257] Engine 000: Avg prompt throughput: 930.8 tokens/s, Avg generation throughput: 1840.6 tokens/s, Running: 61 reqs, Waiting: 65 reqs, GPU KV cache usage: 94.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 781.52 tokens/s, Drafted throughput: 1050.60 tokens/s, Accepted: 7816 tokens, Drafted: 10507 tokens, Per-position acceptance rate: 0.744, Avg Draft acceptance rate: 74.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1100.4 tokens/s, Avg generation throughput: 1733.4 tokens/s, Running: 54 reqs, Waiting: 68 reqs, GPU KV cache usage: 90.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:47:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 744.43 tokens/s, Drafted throughput: 979.01 tokens/s, Accepted: 7445 tokens, Drafted: 9791 tokens, Per-position acceptance rate: 0.760, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1952.5 tokens/s, Avg generation throughput: 1738.6 tokens/s, Running: 94 reqs, Waiting: 34 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 748.15 tokens/s, Drafted throughput: 975.01 tokens/s, Accepted: 7486 tokens, Drafted: 9756 tokens, Per-position acceptance rate: 0.767, Avg Draft acceptance rate: 76.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1159.0 tokens/s, Avg generation throughput: 1993.7 tokens/s, Running: 73 reqs, Waiting: 55 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 861.40 tokens/s, Drafted throughput: 1121.28 tokens/s, Accepted: 8618 tokens, Drafted: 11218 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1014.0 tokens/s, Avg generation throughput: 1784.3 tokens/s, Running: 58 reqs, Waiting: 70 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 763.81 tokens/s, Drafted throughput: 1011.32 tokens/s, Accepted: 7641 tokens, Drafted: 10117 tokens, Per-position acceptance rate: 0.755, Avg Draft acceptance rate: 75.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1431.8 tokens/s, Avg generation throughput: 1619.0 tokens/s, Running: 71 reqs, Waiting: 45 reqs, GPU KV cache usage: 90.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 696.54 tokens/s, Drafted throughput: 909.33 tokens/s, Accepted: 6966 tokens, Drafted: 9094 tokens, Per-position acceptance rate: 0.766, Avg Draft acceptance rate: 76.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1719.1 tokens/s, Avg generation throughput: 1830.4 tokens/s, Running: 88 reqs, Waiting: 40 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 797.69 tokens/s, Drafted throughput: 1018.77 tokens/s, Accepted: 7981 tokens, Drafted: 10193 tokens, Per-position acceptance rate: 0.783, Avg Draft acceptance rate: 78.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1059.6 tokens/s, Avg generation throughput: 1907.9 tokens/s, Running: 67 reqs, Waiting: 61 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:48:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 820.69 tokens/s, Drafted throughput: 1080.20 tokens/s, Accepted: 8210 tokens, Drafted: 10806 tokens, Per-position acceptance rate: 0.760, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:08 [loggers.py:257] Engine 000: Avg prompt throughput: 858.3 tokens/s, Avg generation throughput: 1809.8 tokens/s, Running: 56 reqs, Waiting: 10 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 779.69 tokens/s, Drafted throughput: 1022.59 tokens/s, Accepted: 7797 tokens, Drafted: 10226 tokens, Per-position acceptance rate: 0.762, Avg Draft acceptance rate: 76.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  144.29    
Total input tokens:                      189093    
Total generated tokens:                  255975    
Request throughput (req/s):              8.87      
Output token throughput (tok/s):         1773.98   
Peak output token throughput (tok/s):    1657.00   
Peak concurrent requests:                153.00    
Total token throughput (tok/s):          3084.44   
---------------Time to First Token----------------
Mean TTFT (ms):                          4500.22   
Median TTFT (ms):                        4149.35   
P99 TTFT (ms):                           8067.48   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          46.64     
Median TPOT (ms):                        41.55     
P99 TPOT (ms):                           71.17     
---------------Inter-token Latency----------------
Mean ITL (ms):                           81.99     
Median ITL (ms):                         53.49     
P99 ITL (ms):                            287.35    
---------------Speculative Decoding---------------
Acceptance rate (%):                     76.60     
Acceptance length:                       1.77      
Drafts:                                  144070    
Draft tokens:                            144070    
Accepted tokens:                         110364    
Per-position acceptance (%):
  Position 0:                            76.60     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 269.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 113.80 tokens/s, Drafted throughput: 157.20 tokens/s, Accepted: 1138 tokens, Drafted: 1572 tokens, Per-position acceptance rate: 0.724, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f22b1ab2fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15015, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c434ed7b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:38 [loggers.py:257] Engine 000: Avg prompt throughput: 707.2 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 47 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 4.90 tokens/s, Drafted throughput: 6.25 tokens/s, Accepted: 98 tokens, Drafted: 125 tokens, Per-position acceptance rate: 0.784, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1342.2 tokens/s, Avg generation throughput: 1878.7 tokens/s, Running: 72 reqs, Waiting: 183 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 822.93 tokens/s, Drafted throughput: 1042.43 tokens/s, Accepted: 8233 tokens, Drafted: 10429 tokens, Per-position acceptance rate: 0.789, Avg Draft acceptance rate: 78.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1359.5 tokens/s, Avg generation throughput: 1900.9 tokens/s, Running: 59 reqs, Waiting: 196 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:49:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 820.29 tokens/s, Drafted throughput: 1071.66 tokens/s, Accepted: 8204 tokens, Drafted: 10718 tokens, Per-position acceptance rate: 0.765, Avg Draft acceptance rate: 76.5%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1208.1 tokens/s, Avg generation throughput: 1727.6 tokens/s, Running: 67 reqs, Waiting: 185 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 741.31 tokens/s, Drafted throughput: 975.12 tokens/s, Accepted: 7413 tokens, Drafted: 9751 tokens, Per-position acceptance rate: 0.760, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1905.9 tokens/s, Avg generation throughput: 1806.6 tokens/s, Running: 86 reqs, Waiting: 169 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 784.35 tokens/s, Drafted throughput: 1009.13 tokens/s, Accepted: 7844 tokens, Drafted: 10092 tokens, Per-position acceptance rate: 0.777, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1110.6 tokens/s, Avg generation throughput: 1952.0 tokens/s, Running: 67 reqs, Waiting: 188 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 840.14 tokens/s, Drafted throughput: 1102.23 tokens/s, Accepted: 8402 tokens, Drafted: 11023 tokens, Per-position acceptance rate: 0.762, Avg Draft acceptance rate: 76.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1035.7 tokens/s, Avg generation throughput: 1812.7 tokens/s, Running: 59 reqs, Waiting: 196 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 773.65 tokens/s, Drafted throughput: 1029.97 tokens/s, Accepted: 7739 tokens, Drafted: 10303 tokens, Per-position acceptance rate: 0.751, Avg Draft acceptance rate: 75.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1481.9 tokens/s, Avg generation throughput: 1606.3 tokens/s, Running: 76 reqs, Waiting: 167 reqs, GPU KV cache usage: 88.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 689.55 tokens/s, Drafted throughput: 903.73 tokens/s, Accepted: 6896 tokens, Drafted: 9038 tokens, Per-position acceptance rate: 0.763, Avg Draft acceptance rate: 76.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1576.5 tokens/s, Avg generation throughput: 1942.7 tokens/s, Running: 86 reqs, Waiting: 170 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:50:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 845.96 tokens/s, Drafted throughput: 1083.24 tokens/s, Accepted: 8464 tokens, Drafted: 10838 tokens, Per-position acceptance rate: 0.781, Avg Draft acceptance rate: 78.1%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1100.4 tokens/s, Avg generation throughput: 1925.8 tokens/s, Running: 66 reqs, Waiting: 190 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 826.78 tokens/s, Drafted throughput: 1090.37 tokens/s, Accepted: 8271 tokens, Drafted: 10908 tokens, Per-position acceptance rate: 0.758, Avg Draft acceptance rate: 75.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1067.1 tokens/s, Avg generation throughput: 1752.1 tokens/s, Running: 55 reqs, Waiting: 201 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 750.18 tokens/s, Drafted throughput: 992.37 tokens/s, Accepted: 7502 tokens, Drafted: 9924 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:28 [loggers.py:257] Engine 000: Avg prompt throughput: 2016.6 tokens/s, Avg generation throughput: 1531.3 tokens/s, Running: 102 reqs, Waiting: 151 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 658.47 tokens/s, Drafted throughput: 856.36 tokens/s, Accepted: 6585 tokens, Drafted: 8564 tokens, Per-position acceptance rate: 0.769, Avg Draft acceptance rate: 76.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1104.6 tokens/s, Avg generation throughput: 2006.2 tokens/s, Running: 75 reqs, Waiting: 180 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 874.79 tokens/s, Drafted throughput: 1122.26 tokens/s, Accepted: 8749 tokens, Drafted: 11224 tokens, Per-position acceptance rate: 0.779, Avg Draft acceptance rate: 77.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:48 [loggers.py:257] Engine 000: Avg prompt throughput: 964.5 tokens/s, Avg generation throughput: 1859.3 tokens/s, Running: 61 reqs, Waiting: 195 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 797.27 tokens/s, Drafted throughput: 1053.96 tokens/s, Accepted: 7976 tokens, Drafted: 10544 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1145.7 tokens/s, Avg generation throughput: 1719.5 tokens/s, Running: 54 reqs, Waiting: 191 reqs, GPU KV cache usage: 89.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:51:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 743.51 tokens/s, Drafted throughput: 965.76 tokens/s, Accepted: 7437 tokens, Drafted: 9660 tokens, Per-position acceptance rate: 0.770, Avg Draft acceptance rate: 77.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:08 [loggers.py:257] Engine 000: Avg prompt throughput: 2094.1 tokens/s, Avg generation throughput: 1711.0 tokens/s, Running: 92 reqs, Waiting: 164 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 737.49 tokens/s, Drafted throughput: 958.27 tokens/s, Accepted: 7379 tokens, Drafted: 9588 tokens, Per-position acceptance rate: 0.770, Avg Draft acceptance rate: 77.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1020.2 tokens/s, Avg generation throughput: 1961.3 tokens/s, Running: 65 reqs, Waiting: 191 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 844.33 tokens/s, Drafted throughput: 1108.22 tokens/s, Accepted: 8447 tokens, Drafted: 11087 tokens, Per-position acceptance rate: 0.762, Avg Draft acceptance rate: 76.2%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1105.7 tokens/s, Avg generation throughput: 1784.4 tokens/s, Running: 59 reqs, Waiting: 196 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 766.29 tokens/s, Drafted throughput: 1008.20 tokens/s, Accepted: 7666 tokens, Drafted: 10086 tokens, Per-position acceptance rate: 0.760, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1437.3 tokens/s, Avg generation throughput: 1644.6 tokens/s, Running: 75 reqs, Waiting: 169 reqs, GPU KV cache usage: 88.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 709.44 tokens/s, Drafted throughput: 922.62 tokens/s, Accepted: 7095 tokens, Drafted: 9227 tokens, Per-position acceptance rate: 0.769, Avg Draft acceptance rate: 76.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1709.0 tokens/s, Avg generation throughput: 1854.2 tokens/s, Running: 90 reqs, Waiting: 166 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 812.41 tokens/s, Drafted throughput: 1029.00 tokens/s, Accepted: 8128 tokens, Drafted: 10295 tokens, Per-position acceptance rate: 0.790, Avg Draft acceptance rate: 79.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:58 [loggers.py:257] Engine 000: Avg prompt throughput: 858.7 tokens/s, Avg generation throughput: 1914.2 tokens/s, Running: 63 reqs, Waiting: 192 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:52:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 824.72 tokens/s, Drafted throughput: 1080.89 tokens/s, Accepted: 8248 tokens, Drafted: 10810 tokens, Per-position acceptance rate: 0.763, Avg Draft acceptance rate: 76.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1181.1 tokens/s, Avg generation throughput: 1890.5 tokens/s, Running: 54 reqs, Waiting: 202 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 813.27 tokens/s, Drafted throughput: 1066.56 tokens/s, Accepted: 8136 tokens, Drafted: 10670 tokens, Per-position acceptance rate: 0.763, Avg Draft acceptance rate: 76.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1886.7 tokens/s, Avg generation throughput: 1624.9 tokens/s, Running: 96 reqs, Waiting: 156 reqs, GPU KV cache usage: 95.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 703.66 tokens/s, Drafted throughput: 904.79 tokens/s, Accepted: 7039 tokens, Drafted: 9051 tokens, Per-position acceptance rate: 0.778, Avg Draft acceptance rate: 77.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1221.0 tokens/s, Avg generation throughput: 2022.9 tokens/s, Running: 76 reqs, Waiting: 180 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 884.09 tokens/s, Drafted throughput: 1129.46 tokens/s, Accepted: 8842 tokens, Drafted: 11296 tokens, Per-position acceptance rate: 0.783, Avg Draft acceptance rate: 78.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:38 [loggers.py:257] Engine 000: Avg prompt throughput: 953.3 tokens/s, Avg generation throughput: 1848.3 tokens/s, Running: 60 reqs, Waiting: 196 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 793.16 tokens/s, Drafted throughput: 1046.25 tokens/s, Accepted: 7935 tokens, Drafted: 10467 tokens, Per-position acceptance rate: 0.758, Avg Draft acceptance rate: 75.8%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1125.9 tokens/s, Avg generation throughput: 1775.2 tokens/s, Running: 52 reqs, Waiting: 196 reqs, GPU KV cache usage: 85.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 761.16 tokens/s, Drafted throughput: 1003.24 tokens/s, Accepted: 7612 tokens, Drafted: 10033 tokens, Per-position acceptance rate: 0.759, Avg Draft acceptance rate: 75.9%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:58 [loggers.py:257] Engine 000: Avg prompt throughput: 2023.1 tokens/s, Avg generation throughput: 1746.8 tokens/s, Running: 96 reqs, Waiting: 123 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:53:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 758.73 tokens/s, Drafted throughput: 972.86 tokens/s, Accepted: 7593 tokens, Drafted: 9736 tokens, Per-position acceptance rate: 0.780, Avg Draft acceptance rate: 78.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:54:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1085.8 tokens/s, Avg generation throughput: 2045.5 tokens/s, Running: 74 reqs, Waiting: 69 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:54:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 887.74 tokens/s, Drafted throughput: 1147.73 tokens/s, Accepted: 8881 tokens, Drafted: 11482 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:54:18 [loggers.py:257] Engine 000: Avg prompt throughput: 501.5 tokens/s, Avg generation throughput: 1883.8 tokens/s, Running: 40 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:54:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 809.06 tokens/s, Drafted throughput: 1070.98 tokens/s, Accepted: 8093 tokens, Drafted: 10713 tokens, Per-position acceptance rate: 0.755, Avg Draft acceptance rate: 75.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  281.88    
Total input tokens:                      373233    
Total generated tokens:                  511917    
Request throughput (req/s):              9.08      
Output token throughput (tok/s):         1816.06   
Peak output token throughput (tok/s):    1785.00   
Peak concurrent requests:                287.00    
Total token throughput (tok/s):          3140.12   
---------------Time to First Token----------------
Mean TTFT (ms):                          17747.37  
Median TTFT (ms):                        17371.03  
P99 TTFT (ms):                           23084.68  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          47.29     
Median TPOT (ms):                        41.70     
P99 TPOT (ms):                           71.28     
---------------Inter-token Latency----------------
Mean ITL (ms):                           83.22     
Median ITL (ms):                         53.66     
P99 ITL (ms):                            309.98    
---------------Speculative Decoding---------------
Acceptance rate (%):                     76.79     
Acceptance length:                       1.77      
Drafts:                                  287802    
Draft tokens:                            287802    
Accepted tokens:                         221003    
Per-position acceptance (%):
  Position 0:                            76.79     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k1-t0.0-tp1...
[0;36m(APIServer pid=199636)[0;0m INFO 01-22 17:54:19 [launcher.py:110] Shutting down FastAPI HTTP server.
