Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k2-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k2-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 207481
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:21 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:21 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15016, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 2, 'max_model_len': 5000}}
[0;36m(APIServer pid=207481)[0;0m WARNING 01-22 17:55:21 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:22 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:22 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:23 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:23 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:23 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=207481)[0;0m WARNING 01-22 17:55:23 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:55:23 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f232ace6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-24b38517-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 17:55:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:55:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:55:34 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=2), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:55:36 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.49:42753 backend=nccl
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:55:36 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=207582)[0;0m WARNING 01-22 17:55:36 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:55:36 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:55:37 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 17:55:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:55:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:55:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:55:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:55:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:20 [default_loader.py:291] Loading weights took 41.58 seconds
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:20 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:20 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:20 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-22 17:56:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:26 [default_loader.py:291] Loading weights took 4.98 seconds
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:27 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 49.308177 seconds
WARNING 01-22 17:56:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:39 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:39 [backends.py:704] Dynamo bytecode transform time: 11.73 s
WARNING 01-22 17:56:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:56:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:54 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.390 s
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:56:54 [monitor.py:34] torch.compile takes 14.12 s in total
WARNING 01-22 17:56:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:00 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:00 [backends.py:704] Dynamo bytecode transform time: 6.12 s
WARNING 01-22 17:57:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:08 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 0.904 s
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:08 [monitor.py:34] torch.compile takes 21.15 s in total
WARNING 01-22 17:57:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:09 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:09 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:09 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-22 17:57:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:57:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
WARNING 01-22 17:57:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:24 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took 0.05 GiB
[0;36m(EngineCore_DP0 pid=207582)[0;0m INFO 01-22 17:57:24 [core.py:272] init engine (profile, create kv cache, warmup model) took 57.49 seconds
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:26 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=207481)[0;0m WARNING 01-22 17:57:26 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:26 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:26 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:26 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:28 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:28 [serving.py:221] Chat template warmup completed in 1769.4ms
WARNING 01-22 17:57:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15016)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15016 ssl:default [Connect call failed (\'127.0.0.1\', 15016)]\n''
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:28 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15016
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:29 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:39 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 10.33 tokens/s, Drafted throughput: 14.85 tokens/s, Accepted: 135 tokens, Drafted: 194 tokens, Per-position acceptance rate: 0.784, 0.608, Avg Draft acceptance rate: 69.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:49 [loggers.py:257] Engine 000: Avg prompt throughput: 32.8 tokens/s, Avg generation throughput: 48.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 27.40 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 274 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.760, 0.558, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:59 [loggers.py:257] Engine 000: Avg prompt throughput: 24.1 tokens/s, Avg generation throughput: 48.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:57:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 27.40 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 274 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.779, 0.538, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:09 [loggers.py:257] Engine 000: Avg prompt throughput: 43.3 tokens/s, Avg generation throughput: 48.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 27.20 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 272 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.734, 0.580, Avg Draft acceptance rate: 65.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:19 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 27.50 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 275 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.760, 0.562, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:29 [loggers.py:257] Engine 000: Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 25.00 tokens/s, Drafted throughput: 41.20 tokens/s, Accepted: 250 tokens, Drafted: 412 tokens, Per-position acceptance rate: 0.680, 0.534, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:39 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 51.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 30.20 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 302 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.808, 0.644, Avg Draft acceptance rate: 72.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:49 [loggers.py:257] Engine 000: Avg prompt throughput: 48.3 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 27.20 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 272 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.768, 0.546, Avg Draft acceptance rate: 65.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:59 [loggers.py:257] Engine 000: Avg prompt throughput: 28.8 tokens/s, Avg generation throughput: 50.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:58:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 29.80 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 298 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.807, 0.633, Avg Draft acceptance rate: 72.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:09 [loggers.py:257] Engine 000: Avg prompt throughput: 44.8 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 30.30 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 303 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.826, 0.638, Avg Draft acceptance rate: 73.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:19 [loggers.py:257] Engine 000: Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 46.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 26.00 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 260 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.726, 0.524, Avg Draft acceptance rate: 62.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:29 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 46.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 25.60 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 256 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.696, 0.541, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:39 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 48.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 27.90 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 279 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.758, 0.589, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:49 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 27.60 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 276 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.758, 0.575, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:59 [loggers.py:257] Engine 000: Avg prompt throughput: 49.2 tokens/s, Avg generation throughput: 48.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 17:59:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 28.10 tokens/s, Drafted throughput: 41.20 tokens/s, Accepted: 281 tokens, Drafted: 412 tokens, Per-position acceptance rate: 0.757, 0.607, Avg Draft acceptance rate: 68.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:09 [loggers.py:257] Engine 000: Avg prompt throughput: 14.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 27.70 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 277 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.746, 0.579, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:19 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 31.30 tokens/s, Drafted throughput: 41.20 tokens/s, Accepted: 313 tokens, Drafted: 412 tokens, Per-position acceptance rate: 0.830, 0.689, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:29 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 26.90 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 269 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.731, 0.562, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:39 [loggers.py:257] Engine 000: Avg prompt throughput: 17.4 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 28.10 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 281 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.773, 0.585, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:49 [loggers.py:257] Engine 000: Avg prompt throughput: 48.4 tokens/s, Avg generation throughput: 50.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 29.20 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 292 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.769, 0.635, Avg Draft acceptance rate: 70.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:59 [loggers.py:257] Engine 000: Avg prompt throughput: 25.0 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:00:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 27.80 tokens/s, Drafted throughput: 41.79 tokens/s, Accepted: 278 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.737, 0.593, Avg Draft acceptance rate: 66.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  205.78    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.24      
Output token throughput (tok/s):         48.60     
Peak output token throughput (tok/s):    22.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          83.72     
---------------Time to First Token----------------
Mean TTFT (ms):                          62.17     
Median TTFT (ms):                        61.97     
P99 TTFT (ms):                           70.57     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.29     
Median TPOT (ms):                        20.28     
P99 TPOT (ms):                           22.34     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.49     
Median ITL (ms):                         47.52     
P99 ITL (ms):                            48.00     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.45     
Acceptance length:                       2.35      
Drafts:                                  4251      
Draft tokens:                            8502      
Accepted tokens:                         5735      
Per-position acceptance (%):
  Position 0:                            76.22     
  Position 1:                            58.69     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:09 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 22.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 13.30 tokens/s, Drafted throughput: 18.20 tokens/s, Accepted: 133 tokens, Drafted: 182 tokens, Per-position acceptance rate: 0.846, 0.615, Avg Draft acceptance rate: 73.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7cc4d92fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-55e15639-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:29 [loggers.py:257] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 14.95 tokens/s, Drafted throughput: 22.20 tokens/s, Accepted: 299 tokens, Drafted: 444 tokens, Per-position acceptance rate: 0.779, 0.568, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:39 [loggers.py:257] Engine 000: Avg prompt throughput: 82.9 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 53.75 tokens/s, Drafted throughput: 81.52 tokens/s, Accepted: 538 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.760, 0.559, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:49 [loggers.py:257] Engine 000: Avg prompt throughput: 49.0 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 52.30 tokens/s, Drafted throughput: 82.40 tokens/s, Accepted: 523 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.726, 0.544, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:59 [loggers.py:257] Engine 000: Avg prompt throughput: 47.8 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:01:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 54.58 tokens/s, Drafted throughput: 82.38 tokens/s, Accepted: 546 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.755, 0.570, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:09 [loggers.py:257] Engine 000: Avg prompt throughput: 102.5 tokens/s, Avg generation throughput: 97.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 56.69 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 567 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.787, 0.603, Avg Draft acceptance rate: 69.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:19 [loggers.py:257] Engine 000: Avg prompt throughput: 45.6 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 53.19 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 532 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.731, 0.561, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:29 [loggers.py:257] Engine 000: Avg prompt throughput: 86.7 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 53.40 tokens/s, Drafted throughput: 81.60 tokens/s, Accepted: 534 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.738, 0.571, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:39 [loggers.py:257] Engine 000: Avg prompt throughput: 72.6 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 55.59 tokens/s, Drafted throughput: 81.99 tokens/s, Accepted: 556 tokens, Drafted: 820 tokens, Per-position acceptance rate: 0.763, 0.593, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:49 [loggers.py:257] Engine 000: Avg prompt throughput: 73.5 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 53.80 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 538 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.748, 0.558, Avg Draft acceptance rate: 65.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:59 [loggers.py:257] Engine 000: Avg prompt throughput: 82.8 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:02:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 53.20 tokens/s, Drafted throughput: 81.59 tokens/s, Accepted: 532 tokens, Drafted: 816 tokens, Per-position acceptance rate: 0.743, 0.561, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:09 [loggers.py:257] Engine 000: Avg prompt throughput: 43.7 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 56.49 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 565 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.760, 0.612, Avg Draft acceptance rate: 68.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  105.63    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.47      
Output token throughput (tok/s):         94.67     
Peak output token throughput (tok/s):    42.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          163.09    
---------------Time to First Token----------------
Mean TTFT (ms):                          99.18     
Median TTFT (ms):                        99.58     
P99 TTFT (ms):                           107.52    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.54     
Median TPOT (ms):                        20.56     
P99 TPOT (ms):                           22.49     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.60     
Median ITL (ms):                         47.53     
P99 ITL (ms):                            51.62     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.35     
Acceptance length:                       2.33      
Drafts:                                  4293      
Draft tokens:                            8586      
Accepted tokens:                         5697      
Per-position acceptance (%):
  Position 0:                            75.36     
  Position 1:                            57.35     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 8.20 tokens/s, Drafted throughput: 10.80 tokens/s, Accepted: 82 tokens, Drafted: 108 tokens, Per-position acceptance rate: 0.889, 0.630, Avg Draft acceptance rate: 75.9%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff600272fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8bcc4bab-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:29 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 4.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 2.70 tokens/s, Drafted throughput: 3.60 tokens/s, Accepted: 27 tokens, Drafted: 36 tokens, Per-position acceptance rate: 0.778, 0.722, Avg Draft acceptance rate: 75.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:39 [loggers.py:257] Engine 000: Avg prompt throughput: 118.5 tokens/s, Avg generation throughput: 132.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 75.40 tokens/s, Drafted throughput: 113.20 tokens/s, Accepted: 754 tokens, Drafted: 1132 tokens, Per-position acceptance rate: 0.754, 0.578, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:49 [loggers.py:257] Engine 000: Avg prompt throughput: 96.8 tokens/s, Avg generation throughput: 186.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 105.28 tokens/s, Drafted throughput: 162.37 tokens/s, Accepted: 1053 tokens, Drafted: 1624 tokens, Per-position acceptance rate: 0.748, 0.549, Avg Draft acceptance rate: 64.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:59 [loggers.py:257] Engine 000: Avg prompt throughput: 165.1 tokens/s, Avg generation throughput: 187.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:03:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 107.19 tokens/s, Drafted throughput: 160.39 tokens/s, Accepted: 1072 tokens, Drafted: 1604 tokens, Per-position acceptance rate: 0.762, 0.575, Avg Draft acceptance rate: 66.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:09 [loggers.py:257] Engine 000: Avg prompt throughput: 142.3 tokens/s, Avg generation throughput: 188.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 108.40 tokens/s, Drafted throughput: 160.40 tokens/s, Accepted: 1084 tokens, Drafted: 1604 tokens, Per-position acceptance rate: 0.762, 0.590, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:19 [loggers.py:257] Engine 000: Avg prompt throughput: 136.6 tokens/s, Avg generation throughput: 188.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 107.29 tokens/s, Drafted throughput: 161.98 tokens/s, Accepted: 1073 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.751, 0.574, Avg Draft acceptance rate: 66.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  55.18     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.91      
Output token throughput (tok/s):         181.21    
Peak output token throughput (tok/s):    84.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          312.18    
---------------Time to First Token----------------
Mean TTFT (ms):                          99.82     
Median TTFT (ms):                        100.63    
P99 TTFT (ms):                           109.21    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.86     
Median TPOT (ms):                        20.95     
P99 TPOT (ms):                           22.78     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.45     
Median ITL (ms):                         48.28     
P99 ITL (ms):                            54.10     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.65     
Acceptance length:                       2.33      
Drafts:                                  4283      
Draft tokens:                            8566      
Accepted tokens:                         5709      
Per-position acceptance (%):
  Position 0:                            75.62     
  Position 1:                            57.67     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:29 [loggers.py:257] Engine 000: Avg prompt throughput: 63.4 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 76.10 tokens/s, Drafted throughput: 111.60 tokens/s, Accepted: 761 tokens, Drafted: 1116 tokens, Per-position acceptance rate: 0.765, 0.599, Avg Draft acceptance rate: 68.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2aa41fefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f20a4178-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:49 [loggers.py:257] Engine 000: Avg prompt throughput: 36.6 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 5.75 tokens/s, Drafted throughput: 8.50 tokens/s, Accepted: 115 tokens, Drafted: 170 tokens, Per-position acceptance rate: 0.776, 0.576, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:59 [loggers.py:257] Engine 000: Avg prompt throughput: 320.8 tokens/s, Avg generation throughput: 358.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:04:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 206.27 tokens/s, Drafted throughput: 302.16 tokens/s, Accepted: 2063 tokens, Drafted: 3022 tokens, Per-position acceptance rate: 0.771, 0.594, Avg Draft acceptance rate: 68.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:09 [loggers.py:257] Engine 000: Avg prompt throughput: 257.0 tokens/s, Avg generation throughput: 369.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 210.98 tokens/s, Drafted throughput: 316.77 tokens/s, Accepted: 2110 tokens, Drafted: 3168 tokens, Per-position acceptance rate: 0.756, 0.576, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:19 [loggers.py:257] Engine 000: Avg prompt throughput: 215.7 tokens/s, Avg generation throughput: 370.1 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 211.37 tokens/s, Drafted throughput: 316.75 tokens/s, Accepted: 2114 tokens, Drafted: 3168 tokens, Per-position acceptance rate: 0.758, 0.577, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:29 [loggers.py:257] Engine 000: Avg prompt throughput: 337.1 tokens/s, Avg generation throughput: 370.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 213.27 tokens/s, Drafted throughput: 312.36 tokens/s, Accepted: 2133 tokens, Drafted: 3124 tokens, Per-position acceptance rate: 0.764, 0.602, Avg Draft acceptance rate: 68.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  44.55     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.80      
Output token throughput (tok/s):         359.16    
Peak output token throughput (tok/s):    168.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          621.57    
---------------Time to First Token----------------
Mean TTFT (ms):                          104.88    
Median TTFT (ms):                        104.46    
P99 TTFT (ms):                           121.14    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.21     
Median TPOT (ms):                        21.33     
P99 TPOT (ms):                           23.17     
---------------Inter-token Latency----------------
Mean ITL (ms):                           49.41     
Median ITL (ms):                         48.96     
P99 ITL (ms):                            59.59     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.92     
Acceptance length:                       2.34      
Drafts:                                  6835      
Draft tokens:                            13670     
Accepted tokens:                         9148      
Per-position acceptance (%):
  Position 0:                            75.64     
  Position 1:                            58.20     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:39 [loggers.py:257] Engine 000: Avg prompt throughput: 19.9 tokens/s, Avg generation throughput: 131.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 72.80 tokens/s, Drafted throughput: 118.80 tokens/s, Accepted: 728 tokens, Drafted: 1188 tokens, Per-position acceptance rate: 0.699, 0.527, Avg Draft acceptance rate: 61.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f286f98afc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-09be0d68-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:59 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 265.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:05:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 76.54 tokens/s, Drafted throughput: 110.79 tokens/s, Accepted: 1531 tokens, Drafted: 2216 tokens, Per-position acceptance rate: 0.783, 0.598, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:09 [loggers.py:257] Engine 000: Avg prompt throughput: 547.5 tokens/s, Avg generation throughput: 691.7 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 393.34 tokens/s, Drafted throughput: 596.11 tokens/s, Accepted: 3934 tokens, Drafted: 5962 tokens, Per-position acceptance rate: 0.742, 0.577, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:19 [loggers.py:257] Engine 000: Avg prompt throughput: 536.6 tokens/s, Avg generation throughput: 701.2 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 403.26 tokens/s, Drafted throughput: 596.54 tokens/s, Accepted: 4033 tokens, Drafted: 5966 tokens, Per-position acceptance rate: 0.760, 0.592, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:29 [loggers.py:257] Engine 000: Avg prompt throughput: 625.8 tokens/s, Avg generation throughput: 689.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 395.44 tokens/s, Drafted throughput: 586.31 tokens/s, Accepted: 3955 tokens, Drafted: 5864 tokens, Per-position acceptance rate: 0.760, 0.589, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:39 [loggers.py:257] Engine 000: Avg prompt throughput: 510.6 tokens/s, Avg generation throughput: 714.9 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 414.78 tokens/s, Drafted throughput: 598.38 tokens/s, Accepted: 4148 tokens, Drafted: 5984 tokens, Per-position acceptance rate: 0.773, 0.614, Avg Draft acceptance rate: 69.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  47.25     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.39      
Output token throughput (tok/s):         677.12    
Peak output token throughput (tok/s):    320.00    
Peak concurrent requests:                25.00     
Total token throughput (tok/s):          1192.75   
---------------Time to First Token----------------
Mean TTFT (ms):                          112.78    
Median TTFT (ms):                        110.91    
P99 TTFT (ms):                           190.39    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.31     
Median TPOT (ms):                        22.39     
P99 TPOT (ms):                           24.70     
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.28     
Median ITL (ms):                         51.32     
P99 ITL (ms):                            65.73     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.65     
Acceptance length:                       2.35      
Drafts:                                  13587     
Draft tokens:                            27174     
Accepted tokens:                         18382     
Per-position acceptance (%):
  Position 0:                            75.97     
  Position 1:                            59.32     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 89.59 tokens/s, Drafted throughput: 135.19 tokens/s, Accepted: 896 tokens, Drafted: 1352 tokens, Per-position acceptance rate: 0.741, 0.584, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:06:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc59da96fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7b400028-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:09 [loggers.py:257] Engine 000: Avg prompt throughput: 487.2 tokens/s, Avg generation throughput: 583.9 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 51.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 166.93 tokens/s, Drafted throughput: 247.37 tokens/s, Accepted: 3339 tokens, Drafted: 4948 tokens, Per-position acceptance rate: 0.770, 0.580, Avg Draft acceptance rate: 67.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:19 [loggers.py:257] Engine 000: Avg prompt throughput: 931.6 tokens/s, Avg generation throughput: 1275.4 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 51.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 734.25 tokens/s, Drafted throughput: 1080.92 tokens/s, Accepted: 7343 tokens, Drafted: 10810 tokens, Per-position acceptance rate: 0.764, 0.595, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1065.1 tokens/s, Avg generation throughput: 1271.5 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 55.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 735.25 tokens/s, Drafted throughput: 1068.52 tokens/s, Accepted: 7353 tokens, Drafted: 10686 tokens, Per-position acceptance rate: 0.773, 0.603, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:39 [loggers.py:257] Engine 000: Avg prompt throughput: 934.8 tokens/s, Avg generation throughput: 1290.4 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 47.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 743.02 tokens/s, Drafted throughput: 1093.54 tokens/s, Accepted: 7432 tokens, Drafted: 10938 tokens, Per-position acceptance rate: 0.766, 0.593, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:49 [loggers.py:257] Engine 000: Avg prompt throughput: 972.4 tokens/s, Avg generation throughput: 1265.5 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 723.49 tokens/s, Drafted throughput: 1081.49 tokens/s, Accepted: 7237 tokens, Drafted: 10818 tokens, Per-position acceptance rate: 0.758, 0.580, Avg Draft acceptance rate: 66.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  51.90     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.17      
Output token throughput (tok/s):         1232.96   
Peak output token throughput (tok/s):    608.00    
Peak concurrent requests:                54.00     
Total token throughput (tok/s):          2162.93   
---------------Time to First Token----------------
Mean TTFT (ms):                          135.98    
Median TTFT (ms):                        126.07    
P99 TTFT (ms):                           235.94    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.44     
Median TPOT (ms):                        24.36     
P99 TPOT (ms):                           27.53     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.28     
Median ITL (ms):                         54.55     
P99 ITL (ms):                            105.48    
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.66     
Acceptance length:                       2.35      
Drafts:                                  27170     
Draft tokens:                            54340     
Accepted tokens:                         36764     
Per-position acceptance (%):
  Position 0:                            76.32     
  Position 1:                            58.99     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:59 [loggers.py:257] Engine 000: Avg prompt throughput: 453.0 tokens/s, Avg generation throughput: 731.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:07:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 417.48 tokens/s, Drafted throughput: 630.96 tokens/s, Accepted: 4175 tokens, Drafted: 6310 tokens, Per-position acceptance rate: 0.745, 0.579, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f400d60afc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c4019f84-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:19 [loggers.py:257] Engine 000: Avg prompt throughput: 550.4 tokens/s, Avg generation throughput: 46.7 tokens/s, Running: 37 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 12.65 tokens/s, Drafted throughput: 17.70 tokens/s, Accepted: 253 tokens, Drafted: 354 tokens, Per-position acceptance rate: 0.774, 0.655, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1392.9 tokens/s, Avg generation throughput: 2111.5 tokens/s, Running: 61 reqs, Waiting: 3 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1217.87 tokens/s, Drafted throughput: 1777.97 tokens/s, Accepted: 12183 tokens, Drafted: 17786 tokens, Per-position acceptance rate: 0.768, 0.602, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1776.7 tokens/s, Avg generation throughput: 1837.3 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 1056.99 tokens/s, Drafted throughput: 1552.86 tokens/s, Accepted: 10575 tokens, Drafted: 15536 tokens, Per-position acceptance rate: 0.765, 0.597, Avg Draft acceptance rate: 68.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1030.2 tokens/s, Avg generation throughput: 2117.3 tokens/s, Running: 51 reqs, Waiting: 10 reqs, GPU KV cache usage: 92.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1213.26 tokens/s, Drafted throughput: 1804.79 tokens/s, Accepted: 12134 tokens, Drafted: 18050 tokens, Per-position acceptance rate: 0.762, 0.582, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1771.5 tokens/s, Avg generation throughput: 1976.3 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 87.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:08:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1139.60 tokens/s, Drafted throughput: 1666.41 tokens/s, Accepted: 11400 tokens, Drafted: 16670 tokens, Per-position acceptance rate: 0.764, 0.604, Avg Draft acceptance rate: 68.4%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1473.6 tokens/s, Avg generation throughput: 1971.1 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 74.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1119.37 tokens/s, Drafted throughput: 1697.81 tokens/s, Accepted: 11195 tokens, Drafted: 16980 tokens, Per-position acceptance rate: 0.746, 0.573, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1349.0 tokens/s, Avg generation throughput: 2098.9 tokens/s, Running: 59 reqs, Waiting: 5 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1203.16 tokens/s, Drafted throughput: 1785.99 tokens/s, Accepted: 12033 tokens, Drafted: 17862 tokens, Per-position acceptance rate: 0.758, 0.589, Avg Draft acceptance rate: 67.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  67.36     
Total input tokens:                      94775     
Total generated tokens:                  127939    
Request throughput (req/s):              9.50      
Output token throughput (tok/s):         1899.25   
Peak output token throughput (tok/s):    1088.00   
Peak concurrent requests:                94.00     
Total token throughput (tok/s):          3306.18   
---------------Time to First Token----------------
Mean TTFT (ms):                          268.30    
Median TTFT (ms):                        212.47    
P99 TTFT (ms):                           950.16    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          30.62     
Median TPOT (ms):                        30.50     
P99 TPOT (ms):                           38.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.56     
Median ITL (ms):                         60.28     
P99 ITL (ms):                            195.34    
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.43     
Acceptance length:                       2.35      
Drafts:                                  54390     
Draft tokens:                            108780    
Accepted tokens:                         73349     
Per-position acceptance (%):
  Position 0:                            75.90     
  Position 1:                            58.95     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:29 [loggers.py:257] Engine 000: Avg prompt throughput: 148.9 tokens/s, Avg generation throughput: 651.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 369.08 tokens/s, Drafted throughput: 571.17 tokens/s, Accepted: 3691 tokens, Drafted: 5712 tokens, Per-position acceptance rate: 0.733, 0.560, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f28b90c2fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-98d9d061-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:49 [loggers.py:257] Engine 000: Avg prompt throughput: 722.4 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 40.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 8.10 tokens/s, Drafted throughput: 11.30 tokens/s, Accepted: 162 tokens, Drafted: 226 tokens, Per-position acceptance rate: 0.796, 0.637, Avg Draft acceptance rate: 71.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1369.1 tokens/s, Avg generation throughput: 1929.6 tokens/s, Running: 72 reqs, Waiting: 54 reqs, GPU KV cache usage: 97.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:09:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 1112.89 tokens/s, Drafted throughput: 1612.83 tokens/s, Accepted: 11130 tokens, Drafted: 16130 tokens, Per-position acceptance rate: 0.771, 0.609, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1246.1 tokens/s, Avg generation throughput: 2009.0 tokens/s, Running: 59 reqs, Waiting: 66 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1145.68 tokens/s, Drafted throughput: 1714.82 tokens/s, Accepted: 11458 tokens, Drafted: 17150 tokens, Per-position acceptance rate: 0.754, 0.582, Avg Draft acceptance rate: 66.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:19 [loggers.py:257] Engine 000: Avg prompt throughput: 2145.8 tokens/s, Avg generation throughput: 1790.8 tokens/s, Running: 97 reqs, Waiting: 28 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 1024.86 tokens/s, Drafted throughput: 1509.06 tokens/s, Accepted: 10255 tokens, Drafted: 15100 tokens, Per-position acceptance rate: 0.767, 0.592, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:29 [loggers.py:257] Engine 000: Avg prompt throughput: 971.5 tokens/s, Avg generation throughput: 2156.4 tokens/s, Running: 63 reqs, Waiting: 64 reqs, GPU KV cache usage: 96.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1231.09 tokens/s, Drafted throughput: 1841.64 tokens/s, Accepted: 12312 tokens, Drafted: 18418 tokens, Per-position acceptance rate: 0.752, 0.585, Avg Draft acceptance rate: 66.8%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1244.2 tokens/s, Avg generation throughput: 1809.4 tokens/s, Running: 69 reqs, Waiting: 55 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1022.56 tokens/s, Drafted throughput: 1558.94 tokens/s, Accepted: 10226 tokens, Drafted: 15590 tokens, Per-position acceptance rate: 0.748, 0.564, Avg Draft acceptance rate: 65.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1774.2 tokens/s, Avg generation throughput: 1971.0 tokens/s, Running: 76 reqs, Waiting: 51 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1134.13 tokens/s, Drafted throughput: 1660.50 tokens/s, Accepted: 11342 tokens, Drafted: 16606 tokens, Per-position acceptance rate: 0.765, 0.601, Avg Draft acceptance rate: 68.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1111.1 tokens/s, Avg generation throughput: 2034.3 tokens/s, Running: 60 reqs, Waiting: 67 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:10:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 1147.55 tokens/s, Drafted throughput: 1760.11 tokens/s, Accepted: 11480 tokens, Drafted: 17608 tokens, Per-position acceptance rate: 0.740, 0.564, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1700.8 tokens/s, Avg generation throughput: 1738.4 tokens/s, Running: 84 reqs, Waiting: 33 reqs, GPU KV cache usage: 87.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 993.62 tokens/s, Drafted throughput: 1469.08 tokens/s, Accepted: 9937 tokens, Drafted: 14692 tokens, Per-position acceptance rate: 0.762, 0.591, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1285.9 tokens/s, Avg generation throughput: 2091.8 tokens/s, Running: 66 reqs, Waiting: 59 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1191.05 tokens/s, Drafted throughput: 1789.13 tokens/s, Accepted: 11911 tokens, Drafted: 17892 tokens, Per-position acceptance rate: 0.749, 0.582, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1174.5 tokens/s, Avg generation throughput: 1880.3 tokens/s, Running: 50 reqs, Waiting: 71 reqs, GPU KV cache usage: 90.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1067.01 tokens/s, Drafted throughput: 1616.71 tokens/s, Accepted: 10672 tokens, Drafted: 16170 tokens, Per-position acceptance rate: 0.748, 0.572, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:39 [loggers.py:257] Engine 000: Avg prompt throughput: 2028.3 tokens/s, Avg generation throughput: 1781.4 tokens/s, Running: 93 reqs, Waiting: 34 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 1027.58 tokens/s, Drafted throughput: 1482.82 tokens/s, Accepted: 10277 tokens, Drafted: 14830 tokens, Per-position acceptance rate: 0.774, 0.612, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:49 [loggers.py:257] Engine 000: Avg prompt throughput: 905.7 tokens/s, Avg generation throughput: 2156.3 tokens/s, Running: 59 reqs, Waiting: 69 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1237.47 tokens/s, Drafted throughput: 1827.61 tokens/s, Accepted: 12380 tokens, Drafted: 18284 tokens, Per-position acceptance rate: 0.765, 0.590, Avg Draft acceptance rate: 67.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1243.6 tokens/s, Avg generation throughput: 1811.2 tokens/s, Running: 33 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:11:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1034.08 tokens/s, Drafted throughput: 1546.33 tokens/s, Accepted: 10344 tokens, Drafted: 15468 tokens, Per-position acceptance rate: 0.761, 0.577, Avg Draft acceptance rate: 66.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  135.41    
Total input tokens:                      189093    
Total generated tokens:                  255943    
Request throughput (req/s):              9.45      
Output token throughput (tok/s):         1890.09   
Peak output token throughput (tok/s):    1355.00   
Peak concurrent requests:                154.00    
Total token throughput (tok/s):          3286.51   
---------------Time to First Token----------------
Mean TTFT (ms):                          4531.67   
Median TTFT (ms):                        4916.25   
P99 TTFT (ms):                           7786.05   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          42.13     
Median TPOT (ms):                        38.54     
P99 TPOT (ms):                           67.85     
---------------Inter-token Latency----------------
Mean ITL (ms):                           97.94     
Median ITL (ms):                         64.40     
P99 ITL (ms):                            319.55    
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.14     
Acceptance length:                       2.34      
Drafts:                                  108827    
Draft tokens:                            217654    
Accepted tokens:                         146136    
Per-position acceptance (%):
  Position 0:                            75.76     
  Position 1:                            58.53     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 416.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 236.49 tokens/s, Drafted throughput: 365.98 tokens/s, Accepted: 2365 tokens, Drafted: 3660 tokens, Per-position acceptance rate: 0.737, 0.556, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7feddc62afc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15016, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-29173f13-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:29 [loggers.py:257] Engine 000: Avg prompt throughput: 722.4 tokens/s, Avg generation throughput: 28.7 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 40.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 7.05 tokens/s, Drafted throughput: 9.80 tokens/s, Accepted: 141 tokens, Drafted: 196 tokens, Per-position acceptance rate: 0.806, 0.633, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1405.0 tokens/s, Avg generation throughput: 2063.0 tokens/s, Running: 68 reqs, Waiting: 188 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 1189.58 tokens/s, Drafted throughput: 1724.63 tokens/s, Accepted: 11897 tokens, Drafted: 17248 tokens, Per-position acceptance rate: 0.773, 0.606, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1244.4 tokens/s, Avg generation throughput: 1950.8 tokens/s, Running: 61 reqs, Waiting: 193 reqs, GPU KV cache usage: 98.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1115.36 tokens/s, Drafted throughput: 1660.54 tokens/s, Accepted: 11154 tokens, Drafted: 16606 tokens, Per-position acceptance rate: 0.757, 0.586, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:59 [loggers.py:257] Engine 000: Avg prompt throughput: 2133.4 tokens/s, Avg generation throughput: 1869.0 tokens/s, Running: 89 reqs, Waiting: 167 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:12:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1074.49 tokens/s, Drafted throughput: 1568.71 tokens/s, Accepted: 10751 tokens, Drafted: 15696 tokens, Per-position acceptance rate: 0.769, 0.601, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:09 [loggers.py:257] Engine 000: Avg prompt throughput: 884.4 tokens/s, Avg generation throughput: 2064.6 tokens/s, Running: 62 reqs, Waiting: 194 reqs, GPU KV cache usage: 97.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1175.27 tokens/s, Drafted throughput: 1771.81 tokens/s, Accepted: 11758 tokens, Drafted: 17726 tokens, Per-position acceptance rate: 0.748, 0.578, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1353.7 tokens/s, Avg generation throughput: 1811.8 tokens/s, Running: 68 reqs, Waiting: 182 reqs, GPU KV cache usage: 92.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1030.47 tokens/s, Drafted throughput: 1548.60 tokens/s, Accepted: 10306 tokens, Drafted: 15488 tokens, Per-position acceptance rate: 0.755, 0.576, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1862.6 tokens/s, Avg generation throughput: 2072.5 tokens/s, Running: 72 reqs, Waiting: 183 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1194.22 tokens/s, Drafted throughput: 1741.49 tokens/s, Accepted: 11943 tokens, Drafted: 17416 tokens, Per-position acceptance rate: 0.768, 0.604, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:39 [loggers.py:257] Engine 000: Avg prompt throughput: 951.9 tokens/s, Avg generation throughput: 1987.1 tokens/s, Running: 59 reqs, Waiting: 197 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1117.39 tokens/s, Drafted throughput: 1726.81 tokens/s, Accepted: 11179 tokens, Drafted: 17276 tokens, Per-position acceptance rate: 0.738, 0.557, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:49 [loggers.py:257] Engine 000: Avg prompt throughput: 2048.8 tokens/s, Avg generation throughput: 1806.4 tokens/s, Running: 96 reqs, Waiting: 158 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1038.43 tokens/s, Drafted throughput: 1514.62 tokens/s, Accepted: 10391 tokens, Drafted: 15156 tokens, Per-position acceptance rate: 0.769, 0.602, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1009.2 tokens/s, Avg generation throughput: 2102.1 tokens/s, Running: 63 reqs, Waiting: 193 reqs, GPU KV cache usage: 99.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:13:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1194.88 tokens/s, Drafted throughput: 1802.22 tokens/s, Accepted: 11954 tokens, Drafted: 18030 tokens, Per-position acceptance rate: 0.748, 0.578, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1121.2 tokens/s, Avg generation throughput: 1784.7 tokens/s, Running: 55 reqs, Waiting: 196 reqs, GPU KV cache usage: 93.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1012.39 tokens/s, Drafted throughput: 1531.43 tokens/s, Accepted: 10125 tokens, Drafted: 15316 tokens, Per-position acceptance rate: 0.749, 0.573, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:19 [loggers.py:257] Engine 000: Avg prompt throughput: 2058.2 tokens/s, Avg generation throughput: 1934.0 tokens/s, Running: 81 reqs, Waiting: 175 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 1115.76 tokens/s, Drafted throughput: 1614.47 tokens/s, Accepted: 11164 tokens, Drafted: 16154 tokens, Per-position acceptance rate: 0.772, 0.610, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1042.2 tokens/s, Avg generation throughput: 2076.8 tokens/s, Running: 54 reqs, Waiting: 198 reqs, GPU KV cache usage: 92.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1184.81 tokens/s, Drafted throughput: 1771.72 tokens/s, Accepted: 11850 tokens, Drafted: 17720 tokens, Per-position acceptance rate: 0.755, 0.583, Avg Draft acceptance rate: 66.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1684.1 tokens/s, Avg generation throughput: 1685.5 tokens/s, Running: 80 reqs, Waiting: 170 reqs, GPU KV cache usage: 90.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 953.04 tokens/s, Drafted throughput: 1441.76 tokens/s, Accepted: 9532 tokens, Drafted: 14420 tokens, Per-position acceptance rate: 0.750, 0.572, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1325.5 tokens/s, Avg generation throughput: 2086.2 tokens/s, Running: 68 reqs, Waiting: 186 reqs, GPU KV cache usage: 94.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1192.87 tokens/s, Drafted throughput: 1773.26 tokens/s, Accepted: 11931 tokens, Drafted: 17736 tokens, Per-position acceptance rate: 0.757, 0.589, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1164.2 tokens/s, Avg generation throughput: 1904.2 tokens/s, Running: 53 reqs, Waiting: 202 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:14:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1079.36 tokens/s, Drafted throughput: 1636.19 tokens/s, Accepted: 10795 tokens, Drafted: 16364 tokens, Per-position acceptance rate: 0.747, 0.572, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:09 [loggers.py:257] Engine 000: Avg prompt throughput: 2017.0 tokens/s, Avg generation throughput: 1742.2 tokens/s, Running: 92 reqs, Waiting: 163 reqs, GPU KV cache usage: 98.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 995.02 tokens/s, Drafted throughput: 1474.09 tokens/s, Accepted: 9951 tokens, Drafted: 14742 tokens, Per-position acceptance rate: 0.759, 0.591, Avg Draft acceptance rate: 67.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:19 [loggers.py:257] Engine 000: Avg prompt throughput: 955.3 tokens/s, Avg generation throughput: 2092.2 tokens/s, Running: 65 reqs, Waiting: 189 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1195.43 tokens/s, Drafted throughput: 1783.09 tokens/s, Accepted: 11955 tokens, Drafted: 17832 tokens, Per-position acceptance rate: 0.755, 0.586, Avg Draft acceptance rate: 67.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1148.2 tokens/s, Avg generation throughput: 1884.7 tokens/s, Running: 63 reqs, Waiting: 186 reqs, GPU KV cache usage: 89.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1069.44 tokens/s, Drafted throughput: 1613.31 tokens/s, Accepted: 10695 tokens, Drafted: 16134 tokens, Per-position acceptance rate: 0.753, 0.573, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:39 [loggers.py:257] Engine 000: Avg prompt throughput: 2040.2 tokens/s, Avg generation throughput: 1951.4 tokens/s, Running: 82 reqs, Waiting: 174 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1123.16 tokens/s, Drafted throughput: 1635.94 tokens/s, Accepted: 11232 tokens, Drafted: 16360 tokens, Per-position acceptance rate: 0.767, 0.607, Avg Draft acceptance rate: 68.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:49 [loggers.py:257] Engine 000: Avg prompt throughput: 917.5 tokens/s, Avg generation throughput: 2000.3 tokens/s, Running: 62 reqs, Waiting: 193 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1130.05 tokens/s, Drafted throughput: 1727.37 tokens/s, Accepted: 11302 tokens, Drafted: 17276 tokens, Per-position acceptance rate: 0.748, 0.560, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:59 [loggers.py:257] Engine 000: Avg prompt throughput: 1584.2 tokens/s, Avg generation throughput: 1776.0 tokens/s, Running: 80 reqs, Waiting: 168 reqs, GPU KV cache usage: 87.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:15:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 1018.51 tokens/s, Drafted throughput: 1496.81 tokens/s, Accepted: 10185 tokens, Drafted: 14968 tokens, Per-position acceptance rate: 0.763, 0.598, Avg Draft acceptance rate: 68.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:09 [loggers.py:257] Engine 000: Avg prompt throughput: 1439.6 tokens/s, Avg generation throughput: 2160.8 tokens/s, Running: 67 reqs, Waiting: 188 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1245.44 tokens/s, Drafted throughput: 1818.77 tokens/s, Accepted: 12456 tokens, Drafted: 18190 tokens, Per-position acceptance rate: 0.770, 0.600, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:19 [loggers.py:257] Engine 000: Avg prompt throughput: 1129.2 tokens/s, Avg generation throughput: 1956.3 tokens/s, Running: 55 reqs, Waiting: 195 reqs, GPU KV cache usage: 93.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1116.28 tokens/s, Drafted throughput: 1668.58 tokens/s, Accepted: 11163 tokens, Drafted: 16686 tokens, Per-position acceptance rate: 0.757, 0.581, Avg Draft acceptance rate: 66.9%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:29 [loggers.py:257] Engine 000: Avg prompt throughput: 1885.0 tokens/s, Avg generation throughput: 1929.4 tokens/s, Running: 84 reqs, Waiting: 168 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1103.73 tokens/s, Drafted throughput: 1630.55 tokens/s, Accepted: 11039 tokens, Drafted: 16308 tokens, Per-position acceptance rate: 0.763, 0.591, Avg Draft acceptance rate: 67.7%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:39 [loggers.py:257] Engine 000: Avg prompt throughput: 1065.3 tokens/s, Avg generation throughput: 2104.3 tokens/s, Running: 61 reqs, Waiting: 112 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1201.12 tokens/s, Drafted throughput: 1793.27 tokens/s, Accepted: 12012 tokens, Drafted: 17934 tokens, Per-position acceptance rate: 0.755, 0.585, Avg Draft acceptance rate: 67.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:49 [loggers.py:257] Engine 000: Avg prompt throughput: 1140.2 tokens/s, Avg generation throughput: 1889.1 tokens/s, Running: 40 reqs, Waiting: 0 reqs, GPU KV cache usage: 48.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1079.80 tokens/s, Drafted throughput: 1608.71 tokens/s, Accepted: 10800 tokens, Drafted: 16090 tokens, Per-position acceptance rate: 0.754, 0.588, Avg Draft acceptance rate: 67.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  265.91    
Total input tokens:                      373233    
Total generated tokens:                  511927    
Request throughput (req/s):              9.63      
Output token throughput (tok/s):         1925.18   
Peak output token throughput (tok/s):    1343.00   
Peak concurrent requests:                281.00    
Total token throughput (tok/s):          3328.77   
---------------Time to First Token----------------
Mean TTFT (ms):                          17024.17  
Median TTFT (ms):                        16664.66  
P99 TTFT (ms):                           21475.94  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          42.30     
Median TPOT (ms):                        38.66     
P99 TPOT (ms):                           67.19     
---------------Inter-token Latency----------------
Mean ITL (ms):                           98.37     
Median ITL (ms):                         65.12     
P99 ITL (ms):                            298.60    
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.16     
Acceptance length:                       2.34      
Drafts:                                  217600    
Draft tokens:                            435200    
Accepted tokens:                         292301    
Per-position acceptance (%):
  Position 0:                            75.73     
  Position 1:                            58.60     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k2-t0.0-tp1...
[0;36m(APIServer pid=207481)[0;0m INFO 01-22 18:16:54 [launcher.py:110] Shutting down FastAPI HTTP server.
