Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k3-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k3-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 818184
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:56 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:56 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15017, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 3, 'max_model_len': 5000}}
[0;36m(APIServer pid=818184)[0;0m WARNING 01-22 20:52:56 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:57 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:57 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:59 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:59 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:59 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=818184)[0;0m WARNING 01-22 20:52:59 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:52:59 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2ee61d2fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-09b527b2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 20:53:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:53:10 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=3), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:53:11 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.63:52581 backend=nccl
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:53:11 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=818597)[0;0m WARNING 01-22 20:53:14 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:53:15 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-22 20:53:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:53:17 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 20:53:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:53:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:15 [default_loader.py:291] Loading weights took 57.19 seconds
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:15 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:15 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:15 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-22 20:54:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:32 [default_loader.py:291] Loading weights took 15.81 seconds
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:33 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 76.989409 seconds
WARNING 01-22 20:54:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:46 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:54:46 [backends.py:704] Dynamo bytecode transform time: 13.16 s
WARNING 01-22 20:54:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:54:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:55:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:04 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 5.056 s
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:04 [monitor.py:34] torch.compile takes 18.22 s in total
WARNING 01-22 20:55:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:55:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:10 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:10 [backends.py:704] Dynamo bytecode transform time: 6.09 s
WARNING 01-22 20:55:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:19 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.208 s
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:19 [monitor.py:34] torch.compile takes 26.51 s in total
WARNING 01-22 20:55:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:21 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:21 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:21 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-22 20:55:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:55:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-22 20:55:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:38 [gpu_model_runner.py:4880] Graph capturing finished in 16 secs, took 0.07 GiB
[0;36m(EngineCore_DP0 pid=818597)[0;0m INFO 01-22 20:55:39 [core.py:272] init engine (profile, create kv cache, warmup model) took 65.86 seconds
WARNING 01-22 20:55:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:41 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=818184)[0;0m WARNING 01-22 20:55:41 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:41 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:41 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:41 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [serving.py:221] Chat template warmup completed in 1764.1ms
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15017
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:43 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:53 [loggers.py:257] Engine 000: Avg prompt throughput: 27.9 tokens/s, Avg generation throughput: 28.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:55:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 18.37 tokens/s, Drafted throughput: 30.87 tokens/s, Accepted: 241 tokens, Drafted: 405 tokens, Per-position acceptance rate: 0.741, 0.585, 0.459, Avg Draft acceptance rate: 59.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:03 [loggers.py:257] Engine 000: Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 50.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 32.70 tokens/s, Drafted throughput: 55.19 tokens/s, Accepted: 327 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.766, 0.554, 0.457, Avg Draft acceptance rate: 59.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:13 [loggers.py:257] Engine 000: Avg prompt throughput: 33.0 tokens/s, Avg generation throughput: 49.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.66, Accepted throughput: 30.70 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 307 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.724, 0.519, 0.416, Avg Draft acceptance rate: 55.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:23 [loggers.py:257] Engine 000: Avg prompt throughput: 36.2 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 34.60 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 346 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.778, 0.616, 0.476, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:33 [loggers.py:257] Engine 000: Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 51.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 32.50 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 325 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.761, 0.565, 0.440, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:43 [loggers.py:257] Engine 000: Avg prompt throughput: 21.0 tokens/s, Avg generation throughput: 55.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 36.60 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 366 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.796, 0.645, 0.527, Avg Draft acceptance rate: 65.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:53 [loggers.py:257] Engine 000: Avg prompt throughput: 52.2 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:56:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.62, Accepted throughput: 29.90 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 299 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.730, 0.508, 0.378, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:03 [loggers.py:257] Engine 000: Avg prompt throughput: 30.0 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 33.70 tokens/s, Drafted throughput: 56.10 tokens/s, Accepted: 337 tokens, Drafted: 561 tokens, Per-position acceptance rate: 0.781, 0.583, 0.439, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:13 [loggers.py:257] Engine 000: Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 54.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 35.70 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 357 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.768, 0.611, 0.551, Avg Draft acceptance rate: 64.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:23 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.58, Accepted throughput: 29.20 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 292 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.714, 0.486, 0.378, Avg Draft acceptance rate: 52.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:33 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 51.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 33.20 tokens/s, Drafted throughput: 56.10 tokens/s, Accepted: 332 tokens, Drafted: 561 tokens, Per-position acceptance rate: 0.738, 0.588, 0.449, Avg Draft acceptance rate: 59.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:43 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 33.20 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 332 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.778, 0.589, 0.427, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:53 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:57:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 34.10 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 341 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.763, 0.597, 0.473, Avg Draft acceptance rate: 61.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:03 [loggers.py:257] Engine 000: Avg prompt throughput: 49.2 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.98, Accepted throughput: 36.70 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 367 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.832, 0.659, 0.492, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:13 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 32.40 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 324 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.726, 0.565, 0.452, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:23 [loggers.py:257] Engine 000: Avg prompt throughput: 42.7 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 36.50 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 365 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.785, 0.651, 0.527, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:33 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 49.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.66, Accepted throughput: 30.60 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 306 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.696, 0.533, 0.435, Avg Draft acceptance rate: 55.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:43 [loggers.py:257] Engine 000: Avg prompt throughput: 51.1 tokens/s, Avg generation throughput: 53.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 34.90 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 349 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.784, 0.611, 0.492, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:53 [loggers.py:257] Engine 000: Avg prompt throughput: 25.2 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:58:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 37.40 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 374 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.780, 0.699, 0.532, Avg Draft acceptance rate: 67.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  192.35    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.26      
Output token throughput (tok/s):         51.99     
Peak output token throughput (tok/s):    19.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          89.56     
---------------Time to First Token----------------
Mean TTFT (ms):                          69.53     
Median TTFT (ms):                        67.41     
P99 TTFT (ms):                           124.04    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.90     
Median TPOT (ms):                        19.00     
P99 TPOT (ms):                           21.29     
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.97     
Median ITL (ms):                         52.93     
P99 ITL (ms):                            53.48     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.49     
Acceptance length:                       2.81      
Drafts:                                  3551      
Draft tokens:                            10653     
Accepted tokens:                         6444      
Per-position acceptance (%):
  Position 0:                            76.18     
  Position 1:                            58.88     
  Position 2:                            46.41     
==================================================
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:03 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 28.60 tokens/s, Drafted throughput: 45.60 tokens/s, Accepted: 286 tokens, Drafted: 456 tokens, Per-position acceptance rate: 0.789, 0.612, 0.480, Avg Draft acceptance rate: 62.7%
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8defd0efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b45b43f7-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:23 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 13.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 4.30 tokens/s, Drafted throughput: 7.35 tokens/s, Accepted: 86 tokens, Drafted: 147 tokens, Per-position acceptance rate: 0.714, 0.571, 0.469, Avg Draft acceptance rate: 58.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:33 [loggers.py:257] Engine 000: Avg prompt throughput: 75.2 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 56.20 tokens/s, Drafted throughput: 94.50 tokens/s, Accepted: 562 tokens, Drafted: 945 tokens, Per-position acceptance rate: 0.775, 0.552, 0.457, Avg Draft acceptance rate: 59.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:43 [loggers.py:257] Engine 000: Avg prompt throughput: 62.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 65.39 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 654 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.769, 0.574, 0.453, Avg Draft acceptance rate: 59.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:53 [loggers.py:257] Engine 000: Avg prompt throughput: 69.5 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 20:59:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 65.90 tokens/s, Drafted throughput: 108.90 tokens/s, Accepted: 659 tokens, Drafted: 1089 tokens, Per-position acceptance rate: 0.769, 0.584, 0.463, Avg Draft acceptance rate: 60.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:03 [loggers.py:257] Engine 000: Avg prompt throughput: 77.1 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 64.99 tokens/s, Drafted throughput: 109.48 tokens/s, Accepted: 650 tokens, Drafted: 1095 tokens, Per-position acceptance rate: 0.770, 0.578, 0.433, Avg Draft acceptance rate: 59.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:13 [loggers.py:257] Engine 000: Avg prompt throughput: 67.9 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 63.90 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 639 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.750, 0.549, 0.456, Avg Draft acceptance rate: 58.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:23 [loggers.py:257] Engine 000: Avg prompt throughput: 98.4 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 66.19 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 662 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.773, 0.594, 0.461, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:33 [loggers.py:257] Engine 000: Avg prompt throughput: 72.6 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 70.20 tokens/s, Drafted throughput: 109.20 tokens/s, Accepted: 702 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.808, 0.629, 0.492, Avg Draft acceptance rate: 64.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:43 [loggers.py:257] Engine 000: Avg prompt throughput: 82.4 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 68.29 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 683 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.764, 0.615, 0.497, Avg Draft acceptance rate: 62.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:53 [loggers.py:257] Engine 000: Avg prompt throughput: 73.9 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:00:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 64.10 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 641 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.736, 0.571, 0.453, Avg Draft acceptance rate: 58.7%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:03 [loggers.py:257] Engine 000: Avg prompt throughput: 43.7 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.91, Accepted throughput: 62.90 tokens/s, Drafted throughput: 99.00 tokens/s, Accepted: 629 tokens, Drafted: 990 tokens, Per-position acceptance rate: 0.788, 0.636, 0.482, Avg Draft acceptance rate: 63.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  99.26     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.50      
Output token throughput (tok/s):         100.75    
Peak output token throughput (tok/s):    38.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          173.56    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.56    
Median TTFT (ms):                        111.28    
P99 TTFT (ms):                           131.13    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.00     
Median TPOT (ms):                        19.05     
P99 TPOT (ms):                           21.20     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.39     
Median ITL (ms):                         53.30     
P99 ITL (ms):                            58.05     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.73     
Acceptance length:                       2.82      
Drafts:                                  3541      
Draft tokens:                            10623     
Accepted tokens:                         6451      
Per-position acceptance (%):
  Position 0:                            76.93     
  Position 1:                            58.80     
  Position 2:                            46.46     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 1.30 tokens/s, Drafted throughput: 2.70 tokens/s, Accepted: 13 tokens, Drafted: 27 tokens, Per-position acceptance rate: 0.667, 0.444, 0.333, Avg Draft acceptance rate: 48.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f74899bafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c11d8336-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:23 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 6.70 tokens/s, Drafted throughput: 10.80 tokens/s, Accepted: 67 tokens, Drafted: 108 tokens, Per-position acceptance rate: 0.722, 0.611, 0.528, Avg Draft acceptance rate: 62.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:33 [loggers.py:257] Engine 000: Avg prompt throughput: 118.5 tokens/s, Avg generation throughput: 159.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 101.79 tokens/s, Drafted throughput: 172.48 tokens/s, Accepted: 1018 tokens, Drafted: 1725 tokens, Per-position acceptance rate: 0.762, 0.563, 0.445, Avg Draft acceptance rate: 59.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:43 [loggers.py:257] Engine 000: Avg prompt throughput: 165.3 tokens/s, Avg generation throughput: 201.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 130.20 tokens/s, Drafted throughput: 214.80 tokens/s, Accepted: 1302 tokens, Drafted: 2148 tokens, Per-position acceptance rate: 0.760, 0.588, 0.471, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:53 [loggers.py:257] Engine 000: Avg prompt throughput: 120.4 tokens/s, Avg generation throughput: 195.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:01:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.70, Accepted throughput: 123.18 tokens/s, Drafted throughput: 217.17 tokens/s, Accepted: 1232 tokens, Drafted: 2172 tokens, Per-position acceptance rate: 0.727, 0.540, 0.435, Avg Draft acceptance rate: 56.7%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:03 [loggers.py:257] Engine 000: Avg prompt throughput: 192.0 tokens/s, Avg generation throughput: 202.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 130.89 tokens/s, Drafted throughput: 213.59 tokens/s, Accepted: 1309 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.770, 0.596, 0.473, Avg Draft acceptance rate: 61.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:13 [loggers.py:257] Engine 000: Avg prompt throughput: 117.7 tokens/s, Avg generation throughput: 206.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 134.39 tokens/s, Drafted throughput: 216.58 tokens/s, Accepted: 1344 tokens, Drafted: 2166 tokens, Per-position acceptance rate: 0.760, 0.614, 0.488, Avg Draft acceptance rate: 62.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  51.84     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.96      
Output token throughput (tok/s):         192.89    
Peak output token throughput (tok/s):    76.00     
Peak concurrent requests:                8.00      
Total token throughput (tok/s):          332.29    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.12    
Median TTFT (ms):                        112.62    
P99 TTFT (ms):                           123.68    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.38     
Median TPOT (ms):                        19.56     
P99 TPOT (ms):                           22.66     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.02     
Median ITL (ms):                         53.80     
P99 ITL (ms):                            61.22     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.07     
Acceptance length:                       2.80      
Drafts:                                  3570      
Draft tokens:                            10710     
Accepted tokens:                         6434      
Per-position acceptance (%):
  Position 0:                            75.57     
  Position 1:                            58.21     
  Position 2:                            46.44     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:23 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 44.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 29.10 tokens/s, Drafted throughput: 47.10 tokens/s, Accepted: 291 tokens, Drafted: 471 tokens, Per-position acceptance rate: 0.771, 0.605, 0.478, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efd866bafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d6b285d3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:43 [loggers.py:257] Engine 000: Avg prompt throughput: 206.8 tokens/s, Avg generation throughput: 185.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 59.55 tokens/s, Drafted throughput: 98.84 tokens/s, Accepted: 1191 tokens, Drafted: 1977 tokens, Per-position acceptance rate: 0.765, 0.575, 0.467, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:53 [loggers.py:257] Engine 000: Avg prompt throughput: 280.4 tokens/s, Avg generation throughput: 394.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:02:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 255.46 tokens/s, Drafted throughput: 418.73 tokens/s, Accepted: 2555 tokens, Drafted: 4188 tokens, Per-position acceptance rate: 0.774, 0.594, 0.462, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:03 [loggers.py:257] Engine 000: Avg prompt throughput: 245.0 tokens/s, Avg generation throughput: 391.1 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 252.20 tokens/s, Drafted throughput: 419.10 tokens/s, Accepted: 2522 tokens, Drafted: 4191 tokens, Per-position acceptance rate: 0.753, 0.587, 0.465, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:13 [loggers.py:257] Engine 000: Avg prompt throughput: 316.7 tokens/s, Avg generation throughput: 399.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 261.18 tokens/s, Drafted throughput: 416.07 tokens/s, Accepted: 2612 tokens, Drafted: 4161 tokens, Per-position acceptance rate: 0.788, 0.608, 0.487, Avg Draft acceptance rate: 62.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  42.20     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.90      
Output token throughput (tok/s):         379.10    
Peak output token throughput (tok/s):    144.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          656.08    
---------------Time to First Token----------------
Mean TTFT (ms):                          115.07    
Median TTFT (ms):                        115.64    
P99 TTFT (ms):                           130.89    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.95     
Median TPOT (ms):                        19.92     
P99 TPOT (ms):                           22.44     
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.78     
Median ITL (ms):                         55.27     
P99 ITL (ms):                            63.38     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.44     
Acceptance length:                       2.81      
Drafts:                                  5695      
Draft tokens:                            17085     
Accepted tokens:                         10327     
Per-position acceptance (%):
  Position 0:                            76.38     
  Position 1:                            58.68     
  Position 2:                            46.27     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:23 [loggers.py:257] Engine 000: Avg prompt throughput: 138.4 tokens/s, Avg generation throughput: 248.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.70, Accepted throughput: 157.59 tokens/s, Drafted throughput: 278.38 tokens/s, Accepted: 1576 tokens, Drafted: 2784 tokens, Per-position acceptance rate: 0.726, 0.553, 0.419, Avg Draft acceptance rate: 56.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7c42a46fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-19c38b1e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:43 [loggers.py:257] Engine 000: Avg prompt throughput: 196.1 tokens/s, Avg generation throughput: 60.6 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 20.10 tokens/s, Drafted throughput: 28.80 tokens/s, Accepted: 402 tokens, Drafted: 576 tokens, Per-position acceptance rate: 0.807, 0.677, 0.609, Avg Draft acceptance rate: 69.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:53 [loggers.py:257] Engine 000: Avg prompt throughput: 526.4 tokens/s, Avg generation throughput: 756.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:03:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 490.09 tokens/s, Drafted throughput: 800.38 tokens/s, Accepted: 4901 tokens, Drafted: 8004 tokens, Per-position acceptance rate: 0.770, 0.594, 0.473, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:03 [loggers.py:257] Engine 000: Avg prompt throughput: 498.1 tokens/s, Avg generation throughput: 744.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 478.73 tokens/s, Drafted throughput: 800.28 tokens/s, Accepted: 4788 tokens, Drafted: 8004 tokens, Per-position acceptance rate: 0.764, 0.582, 0.448, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:13 [loggers.py:257] Engine 000: Avg prompt throughput: 625.0 tokens/s, Avg generation throughput: 733.2 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 474.12 tokens/s, Drafted throughput: 781.37 tokens/s, Accepted: 4742 tokens, Drafted: 7815 tokens, Per-position acceptance rate: 0.775, 0.586, 0.460, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:23 [loggers.py:257] Engine 000: Avg prompt throughput: 608.5 tokens/s, Avg generation throughput: 767.6 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.90, Accepted throughput: 503.50 tokens/s, Drafted throughput: 795.45 tokens/s, Accepted: 5036 tokens, Drafted: 7956 tokens, Per-position acceptance rate: 0.780, 0.623, 0.496, Avg Draft acceptance rate: 63.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  44.46     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.60      
Output token throughput (tok/s):         719.52    
Peak output token throughput (tok/s):    288.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          1267.43   
---------------Time to First Token----------------
Mean TTFT (ms):                          125.64    
Median TTFT (ms):                        123.49    
P99 TTFT (ms):                           182.32    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.74     
Median TPOT (ms):                        20.79     
P99 TPOT (ms):                           23.41     
---------------Inter-token Latency----------------
Mean ITL (ms):                           58.45     
Median ITL (ms):                         57.15     
P99 ITL (ms):                            78.91     
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.21     
Acceptance length:                       2.84      
Drafts:                                  11294     
Draft tokens:                            33882     
Accepted tokens:                         20738     
Per-position acceptance (%):
  Position 0:                            77.18     
  Position 1:                            59.54     
  Position 2:                            46.90     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 99.80 tokens/s, Drafted throughput: 174.29 tokens/s, Accepted: 998 tokens, Drafted: 1743 tokens, Per-position acceptance rate: 0.747, 0.549, 0.422, Avg Draft acceptance rate: 57.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2dba3dafc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-dddd678c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:53 [loggers.py:257] Engine 000: Avg prompt throughput: 550.3 tokens/s, Avg generation throughput: 606.9 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 49.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:04:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 195.48 tokens/s, Drafted throughput: 319.61 tokens/s, Accepted: 3910 tokens, Drafted: 6393 tokens, Per-position acceptance rate: 0.781, 0.589, 0.465, Avg Draft acceptance rate: 61.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1036.5 tokens/s, Avg generation throughput: 1313.7 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 844.28 tokens/s, Drafted throughput: 1407.87 tokens/s, Accepted: 8443 tokens, Drafted: 14079 tokens, Per-position acceptance rate: 0.758, 0.585, 0.456, Avg Draft acceptance rate: 60.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1126.6 tokens/s, Avg generation throughput: 1320.9 tokens/s, Running: 28 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 859.93 tokens/s, Drafted throughput: 1387.08 tokens/s, Accepted: 8600 tokens, Drafted: 13872 tokens, Per-position acceptance rate: 0.774, 0.607, 0.478, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1029.4 tokens/s, Avg generation throughput: 1331.9 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 859.73 tokens/s, Drafted throughput: 1418.58 tokens/s, Accepted: 8598 tokens, Drafted: 14187 tokens, Per-position acceptance rate: 0.765, 0.587, 0.466, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:33 [loggers.py:257] Engine 000: Avg prompt throughput: 982.8 tokens/s, Avg generation throughput: 1317.3 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 844.30 tokens/s, Drafted throughput: 1419.44 tokens/s, Accepted: 8444 tokens, Drafted: 14196 tokens, Per-position acceptance rate: 0.755, 0.575, 0.454, Avg Draft acceptance rate: 59.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  50.91     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.29      
Output token throughput (tok/s):         1256.93   
Peak output token throughput (tok/s):    543.00    
Peak concurrent requests:                53.00     
Total token throughput (tok/s):          2204.99   
---------------Time to First Token----------------
Mean TTFT (ms):                          149.81    
Median TTFT (ms):                        145.93    
P99 TTFT (ms):                           221.81    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.42     
Median TPOT (ms):                        23.41     
P99 TPOT (ms):                           27.20     
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.44     
Median ITL (ms):                         61.80     
P99 ITL (ms):                            112.48    
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.32     
Acceptance length:                       2.81      
Drafts:                                  22786     
Draft tokens:                            68358     
Accepted tokens:                         41233     
Per-position acceptance (%):
  Position 0:                            76.39     
  Position 1:                            58.66     
  Position 2:                            45.91     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:43 [loggers.py:257] Engine 000: Avg prompt throughput: 118.7 tokens/s, Avg generation throughput: 527.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.73, Accepted throughput: 336.69 tokens/s, Drafted throughput: 584.69 tokens/s, Accepted: 3367 tokens, Drafted: 5847 tokens, Per-position acceptance rate: 0.751, 0.564, 0.412, Avg Draft acceptance rate: 57.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:05:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd1857b6fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b2b7868b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:03 [loggers.py:257] Engine 000: Avg prompt throughput: 948.1 tokens/s, Avg generation throughput: 325.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 70.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 109.37 tokens/s, Drafted throughput: 151.01 tokens/s, Accepted: 2188 tokens, Drafted: 3021 tokens, Per-position acceptance rate: 0.842, 0.713, 0.618, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:13 [loggers.py:257] Engine 000: Avg prompt throughput: 995.1 tokens/s, Avg generation throughput: 2050.8 tokens/s, Running: 52 reqs, Waiting: 9 reqs, GPU KV cache usage: 94.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 1314.14 tokens/s, Drafted throughput: 2213.00 tokens/s, Accepted: 13142 tokens, Drafted: 22131 tokens, Per-position acceptance rate: 0.756, 0.578, 0.448, Avg Draft acceptance rate: 59.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1836.7 tokens/s, Avg generation throughput: 1910.1 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 82.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 1239.07 tokens/s, Drafted throughput: 2011.30 tokens/s, Accepted: 12392 tokens, Drafted: 20115 tokens, Per-position acceptance rate: 0.771, 0.599, 0.478, Avg Draft acceptance rate: 61.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1360.0 tokens/s, Avg generation throughput: 1958.2 tokens/s, Running: 55 reqs, Waiting: 0 reqs, GPU KV cache usage: 75.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1249.36 tokens/s, Drafted throughput: 2121.37 tokens/s, Accepted: 12495 tokens, Drafted: 21216 tokens, Per-position acceptance rate: 0.750, 0.572, 0.445, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1425.2 tokens/s, Avg generation throughput: 2052.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 94.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 1322.42 tokens/s, Drafted throughput: 2189.54 tokens/s, Accepted: 13227 tokens, Drafted: 21900 tokens, Per-position acceptance rate: 0.765, 0.587, 0.460, Avg Draft acceptance rate: 60.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1657.4 tokens/s, Avg generation throughput: 1963.1 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 70.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:06:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 1263.06 tokens/s, Drafted throughput: 2103.67 tokens/s, Accepted: 12632 tokens, Drafted: 21039 tokens, Per-position acceptance rate: 0.758, 0.585, 0.458, Avg Draft acceptance rate: 60.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1153.1 tokens/s, Avg generation throughput: 2070.4 tokens/s, Running: 56 reqs, Waiting: 7 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 1320.27 tokens/s, Drafted throughput: 2250.89 tokens/s, Accepted: 13208 tokens, Drafted: 22518 tokens, Per-position acceptance rate: 0.745, 0.571, 0.444, Avg Draft acceptance rate: 58.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  68.36     
Total input tokens:                      94775     
Total generated tokens:                  127928    
Request throughput (req/s):              9.36      
Output token throughput (tok/s):         1871.52   
Peak output token throughput (tok/s):    896.00    
Peak concurrent requests:                94.00     
Total token throughput (tok/s):          3258.03   
---------------Time to First Token----------------
Mean TTFT (ms):                          293.17    
Median TTFT (ms):                        240.19    
P99 TTFT (ms):                           1011.75   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          30.62     
Median TPOT (ms):                        30.51     
P99 TPOT (ms):                           39.73     
---------------Inter-token Latency----------------
Mean ITL (ms):                           85.07     
Median ITL (ms):                         72.23     
P99 ITL (ms):                            196.55    
---------------Speculative Decoding---------------
Acceptance rate (%):                     59.89     
Acceptance length:                       2.80      
Drafts:                                  45745     
Draft tokens:                            137235    
Accepted tokens:                         82186     
Per-position acceptance (%):
  Position 0:                            75.74     
  Position 1:                            58.25     
  Position 2:                            45.67     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:14 [loggers.py:257] Engine 000: Avg prompt throughput: 118.1 tokens/s, Avg generation throughput: 480.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.65, Accepted throughput: 303.09 tokens/s, Drafted throughput: 551.08 tokens/s, Accepted: 3031 tokens, Drafted: 5511 tokens, Per-position acceptance rate: 0.714, 0.529, 0.407, Avg Draft acceptance rate: 55.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3e1d79afc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-37eb8684-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:34 [loggers.py:257] Engine 000: Avg prompt throughput: 767.7 tokens/s, Avg generation throughput: 36.6 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 10.65 tokens/s, Drafted throughput: 15.30 tokens/s, Accepted: 213 tokens, Drafted: 306 tokens, Per-position acceptance rate: 0.804, 0.686, 0.598, Avg Draft acceptance rate: 69.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1310.6 tokens/s, Avg generation throughput: 2053.2 tokens/s, Running: 68 reqs, Waiting: 60 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 1328.60 tokens/s, Drafted throughput: 2149.34 tokens/s, Accepted: 13287 tokens, Drafted: 21495 tokens, Per-position acceptance rate: 0.778, 0.602, 0.475, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1221.1 tokens/s, Avg generation throughput: 1932.6 tokens/s, Running: 58 reqs, Waiting: 70 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:07:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 1238.21 tokens/s, Drafted throughput: 2076.68 tokens/s, Accepted: 12387 tokens, Drafted: 20775 tokens, Per-position acceptance rate: 0.758, 0.583, 0.448, Avg Draft acceptance rate: 59.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:04 [loggers.py:257] Engine 000: Avg prompt throughput: 2056.6 tokens/s, Avg generation throughput: 1747.4 tokens/s, Running: 88 reqs, Waiting: 37 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 1122.44 tokens/s, Drafted throughput: 1859.00 tokens/s, Accepted: 11225 tokens, Drafted: 18591 tokens, Per-position acceptance rate: 0.756, 0.585, 0.471, Avg Draft acceptance rate: 60.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:14 [loggers.py:257] Engine 000: Avg prompt throughput: 945.9 tokens/s, Avg generation throughput: 2095.4 tokens/s, Running: 61 reqs, Waiting: 66 reqs, GPU KV cache usage: 98.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 1330.52 tokens/s, Drafted throughput: 2284.96 tokens/s, Accepted: 13306 tokens, Drafted: 22851 tokens, Per-position acceptance rate: 0.745, 0.568, 0.434, Avg Draft acceptance rate: 58.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1382.8 tokens/s, Avg generation throughput: 1748.9 tokens/s, Running: 71 reqs, Waiting: 53 reqs, GPU KV cache usage: 94.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 1107.71 tokens/s, Drafted throughput: 1913.68 tokens/s, Accepted: 11079 tokens, Drafted: 19140 tokens, Per-position acceptance rate: 0.744, 0.561, 0.432, Avg Draft acceptance rate: 57.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1649.7 tokens/s, Avg generation throughput: 2035.4 tokens/s, Running: 69 reqs, Waiting: 59 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 1306.93 tokens/s, Drafted throughput: 2175.26 tokens/s, Accepted: 13075 tokens, Drafted: 21762 tokens, Per-position acceptance rate: 0.753, 0.587, 0.463, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1012.1 tokens/s, Avg generation throughput: 1948.7 tokens/s, Running: 55 reqs, Waiting: 69 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.73, Accepted throughput: 1235.18 tokens/s, Drafted throughput: 2136.40 tokens/s, Accepted: 12353 tokens, Drafted: 21366 tokens, Per-position acceptance rate: 0.739, 0.559, 0.437, Avg Draft acceptance rate: 57.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1939.2 tokens/s, Avg generation throughput: 1798.5 tokens/s, Running: 92 reqs, Waiting: 35 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:08:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1155.58 tokens/s, Drafted throughput: 1908.11 tokens/s, Accepted: 11557 tokens, Drafted: 19083 tokens, Per-position acceptance rate: 0.760, 0.590, 0.467, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1014.5 tokens/s, Avg generation throughput: 2076.7 tokens/s, Running: 59 reqs, Waiting: 69 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 1322.17 tokens/s, Drafted throughput: 2260.45 tokens/s, Accepted: 13222 tokens, Drafted: 22605 tokens, Per-position acceptance rate: 0.747, 0.565, 0.442, Avg Draft acceptance rate: 58.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1428.2 tokens/s, Avg generation throughput: 1725.7 tokens/s, Running: 63 reqs, Waiting: 56 reqs, GPU KV cache usage: 89.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 1098.48 tokens/s, Drafted throughput: 1875.82 tokens/s, Accepted: 10987 tokens, Drafted: 18762 tokens, Per-position acceptance rate: 0.748, 0.569, 0.440, Avg Draft acceptance rate: 58.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1722.4 tokens/s, Avg generation throughput: 1977.7 tokens/s, Running: 75 reqs, Waiting: 53 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 1283.44 tokens/s, Drafted throughput: 2067.14 tokens/s, Accepted: 12841 tokens, Drafted: 20682 tokens, Per-position acceptance rate: 0.772, 0.608, 0.482, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:34 [loggers.py:257] Engine 000: Avg prompt throughput: 918.6 tokens/s, Avg generation throughput: 1943.7 tokens/s, Running: 56 reqs, Waiting: 71 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.73, Accepted throughput: 1232.00 tokens/s, Drafted throughput: 2131.19 tokens/s, Accepted: 12320 tokens, Drafted: 21312 tokens, Per-position acceptance rate: 0.739, 0.566, 0.429, Avg Draft acceptance rate: 57.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1554.3 tokens/s, Avg generation throughput: 1796.1 tokens/s, Running: 58 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 1153.95 tokens/s, Drafted throughput: 1922.15 tokens/s, Accepted: 11541 tokens, Drafted: 19224 tokens, Per-position acceptance rate: 0.761, 0.584, 0.456, Avg Draft acceptance rate: 60.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  135.87    
Total input tokens:                      189093    
Total generated tokens:                  255919    
Request throughput (req/s):              9.42      
Output token throughput (tok/s):         1883.49   
Peak output token throughput (tok/s):    1146.00   
Peak concurrent requests:                150.00    
Total token throughput (tok/s):          3275.17   
---------------Time to First Token----------------
Mean TTFT (ms):                          5005.88   
Median TTFT (ms):                        6196.13   
P99 TTFT (ms):                           8432.49   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          40.72     
Median TPOT (ms):                        38.07     
P99 TPOT (ms):                           67.46     
---------------Inter-token Latency----------------
Mean ITL (ms):                           112.09    
Median ITL (ms):                         77.86     
P99 ITL (ms):                            316.28    
---------------Speculative Decoding---------------
Acceptance rate (%):                     59.37     
Acceptance length:                       2.78      
Drafts:                                  91866     
Draft tokens:                            275598    
Accepted tokens:                         163615    
Per-position acceptance (%):
  Position 0:                            75.32     
  Position 1:                            57.75     
  Position 2:                            45.03     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 690.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:09:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.66, Accepted throughput: 435.08 tokens/s, Drafted throughput: 785.96 tokens/s, Accepted: 4351 tokens, Drafted: 7860 tokens, Per-position acceptance rate: 0.731, 0.532, 0.397, Avg Draft acceptance rate: 55.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9aef136fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-00d98426-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:14 [loggers.py:257] Engine 000: Avg prompt throughput: 722.5 tokens/s, Avg generation throughput: 30.4 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 40.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 8.55 tokens/s, Drafted throughput: 12.90 tokens/s, Accepted: 171 tokens, Drafted: 258 tokens, Per-position acceptance rate: 0.791, 0.651, 0.547, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1327.5 tokens/s, Avg generation throughput: 2032.4 tokens/s, Running: 69 reqs, Waiting: 185 reqs, GPU KV cache usage: 98.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1308.28 tokens/s, Drafted throughput: 2147.68 tokens/s, Accepted: 13083 tokens, Drafted: 21477 tokens, Per-position acceptance rate: 0.768, 0.591, 0.468, Avg Draft acceptance rate: 60.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1249.9 tokens/s, Avg generation throughput: 1969.4 tokens/s, Running: 55 reqs, Waiting: 198 reqs, GPU KV cache usage: 92.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 1262.58 tokens/s, Drafted throughput: 2117.80 tokens/s, Accepted: 12627 tokens, Drafted: 21180 tokens, Per-position acceptance rate: 0.756, 0.580, 0.453, Avg Draft acceptance rate: 59.6%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:44 [loggers.py:257] Engine 000: Avg prompt throughput: 2122.9 tokens/s, Avg generation throughput: 1741.3 tokens/s, Running: 91 reqs, Waiting: 162 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 1125.37 tokens/s, Drafted throughput: 1831.45 tokens/s, Accepted: 11254 tokens, Drafted: 18315 tokens, Per-position acceptance rate: 0.767, 0.596, 0.480, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:54 [loggers.py:257] Engine 000: Avg prompt throughput: 831.7 tokens/s, Avg generation throughput: 2075.3 tokens/s, Running: 60 reqs, Waiting: 194 reqs, GPU KV cache usage: 94.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:10:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1323.25 tokens/s, Drafted throughput: 2248.67 tokens/s, Accepted: 13235 tokens, Drafted: 22491 tokens, Per-position acceptance rate: 0.747, 0.573, 0.445, Avg Draft acceptance rate: 58.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1478.8 tokens/s, Avg generation throughput: 1801.9 tokens/s, Running: 71 reqs, Waiting: 178 reqs, GPU KV cache usage: 92.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1148.70 tokens/s, Drafted throughput: 1947.44 tokens/s, Accepted: 11488 tokens, Drafted: 19476 tokens, Per-position acceptance rate: 0.750, 0.571, 0.449, Avg Draft acceptance rate: 59.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1683.2 tokens/s, Avg generation throughput: 2069.4 tokens/s, Running: 70 reqs, Waiting: 186 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 1333.16 tokens/s, Drafted throughput: 2197.87 tokens/s, Accepted: 13333 tokens, Drafted: 21981 tokens, Per-position acceptance rate: 0.759, 0.592, 0.469, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:24 [loggers.py:257] Engine 000: Avg prompt throughput: 980.2 tokens/s, Avg generation throughput: 1955.9 tokens/s, Running: 54 reqs, Waiting: 199 reqs, GPU KV cache usage: 95.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 1234.87 tokens/s, Drafted throughput: 2160.07 tokens/s, Accepted: 12350 tokens, Drafted: 21603 tokens, Per-position acceptance rate: 0.734, 0.553, 0.429, Avg Draft acceptance rate: 57.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1934.5 tokens/s, Avg generation throughput: 1860.8 tokens/s, Running: 88 reqs, Waiting: 168 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 1207.60 tokens/s, Drafted throughput: 1942.71 tokens/s, Accepted: 12084 tokens, Drafted: 19440 tokens, Per-position acceptance rate: 0.773, 0.608, 0.484, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1053.0 tokens/s, Avg generation throughput: 2067.4 tokens/s, Running: 56 reqs, Waiting: 198 reqs, GPU KV cache usage: 96.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 1312.72 tokens/s, Drafted throughput: 2260.23 tokens/s, Accepted: 13127 tokens, Drafted: 22602 tokens, Per-position acceptance rate: 0.743, 0.566, 0.434, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1584.8 tokens/s, Avg generation throughput: 1685.2 tokens/s, Running: 71 reqs, Waiting: 176 reqs, GPU KV cache usage: 91.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:11:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1073.48 tokens/s, Drafted throughput: 1823.19 tokens/s, Accepted: 10736 tokens, Drafted: 18234 tokens, Per-position acceptance rate: 0.750, 0.569, 0.447, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1454.4 tokens/s, Avg generation throughput: 2082.1 tokens/s, Running: 71 reqs, Waiting: 184 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 1349.74 tokens/s, Drafted throughput: 2184.59 tokens/s, Accepted: 13503 tokens, Drafted: 21855 tokens, Per-position acceptance rate: 0.775, 0.605, 0.473, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1078.0 tokens/s, Avg generation throughput: 1933.3 tokens/s, Running: 51 reqs, Waiting: 198 reqs, GPU KV cache usage: 92.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 1226.15 tokens/s, Drafted throughput: 2110.08 tokens/s, Accepted: 12264 tokens, Drafted: 21105 tokens, Per-position acceptance rate: 0.738, 0.571, 0.434, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:24 [loggers.py:257] Engine 000: Avg prompt throughput: 2073.7 tokens/s, Avg generation throughput: 1774.5 tokens/s, Running: 89 reqs, Waiting: 167 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 1152.71 tokens/s, Drafted throughput: 1843.60 tokens/s, Accepted: 11534 tokens, Drafted: 18447 tokens, Per-position acceptance rate: 0.779, 0.610, 0.487, Avg Draft acceptance rate: 62.5%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:34 [loggers.py:257] Engine 000: Avg prompt throughput: 927.2 tokens/s, Avg generation throughput: 2017.4 tokens/s, Running: 62 reqs, Waiting: 194 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1287.32 tokens/s, Drafted throughput: 2178.23 tokens/s, Accepted: 12879 tokens, Drafted: 21792 tokens, Per-position acceptance rate: 0.754, 0.577, 0.442, Avg Draft acceptance rate: 59.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1530.3 tokens/s, Avg generation throughput: 1745.7 tokens/s, Running: 74 reqs, Waiting: 174 reqs, GPU KV cache usage: 94.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 1104.65 tokens/s, Drafted throughput: 1907.31 tokens/s, Accepted: 11047 tokens, Drafted: 19074 tokens, Per-position acceptance rate: 0.746, 0.562, 0.430, Avg Draft acceptance rate: 57.9%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1435.2 tokens/s, Avg generation throughput: 2038.4 tokens/s, Running: 72 reqs, Waiting: 184 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:12:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 1315.81 tokens/s, Drafted throughput: 2156.18 tokens/s, Accepted: 13165 tokens, Drafted: 21573 tokens, Per-position acceptance rate: 0.761, 0.599, 0.471, Avg Draft acceptance rate: 61.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1015.5 tokens/s, Avg generation throughput: 1923.2 tokens/s, Running: 51 reqs, Waiting: 196 reqs, GPU KV cache usage: 85.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.73, Accepted throughput: 1217.69 tokens/s, Drafted throughput: 2110.14 tokens/s, Accepted: 12179 tokens, Drafted: 21105 tokens, Per-position acceptance rate: 0.743, 0.560, 0.428, Avg Draft acceptance rate: 57.7%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1981.3 tokens/s, Avg generation throughput: 1913.3 tokens/s, Running: 84 reqs, Waiting: 172 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 1241.40 tokens/s, Drafted throughput: 1992.38 tokens/s, Accepted: 12421 tokens, Drafted: 19935 tokens, Per-position acceptance rate: 0.773, 0.608, 0.488, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1138.9 tokens/s, Avg generation throughput: 2063.8 tokens/s, Running: 58 reqs, Waiting: 198 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 1311.01 tokens/s, Drafted throughput: 2245.18 tokens/s, Accepted: 13112 tokens, Drafted: 22455 tokens, Per-position acceptance rate: 0.747, 0.568, 0.437, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1280.0 tokens/s, Avg generation throughput: 1739.8 tokens/s, Running: 62 reqs, Waiting: 181 reqs, GPU KV cache usage: 79.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 1104.98 tokens/s, Drafted throughput: 1892.49 tokens/s, Accepted: 11051 tokens, Drafted: 18927 tokens, Per-position acceptance rate: 0.748, 0.566, 0.438, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1700.1 tokens/s, Avg generation throughput: 2149.0 tokens/s, Running: 74 reqs, Waiting: 181 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 1401.47 tokens/s, Drafted throughput: 2226.20 tokens/s, Accepted: 14021 tokens, Drafted: 22272 tokens, Per-position acceptance rate: 0.782, 0.616, 0.490, Avg Draft acceptance rate: 63.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1040.1 tokens/s, Avg generation throughput: 1962.2 tokens/s, Running: 56 reqs, Waiting: 199 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:13:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 1244.56 tokens/s, Drafted throughput: 2140.78 tokens/s, Accepted: 12451 tokens, Drafted: 21417 tokens, Per-position acceptance rate: 0.755, 0.562, 0.428, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:04 [loggers.py:257] Engine 000: Avg prompt throughput: 2020.2 tokens/s, Avg generation throughput: 1807.6 tokens/s, Running: 90 reqs, Waiting: 165 reqs, GPU KV cache usage: 96.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 1174.16 tokens/s, Drafted throughput: 1888.14 tokens/s, Accepted: 11742 tokens, Drafted: 18882 tokens, Per-position acceptance rate: 0.772, 0.610, 0.483, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:14 [loggers.py:257] Engine 000: Avg prompt throughput: 972.3 tokens/s, Avg generation throughput: 2135.4 tokens/s, Running: 60 reqs, Waiting: 195 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 1360.48 tokens/s, Drafted throughput: 2318.89 tokens/s, Accepted: 13609 tokens, Drafted: 23196 tokens, Per-position acceptance rate: 0.755, 0.569, 0.436, Avg Draft acceptance rate: 58.7%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1492.0 tokens/s, Avg generation throughput: 1803.7 tokens/s, Running: 76 reqs, Waiting: 91 reqs, GPU KV cache usage: 87.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 1151.23 tokens/s, Drafted throughput: 1947.48 tokens/s, Accepted: 11513 tokens, Drafted: 19476 tokens, Per-position acceptance rate: 0.751, 0.575, 0.447, Avg Draft acceptance rate: 59.1%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1223.3 tokens/s, Avg generation throughput: 2207.1 tokens/s, Running: 70 reqs, Waiting: 4 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 1432.37 tokens/s, Drafted throughput: 2314.84 tokens/s, Accepted: 14329 tokens, Drafted: 23157 tokens, Per-position acceptance rate: 0.769, 0.607, 0.480, Avg Draft acceptance rate: 61.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  265.79    
Total input tokens:                      373233    
Total generated tokens:                  511903    
Request throughput (req/s):              9.63      
Output token throughput (tok/s):         1925.96   
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                280.00    
Total token throughput (tok/s):          3330.20   
---------------Time to First Token----------------
Mean TTFT (ms):                          17371.79  
Median TTFT (ms):                        17330.96  
P99 TTFT (ms):                           21421.69  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          40.96     
Median TPOT (ms):                        38.11     
P99 TPOT (ms):                           65.10     
---------------Inter-token Latency----------------
Mean ITL (ms):                           113.34    
Median ITL (ms):                         78.40     
P99 ITL (ms):                            330.76    
---------------Speculative Decoding---------------
Acceptance rate (%):                     59.83     
Acceptance length:                       2.79      
Drafts:                                  182775    
Draft tokens:                            548325    
Accepted tokens:                         328074    
Per-position acceptance (%):
  Position 0:                            75.72     
  Position 1:                            58.27     
  Position 2:                            45.51     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k3-t0.0-tp1...
[0;36m(APIServer pid=818184)[0;0m INFO 01-22 21:14:38 [launcher.py:110] Shutting down FastAPI HTTP server.
