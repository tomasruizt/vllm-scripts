Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k4-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k4-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 825760
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:30 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:30 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15018, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 4, 'max_model_len': 5000}}
[0;36m(APIServer pid=825760)[0;0m WARNING 01-22 21:15:30 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:31 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:31 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:32 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:32 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:32 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=825760)[0;0m WARNING 01-22 21:15:32 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:15:32 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc279236fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6aaa4532-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 21:15:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:15:43 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=4), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-22 21:15:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:15:45 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.63:53621 backend=nccl
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:15:45 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=825860)[0;0m WARNING 01-22 21:15:45 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:15:46 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:15:46 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 21:15:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:15:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:15:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:39 [default_loader.py:291] Loading weights took 50.69 seconds
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:39 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:39 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:39 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-22 21:16:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:45 [default_loader.py:291] Loading weights took 5.03 seconds
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:45 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 58.749419 seconds
WARNING 01-22 21:16:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:16:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:58 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:16:58 [backends.py:704] Dynamo bytecode transform time: 11.81 s
WARNING 01-22 21:17:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:17:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:17:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:13 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.392 s
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:13 [monitor.py:34] torch.compile takes 14.20 s in total
WARNING 01-22 21:17:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:19 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:19 [backends.py:704] Dynamo bytecode transform time: 6.00 s
WARNING 01-22 21:17:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:17:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:26 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 0.911 s
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:26 [monitor.py:34] torch.compile takes 21.11 s in total
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:28 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:28 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:28 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-22 21:17:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:17:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:17:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
WARNING 01-22 21:17:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:45 [gpu_model_runner.py:4880] Graph capturing finished in 16 secs, took 0.03 GiB
[0;36m(EngineCore_DP0 pid=825860)[0;0m INFO 01-22 21:17:45 [core.py:272] init engine (profile, create kv cache, warmup model) took 59.88 seconds
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:47 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=825760)[0;0m WARNING 01-22 21:17:47 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:47 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:48 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:48 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:49 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:49 [serving.py:221] Chat template warmup completed in 1741.9ms
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
WARNING 01-22 21:17:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15018 ssl:default [Connect call failed (\'127.0.0.1\', 15018)]\n''
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15018
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:17:50 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:00 [loggers.py:257] Engine 000: Avg prompt throughput: 28.3 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 13.60 tokens/s, Drafted throughput: 23.49 tokens/s, Accepted: 176 tokens, Drafted: 304 tokens, Per-position acceptance rate: 0.763, 0.618, 0.526, 0.408, Avg Draft acceptance rate: 57.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:10 [loggers.py:257] Engine 000: Avg prompt throughput: 32.8 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 33.90 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 339 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.754, 0.569, 0.407, 0.299, Avg Draft acceptance rate: 50.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:20 [loggers.py:257] Engine 000: Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 34.30 tokens/s, Drafted throughput: 65.99 tokens/s, Accepted: 343 tokens, Drafted: 660 tokens, Per-position acceptance rate: 0.758, 0.564, 0.442, 0.315, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:30 [loggers.py:257] Engine 000: Avg prompt throughput: 25.9 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 34.90 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 349 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.772, 0.557, 0.437, 0.323, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:40 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 35.40 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 354 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.759, 0.566, 0.464, 0.343, Avg Draft acceptance rate: 53.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:50 [loggers.py:257] Engine 000: Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 55.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:18:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 38.79 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 388 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.807, 0.627, 0.524, 0.380, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:00 [loggers.py:257] Engine 000: Avg prompt throughput: 26.8 tokens/s, Avg generation throughput: 49.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 33.30 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 333 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.737, 0.533, 0.413, 0.311, Avg Draft acceptance rate: 49.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:10 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 34.90 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 349 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.777, 0.572, 0.416, 0.337, Avg Draft acceptance rate: 52.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:20 [loggers.py:257] Engine 000: Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 39.30 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 393 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.802, 0.623, 0.521, 0.407, Avg Draft acceptance rate: 58.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:30 [loggers.py:257] Engine 000: Avg prompt throughput: 21.4 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 33.00 tokens/s, Drafted throughput: 67.20 tokens/s, Accepted: 330 tokens, Drafted: 672 tokens, Per-position acceptance rate: 0.702, 0.506, 0.417, 0.339, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:40 [loggers.py:257] Engine 000: Avg prompt throughput: 41.2 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 35.20 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 352 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.743, 0.575, 0.467, 0.323, Avg Draft acceptance rate: 52.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:50 [loggers.py:257] Engine 000: Avg prompt throughput: 38.6 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:19:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 35.30 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 353 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.784, 0.569, 0.431, 0.329, Avg Draft acceptance rate: 52.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:00 [loggers.py:257] Engine 000: Avg prompt throughput: 49.9 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 36.60 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 366 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.783, 0.602, 0.464, 0.355, Avg Draft acceptance rate: 55.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:10 [loggers.py:257] Engine 000: Avg prompt throughput: 49.2 tokens/s, Avg generation throughput: 51.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 34.70 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 347 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.743, 0.569, 0.437, 0.329, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:20 [loggers.py:257] Engine 000: Avg prompt throughput: 14.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 36.40 tokens/s, Drafted throughput: 67.20 tokens/s, Accepted: 364 tokens, Drafted: 672 tokens, Per-position acceptance rate: 0.744, 0.583, 0.488, 0.351, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:30 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 57.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 40.70 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 407 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.825, 0.663, 0.530, 0.434, Avg Draft acceptance rate: 61.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:40 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 52.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 35.70 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 357 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.737, 0.611, 0.467, 0.323, Avg Draft acceptance rate: 53.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:50 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:20:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 34.70 tokens/s, Drafted throughput: 67.19 tokens/s, Accepted: 347 tokens, Drafted: 672 tokens, Per-position acceptance rate: 0.732, 0.607, 0.435, 0.292, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:00 [loggers.py:257] Engine 000: Avg prompt throughput: 44.9 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 38.50 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 385 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.789, 0.675, 0.500, 0.355, Avg Draft acceptance rate: 58.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  190.79    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.26      
Output token throughput (tok/s):         52.41     
Peak output token throughput (tok/s):    18.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          90.29     
---------------Time to First Token----------------
Mean TTFT (ms):                          74.52     
Median TTFT (ms):                        73.10     
P99 TTFT (ms):                           104.84    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.72     
Median TPOT (ms):                        18.47     
P99 TPOT (ms):                           21.01     
---------------Inter-token Latency----------------
Mean ITL (ms):                           58.80     
Median ITL (ms):                         58.76     
P99 ITL (ms):                            59.36     
---------------Speculative Decoding---------------
Acceptance rate (%):                     54.06     
Acceptance length:                       3.16      
Drafts:                                  3168      
Draft tokens:                            12672     
Accepted tokens:                         6851      
Per-position acceptance (%):
  Position 0:                            76.52     
  Position 1:                            59.03     
  Position 2:                            46.18     
  Position 3:                            34.53     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:10 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 35.80 tokens/s, Drafted throughput: 61.60 tokens/s, Accepted: 358 tokens, Drafted: 616 tokens, Per-position acceptance rate: 0.792, 0.636, 0.494, 0.403, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f66181a2fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1a1f3b3a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:30 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 5.80 tokens/s, Drafted throughput: 10.40 tokens/s, Accepted: 116 tokens, Drafted: 208 tokens, Per-position acceptance rate: 0.731, 0.615, 0.500, 0.385, Avg Draft acceptance rate: 55.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:40 [loggers.py:257] Engine 000: Avg prompt throughput: 92.6 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 63.39 tokens/s, Drafted throughput: 116.79 tokens/s, Accepted: 634 tokens, Drafted: 1168 tokens, Per-position acceptance rate: 0.795, 0.582, 0.449, 0.346, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:50 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:21:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 68.29 tokens/s, Drafted throughput: 131.59 tokens/s, Accepted: 683 tokens, Drafted: 1316 tokens, Per-position acceptance rate: 0.754, 0.553, 0.444, 0.325, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:00 [loggers.py:257] Engine 000: Avg prompt throughput: 78.1 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 69.60 tokens/s, Drafted throughput: 130.79 tokens/s, Accepted: 696 tokens, Drafted: 1308 tokens, Per-position acceptance rate: 0.752, 0.560, 0.468, 0.349, Avg Draft acceptance rate: 53.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:10 [loggers.py:257] Engine 000: Avg prompt throughput: 81.1 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 71.00 tokens/s, Drafted throughput: 131.19 tokens/s, Accepted: 710 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.765, 0.579, 0.454, 0.366, Avg Draft acceptance rate: 54.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:20 [loggers.py:257] Engine 000: Avg prompt throughput: 67.0 tokens/s, Avg generation throughput: 103.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 70.19 tokens/s, Drafted throughput: 131.18 tokens/s, Accepted: 702 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.747, 0.570, 0.466, 0.357, Avg Draft acceptance rate: 53.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:30 [loggers.py:257] Engine 000: Avg prompt throughput: 86.7 tokens/s, Avg generation throughput: 103.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 70.80 tokens/s, Drafted throughput: 131.19 tokens/s, Accepted: 708 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.793, 0.595, 0.460, 0.311, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:40 [loggers.py:257] Engine 000: Avg prompt throughput: 82.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 70.00 tokens/s, Drafted throughput: 129.60 tokens/s, Accepted: 700 tokens, Drafted: 1296 tokens, Per-position acceptance rate: 0.759, 0.580, 0.472, 0.349, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:50 [loggers.py:257] Engine 000: Avg prompt throughput: 73.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:22:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 73.49 tokens/s, Drafted throughput: 131.99 tokens/s, Accepted: 735 tokens, Drafted: 1320 tokens, Per-position acceptance rate: 0.761, 0.597, 0.488, 0.382, Avg Draft acceptance rate: 55.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:00 [loggers.py:257] Engine 000: Avg prompt throughput: 83.8 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 72.39 tokens/s, Drafted throughput: 129.59 tokens/s, Accepted: 724 tokens, Drafted: 1296 tokens, Per-position acceptance rate: 0.778, 0.623, 0.481, 0.352, Avg Draft acceptance rate: 55.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  98.97     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.51      
Output token throughput (tok/s):         101.04    
Peak output token throughput (tok/s):    34.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          174.06    
---------------Time to First Token----------------
Mean TTFT (ms):                          124.44    
Median TTFT (ms):                        122.82    
P99 TTFT (ms):                           154.12    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.83     
Median TPOT (ms):                        18.86     
P99 TPOT (ms):                           21.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           59.14     
Median ITL (ms):                         59.02     
P99 ITL (ms):                            64.04     
---------------Speculative Decoding---------------
Acceptance rate (%):                     54.10     
Acceptance length:                       3.16      
Drafts:                                  3169      
Draft tokens:                            12676     
Accepted tokens:                         6858      
Per-position acceptance (%):
  Position 0:                            76.65     
  Position 1:                            58.50     
  Position 2:                            46.48     
  Position 3:                            34.77     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:10 [loggers.py:257] Engine 000: Avg prompt throughput: 33.8 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 58.90 tokens/s, Drafted throughput: 108.00 tokens/s, Accepted: 589 tokens, Drafted: 1080 tokens, Per-position acceptance rate: 0.770, 0.611, 0.459, 0.341, Avg Draft acceptance rate: 54.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fa8f926efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-130ec07b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:30 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 6.00 tokens/s, Drafted throughput: 10.60 tokens/s, Accepted: 120 tokens, Drafted: 212 tokens, Per-position acceptance rate: 0.736, 0.623, 0.509, 0.396, Avg Draft acceptance rate: 56.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:40 [loggers.py:257] Engine 000: Avg prompt throughput: 147.4 tokens/s, Avg generation throughput: 175.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 118.19 tokens/s, Drafted throughput: 229.17 tokens/s, Accepted: 1182 tokens, Drafted: 2292 tokens, Per-position acceptance rate: 0.752, 0.550, 0.442, 0.319, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:50 [loggers.py:257] Engine 000: Avg prompt throughput: 136.4 tokens/s, Avg generation throughput: 196.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:23:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 132.39 tokens/s, Drafted throughput: 258.39 tokens/s, Accepted: 1324 tokens, Drafted: 2584 tokens, Per-position acceptance rate: 0.738, 0.548, 0.433, 0.330, Avg Draft acceptance rate: 51.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:00 [loggers.py:257] Engine 000: Avg prompt throughput: 150.2 tokens/s, Avg generation throughput: 199.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 135.29 tokens/s, Drafted throughput: 257.18 tokens/s, Accepted: 1353 tokens, Drafted: 2572 tokens, Per-position acceptance rate: 0.743, 0.560, 0.456, 0.345, Avg Draft acceptance rate: 52.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:10 [loggers.py:257] Engine 000: Avg prompt throughput: 171.1 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 144.00 tokens/s, Drafted throughput: 257.20 tokens/s, Accepted: 1440 tokens, Drafted: 2572 tokens, Per-position acceptance rate: 0.781, 0.596, 0.485, 0.378, Avg Draft acceptance rate: 56.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:20 [loggers.py:257] Engine 000: Avg prompt throughput: 117.6 tokens/s, Avg generation throughput: 197.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 135.30 tokens/s, Drafted throughput: 247.20 tokens/s, Accepted: 1353 tokens, Drafted: 2472 tokens, Per-position acceptance rate: 0.761, 0.620, 0.466, 0.343, Avg Draft acceptance rate: 54.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  51.20     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.98      
Output token throughput (tok/s):         195.31    
Peak output token throughput (tok/s):    68.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          336.46    
---------------Time to First Token----------------
Mean TTFT (ms):                          125.05    
Median TTFT (ms):                        124.66    
P99 TTFT (ms):                           155.94    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.33     
Median TPOT (ms):                        19.18     
P99 TPOT (ms):                           21.96     
---------------Inter-token Latency----------------
Mean ITL (ms):                           60.19     
Median ITL (ms):                         59.88     
P99 ITL (ms):                            67.58     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.29     
Acceptance length:                       3.13      
Drafts:                                  3196      
Draft tokens:                            12784     
Accepted tokens:                         6813      
Per-position acceptance (%):
  Position 0:                            75.56     
  Position 1:                            57.60     
  Position 2:                            45.59     
  Position 3:                            34.42     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 18.00 tokens/s, Drafted throughput: 33.20 tokens/s, Accepted: 180 tokens, Drafted: 332 tokens, Per-position acceptance rate: 0.795, 0.602, 0.410, 0.361, Avg Draft acceptance rate: 54.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f18b84c2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-74a17c7a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:40 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 4.80 tokens/s, Drafted throughput: 7.20 tokens/s, Accepted: 48 tokens, Drafted: 72 tokens, Per-position acceptance rate: 0.778, 0.722, 0.667, 0.500, Avg Draft acceptance rate: 66.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:50 [loggers.py:257] Engine 000: Avg prompt throughput: 215.3 tokens/s, Avg generation throughput: 280.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:24:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 191.09 tokens/s, Drafted throughput: 355.98 tokens/s, Accepted: 1911 tokens, Drafted: 3560 tokens, Per-position acceptance rate: 0.766, 0.579, 0.454, 0.348, Avg Draft acceptance rate: 53.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:00 [loggers.py:257] Engine 000: Avg prompt throughput: 316.8 tokens/s, Avg generation throughput: 379.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 258.48 tokens/s, Drafted throughput: 488.77 tokens/s, Accepted: 2585 tokens, Drafted: 4888 tokens, Per-position acceptance rate: 0.764, 0.566, 0.453, 0.331, Avg Draft acceptance rate: 52.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:10 [loggers.py:257] Engine 000: Avg prompt throughput: 279.7 tokens/s, Avg generation throughput: 390.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 267.47 tokens/s, Drafted throughput: 493.54 tokens/s, Accepted: 2675 tokens, Drafted: 4936 tokens, Per-position acceptance rate: 0.759, 0.594, 0.465, 0.350, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:20 [loggers.py:257] Engine 000: Avg prompt throughput: 259.3 tokens/s, Avg generation throughput: 390.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 268.16 tokens/s, Drafted throughput: 497.53 tokens/s, Accepted: 2682 tokens, Drafted: 4976 tokens, Per-position acceptance rate: 0.757, 0.591, 0.457, 0.350, Avg Draft acceptance rate: 53.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  43.08     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.86      
Output token throughput (tok/s):         371.40    
Peak output token throughput (tok/s):    136.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          642.76    
---------------Time to First Token----------------
Mean TTFT (ms):                          128.56    
Median TTFT (ms):                        128.96    
P99 TTFT (ms):                           159.29    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.23     
Median TPOT (ms):                        20.21     
P99 TPOT (ms):                           23.14     
---------------Inter-token Latency----------------
Mean ITL (ms):                           62.70     
Median ITL (ms):                         62.12     
P99 ITL (ms):                            72.88     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.08     
Acceptance length:                       3.12      
Drafts:                                  5136      
Draft tokens:                            20544     
Accepted tokens:                         10904     
Per-position acceptance (%):
  Position 0:                            75.58     
  Position 1:                            57.59     
  Position 2:                            45.13     
  Position 3:                            34.00     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:30 [loggers.py:257] Engine 000: Avg prompt throughput: 97.8 tokens/s, Avg generation throughput: 172.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 114.18 tokens/s, Drafted throughput: 236.36 tokens/s, Accepted: 1142 tokens, Drafted: 2364 tokens, Per-position acceptance rate: 0.714, 0.519, 0.398, 0.301, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc5158befc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f6ae9c44-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:50 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 219.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:25:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 75.29 tokens/s, Drafted throughput: 134.99 tokens/s, Accepted: 1506 tokens, Drafted: 2700 tokens, Per-position acceptance rate: 0.775, 0.599, 0.492, 0.366, Avg Draft acceptance rate: 55.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:00 [loggers.py:257] Engine 000: Avg prompt throughput: 521.3 tokens/s, Avg generation throughput: 717.6 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 489.08 tokens/s, Drafted throughput: 919.16 tokens/s, Accepted: 4891 tokens, Drafted: 9192 tokens, Per-position acceptance rate: 0.753, 0.577, 0.457, 0.341, Avg Draft acceptance rate: 53.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:10 [loggers.py:257] Engine 000: Avg prompt throughput: 550.2 tokens/s, Avg generation throughput: 726.0 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 501.08 tokens/s, Drafted throughput: 909.96 tokens/s, Accepted: 5011 tokens, Drafted: 9100 tokens, Per-position acceptance rate: 0.768, 0.593, 0.473, 0.369, Avg Draft acceptance rate: 55.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:20 [loggers.py:257] Engine 000: Avg prompt throughput: 638.6 tokens/s, Avg generation throughput: 716.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 491.99 tokens/s, Drafted throughput: 903.40 tokens/s, Accepted: 4921 tokens, Drafted: 9036 tokens, Per-position acceptance rate: 0.768, 0.601, 0.463, 0.346, Avg Draft acceptance rate: 54.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:30 [loggers.py:257] Engine 000: Avg prompt throughput: 510.6 tokens/s, Avg generation throughput: 733.1 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 509.40 tokens/s, Drafted throughput: 901.59 tokens/s, Accepted: 5094 tokens, Drafted: 9016 tokens, Per-position acceptance rate: 0.786, 0.601, 0.483, 0.390, Avg Draft acceptance rate: 56.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  45.91     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.49      
Output token throughput (tok/s):         696.85    
Peak output token throughput (tok/s):    255.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          1227.49   
---------------Time to First Token----------------
Mean TTFT (ms):                          145.29    
Median TTFT (ms):                        141.52    
P99 TTFT (ms):                           213.59    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.31     
Median TPOT (ms):                        21.48     
P99 TPOT (ms):                           24.97     
---------------Inter-token Latency----------------
Mean ITL (ms):                           67.56     
Median ITL (ms):                         66.10     
P99 ITL (ms):                            99.50     
---------------Speculative Decoding---------------
Acceptance rate (%):                     54.82     
Acceptance length:                       3.19      
Drafts:                                  10040     
Draft tokens:                            40160     
Accepted tokens:                         22017     
Per-position acceptance (%):
  Position 0:                            76.88     
  Position 1:                            59.27     
  Position 2:                            46.95     
  Position 3:                            36.19     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 73.29 tokens/s, Drafted throughput: 136.79 tokens/s, Accepted: 733 tokens, Drafted: 1368 tokens, Per-position acceptance rate: 0.760, 0.570, 0.444, 0.368, Avg Draft acceptance rate: 53.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:26:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fab7d19efc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-70dcfeae-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:00 [loggers.py:257] Engine 000: Avg prompt throughput: 754.8 tokens/s, Avg generation throughput: 714.7 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 38.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 243.83 tokens/s, Drafted throughput: 450.97 tokens/s, Accepted: 4877 tokens, Drafted: 9020 tokens, Per-position acceptance rate: 0.768, 0.581, 0.466, 0.347, Avg Draft acceptance rate: 54.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:10 [loggers.py:257] Engine 000: Avg prompt throughput: 979.3 tokens/s, Avg generation throughput: 1304.6 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 897.29 tokens/s, Drafted throughput: 1648.59 tokens/s, Accepted: 8974 tokens, Drafted: 16488 tokens, Per-position acceptance rate: 0.764, 0.593, 0.465, 0.356, Avg Draft acceptance rate: 54.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:20 [loggers.py:257] Engine 000: Avg prompt throughput: 1104.3 tokens/s, Avg generation throughput: 1301.3 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 898.59 tokens/s, Drafted throughput: 1622.42 tokens/s, Accepted: 8988 tokens, Drafted: 16228 tokens, Per-position acceptance rate: 0.772, 0.597, 0.474, 0.372, Avg Draft acceptance rate: 55.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:30 [loggers.py:257] Engine 000: Avg prompt throughput: 941.8 tokens/s, Avg generation throughput: 1319.0 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 907.82 tokens/s, Drafted throughput: 1663.45 tokens/s, Accepted: 9079 tokens, Drafted: 16636 tokens, Per-position acceptance rate: 0.769, 0.588, 0.463, 0.363, Avg Draft acceptance rate: 54.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:40 [loggers.py:257] Engine 000: Avg prompt throughput: 970.6 tokens/s, Avg generation throughput: 1314.9 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 42.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 899.44 tokens/s, Drafted throughput: 1663.52 tokens/s, Accepted: 8997 tokens, Drafted: 16640 tokens, Per-position acceptance rate: 0.757, 0.587, 0.460, 0.358, Avg Draft acceptance rate: 54.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  51.23     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.25      
Output token throughput (tok/s):         1249.21   
Peak output token throughput (tok/s):    479.00    
Peak concurrent requests:                52.00     
Total token throughput (tok/s):          2191.45   
---------------Time to First Token----------------
Mean TTFT (ms):                          166.02    
Median TTFT (ms):                        163.39    
P99 TTFT (ms):                           263.24    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.71     
Median TPOT (ms):                        23.75     
P99 TPOT (ms):                           28.20     
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.56     
Median ITL (ms):                         70.43     
P99 ITL (ms):                            116.30    
---------------Speculative Decoding---------------
Acceptance rate (%):                     54.22     
Acceptance length:                       3.17      
Drafts:                                  20245     
Draft tokens:                            80980     
Accepted tokens:                         43904     
Per-position acceptance (%):
  Position 0:                            76.31     
  Position 1:                            58.76     
  Position 2:                            46.12     
  Position 3:                            35.68     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:50 [loggers.py:257] Engine 000: Avg prompt throughput: 93.2 tokens/s, Avg generation throughput: 463.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:27:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 312.78 tokens/s, Drafted throughput: 621.96 tokens/s, Accepted: 3128 tokens, Drafted: 6220 tokens, Per-position acceptance rate: 0.730, 0.559, 0.409, 0.314, Avg Draft acceptance rate: 50.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f66e9436fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3845c388-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:10 [loggers.py:257] Engine 000: Avg prompt throughput: 948.2 tokens/s, Avg generation throughput: 572.4 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 84.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 200.75 tokens/s, Drafted throughput: 329.53 tokens/s, Accepted: 4016 tokens, Drafted: 6592 tokens, Per-position acceptance rate: 0.809, 0.658, 0.547, 0.422, Avg Draft acceptance rate: 60.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:20 [loggers.py:257] Engine 000: Avg prompt throughput: 1012.7 tokens/s, Avg generation throughput: 1853.6 tokens/s, Running: 51 reqs, Waiting: 8 reqs, GPU KV cache usage: 91.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 1258.38 tokens/s, Drafted throughput: 2398.57 tokens/s, Accepted: 12585 tokens, Drafted: 23988 tokens, Per-position acceptance rate: 0.753, 0.575, 0.443, 0.328, Avg Draft acceptance rate: 52.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1819.3 tokens/s, Avg generation throughput: 1883.6 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 84.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 1302.73 tokens/s, Drafted throughput: 2335.87 tokens/s, Accepted: 13028 tokens, Drafted: 23360 tokens, Per-position acceptance rate: 0.773, 0.604, 0.476, 0.378, Avg Draft acceptance rate: 55.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1190.2 tokens/s, Avg generation throughput: 1878.9 tokens/s, Running: 55 reqs, Waiting: 0 reqs, GPU KV cache usage: 86.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 1268.14 tokens/s, Drafted throughput: 2455.08 tokens/s, Accepted: 12682 tokens, Drafted: 24552 tokens, Per-position acceptance rate: 0.746, 0.560, 0.429, 0.331, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1594.6 tokens/s, Avg generation throughput: 1942.7 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 86.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:28:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 1330.65 tokens/s, Drafted throughput: 2463.20 tokens/s, Accepted: 13313 tokens, Drafted: 24644 tokens, Per-position acceptance rate: 0.754, 0.585, 0.462, 0.360, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1310.8 tokens/s, Avg generation throughput: 1949.8 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 83.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1322.43 tokens/s, Drafted throughput: 2530.87 tokens/s, Accepted: 13226 tokens, Drafted: 25312 tokens, Per-position acceptance rate: 0.749, 0.569, 0.438, 0.334, Avg Draft acceptance rate: 52.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1500.1 tokens/s, Avg generation throughput: 1930.9 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 88.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 1324.02 tokens/s, Drafted throughput: 2444.26 tokens/s, Accepted: 13241 tokens, Drafted: 24444 tokens, Per-position acceptance rate: 0.756, 0.588, 0.458, 0.364, Avg Draft acceptance rate: 54.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  69.84     
Total input tokens:                      94775     
Total generated tokens:                  127983    
Request throughput (req/s):              9.16      
Output token throughput (tok/s):         1832.43   
Peak output token throughput (tok/s):    768.00    
Peak concurrent requests:                94.00     
Total token throughput (tok/s):          3189.40   
---------------Time to First Token----------------
Mean TTFT (ms):                          301.19    
Median TTFT (ms):                        251.03    
P99 TTFT (ms):                           1014.02   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          31.87     
Median TPOT (ms):                        31.65     
P99 TPOT (ms):                           42.49     
---------------Inter-token Latency----------------
Mean ITL (ms):                           99.04     
Median ITL (ms):                         86.50     
P99 ITL (ms):                            199.35    
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.36     
Acceptance length:                       3.13      
Drafts:                                  40923     
Draft tokens:                            163692    
Accepted tokens:                         87339     
Per-position acceptance (%):
  Position 0:                            75.37     
  Position 1:                            57.98     
  Position 2:                            45.18     
  Position 3:                            34.89     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:20 [loggers.py:257] Engine 000: Avg prompt throughput: 118.1 tokens/s, Avg generation throughput: 804.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 538.68 tokens/s, Drafted throughput: 1105.17 tokens/s, Accepted: 5387 tokens, Drafted: 11052 tokens, Per-position acceptance rate: 0.705, 0.531, 0.409, 0.305, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f4839152fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-018664b4-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:40 [loggers.py:257] Engine 000: Avg prompt throughput: 336.1 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 22 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 8.50 tokens/s, Drafted throughput: 14.40 tokens/s, Accepted: 170 tokens, Drafted: 288 tokens, Per-position acceptance rate: 0.792, 0.639, 0.542, 0.389, Avg Draft acceptance rate: 59.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1742.3 tokens/s, Avg generation throughput: 1794.1 tokens/s, Running: 76 reqs, Waiting: 50 reqs, GPU KV cache usage: 97.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:29:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.21, Accepted throughput: 1229.57 tokens/s, Drafted throughput: 2222.75 tokens/s, Accepted: 12296 tokens, Drafted: 22228 tokens, Per-position acceptance rate: 0.770, 0.598, 0.479, 0.365, Avg Draft acceptance rate: 55.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1189.4 tokens/s, Avg generation throughput: 1937.2 tokens/s, Running: 59 reqs, Waiting: 69 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1319.77 tokens/s, Drafted throughput: 2470.16 tokens/s, Accepted: 13199 tokens, Drafted: 24704 tokens, Per-position acceptance rate: 0.759, 0.577, 0.449, 0.352, Avg Draft acceptance rate: 53.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1499.8 tokens/s, Avg generation throughput: 1658.6 tokens/s, Running: 71 reqs, Waiting: 49 reqs, GPU KV cache usage: 91.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 1122.66 tokens/s, Drafted throughput: 2142.14 tokens/s, Accepted: 11228 tokens, Drafted: 21424 tokens, Per-position acceptance rate: 0.744, 0.570, 0.443, 0.339, Avg Draft acceptance rate: 52.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:20 [loggers.py:257] Engine 000: Avg prompt throughput: 1486.1 tokens/s, Avg generation throughput: 1944.1 tokens/s, Running: 79 reqs, Waiting: 49 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 1335.97 tokens/s, Drafted throughput: 2423.87 tokens/s, Accepted: 13367 tokens, Drafted: 24252 tokens, Per-position acceptance rate: 0.766, 0.598, 0.469, 0.372, Avg Draft acceptance rate: 55.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:30 [loggers.py:257] Engine 000: Avg prompt throughput: 946.4 tokens/s, Avg generation throughput: 1874.1 tokens/s, Running: 60 reqs, Waiting: 68 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 1258.33 tokens/s, Drafted throughput: 2469.87 tokens/s, Accepted: 12588 tokens, Drafted: 24708 tokens, Per-position acceptance rate: 0.740, 0.554, 0.421, 0.324, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1533.7 tokens/s, Avg generation throughput: 1754.9 tokens/s, Running: 66 reqs, Waiting: 53 reqs, GPU KV cache usage: 88.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 1188.49 tokens/s, Drafted throughput: 2265.99 tokens/s, Accepted: 11885 tokens, Drafted: 22660 tokens, Per-position acceptance rate: 0.752, 0.568, 0.437, 0.342, Avg Draft acceptance rate: 52.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1579.5 tokens/s, Avg generation throughput: 1940.4 tokens/s, Running: 73 reqs, Waiting: 55 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:30:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 1324.53 tokens/s, Drafted throughput: 2452.35 tokens/s, Accepted: 13252 tokens, Drafted: 24536 tokens, Per-position acceptance rate: 0.757, 0.587, 0.459, 0.358, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:00 [loggers.py:257] Engine 000: Avg prompt throughput: 958.6 tokens/s, Avg generation throughput: 1957.5 tokens/s, Running: 52 reqs, Waiting: 72 reqs, GPU KV cache usage: 91.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1332.27 tokens/s, Drafted throughput: 2508.40 tokens/s, Accepted: 13327 tokens, Drafted: 25092 tokens, Per-position acceptance rate: 0.752, 0.575, 0.448, 0.350, Avg Draft acceptance rate: 53.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1629.8 tokens/s, Avg generation throughput: 1659.6 tokens/s, Running: 75 reqs, Waiting: 47 reqs, GPU KV cache usage: 92.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1121.59 tokens/s, Drafted throughput: 2143.38 tokens/s, Accepted: 11217 tokens, Drafted: 21436 tokens, Per-position acceptance rate: 0.743, 0.564, 0.444, 0.342, Avg Draft acceptance rate: 52.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:20 [loggers.py:257] Engine 000: Avg prompt throughput: 1337.2 tokens/s, Avg generation throughput: 1958.2 tokens/s, Running: 68 reqs, Waiting: 60 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 1336.94 tokens/s, Drafted throughput: 2487.18 tokens/s, Accepted: 13376 tokens, Drafted: 24884 tokens, Per-position acceptance rate: 0.757, 0.581, 0.455, 0.357, Avg Draft acceptance rate: 53.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1046.4 tokens/s, Avg generation throughput: 1791.2 tokens/s, Running: 53 reqs, Waiting: 72 reqs, GPU KV cache usage: 96.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 1206.69 tokens/s, Drafted throughput: 2344.18 tokens/s, Accepted: 12068 tokens, Drafted: 23444 tokens, Per-position acceptance rate: 0.735, 0.560, 0.431, 0.332, Avg Draft acceptance rate: 51.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1906.7 tokens/s, Avg generation throughput: 1788.0 tokens/s, Running: 84 reqs, Waiting: 41 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 1221.95 tokens/s, Drafted throughput: 2255.10 tokens/s, Accepted: 12220 tokens, Drafted: 22552 tokens, Per-position acceptance rate: 0.764, 0.590, 0.458, 0.356, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1044.9 tokens/s, Avg generation throughput: 1966.8 tokens/s, Running: 60 reqs, Waiting: 64 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:31:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1339.20 tokens/s, Drafted throughput: 2521.46 tokens/s, Accepted: 13397 tokens, Drafted: 25224 tokens, Per-position acceptance rate: 0.754, 0.581, 0.446, 0.344, Avg Draft acceptance rate: 53.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  139.91    
Total input tokens:                      189093    
Total generated tokens:                  255939    
Request throughput (req/s):              9.15      
Output token throughput (tok/s):         1829.29   
Peak output token throughput (tok/s):    970.00    
Peak concurrent requests:                153.00    
Total token throughput (tok/s):          3180.81   
---------------Time to First Token----------------
Mean TTFT (ms):                          5367.74   
Median TTFT (ms):                        6425.22   
P99 TTFT (ms):                           8518.27   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          41.10     
Median TPOT (ms):                        38.75     
P99 TPOT (ms):                           70.99     
---------------Inter-token Latency----------------
Mean ITL (ms):                           126.92    
Median ITL (ms):                         90.37     
P99 ITL (ms):                            338.12    
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.11     
Acceptance length:                       3.12      
Drafts:                                  81916     
Draft tokens:                            327664    
Accepted tokens:                         174014    
Per-position acceptance (%):
  Position 0:                            75.31     
  Position 1:                            57.62     
  Position 2:                            44.77     
  Position 3:                            34.73     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:00 [loggers.py:257] Engine 000: Avg prompt throughput: 686.1 tokens/s, Avg generation throughput: 1556.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 1056.23 tokens/s, Drafted throughput: 2048.27 tokens/s, Accepted: 10563 tokens, Drafted: 20484 tokens, Per-position acceptance rate: 0.748, 0.563, 0.426, 0.326, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1453c0efc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15018, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-333fa2cd-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:20 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 0.75 tokens/s, Drafted throughput: 1.00 tokens/s, Accepted: 15 tokens, Drafted: 20 tokens, Per-position acceptance rate: 0.800, 0.800, 0.800, 0.600, Avg Draft acceptance rate: 75.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1693.5 tokens/s, Avg generation throughput: 1280.0 tokens/s, Running: 57 reqs, Waiting: 198 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 881.81 tokens/s, Drafted throughput: 1549.04 tokens/s, Accepted: 8819 tokens, Drafted: 15492 tokens, Per-position acceptance rate: 0.785, 0.613, 0.502, 0.377, Avg Draft acceptance rate: 56.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1090.2 tokens/s, Avg generation throughput: 1589.0 tokens/s, Running: 70 reqs, Waiting: 180 reqs, GPU KV cache usage: 95.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 1082.74 tokens/s, Drafted throughput: 2029.09 tokens/s, Accepted: 10828 tokens, Drafted: 20292 tokens, Per-position acceptance rate: 0.764, 0.583, 0.446, 0.342, Avg Draft acceptance rate: 53.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1407.0 tokens/s, Avg generation throughput: 1892.7 tokens/s, Running: 69 reqs, Waiting: 186 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:32:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 1293.48 tokens/s, Drafted throughput: 2402.77 tokens/s, Accepted: 12935 tokens, Drafted: 24028 tokens, Per-position acceptance rate: 0.757, 0.581, 0.458, 0.358, Avg Draft acceptance rate: 53.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1014.0 tokens/s, Avg generation throughput: 1851.4 tokens/s, Running: 56 reqs, Waiting: 198 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 1240.42 tokens/s, Drafted throughput: 2441.45 tokens/s, Accepted: 12405 tokens, Drafted: 24416 tokens, Per-position acceptance rate: 0.730, 0.558, 0.421, 0.323, Avg Draft acceptance rate: 50.8%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1987.0 tokens/s, Avg generation throughput: 1759.2 tokens/s, Running: 86 reqs, Waiting: 168 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 1193.43 tokens/s, Drafted throughput: 2253.47 tokens/s, Accepted: 11935 tokens, Drafted: 22536 tokens, Per-position acceptance rate: 0.747, 0.577, 0.448, 0.346, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:20 [loggers.py:257] Engine 000: Avg prompt throughput: 843.9 tokens/s, Avg generation throughput: 1944.4 tokens/s, Running: 66 reqs, Waiting: 190 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 1300.98 tokens/s, Drafted throughput: 2575.95 tokens/s, Accepted: 13010 tokens, Drafted: 25760 tokens, Per-position acceptance rate: 0.733, 0.544, 0.416, 0.327, Avg Draft acceptance rate: 50.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1261.0 tokens/s, Avg generation throughput: 1743.5 tokens/s, Running: 60 reqs, Waiting: 193 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 1170.21 tokens/s, Drafted throughput: 2279.83 tokens/s, Accepted: 11703 tokens, Drafted: 22800 tokens, Per-position acceptance rate: 0.732, 0.556, 0.431, 0.335, Avg Draft acceptance rate: 51.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1725.5 tokens/s, Avg generation throughput: 1884.0 tokens/s, Running: 80 reqs, Waiting: 176 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 1286.69 tokens/s, Drafted throughput: 2367.71 tokens/s, Accepted: 12875 tokens, Drafted: 23692 tokens, Per-position acceptance rate: 0.754, 0.591, 0.466, 0.363, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1084.1 tokens/s, Avg generation throughput: 2050.6 tokens/s, Running: 61 reqs, Waiting: 195 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:33:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1396.43 tokens/s, Drafted throughput: 2615.15 tokens/s, Accepted: 13971 tokens, Drafted: 26164 tokens, Per-position acceptance rate: 0.751, 0.575, 0.454, 0.355, Avg Draft acceptance rate: 53.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1604.8 tokens/s, Avg generation throughput: 1651.9 tokens/s, Running: 77 reqs, Waiting: 175 reqs, GPU KV cache usage: 98.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 1108.88 tokens/s, Drafted throughput: 2171.97 tokens/s, Accepted: 11089 tokens, Drafted: 21720 tokens, Per-position acceptance rate: 0.736, 0.553, 0.428, 0.325, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1321.5 tokens/s, Avg generation throughput: 1910.0 tokens/s, Running: 68 reqs, Waiting: 186 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 1307.60 tokens/s, Drafted throughput: 2400.31 tokens/s, Accepted: 13083 tokens, Drafted: 24016 tokens, Per-position acceptance rate: 0.761, 0.593, 0.464, 0.361, Avg Draft acceptance rate: 54.5%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:20 [loggers.py:257] Engine 000: Avg prompt throughput: 1091.1 tokens/s, Avg generation throughput: 1904.1 tokens/s, Running: 56 reqs, Waiting: 197 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 1277.82 tokens/s, Drafted throughput: 2500.64 tokens/s, Accepted: 12779 tokens, Drafted: 25008 tokens, Per-position acceptance rate: 0.738, 0.557, 0.422, 0.328, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1894.5 tokens/s, Avg generation throughput: 1666.2 tokens/s, Running: 82 reqs, Waiting: 170 reqs, GPU KV cache usage: 95.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 1135.76 tokens/s, Drafted throughput: 2115.52 tokens/s, Accepted: 11358 tokens, Drafted: 21156 tokens, Per-position acceptance rate: 0.762, 0.587, 0.452, 0.347, Avg Draft acceptance rate: 53.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1164.7 tokens/s, Avg generation throughput: 1970.9 tokens/s, Running: 68 reqs, Waiting: 188 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 1353.26 tokens/s, Drafted throughput: 2474.44 tokens/s, Accepted: 13539 tokens, Drafted: 24756 tokens, Per-position acceptance rate: 0.766, 0.595, 0.460, 0.366, Avg Draft acceptance rate: 54.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:50 [loggers.py:257] Engine 000: Avg prompt throughput: 880.3 tokens/s, Avg generation throughput: 1755.1 tokens/s, Running: 62 reqs, Waiting: 192 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:34:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 1170.21 tokens/s, Drafted throughput: 2334.62 tokens/s, Accepted: 11707 tokens, Drafted: 23356 tokens, Per-position acceptance rate: 0.733, 0.546, 0.413, 0.314, Avg Draft acceptance rate: 50.1%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1973.1 tokens/s, Avg generation throughput: 1739.9 tokens/s, Running: 84 reqs, Waiting: 169 reqs, GPU KV cache usage: 96.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 1189.57 tokens/s, Drafted throughput: 2192.35 tokens/s, Accepted: 11896 tokens, Drafted: 21924 tokens, Per-position acceptance rate: 0.760, 0.594, 0.461, 0.356, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1203.5 tokens/s, Avg generation throughput: 1981.0 tokens/s, Running: 64 reqs, Waiting: 190 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 1351.47 tokens/s, Drafted throughput: 2520.82 tokens/s, Accepted: 13521 tokens, Drafted: 25220 tokens, Per-position acceptance rate: 0.748, 0.581, 0.457, 0.358, Avg Draft acceptance rate: 53.6%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:20 [loggers.py:257] Engine 000: Avg prompt throughput: 978.2 tokens/s, Avg generation throughput: 1742.9 tokens/s, Running: 61 reqs, Waiting: 190 reqs, GPU KV cache usage: 96.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 1175.85 tokens/s, Drafted throughput: 2262.31 tokens/s, Accepted: 11759 tokens, Drafted: 22624 tokens, Per-position acceptance rate: 0.751, 0.566, 0.432, 0.330, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1883.1 tokens/s, Avg generation throughput: 1835.5 tokens/s, Running: 84 reqs, Waiting: 170 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 1259.15 tokens/s, Drafted throughput: 2289.11 tokens/s, Accepted: 12592 tokens, Drafted: 22892 tokens, Per-position acceptance rate: 0.765, 0.594, 0.473, 0.368, Avg Draft acceptance rate: 55.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1017.5 tokens/s, Avg generation throughput: 1977.0 tokens/s, Running: 60 reqs, Waiting: 196 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1337.71 tokens/s, Drafted throughput: 2559.02 tokens/s, Accepted: 13378 tokens, Drafted: 25592 tokens, Per-position acceptance rate: 0.752, 0.567, 0.436, 0.336, Avg Draft acceptance rate: 52.3%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1214.1 tokens/s, Avg generation throughput: 1728.4 tokens/s, Running: 65 reqs, Waiting: 185 reqs, GPU KV cache usage: 91.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:35:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 1163.22 tokens/s, Drafted throughput: 2262.25 tokens/s, Accepted: 11633 tokens, Drafted: 22624 tokens, Per-position acceptance rate: 0.739, 0.557, 0.429, 0.333, Avg Draft acceptance rate: 51.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1788.6 tokens/s, Avg generation throughput: 1892.6 tokens/s, Running: 81 reqs, Waiting: 173 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 1300.92 tokens/s, Drafted throughput: 2347.67 tokens/s, Accepted: 13011 tokens, Drafted: 23480 tokens, Per-position acceptance rate: 0.777, 0.599, 0.474, 0.366, Avg Draft acceptance rate: 55.4%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:10 [loggers.py:257] Engine 000: Avg prompt throughput: 810.7 tokens/s, Avg generation throughput: 1927.9 tokens/s, Running: 60 reqs, Waiting: 195 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:10 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 1298.91 tokens/s, Drafted throughput: 2512.84 tokens/s, Accepted: 12991 tokens, Drafted: 25132 tokens, Per-position acceptance rate: 0.749, 0.561, 0.431, 0.326, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:20 [loggers.py:257] Engine 000: Avg prompt throughput: 1612.0 tokens/s, Avg generation throughput: 1676.6 tokens/s, Running: 73 reqs, Waiting: 174 reqs, GPU KV cache usage: 91.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:20 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1133.10 tokens/s, Drafted throughput: 2168.80 tokens/s, Accepted: 11331 tokens, Drafted: 21688 tokens, Per-position acceptance rate: 0.745, 0.567, 0.440, 0.338, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1368.2 tokens/s, Avg generation throughput: 1959.4 tokens/s, Running: 78 reqs, Waiting: 177 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:30 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 1337.98 tokens/s, Drafted throughput: 2476.00 tokens/s, Accepted: 13382 tokens, Drafted: 24764 tokens, Per-position acceptance rate: 0.757, 0.588, 0.460, 0.356, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:40 [loggers.py:257] Engine 000: Avg prompt throughput: 1083.8 tokens/s, Avg generation throughput: 1973.7 tokens/s, Running: 54 reqs, Waiting: 193 reqs, GPU KV cache usage: 95.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:40 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 1335.59 tokens/s, Drafted throughput: 2559.98 tokens/s, Accepted: 13356 tokens, Drafted: 25600 tokens, Per-position acceptance rate: 0.752, 0.570, 0.436, 0.329, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:50 [loggers.py:257] Engine 000: Avg prompt throughput: 1731.4 tokens/s, Avg generation throughput: 1691.1 tokens/s, Running: 93 reqs, Waiting: 43 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:36:50 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 1152.63 tokens/s, Drafted throughput: 2140.27 tokens/s, Accepted: 11527 tokens, Drafted: 21404 tokens, Per-position acceptance rate: 0.757, 0.584, 0.462, 0.351, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:37:00 [loggers.py:257] Engine 000: Avg prompt throughput: 589.6 tokens/s, Avg generation throughput: 2025.9 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:37:00 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 1382.92 tokens/s, Drafted throughput: 2595.66 tokens/s, Accepted: 13831 tokens, Drafted: 25960 tokens, Per-position acceptance rate: 0.749, 0.580, 0.451, 0.351, Avg Draft acceptance rate: 53.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  279.62    
Total input tokens:                      373233    
Total generated tokens:                  511935    
Request throughput (req/s):              9.16      
Output token throughput (tok/s):         1830.82   
Peak output token throughput (tok/s):    975.00    
Peak concurrent requests:                278.00    
Total token throughput (tok/s):          3165.61   
---------------Time to First Token----------------
Mean TTFT (ms):                          18510.69  
Median TTFT (ms):                        19704.14  
P99 TTFT (ms):                           22231.25  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          41.37     
Median TPOT (ms):                        38.83     
P99 TPOT (ms):                           68.75     
---------------Inter-token Latency----------------
Mean ITL (ms):                           127.38    
Median ITL (ms):                         91.64     
P99 ITL (ms):                            313.52    
---------------Speculative Decoding---------------
Acceptance rate (%):                     52.91     
Acceptance length:                       3.12      
Drafts:                                  164166    
Draft tokens:                            656664    
Accepted tokens:                         347441    
Per-position acceptance (%):
  Position 0:                            75.11     
  Position 1:                            57.47     
  Position 2:                            44.61     
  Position 3:                            34.44     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k4-t0.0-tp1...
[0;36m(APIServer pid=825760)[0;0m INFO 01-22 21:37:04 [launcher.py:110] Shutting down FastAPI HTTP server.
