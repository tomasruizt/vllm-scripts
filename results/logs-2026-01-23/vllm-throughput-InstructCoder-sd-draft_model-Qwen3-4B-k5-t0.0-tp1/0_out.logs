Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k5-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k5-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 834350
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:04 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:04 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15019, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=834350)[0;0m WARNING 01-22 21:38:04 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:05 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:05 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:06 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:06 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:06 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=834350)[0;0m WARNING 01-22 21:38:06 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:38:06 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2668d2afc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d405d548-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 21:38:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:38:17 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=5), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:38:18 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.63:45423 backend=nccl
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:38:18 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=834552)[0;0m WARNING 01-22 21:38:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:38:19 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:38:20 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 21:38:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:38:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:12 [default_loader.py:291] Loading weights took 51.18 seconds
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:13 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:13 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:13 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-22 21:39:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:19 [default_loader.py:291] Loading weights took 5.04 seconds
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:19 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 59.047870 seconds
WARNING 01-22 21:39:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:31 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:31 [backends.py:704] Dynamo bytecode transform time: 11.71 s
WARNING 01-22 21:39:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:39:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:47 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.413 s
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:47 [monitor.py:34] torch.compile takes 14.13 s in total
WARNING 01-22 21:39:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:53 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:39:53 [backends.py:704] Dynamo bytecode transform time: 6.09 s
WARNING 01-22 21:39:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 0.916 s
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:00 [monitor.py:34] torch.compile takes 21.14 s in total
WARNING 01-22 21:40:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:02 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:02 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:02 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-22 21:40:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:40:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
WARNING 01-22 21:40:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:20 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 0.02 GiB
[0;36m(EngineCore_DP0 pid=834552)[0;0m INFO 01-22 21:40:21 [core.py:272] init engine (profile, create kv cache, warmup model) took 61.26 seconds
WARNING 01-22 21:40:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15019)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15019 ssl:default [Connect call failed (\'127.0.0.1\', 15019)]\n''
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:23 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=834350)[0;0m WARNING 01-22 21:40:23 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:23 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:23 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:23 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [serving.py:221] Chat template warmup completed in 1724.1ms
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15019
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:25 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:35 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 36.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 26.48 tokens/s, Drafted throughput: 49.89 tokens/s, Accepted: 345 tokens, Drafted: 650 tokens, Per-position acceptance rate: 0.808, 0.638, 0.531, 0.408, 0.269, Avg Draft acceptance rate: 53.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:45 [loggers.py:257] Engine 000: Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 33.40 tokens/s, Drafted throughput: 76.00 tokens/s, Accepted: 334 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.789, 0.553, 0.395, 0.243, 0.217, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:55 [loggers.py:257] Engine 000: Avg prompt throughput: 45.5 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:40:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 35.20 tokens/s, Drafted throughput: 75.00 tokens/s, Accepted: 352 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.753, 0.573, 0.447, 0.327, 0.247, Avg Draft acceptance rate: 46.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:06 [loggers.py:257] Engine 000: Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 49.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 35.20 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 352 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.730, 0.553, 0.454, 0.316, 0.263, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:16 [loggers.py:257] Engine 000: Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 49.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 34.80 tokens/s, Drafted throughput: 75.00 tokens/s, Accepted: 348 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.760, 0.560, 0.447, 0.313, 0.240, Avg Draft acceptance rate: 46.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:26 [loggers.py:257] Engine 000: Avg prompt throughput: 21.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 39.39 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 394 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.763, 0.605, 0.500, 0.401, 0.322, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:36 [loggers.py:257] Engine 000: Avg prompt throughput: 52.2 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 36.10 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 361 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.781, 0.570, 0.444, 0.338, 0.258, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:46 [loggers.py:257] Engine 000: Avg prompt throughput: 43.1 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 37.30 tokens/s, Drafted throughput: 75.00 tokens/s, Accepted: 373 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.800, 0.587, 0.447, 0.360, 0.293, Avg Draft acceptance rate: 49.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:56 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:41:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.79, Accepted throughput: 42.20 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 422 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.815, 0.642, 0.543, 0.457, 0.338, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:06 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 47.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 32.10 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 321 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.709, 0.497, 0.391, 0.298, 0.232, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:16 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 32.80 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 328 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.722, 0.556, 0.424, 0.265, 0.205, Avg Draft acceptance rate: 43.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:26 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 51.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 36.00 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 360 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.722, 0.543, 0.437, 0.371, 0.311, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:36 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 50.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 34.90 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 349 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.748, 0.550, 0.417, 0.325, 0.272, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:46 [loggers.py:257] Engine 000: Avg prompt throughput: 49.2 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 37.10 tokens/s, Drafted throughput: 75.00 tokens/s, Accepted: 371 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.780, 0.620, 0.460, 0.340, 0.273, Avg Draft acceptance rate: 49.5%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:56 [loggers.py:257] Engine 000: Avg prompt throughput: 14.0 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:42:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 35.20 tokens/s, Drafted throughput: 76.00 tokens/s, Accepted: 352 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.730, 0.572, 0.454, 0.329, 0.230, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:06 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 56.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 41.70 tokens/s, Drafted throughput: 74.99 tokens/s, Accepted: 417 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.793, 0.647, 0.533, 0.453, 0.353, Avg Draft acceptance rate: 55.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:16 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 36.20 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 362 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.775, 0.609, 0.470, 0.318, 0.225, Avg Draft acceptance rate: 47.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:26 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 39.00 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 390 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.808, 0.649, 0.510, 0.331, 0.285, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:36 [loggers.py:257] Engine 000: Avg prompt throughput: 44.9 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 37.00 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 370 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.768, 0.623, 0.450, 0.331, 0.278, Avg Draft acceptance rate: 49.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  194.87    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.26      
Output token throughput (tok/s):         51.32     
Peak output token throughput (tok/s):    16.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          88.40     
---------------Time to First Token----------------
Mean TTFT (ms):                          78.90     
Median TTFT (ms):                        78.67     
P99 TTFT (ms):                           87.52     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.11     
Median TPOT (ms):                        18.90     
P99 TPOT (ms):                           22.22     
---------------Inter-token Latency----------------
Mean ITL (ms):                           64.92     
Median ITL (ms):                         64.92     
P99 ITL (ms):                            65.47     
---------------Speculative Decoding---------------
Acceptance rate (%):                     48.47     
Acceptance length:                       3.42      
Drafts:                                  2929      
Draft tokens:                            14645     
Accepted tokens:                         7098      
Per-position acceptance (%):
  Position 0:                            76.68     
  Position 1:                            58.48     
  Position 2:                            45.92     
  Position 3:                            34.28     
  Position 4:                            26.97     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:46 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 34.30 tokens/s, Drafted throughput: 69.49 tokens/s, Accepted: 343 tokens, Drafted: 695 tokens, Per-position acceptance rate: 0.799, 0.576, 0.460, 0.360, 0.273, Avg Draft acceptance rate: 49.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:43:56 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fcdfac6efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0f0fde54-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:06 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 6.30 tokens/s, Drafted throughput: 12.00 tokens/s, Accepted: 126 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.792, 0.667, 0.542, 0.396, 0.229, Avg Draft acceptance rate: 52.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:16 [loggers.py:257] Engine 000: Avg prompt throughput: 75.2 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 60.89 tokens/s, Drafted throughput: 133.49 tokens/s, Accepted: 609 tokens, Drafted: 1335 tokens, Per-position acceptance rate: 0.764, 0.558, 0.416, 0.311, 0.232, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:26 [loggers.py:257] Engine 000: Avg prompt throughput: 62.0 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 66.69 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 667 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.735, 0.541, 0.442, 0.306, 0.245, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:36 [loggers.py:257] Engine 000: Avg prompt throughput: 69.5 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 73.40 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 734 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.765, 0.592, 0.473, 0.371, 0.296, Avg Draft acceptance rate: 49.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:46 [loggers.py:257] Engine 000: Avg prompt throughput: 77.1 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 68.49 tokens/s, Drafted throughput: 147.99 tokens/s, Accepted: 685 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.767, 0.564, 0.426, 0.318, 0.240, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:56 [loggers.py:257] Engine 000: Avg prompt throughput: 67.9 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:44:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 72.70 tokens/s, Drafted throughput: 147.99 tokens/s, Accepted: 727 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.760, 0.571, 0.473, 0.375, 0.277, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:06 [loggers.py:257] Engine 000: Avg prompt throughput: 82.3 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 67.99 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 680 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.752, 0.554, 0.435, 0.306, 0.265, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:16 [loggers.py:257] Engine 000: Avg prompt throughput: 84.1 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 75.49 tokens/s, Drafted throughput: 147.98 tokens/s, Accepted: 755 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.774, 0.598, 0.486, 0.382, 0.311, Avg Draft acceptance rate: 51.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:26 [loggers.py:257] Engine 000: Avg prompt throughput: 87.0 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 76.39 tokens/s, Drafted throughput: 145.99 tokens/s, Accepted: 764 tokens, Drafted: 1460 tokens, Per-position acceptance rate: 0.777, 0.616, 0.503, 0.401, 0.318, Avg Draft acceptance rate: 52.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:36 [loggers.py:257] Engine 000: Avg prompt throughput: 54.2 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 64.99 tokens/s, Drafted throughput: 148.98 tokens/s, Accepted: 650 tokens, Drafted: 1490 tokens, Per-position acceptance rate: 0.728, 0.547, 0.403, 0.292, 0.211, Avg Draft acceptance rate: 43.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:46 [loggers.py:257] Engine 000: Avg prompt throughput: 63.4 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 71.70 tokens/s, Drafted throughput: 147.99 tokens/s, Accepted: 717 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.757, 0.598, 0.453, 0.341, 0.274, Avg Draft acceptance rate: 48.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  102.26    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.49      
Output token throughput (tok/s):         97.79     
Peak output token throughput (tok/s):    32.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          168.46    
---------------Time to First Token----------------
Mean TTFT (ms):                          134.24    
Median TTFT (ms):                        134.88    
P99 TTFT (ms):                           143.00    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.47     
Median TPOT (ms):                        19.43     
P99 TPOT (ms):                           22.10     
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.48     
Median ITL (ms):                         65.39     
P99 ITL (ms):                            70.43     
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.79     
Acceptance length:                       3.39      
Drafts:                                  2959      
Draft tokens:                            14795     
Accepted tokens:                         7071      
Per-position acceptance (%):
  Position 0:                            75.90     
  Position 1:                            57.45     
  Position 2:                            45.05     
  Position 3:                            33.96     
  Position 4:                            26.60     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:56 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:45:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 10.30 tokens/s, Drafted throughput: 22.50 tokens/s, Accepted: 103 tokens, Drafted: 225 tokens, Per-position acceptance rate: 0.844, 0.578, 0.378, 0.289, 0.200, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f336aaf6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c7b4d45d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:16 [loggers.py:257] Engine 000: Avg prompt throughput: 136.8 tokens/s, Avg generation throughput: 126.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 45.05 tokens/s, Drafted throughput: 92.25 tokens/s, Accepted: 901 tokens, Drafted: 1845 tokens, Per-position acceptance rate: 0.783, 0.599, 0.463, 0.336, 0.260, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:26 [loggers.py:257] Engine 000: Avg prompt throughput: 122.2 tokens/s, Avg generation throughput: 201.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 143.58 tokens/s, Drafted throughput: 292.97 tokens/s, Accepted: 1436 tokens, Drafted: 2930 tokens, Per-position acceptance rate: 0.770, 0.585, 0.468, 0.352, 0.276, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:36 [loggers.py:257] Engine 000: Avg prompt throughput: 139.7 tokens/s, Avg generation throughput: 192.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 134.68 tokens/s, Drafted throughput: 289.96 tokens/s, Accepted: 1347 tokens, Drafted: 2900 tokens, Per-position acceptance rate: 0.750, 0.559, 0.438, 0.326, 0.250, Avg Draft acceptance rate: 46.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:46 [loggers.py:257] Engine 000: Avg prompt throughput: 151.7 tokens/s, Avg generation throughput: 193.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 134.98 tokens/s, Drafted throughput: 289.97 tokens/s, Accepted: 1350 tokens, Drafted: 2900 tokens, Per-position acceptance rate: 0.750, 0.559, 0.429, 0.319, 0.271, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:56 [loggers.py:257] Engine 000: Avg prompt throughput: 146.9 tokens/s, Avg generation throughput: 197.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:46:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 139.89 tokens/s, Drafted throughput: 292.97 tokens/s, Accepted: 1399 tokens, Drafted: 2930 tokens, Per-position acceptance rate: 0.746, 0.585, 0.468, 0.336, 0.253, Avg Draft acceptance rate: 47.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  52.31     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.96      
Output token throughput (tok/s):         191.15    
Peak output token throughput (tok/s):    64.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          329.29    
---------------Time to First Token----------------
Mean TTFT (ms):                          133.83    
Median TTFT (ms):                        135.88    
P99 TTFT (ms):                           144.19    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.77     
Median TPOT (ms):                        19.54     
P99 TPOT (ms):                           22.87     
---------------Inter-token Latency----------------
Mean ITL (ms):                           66.41     
Median ITL (ms):                         66.18     
P99 ITL (ms):                            72.42     
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.70     
Acceptance length:                       3.39      
Drafts:                                  2962      
Draft tokens:                            14810     
Accepted tokens:                         7065      
Per-position acceptance (%):
  Position 0:                            76.06     
  Position 1:                            57.63     
  Position 2:                            45.07     
  Position 3:                            33.39     
  Position 4:                            26.37     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:06 [loggers.py:257] Engine 000: Avg prompt throughput: 43.7 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 77.78 tokens/s, Drafted throughput: 158.96 tokens/s, Accepted: 778 tokens, Drafted: 1590 tokens, Per-position acceptance rate: 0.789, 0.591, 0.447, 0.346, 0.274, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3fd580afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ae18f7c5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:26 [loggers.py:257] Engine 000: Avg prompt throughput: 136.8 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 37.35 tokens/s, Drafted throughput: 70.74 tokens/s, Accepted: 747 tokens, Drafted: 1415 tokens, Per-position acceptance rate: 0.795, 0.633, 0.530, 0.389, 0.293, Avg Draft acceptance rate: 52.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:36 [loggers.py:257] Engine 000: Avg prompt throughput: 244.9 tokens/s, Avg generation throughput: 378.1 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 267.48 tokens/s, Drafted throughput: 561.95 tokens/s, Accepted: 2675 tokens, Drafted: 5620 tokens, Per-position acceptance rate: 0.763, 0.566, 0.444, 0.343, 0.264, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:46 [loggers.py:257] Engine 000: Avg prompt throughput: 325.5 tokens/s, Avg generation throughput: 386.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 275.02 tokens/s, Drafted throughput: 558.84 tokens/s, Accepted: 2751 tokens, Drafted: 5590 tokens, Per-position acceptance rate: 0.755, 0.585, 0.483, 0.356, 0.282, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:56 [loggers.py:257] Engine 000: Avg prompt throughput: 241.3 tokens/s, Avg generation throughput: 391.9 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:47:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 281.17 tokens/s, Drafted throughput: 566.93 tokens/s, Accepted: 2812 tokens, Drafted: 5670 tokens, Per-position acceptance rate: 0.770, 0.604, 0.470, 0.354, 0.281, Avg Draft acceptance rate: 49.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:06 [loggers.py:257] Engine 000: Avg prompt throughput: 238.7 tokens/s, Avg generation throughput: 348.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 245.08 tokens/s, Drafted throughput: 528.46 tokens/s, Accepted: 2451 tokens, Drafted: 5285 tokens, Per-position acceptance rate: 0.744, 0.552, 0.426, 0.332, 0.266, Avg Draft acceptance rate: 46.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  43.86     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.82      
Output token throughput (tok/s):         364.81    
Peak output token throughput (tok/s):    120.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          631.35    
---------------Time to First Token----------------
Mean TTFT (ms):                          139.35    
Median TTFT (ms):                        139.65    
P99 TTFT (ms):                           155.39    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.27     
Median TPOT (ms):                        20.05     
P99 TPOT (ms):                           25.54     
---------------Inter-token Latency----------------
Mean ITL (ms):                           68.69     
Median ITL (ms):                         68.17     
P99 ITL (ms):                            79.12     
---------------Speculative Decoding---------------
Acceptance rate (%):                     48.40     
Acceptance length:                       3.42      
Drafts:                                  4698      
Draft tokens:                            23490     
Accepted tokens:                         11369     
Per-position acceptance (%):
  Position 0:                            76.05     
  Position 1:                            57.92     
  Position 2:                            45.89     
  Position 3:                            34.74     
  Position 4:                            27.39     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 7.90 tokens/s, Drafted throughput: 19.50 tokens/s, Accepted: 79 tokens, Drafted: 195 tokens, Per-position acceptance rate: 0.821, 0.513, 0.333, 0.205, 0.154, Avg Draft acceptance rate: 40.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0aa0b56fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a75bbc80-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:26 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 6.30 tokens/s, Drafted throughput: 10.50 tokens/s, Accepted: 63 tokens, Drafted: 105 tokens, Per-position acceptance rate: 0.857, 0.762, 0.619, 0.476, 0.286, Avg Draft acceptance rate: 60.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:36 [loggers.py:257] Engine 000: Avg prompt throughput: 468.8 tokens/s, Avg generation throughput: 525.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 370.36 tokens/s, Drafted throughput: 769.41 tokens/s, Accepted: 3704 tokens, Drafted: 7695 tokens, Per-position acceptance rate: 0.766, 0.571, 0.455, 0.339, 0.276, Avg Draft acceptance rate: 48.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:46 [loggers.py:257] Engine 000: Avg prompt throughput: 492.7 tokens/s, Avg generation throughput: 721.7 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 513.51 tokens/s, Drafted throughput: 1054.03 tokens/s, Accepted: 5135 tokens, Drafted: 10540 tokens, Per-position acceptance rate: 0.766, 0.588, 0.467, 0.345, 0.270, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:56 [loggers.py:257] Engine 000: Avg prompt throughput: 580.8 tokens/s, Avg generation throughput: 720.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:48:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 513.61 tokens/s, Drafted throughput: 1057.82 tokens/s, Accepted: 5137 tokens, Drafted: 10580 tokens, Per-position acceptance rate: 0.753, 0.588, 0.459, 0.348, 0.278, Avg Draft acceptance rate: 48.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:06 [loggers.py:257] Engine 000: Avg prompt throughput: 616.3 tokens/s, Avg generation throughput: 719.9 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 516.77 tokens/s, Drafted throughput: 1039.93 tokens/s, Accepted: 5168 tokens, Drafted: 10400 tokens, Per-position acceptance rate: 0.777, 0.597, 0.470, 0.354, 0.286, Avg Draft acceptance rate: 49.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  46.82     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.42      
Output token throughput (tok/s):         683.27    
Peak output token throughput (tok/s):    224.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          1203.57   
---------------Time to First Token----------------
Mean TTFT (ms):                          152.95    
Median TTFT (ms):                        151.93    
P99 TTFT (ms):                           197.81    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.48     
Median TPOT (ms):                        21.49     
P99 TPOT (ms):                           25.86     
---------------Inter-token Latency----------------
Mean ITL (ms):                           73.05     
Median ITL (ms):                         71.76     
P99 ITL (ms):                            102.66    
---------------Speculative Decoding---------------
Acceptance rate (%):                     48.62     
Acceptance length:                       3.43      
Drafts:                                  9362      
Draft tokens:                            46810     
Accepted tokens:                         22759     
Per-position acceptance (%):
  Position 0:                            76.12     
  Position 1:                            58.50     
  Position 2:                            46.01     
  Position 3:                            34.67     
  Position 4:                            27.80     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:16 [loggers.py:257] Engine 000: Avg prompt throughput: 277.3 tokens/s, Avg generation throughput: 522.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 369.78 tokens/s, Drafted throughput: 777.46 tokens/s, Accepted: 3698 tokens, Drafted: 7775 tokens, Per-position acceptance rate: 0.739, 0.573, 0.442, 0.345, 0.279, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f587617afc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-53276e38-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:36 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 16.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 6.00 tokens/s, Drafted throughput: 11.25 tokens/s, Accepted: 120 tokens, Drafted: 225 tokens, Per-position acceptance rate: 0.800, 0.689, 0.556, 0.400, 0.222, Avg Draft acceptance rate: 53.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:46 [loggers.py:257] Engine 000: Avg prompt throughput: 944.0 tokens/s, Avg generation throughput: 1125.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 799.00 tokens/s, Drafted throughput: 1628.30 tokens/s, Accepted: 7991 tokens, Drafted: 16285 tokens, Per-position acceptance rate: 0.764, 0.591, 0.469, 0.350, 0.279, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1010.3 tokens/s, Avg generation throughput: 1284.6 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:49:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 916.56 tokens/s, Drafted throughput: 1869.52 tokens/s, Accepted: 9168 tokens, Drafted: 18700 tokens, Per-position acceptance rate: 0.771, 0.590, 0.461, 0.353, 0.276, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:06 [loggers.py:257] Engine 000: Avg prompt throughput: 958.3 tokens/s, Avg generation throughput: 1299.5 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 46.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 931.93 tokens/s, Drafted throughput: 1872.36 tokens/s, Accepted: 9320 tokens, Drafted: 18725 tokens, Per-position acceptance rate: 0.763, 0.587, 0.472, 0.366, 0.300, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:16 [loggers.py:257] Engine 000: Avg prompt throughput: 998.4 tokens/s, Avg generation throughput: 1283.9 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 48.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 916.75 tokens/s, Drafted throughput: 1868.90 tokens/s, Accepted: 9168 tokens, Drafted: 18690 tokens, Per-position acceptance rate: 0.769, 0.581, 0.459, 0.359, 0.284, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:26 [loggers.py:257] Engine 000: Avg prompt throughput: 914.9 tokens/s, Avg generation throughput: 1264.5 tokens/s, Running: 25 reqs, Waiting: 0 reqs, GPU KV cache usage: 42.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 895.07 tokens/s, Drafted throughput: 1867.93 tokens/s, Accepted: 8951 tokens, Drafted: 18680 tokens, Per-position acceptance rate: 0.756, 0.577, 0.443, 0.341, 0.278, Avg Draft acceptance rate: 47.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  52.12     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.14      
Output token throughput (tok/s):         1227.72   
Peak output token throughput (tok/s):    448.00    
Peak concurrent requests:                49.00     
Total token throughput (tok/s):          2153.74   
---------------Time to First Token----------------
Mean TTFT (ms):                          180.75    
Median TTFT (ms):                        173.82    
P99 TTFT (ms):                           265.50    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.97     
Median TPOT (ms):                        24.00     
P99 TPOT (ms):                           28.91     
---------------Inter-token Latency----------------
Mean ITL (ms):                           81.70     
Median ITL (ms):                         76.50     
P99 ITL (ms):                            140.24    
---------------Speculative Decoding---------------
Acceptance rate (%):                     48.77     
Acceptance length:                       3.44      
Drafts:                                  18684     
Draft tokens:                            93420     
Accepted tokens:                         45563     
Per-position acceptance (%):
  Position 0:                            76.38     
  Position 1:                            58.41     
  Position 2:                            45.81     
  Position 3:                            35.16     
  Position 4:                            28.09     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 143.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 99.09 tokens/s, Drafted throughput: 239.98 tokens/s, Accepted: 991 tokens, Drafted: 2400 tokens, Per-position acceptance rate: 0.733, 0.540, 0.350, 0.258, 0.183, Avg Draft acceptance rate: 41.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:46 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f285a42afc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e75269d7-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:56 [loggers.py:257] Engine 000: Avg prompt throughput: 948.3 tokens/s, Avg generation throughput: 1004.2 tokens/s, Running: 58 reqs, Waiting: 6 reqs, GPU KV cache usage: 98.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:50:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 358.13 tokens/s, Drafted throughput: 704.87 tokens/s, Accepted: 7164 tokens, Drafted: 14100 tokens, Per-position acceptance rate: 0.781, 0.607, 0.488, 0.375, 0.290, Avg Draft acceptance rate: 50.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1727.7 tokens/s, Avg generation throughput: 1646.6 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 74.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 1174.23 tokens/s, Drafted throughput: 2402.36 tokens/s, Accepted: 11743 tokens, Drafted: 24025 tokens, Per-position acceptance rate: 0.764, 0.591, 0.462, 0.348, 0.278, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1132.3 tokens/s, Avg generation throughput: 1974.9 tokens/s, Running: 62 reqs, Waiting: 2 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 1405.15 tokens/s, Drafted throughput: 2858.89 tokens/s, Accepted: 14057 tokens, Drafted: 28600 tokens, Per-position acceptance rate: 0.766, 0.587, 0.462, 0.359, 0.284, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1460.3 tokens/s, Avg generation throughput: 1729.1 tokens/s, Running: 60 reqs, Waiting: 0 reqs, GPU KV cache usage: 82.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 1217.81 tokens/s, Drafted throughput: 2589.10 tokens/s, Accepted: 12180 tokens, Drafted: 25895 tokens, Per-position acceptance rate: 0.750, 0.570, 0.431, 0.336, 0.264, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1296.0 tokens/s, Avg generation throughput: 1898.9 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 94.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 1340.16 tokens/s, Drafted throughput: 2812.36 tokens/s, Accepted: 13407 tokens, Drafted: 28135 tokens, Per-position acceptance rate: 0.746, 0.574, 0.450, 0.344, 0.269, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1345.1 tokens/s, Avg generation throughput: 1862.8 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 82.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1307.91 tokens/s, Drafted throughput: 2812.58 tokens/s, Accepted: 13081 tokens, Drafted: 28130 tokens, Per-position acceptance rate: 0.742, 0.560, 0.433, 0.331, 0.258, Avg Draft acceptance rate: 46.5%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1456.7 tokens/s, Avg generation throughput: 1844.4 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 93.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:51:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 1310.19 tokens/s, Drafted throughput: 2704.75 tokens/s, Accepted: 13108 tokens, Drafted: 27060 tokens, Per-position acceptance rate: 0.749, 0.586, 0.452, 0.356, 0.278, Avg Draft acceptance rate: 48.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  71.92     
Total input tokens:                      94775     
Total generated tokens:                  127921    
Request throughput (req/s):              8.90      
Output token throughput (tok/s):         1778.74   
Peak output token throughput (tok/s):    704.00    
Peak concurrent requests:                92.00     
Total token throughput (tok/s):          3096.59   
---------------Time to First Token----------------
Mean TTFT (ms):                          320.59    
Median TTFT (ms):                        271.14    
P99 TTFT (ms):                           949.82    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          32.90     
Median TPOT (ms):                        32.79     
P99 TPOT (ms):                           43.87     
---------------Inter-token Latency----------------
Mean ITL (ms):                           110.63    
Median ITL (ms):                         94.20     
P99 ITL (ms):                            208.75    
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.89     
Acceptance length:                       3.39      
Drafts:                                  37795     
Draft tokens:                            188975    
Accepted tokens:                         90505     
Per-position acceptance (%):
  Position 0:                            75.32     
  Position 1:                            57.75     
  Position 2:                            44.86     
  Position 3:                            34.48     
  Position 4:                            27.05     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:06 [loggers.py:257] Engine 000: Avg prompt throughput: 126.6 tokens/s, Avg generation throughput: 847.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 591.06 tokens/s, Drafted throughput: 1331.42 tokens/s, Accepted: 5911 tokens, Drafted: 13315 tokens, Per-position acceptance rate: 0.729, 0.541, 0.413, 0.305, 0.232, Avg Draft acceptance rate: 44.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd875d8afc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-623d0279-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1034.5 tokens/s, Avg generation throughput: 42.0 tokens/s, Running: 70 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.97, Accepted throughput: 13.20 tokens/s, Drafted throughput: 22.25 tokens/s, Accepted: 264 tokens, Drafted: 445 tokens, Per-position acceptance rate: 0.854, 0.753, 0.674, 0.404, 0.281, Avg Draft acceptance rate: 59.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:36 [loggers.py:257] Engine 000: Avg prompt throughput: 997.2 tokens/s, Avg generation throughput: 1801.5 tokens/s, Running: 73 reqs, Waiting: 52 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 1282.81 tokens/s, Drafted throughput: 2585.33 tokens/s, Accepted: 12829 tokens, Drafted: 25855 tokens, Per-position acceptance rate: 0.765, 0.596, 0.473, 0.358, 0.288, Avg Draft acceptance rate: 49.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1177.4 tokens/s, Avg generation throughput: 1859.2 tokens/s, Running: 57 reqs, Waiting: 69 reqs, GPU KV cache usage: 95.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 1309.35 tokens/s, Drafted throughput: 2752.18 tokens/s, Accepted: 13095 tokens, Drafted: 27525 tokens, Per-position acceptance rate: 0.748, 0.570, 0.442, 0.345, 0.274, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1230.0 tokens/s, Avg generation throughput: 1621.8 tokens/s, Running: 63 reqs, Waiting: 62 reqs, GPU KV cache usage: 95.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:52:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 1133.20 tokens/s, Drafted throughput: 2461.28 tokens/s, Accepted: 11333 tokens, Drafted: 24615 tokens, Per-position acceptance rate: 0.744, 0.550, 0.426, 0.325, 0.257, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1637.3 tokens/s, Avg generation throughput: 1747.2 tokens/s, Running: 83 reqs, Waiting: 43 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 1239.58 tokens/s, Drafted throughput: 2543.95 tokens/s, Accepted: 12396 tokens, Drafted: 25440 tokens, Per-position acceptance rate: 0.753, 0.588, 0.458, 0.350, 0.287, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1094.6 tokens/s, Avg generation throughput: 1872.0 tokens/s, Running: 61 reqs, Waiting: 65 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 1305.78 tokens/s, Drafted throughput: 2846.75 tokens/s, Accepted: 13059 tokens, Drafted: 28470 tokens, Per-position acceptance rate: 0.734, 0.553, 0.424, 0.327, 0.255, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1007.5 tokens/s, Avg generation throughput: 1639.1 tokens/s, Running: 65 reqs, Waiting: 56 reqs, GPU KV cache usage: 95.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 1129.86 tokens/s, Drafted throughput: 2563.41 tokens/s, Accepted: 11299 tokens, Drafted: 25635 tokens, Per-position acceptance rate: 0.718, 0.533, 0.403, 0.311, 0.239, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1771.2 tokens/s, Avg generation throughput: 1770.6 tokens/s, Running: 80 reqs, Waiting: 45 reqs, GPU KV cache usage: 95.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 1257.32 tokens/s, Drafted throughput: 2570.22 tokens/s, Accepted: 12577 tokens, Drafted: 25710 tokens, Per-position acceptance rate: 0.760, 0.583, 0.460, 0.358, 0.284, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1009.1 tokens/s, Avg generation throughput: 1861.4 tokens/s, Running: 64 reqs, Waiting: 64 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 1301.54 tokens/s, Drafted throughput: 2811.37 tokens/s, Accepted: 13016 tokens, Drafted: 28115 tokens, Per-position acceptance rate: 0.731, 0.557, 0.434, 0.332, 0.260, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1106.6 tokens/s, Avg generation throughput: 1680.0 tokens/s, Running: 67 reqs, Waiting: 56 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:53:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 1169.88 tokens/s, Drafted throughput: 2555.23 tokens/s, Accepted: 11700 tokens, Drafted: 25555 tokens, Per-position acceptance rate: 0.728, 0.554, 0.426, 0.327, 0.254, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1664.7 tokens/s, Avg generation throughput: 1722.3 tokens/s, Running: 72 reqs, Waiting: 53 reqs, GPU KV cache usage: 93.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1207.22 tokens/s, Drafted throughput: 2589.33 tokens/s, Accepted: 12073 tokens, Drafted: 25895 tokens, Per-position acceptance rate: 0.736, 0.559, 0.436, 0.335, 0.266, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:16 [loggers.py:257] Engine 000: Avg prompt throughput: 992.1 tokens/s, Avg generation throughput: 1757.7 tokens/s, Running: 61 reqs, Waiting: 67 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 1220.39 tokens/s, Drafted throughput: 2681.38 tokens/s, Accepted: 12209 tokens, Drafted: 26825 tokens, Per-position acceptance rate: 0.731, 0.548, 0.421, 0.323, 0.253, Avg Draft acceptance rate: 45.5%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1128.8 tokens/s, Avg generation throughput: 1717.5 tokens/s, Running: 56 reqs, Waiting: 63 reqs, GPU KV cache usage: 90.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 1203.14 tokens/s, Drafted throughput: 2574.87 tokens/s, Accepted: 12032 tokens, Drafted: 25750 tokens, Per-position acceptance rate: 0.748, 0.562, 0.431, 0.330, 0.266, Avg Draft acceptance rate: 46.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1913.0 tokens/s, Avg generation throughput: 1659.4 tokens/s, Running: 86 reqs, Waiting: 37 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 1174.57 tokens/s, Drafted throughput: 2434.23 tokens/s, Accepted: 11747 tokens, Drafted: 24345 tokens, Per-position acceptance rate: 0.758, 0.588, 0.449, 0.343, 0.274, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1045.5 tokens/s, Avg generation throughput: 1904.6 tokens/s, Running: 66 reqs, Waiting: 22 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 1352.21 tokens/s, Drafted throughput: 2758.01 tokens/s, Accepted: 13522 tokens, Drafted: 27580 tokens, Per-position acceptance rate: 0.763, 0.588, 0.459, 0.358, 0.284, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:56 [loggers.py:257] Engine 000: Avg prompt throughput: 115.6 tokens/s, Avg generation throughput: 957.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:54:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 658.03 tokens/s, Drafted throughput: 1555.84 tokens/s, Accepted: 6581 tokens, Drafted: 15560 tokens, Per-position acceptance rate: 0.711, 0.523, 0.383, 0.287, 0.211, Avg Draft acceptance rate: 42.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  151.42    
Total input tokens:                      189093    
Total generated tokens:                  255970    
Request throughput (req/s):              8.45      
Output token throughput (tok/s):         1690.49   
Peak output token throughput (tok/s):    794.00    
Peak concurrent requests:                148.00    
Total token throughput (tok/s):          2939.31   
---------------Time to First Token----------------
Mean TTFT (ms):                          5859.77   
Median TTFT (ms):                        6812.85   
P99 TTFT (ms):                           9112.91   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          42.34     
Median TPOT (ms):                        40.20     
P99 TPOT (ms):                           73.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           139.68    
Median ITL (ms):                         104.07    
P99 ITL (ms):                            358.47    
---------------Speculative Decoding---------------
Acceptance rate (%):                     46.89     
Acceptance length:                       3.34      
Drafts:                                  76607     
Draft tokens:                            383035    
Accepted tokens:                         179586    
Per-position acceptance (%):
  Position 0:                            74.27     
  Position 1:                            56.48     
  Position 2:                            43.66     
  Position 3:                            33.53     
  Position 4:                            26.49     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f55984b6fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15019, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-550152ed-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:16 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 6.00, Accepted throughput: 0.50 tokens/s, Drafted throughput: 0.50 tokens/s, Accepted: 10 tokens, Drafted: 10 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 1.000, 1.000, Avg Draft acceptance rate: 100.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1666.0 tokens/s, Avg generation throughput: 1190.1 tokens/s, Running: 60 reqs, Waiting: 196 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 849.84 tokens/s, Drafted throughput: 1647.88 tokens/s, Accepted: 8499 tokens, Drafted: 16480 tokens, Per-position acceptance rate: 0.779, 0.617, 0.502, 0.379, 0.302, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:36 [loggers.py:257] Engine 000: Avg prompt throughput: 863.1 tokens/s, Avg generation throughput: 1540.2 tokens/s, Running: 63 reqs, Waiting: 188 reqs, GPU KV cache usage: 95.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 1074.76 tokens/s, Drafted throughput: 2355.20 tokens/s, Accepted: 10749 tokens, Drafted: 23555 tokens, Per-position acceptance rate: 0.734, 0.555, 0.422, 0.320, 0.252, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1689.9 tokens/s, Avg generation throughput: 1748.0 tokens/s, Running: 81 reqs, Waiting: 173 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 1242.13 tokens/s, Drafted throughput: 2535.16 tokens/s, Accepted: 12423 tokens, Drafted: 25355 tokens, Per-position acceptance rate: 0.761, 0.586, 0.461, 0.357, 0.286, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:56 [loggers.py:257] Engine 000: Avg prompt throughput: 970.7 tokens/s, Avg generation throughput: 1885.9 tokens/s, Running: 61 reqs, Waiting: 193 reqs, GPU KV cache usage: 98.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:55:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 1329.36 tokens/s, Drafted throughput: 2791.21 tokens/s, Accepted: 13295 tokens, Drafted: 27915 tokens, Per-position acceptance rate: 0.753, 0.576, 0.442, 0.340, 0.270, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1147.8 tokens/s, Avg generation throughput: 1700.2 tokens/s, Running: 59 reqs, Waiting: 192 reqs, GPU KV cache usage: 92.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1191.34 tokens/s, Drafted throughput: 2555.86 tokens/s, Accepted: 11914 tokens, Drafted: 25560 tokens, Per-position acceptance rate: 0.740, 0.570, 0.437, 0.328, 0.256, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1765.8 tokens/s, Avg generation throughput: 1744.2 tokens/s, Running: 84 reqs, Waiting: 168 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 1230.39 tokens/s, Drafted throughput: 2592.92 tokens/s, Accepted: 12309 tokens, Drafted: 25940 tokens, Per-position acceptance rate: 0.742, 0.568, 0.446, 0.345, 0.272, Avg Draft acceptance rate: 47.5%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1010.9 tokens/s, Avg generation throughput: 1808.5 tokens/s, Running: 61 reqs, Waiting: 193 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 1265.85 tokens/s, Drafted throughput: 2726.18 tokens/s, Accepted: 12660 tokens, Drafted: 27265 tokens, Per-position acceptance rate: 0.739, 0.561, 0.428, 0.337, 0.256, Avg Draft acceptance rate: 46.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1060.7 tokens/s, Avg generation throughput: 1746.5 tokens/s, Running: 58 reqs, Waiting: 191 reqs, GPU KV cache usage: 94.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 1223.85 tokens/s, Drafted throughput: 2619.40 tokens/s, Accepted: 12239 tokens, Drafted: 26195 tokens, Per-position acceptance rate: 0.738, 0.564, 0.440, 0.331, 0.262, Avg Draft acceptance rate: 46.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1734.7 tokens/s, Avg generation throughput: 1773.9 tokens/s, Running: 85 reqs, Waiting: 167 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 1268.03 tokens/s, Drafted throughput: 2539.66 tokens/s, Accepted: 12682 tokens, Drafted: 25400 tokens, Per-position acceptance rate: 0.758, 0.593, 0.474, 0.372, 0.300, Avg Draft acceptance rate: 49.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:56 [loggers.py:257] Engine 000: Avg prompt throughput: 997.9 tokens/s, Avg generation throughput: 1860.9 tokens/s, Running: 62 reqs, Waiting: 193 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:56:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 1298.76 tokens/s, Drafted throughput: 2838.82 tokens/s, Accepted: 12993 tokens, Drafted: 28400 tokens, Per-position acceptance rate: 0.733, 0.551, 0.422, 0.325, 0.256, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1079.8 tokens/s, Avg generation throughput: 1678.3 tokens/s, Running: 51 reqs, Waiting: 198 reqs, GPU KV cache usage: 86.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 1169.82 tokens/s, Drafted throughput: 2543.82 tokens/s, Accepted: 11699 tokens, Drafted: 25440 tokens, Per-position acceptance rate: 0.736, 0.564, 0.428, 0.321, 0.250, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1844.7 tokens/s, Avg generation throughput: 1707.7 tokens/s, Running: 81 reqs, Waiting: 174 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 1208.05 tokens/s, Drafted throughput: 2511.20 tokens/s, Accepted: 12082 tokens, Drafted: 25115 tokens, Per-position acceptance rate: 0.753, 0.573, 0.448, 0.347, 0.284, Avg Draft acceptance rate: 48.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1084.0 tokens/s, Avg generation throughput: 1880.4 tokens/s, Running: 64 reqs, Waiting: 192 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1316.29 tokens/s, Drafted throughput: 2826.20 tokens/s, Accepted: 13169 tokens, Drafted: 28275 tokens, Per-position acceptance rate: 0.743, 0.565, 0.429, 0.334, 0.257, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1169.7 tokens/s, Avg generation throughput: 1707.4 tokens/s, Running: 59 reqs, Waiting: 191 reqs, GPU KV cache usage: 92.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 1194.92 tokens/s, Drafted throughput: 2593.12 tokens/s, Accepted: 11951 tokens, Drafted: 25935 tokens, Per-position acceptance rate: 0.735, 0.560, 0.424, 0.328, 0.256, Avg Draft acceptance rate: 46.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1640.0 tokens/s, Avg generation throughput: 1710.6 tokens/s, Running: 77 reqs, Waiting: 178 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1195.69 tokens/s, Drafted throughput: 2566.77 tokens/s, Accepted: 11958 tokens, Drafted: 25670 tokens, Per-position acceptance rate: 0.745, 0.559, 0.434, 0.332, 0.259, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1063.2 tokens/s, Avg generation throughput: 1865.9 tokens/s, Running: 63 reqs, Waiting: 192 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:57:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 1303.92 tokens/s, Drafted throughput: 2812.33 tokens/s, Accepted: 13040 tokens, Drafted: 28125 tokens, Per-position acceptance rate: 0.735, 0.558, 0.431, 0.333, 0.261, Avg Draft acceptance rate: 46.4%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1090.8 tokens/s, Avg generation throughput: 1609.9 tokens/s, Running: 54 reqs, Waiting: 191 reqs, GPU KV cache usage: 85.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 1118.18 tokens/s, Drafted throughput: 2482.23 tokens/s, Accepted: 11183 tokens, Drafted: 24825 tokens, Per-position acceptance rate: 0.725, 0.547, 0.418, 0.321, 0.242, Avg Draft acceptance rate: 45.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1875.9 tokens/s, Avg generation throughput: 1777.8 tokens/s, Running: 82 reqs, Waiting: 172 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 1262.82 tokens/s, Drafted throughput: 2571.84 tokens/s, Accepted: 12629 tokens, Drafted: 25720 tokens, Per-position acceptance rate: 0.762, 0.590, 0.464, 0.357, 0.283, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:26 [loggers.py:257] Engine 000: Avg prompt throughput: 897.8 tokens/s, Avg generation throughput: 1838.1 tokens/s, Running: 65 reqs, Waiting: 189 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 1281.51 tokens/s, Drafted throughput: 2788.80 tokens/s, Accepted: 12816 tokens, Drafted: 27890 tokens, Per-position acceptance rate: 0.731, 0.554, 0.426, 0.328, 0.260, Avg Draft acceptance rate: 46.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1135.4 tokens/s, Avg generation throughput: 1731.5 tokens/s, Running: 60 reqs, Waiting: 189 reqs, GPU KV cache usage: 91.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1213.66 tokens/s, Drafted throughput: 2603.19 tokens/s, Accepted: 12138 tokens, Drafted: 26035 tokens, Per-position acceptance rate: 0.746, 0.557, 0.431, 0.333, 0.264, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1829.8 tokens/s, Avg generation throughput: 1698.8 tokens/s, Running: 83 reqs, Waiting: 172 reqs, GPU KV cache usage: 96.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 1197.67 tokens/s, Drafted throughput: 2512.23 tokens/s, Accepted: 11978 tokens, Drafted: 25125 tokens, Per-position acceptance rate: 0.749, 0.573, 0.448, 0.344, 0.269, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:56 [loggers.py:257] Engine 000: Avg prompt throughput: 916.5 tokens/s, Avg generation throughput: 1873.6 tokens/s, Running: 65 reqs, Waiting: 188 reqs, GPU KV cache usage: 94.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:58:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 1312.13 tokens/s, Drafted throughput: 2814.86 tokens/s, Accepted: 13122 tokens, Drafted: 28150 tokens, Per-position acceptance rate: 0.745, 0.567, 0.432, 0.331, 0.255, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:06 [loggers.py:257] Engine 000: Avg prompt throughput: 1137.3 tokens/s, Avg generation throughput: 1755.5 tokens/s, Running: 50 reqs, Waiting: 198 reqs, GPU KV cache usage: 87.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 1222.84 tokens/s, Drafted throughput: 2667.86 tokens/s, Accepted: 12229 tokens, Drafted: 26680 tokens, Per-position acceptance rate: 0.748, 0.551, 0.424, 0.320, 0.250, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:16 [loggers.py:257] Engine 000: Avg prompt throughput: 1665.0 tokens/s, Avg generation throughput: 1710.2 tokens/s, Running: 80 reqs, Waiting: 171 reqs, GPU KV cache usage: 91.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 1202.19 tokens/s, Drafted throughput: 2527.27 tokens/s, Accepted: 12023 tokens, Drafted: 25275 tokens, Per-position acceptance rate: 0.746, 0.570, 0.444, 0.341, 0.277, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:26 [loggers.py:257] Engine 000: Avg prompt throughput: 1072.3 tokens/s, Avg generation throughput: 1907.0 tokens/s, Running: 64 reqs, Waiting: 192 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 1346.46 tokens/s, Drafted throughput: 2819.66 tokens/s, Accepted: 13471 tokens, Drafted: 28210 tokens, Per-position acceptance rate: 0.752, 0.572, 0.442, 0.349, 0.273, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:36 [loggers.py:257] Engine 000: Avg prompt throughput: 1026.5 tokens/s, Avg generation throughput: 1753.0 tokens/s, Running: 60 reqs, Waiting: 194 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 1222.00 tokens/s, Drafted throughput: 2661.69 tokens/s, Accepted: 12226 tokens, Drafted: 26630 tokens, Per-position acceptance rate: 0.732, 0.556, 0.428, 0.323, 0.256, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:46 [loggers.py:257] Engine 000: Avg prompt throughput: 1778.6 tokens/s, Avg generation throughput: 1671.3 tokens/s, Running: 80 reqs, Waiting: 157 reqs, GPU KV cache usage: 91.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 1173.19 tokens/s, Drafted throughput: 2496.76 tokens/s, Accepted: 11733 tokens, Drafted: 24970 tokens, Per-position acceptance rate: 0.751, 0.570, 0.438, 0.334, 0.256, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:56 [loggers.py:257] Engine 000: Avg prompt throughput: 1132.9 tokens/s, Avg generation throughput: 1891.1 tokens/s, Running: 70 reqs, Waiting: 91 reqs, GPU KV cache usage: 96.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 21:59:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 1336.13 tokens/s, Drafted throughput: 2782.36 tokens/s, Accepted: 13362 tokens, Drafted: 27825 tokens, Per-position acceptance rate: 0.748, 0.571, 0.448, 0.353, 0.282, Avg Draft acceptance rate: 48.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 22:00:06 [loggers.py:257] Engine 000: Avg prompt throughput: 888.2 tokens/s, Avg generation throughput: 1895.0 tokens/s, Running: 56 reqs, Waiting: 25 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 22:00:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 1329.80 tokens/s, Drafted throughput: 2838.29 tokens/s, Accepted: 13299 tokens, Drafted: 28385 tokens, Per-position acceptance rate: 0.741, 0.565, 0.437, 0.336, 0.263, Avg Draft acceptance rate: 46.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  292.12    
Total input tokens:                      373233    
Total generated tokens:                  511879    
Request throughput (req/s):              8.76      
Output token throughput (tok/s):         1752.30   
Peak output token throughput (tok/s):    797.00    
Peak concurrent requests:                277.00    
Total token throughput (tok/s):          3029.98   
---------------Time to First Token----------------
Mean TTFT (ms):                          19482.35  
Median TTFT (ms):                        21226.21  
P99 TTFT (ms):                           23156.44  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          42.40     
Median TPOT (ms):                        40.13     
P99 TPOT (ms):                           70.88     
---------------Inter-token Latency----------------
Mean ITL (ms):                           140.17    
Median ITL (ms):                         106.48    
P99 ITL (ms):                            333.77    
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.00     
Acceptance length:                       3.35      
Drafts:                                  152959    
Draft tokens:                            764795    
Accepted tokens:                         359471    
Per-position acceptance (%):
  Position 0:                            74.38     
  Position 1:                            56.64     
  Position 2:                            43.81     
  Position 3:                            33.69     
  Position 4:                            26.49     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k5-t0.0-tp1...
[0;36m(APIServer pid=834350)[0;0m INFO 01-22 22:00:12 [launcher.py:110] Shutting down FastAPI HTTP server.
