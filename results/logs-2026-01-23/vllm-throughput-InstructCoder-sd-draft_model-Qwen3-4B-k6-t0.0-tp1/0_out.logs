Removing any existing container named vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k6-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k6-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 247151
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:47 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:47 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15023, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=247151)[0;0m WARNING 01-23 19:22:47 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:48 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:48 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:49 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:49 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:49 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=247151)[0;0m WARNING 01-23 19:22:49 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:22:49 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb80673efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a0121497-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 19:22:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:22:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:00 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=6), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:01 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.68:40401 backend=nccl
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:01 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=247292)[0;0m WARNING 01-23 19:23:02 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:02 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:03 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 19:23:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:23:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:55 [default_loader.py:291] Loading weights took 50.58 seconds
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:55 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:55 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:23:55 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 19:23:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:03 [default_loader.py:291] Loading weights took 6.68 seconds
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:04 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 60.465811 seconds
WARNING 01-23 19:24:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:24:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:24:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:16 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:16 [backends.py:704] Dynamo bytecode transform time: 12.50 s
WARNING 01-23 19:24:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:24:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:24:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:33 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.356 s
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:33 [monitor.py:34] torch.compile takes 16.86 s in total
WARNING 01-23 19:24:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:24:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:40 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:40 [backends.py:704] Dynamo bytecode transform time: 6.13 s
WARNING 01-23 19:24:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:48 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.052 s
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:48 [monitor.py:34] torch.compile takes 25.03 s in total
WARNING 01-23 19:24:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:50 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:50 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:24:50 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 19:24:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:24:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:25:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 19:25:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:25:09 [gpu_model_runner.py:4880] Graph capturing finished in 18 secs, took 0.02 GiB
[0;36m(EngineCore_DP0 pid=247292)[0;0m INFO 01-23 19:25:10 [core.py:272] init engine (profile, create kv cache, warmup model) took 65.98 seconds
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:11 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=247151)[0;0m WARNING 01-23 19:25:12 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:12 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:12 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:12 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [serving.py:221] Chat template warmup completed in 1752.1ms
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15023
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:14 [launcher.py:46] Route: /pooling, Methods: POST
WARNING 01-23 19:25:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:24 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.85, Accepted throughput: 13.08 tokens/s, Drafted throughput: 27.53 tokens/s, Accepted: 171 tokens, Drafted: 360 tokens, Per-position acceptance rate: 0.817, 0.633, 0.517, 0.400, 0.267, 0.217, Avg Draft acceptance rate: 47.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:34 [loggers.py:257] Engine 000: Avg prompt throughput: 32.8 tokens/s, Avg generation throughput: 47.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 33.70 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 337 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.761, 0.580, 0.413, 0.312, 0.225, 0.152, Avg Draft acceptance rate: 40.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:44 [loggers.py:257] Engine 000: Avg prompt throughput: 24.1 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 34.00 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 340 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.784, 0.561, 0.432, 0.324, 0.223, 0.122, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:54 [loggers.py:257] Engine 000: Avg prompt throughput: 43.3 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:25:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 36.60 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 366 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.759, 0.591, 0.467, 0.350, 0.270, 0.234, Avg Draft acceptance rate: 44.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:04 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 49.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 35.80 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 358 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.768, 0.558, 0.471, 0.341, 0.261, 0.196, Avg Draft acceptance rate: 43.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:14 [loggers.py:257] Engine 000: Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 33.80 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 338 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.774, 0.577, 0.438, 0.292, 0.204, 0.182, Avg Draft acceptance rate: 41.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:24 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 38.20 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 382 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.734, 0.576, 0.482, 0.410, 0.317, 0.230, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:34 [loggers.py:257] Engine 000: Avg prompt throughput: 48.3 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 34.89 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 349 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.730, 0.569, 0.445, 0.321, 0.255, 0.226, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:44 [loggers.py:257] Engine 000: Avg prompt throughput: 28.8 tokens/s, Avg generation throughput: 50.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 36.59 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 366 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.790, 0.565, 0.428, 0.355, 0.268, 0.246, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:54 [loggers.py:257] Engine 000: Avg prompt throughput: 44.8 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:26:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 41.40 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 414 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.810, 0.628, 0.547, 0.423, 0.336, 0.277, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:04 [loggers.py:257] Engine 000: Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 29.30 tokens/s, Drafted throughput: 83.40 tokens/s, Accepted: 293 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.676, 0.453, 0.338, 0.273, 0.201, 0.165, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:14 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 48.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 34.50 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 345 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.737, 0.591, 0.431, 0.307, 0.248, 0.204, Avg Draft acceptance rate: 42.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:24 [loggers.py:257] Engine 000: Avg prompt throughput: 29.8 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 38.30 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 383 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.754, 0.594, 0.486, 0.391, 0.312, 0.239, Avg Draft acceptance rate: 46.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:34 [loggers.py:257] Engine 000: Avg prompt throughput: 44.1 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 36.90 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 369 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.788, 0.562, 0.453, 0.350, 0.292, 0.248, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:44 [loggers.py:257] Engine 000: Avg prompt throughput: 40.0 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 34.20 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 342 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.763, 0.547, 0.424, 0.295, 0.230, 0.201, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:54 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:27:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 34.90 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 349 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.752, 0.599, 0.438, 0.328, 0.241, 0.190, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:04 [loggers.py:257] Engine 000: Avg prompt throughput: 42.7 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.11, Accepted throughput: 42.90 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 429 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.819, 0.667, 0.572, 0.442, 0.341, 0.268, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:14 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 34.80 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 348 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.717, 0.565, 0.457, 0.326, 0.254, 0.203, Avg Draft acceptance rate: 42.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:24 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 30.90 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 309 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.696, 0.558, 0.391, 0.239, 0.217, 0.138, Avg Draft acceptance rate: 37.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:34 [loggers.py:257] Engine 000: Avg prompt throughput: 44.9 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 38.30 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 383 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.810, 0.628, 0.474, 0.358, 0.285, 0.241, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:44 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.74, Accepted throughput: 38.09 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 381 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.719, 0.590, 0.460, 0.374, 0.309, 0.288, Avg Draft acceptance rate: 45.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  202.82    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.25      
Output token throughput (tok/s):         49.30     
Peak output token throughput (tok/s):    15.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          84.94     
---------------Time to First Token----------------
Mean TTFT (ms):                          84.90     
Median TTFT (ms):                        84.69     
P99 TTFT (ms):                           92.97     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.88     
Median TPOT (ms):                        19.62     
P99 TPOT (ms):                           23.59     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.05     
Median ITL (ms):                         71.05     
P99 ITL (ms):                            71.60     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.38     
Acceptance length:                       3.60      
Drafts:                                  2784      
Draft tokens:                            16704     
Accepted tokens:                         7247      
Per-position acceptance (%):
  Position 0:                            75.75     
  Position 1:                            57.76     
  Position 2:                            45.15     
  Position 3:                            34.02     
  Position 4:                            26.40     
  Position 5:                            21.23     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:28:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 4.40 tokens/s, Drafted throughput: 13.20 tokens/s, Accepted: 44 tokens, Drafted: 132 tokens, Per-position acceptance rate: 0.773, 0.500, 0.273, 0.227, 0.136, 0.091, Avg Draft acceptance rate: 33.3%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0acf736fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3ef669fa-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:04 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.12, Accepted throughput: 5.00 tokens/s, Drafted throughput: 9.60 tokens/s, Accepted: 50 tokens, Drafted: 96 tokens, Per-position acceptance rate: 0.812, 0.625, 0.625, 0.500, 0.312, 0.250, Avg Draft acceptance rate: 52.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:14 [loggers.py:257] Engine 000: Avg prompt throughput: 59.6 tokens/s, Avg generation throughput: 75.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 54.89 tokens/s, Drafted throughput: 128.38 tokens/s, Accepted: 549 tokens, Drafted: 1284 tokens, Per-position acceptance rate: 0.790, 0.607, 0.430, 0.327, 0.234, 0.178, Avg Draft acceptance rate: 42.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:24 [loggers.py:257] Engine 000: Avg prompt throughput: 69.2 tokens/s, Avg generation throughput: 95.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 69.60 tokens/s, Drafted throughput: 161.99 tokens/s, Accepted: 696 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.763, 0.567, 0.459, 0.344, 0.263, 0.181, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:34 [loggers.py:257] Engine 000: Avg prompt throughput: 59.7 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 68.79 tokens/s, Drafted throughput: 160.78 tokens/s, Accepted: 688 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.746, 0.556, 0.459, 0.336, 0.261, 0.209, Avg Draft acceptance rate: 42.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:44 [loggers.py:257] Engine 000: Avg prompt throughput: 66.5 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.72, Accepted throughput: 73.99 tokens/s, Drafted throughput: 163.18 tokens/s, Accepted: 740 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.768, 0.603, 0.463, 0.364, 0.287, 0.235, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:54 [loggers.py:257] Engine 000: Avg prompt throughput: 84.2 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:29:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.88, Accepted throughput: 76.49 tokens/s, Drafted throughput: 159.58 tokens/s, Accepted: 765 tokens, Drafted: 1596 tokens, Per-position acceptance rate: 0.808, 0.602, 0.508, 0.406, 0.308, 0.244, Avg Draft acceptance rate: 47.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:04 [loggers.py:257] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 61.99 tokens/s, Drafted throughput: 163.18 tokens/s, Accepted: 620 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.724, 0.551, 0.415, 0.250, 0.184, 0.154, Avg Draft acceptance rate: 38.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:14 [loggers.py:257] Engine 000: Avg prompt throughput: 73.9 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 74.99 tokens/s, Drafted throughput: 160.78 tokens/s, Accepted: 750 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.765, 0.575, 0.489, 0.384, 0.328, 0.257, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:24 [loggers.py:257] Engine 000: Avg prompt throughput: 90.1 tokens/s, Avg generation throughput: 96.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 70.09 tokens/s, Drafted throughput: 159.57 tokens/s, Accepted: 701 tokens, Drafted: 1596 tokens, Per-position acceptance rate: 0.778, 0.598, 0.455, 0.338, 0.263, 0.203, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:34 [loggers.py:257] Engine 000: Avg prompt throughput: 59.7 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 73.29 tokens/s, Drafted throughput: 163.18 tokens/s, Accepted: 733 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.724, 0.577, 0.485, 0.375, 0.290, 0.243, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:44 [loggers.py:257] Engine 000: Avg prompt throughput: 76.3 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 70.30 tokens/s, Drafted throughput: 160.79 tokens/s, Accepted: 703 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.791, 0.612, 0.440, 0.317, 0.261, 0.201, Avg Draft acceptance rate: 43.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  105.11    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.48      
Output token throughput (tok/s):         95.14     
Peak output token throughput (tok/s):    28.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          163.89    
---------------Time to First Token----------------
Mean TTFT (ms):                          147.21    
Median TTFT (ms):                        146.85    
P99 TTFT (ms):                           178.23    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.91     
Median TPOT (ms):                        19.80     
P99 TPOT (ms):                           23.58     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.63     
Median ITL (ms):                         71.53     
P99 ITL (ms):                            76.38     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.85     
Acceptance length:                       3.63      
Drafts:                                  2765      
Draft tokens:                            16590     
Accepted tokens:                         7275      
Per-position acceptance (%):
  Position 0:                            76.35     
  Position 1:                            58.30     
  Position 2:                            45.90     
  Position 3:                            34.39     
  Position 4:                            26.84     
  Position 5:                            21.34     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:54 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:30:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 42.90 tokens/s, Drafted throughput: 100.79 tokens/s, Accepted: 429 tokens, Drafted: 1008 tokens, Per-position acceptance rate: 0.744, 0.571, 0.423, 0.333, 0.250, 0.232, Avg Draft acceptance rate: 42.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9c8f98afc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-01027a07-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:14 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 10.80 tokens/s, Drafted throughput: 21.60 tokens/s, Accepted: 216 tokens, Drafted: 432 tokens, Per-position acceptance rate: 0.833, 0.667, 0.569, 0.431, 0.292, 0.208, Avg Draft acceptance rate: 50.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:24 [loggers.py:257] Engine 000: Avg prompt throughput: 107.9 tokens/s, Avg generation throughput: 191.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 139.20 tokens/s, Drafted throughput: 319.19 tokens/s, Accepted: 1392 tokens, Drafted: 3192 tokens, Per-position acceptance rate: 0.750, 0.568, 0.461, 0.355, 0.278, 0.205, Avg Draft acceptance rate: 43.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:34 [loggers.py:257] Engine 000: Avg prompt throughput: 128.9 tokens/s, Avg generation throughput: 187.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 135.78 tokens/s, Drafted throughput: 317.96 tokens/s, Accepted: 1358 tokens, Drafted: 3180 tokens, Per-position acceptance rate: 0.751, 0.566, 0.455, 0.336, 0.247, 0.208, Avg Draft acceptance rate: 42.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:44 [loggers.py:257] Engine 000: Avg prompt throughput: 153.7 tokens/s, Avg generation throughput: 190.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 137.19 tokens/s, Drafted throughput: 316.79 tokens/s, Accepted: 1372 tokens, Drafted: 3168 tokens, Per-position acceptance rate: 0.773, 0.583, 0.460, 0.322, 0.256, 0.205, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:54 [loggers.py:257] Engine 000: Avg prompt throughput: 155.0 tokens/s, Avg generation throughput: 195.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:31:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 143.19 tokens/s, Drafted throughput: 316.79 tokens/s, Accepted: 1432 tokens, Drafted: 3168 tokens, Per-position acceptance rate: 0.759, 0.574, 0.481, 0.375, 0.294, 0.229, Avg Draft acceptance rate: 45.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:04 [loggers.py:257] Engine 000: Avg prompt throughput: 117.6 tokens/s, Avg generation throughput: 193.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 140.40 tokens/s, Drafted throughput: 320.39 tokens/s, Accepted: 1404 tokens, Drafted: 3204 tokens, Per-position acceptance rate: 0.755, 0.597, 0.459, 0.333, 0.273, 0.212, Avg Draft acceptance rate: 43.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  53.92     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.93      
Output token throughput (tok/s):         185.48    
Peak output token throughput (tok/s):    56.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          319.52    
---------------Time to First Token----------------
Mean TTFT (ms):                          148.61    
Median TTFT (ms):                        148.56    
P99 TTFT (ms):                           179.38    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.26     
Median TPOT (ms):                        20.12     
P99 TPOT (ms):                           23.39     
---------------Inter-token Latency----------------
Mean ITL (ms):                           72.81     
Median ITL (ms):                         72.49     
P99 ITL (ms):                            79.23     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.76     
Acceptance length:                       3.63      
Drafts:                                  2768      
Draft tokens:                            16608     
Accepted tokens:                         7267      
Per-position acceptance (%):
  Position 0:                            75.83     
  Position 1:                            57.80     
  Position 2:                            46.35     
  Position 3:                            34.43     
  Position 4:                            26.95     
  Position 5:                            21.17     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 33.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 24.20 tokens/s, Drafted throughput: 59.39 tokens/s, Accepted: 242 tokens, Drafted: 594 tokens, Per-position acceptance rate: 0.747, 0.545, 0.414, 0.303, 0.232, 0.202, Avg Draft acceptance rate: 40.7%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7de061efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b4461566-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:34 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 219.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 79.59 tokens/s, Drafted throughput: 183.89 tokens/s, Accepted: 1592 tokens, Drafted: 3678 tokens, Per-position acceptance rate: 0.763, 0.569, 0.465, 0.338, 0.264, 0.197, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:44 [loggers.py:257] Engine 000: Avg prompt throughput: 253.6 tokens/s, Avg generation throughput: 366.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 263.87 tokens/s, Drafted throughput: 619.12 tokens/s, Accepted: 2639 tokens, Drafted: 6192 tokens, Per-position acceptance rate: 0.757, 0.576, 0.449, 0.328, 0.251, 0.198, Avg Draft acceptance rate: 42.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:54 [loggers.py:257] Engine 000: Avg prompt throughput: 253.8 tokens/s, Avg generation throughput: 374.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:32:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 272.18 tokens/s, Drafted throughput: 616.75 tokens/s, Accepted: 2722 tokens, Drafted: 6168 tokens, Per-position acceptance rate: 0.755, 0.573, 0.458, 0.349, 0.289, 0.224, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:04 [loggers.py:257] Engine 000: Avg prompt throughput: 298.8 tokens/s, Avg generation throughput: 380.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 281.36 tokens/s, Drafted throughput: 613.12 tokens/s, Accepted: 2814 tokens, Drafted: 6132 tokens, Per-position acceptance rate: 0.776, 0.614, 0.482, 0.363, 0.286, 0.232, Avg Draft acceptance rate: 45.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  45.09     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.77      
Output token throughput (tok/s):         354.87    
Peak output token throughput (tok/s):    112.00    
Peak concurrent requests:                13.00     
Total token throughput (tok/s):          614.15    
---------------Time to First Token----------------
Mean TTFT (ms):                          153.43    
Median TTFT (ms):                        152.94    
P99 TTFT (ms):                           185.48    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.90     
Median TPOT (ms):                        21.00     
P99 TPOT (ms):                           25.09     
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.79     
Median ITL (ms):                         74.00     
P99 ITL (ms):                            89.96     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.49     
Acceptance length:                       3.61      
Drafts:                                  4450      
Draft tokens:                            26700     
Accepted tokens:                         11613     
Per-position acceptance (%):
  Position 0:                            75.69     
  Position 1:                            58.11     
  Position 2:                            45.73     
  Position 3:                            33.91     
  Position 4:                            26.70     
  Position 5:                            20.83     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:14 [loggers.py:257] Engine 000: Avg prompt throughput: 147.4 tokens/s, Avg generation throughput: 279.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 199.50 tokens/s, Drafted throughput: 485.99 tokens/s, Accepted: 1995 tokens, Drafted: 4860 tokens, Per-position acceptance rate: 0.733, 0.568, 0.432, 0.315, 0.236, 0.179, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f19cd7d6fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a96adef3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:34 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 6.95 tokens/s, Drafted throughput: 15.00 tokens/s, Accepted: 139 tokens, Drafted: 300 tokens, Per-position acceptance rate: 0.800, 0.640, 0.520, 0.400, 0.240, 0.180, Avg Draft acceptance rate: 46.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:44 [loggers.py:257] Engine 000: Avg prompt throughput: 522.6 tokens/s, Avg generation throughput: 619.0 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 446.86 tokens/s, Drafted throughput: 1040.31 tokens/s, Accepted: 4469 tokens, Drafted: 10404 tokens, Per-position acceptance rate: 0.762, 0.572, 0.451, 0.334, 0.255, 0.203, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:54 [loggers.py:257] Engine 000: Avg prompt throughput: 507.8 tokens/s, Avg generation throughput: 687.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:33:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 501.01 tokens/s, Drafted throughput: 1147.59 tokens/s, Accepted: 5011 tokens, Drafted: 11478 tokens, Per-position acceptance rate: 0.750, 0.579, 0.463, 0.341, 0.270, 0.216, Avg Draft acceptance rate: 43.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:04 [loggers.py:257] Engine 000: Avg prompt throughput: 553.3 tokens/s, Avg generation throughput: 687.8 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 498.80 tokens/s, Drafted throughput: 1156.80 tokens/s, Accepted: 4988 tokens, Drafted: 11568 tokens, Per-position acceptance rate: 0.756, 0.580, 0.449, 0.339, 0.262, 0.201, Avg Draft acceptance rate: 43.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:14 [loggers.py:257] Engine 000: Avg prompt throughput: 574.8 tokens/s, Avg generation throughput: 712.2 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.74, Accepted throughput: 524.13 tokens/s, Drafted throughput: 1147.06 tokens/s, Accepted: 5242 tokens, Drafted: 11472 tokens, Per-position acceptance rate: 0.770, 0.600, 0.465, 0.369, 0.298, 0.240, Avg Draft acceptance rate: 45.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  48.21     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.32      
Output token throughput (tok/s):         663.55    
Peak output token throughput (tok/s):    208.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          1168.84   
---------------Time to First Token----------------
Mean TTFT (ms):                          167.85    
Median TTFT (ms):                        166.36    
P99 TTFT (ms):                           208.92    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.27     
Median TPOT (ms):                        22.14     
P99 TPOT (ms):                           28.08     
---------------Inter-token Latency----------------
Mean ITL (ms):                           80.01     
Median ITL (ms):                         78.41     
P99 ITL (ms):                            112.29    
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.83     
Acceptance length:                       3.63      
Drafts:                                  8860      
Draft tokens:                            53160     
Accepted tokens:                         23302     
Per-position acceptance (%):
  Position 0:                            75.89     
  Position 1:                            58.28     
  Position 2:                            45.56     
  Position 3:                            34.57     
  Position 4:                            27.16     
  Position 5:                            21.53     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:24 [loggers.py:257] Engine 000: Avg prompt throughput: 277.3 tokens/s, Avg generation throughput: 493.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 360.18 tokens/s, Drafted throughput: 826.76 tokens/s, Accepted: 3602 tokens, Drafted: 8268 tokens, Per-position acceptance rate: 0.756, 0.583, 0.446, 0.343, 0.271, 0.215, Avg Draft acceptance rate: 43.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efe7c2aefc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f621f88f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:44 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 7.00 tokens/s, Drafted throughput: 15.60 tokens/s, Accepted: 140 tokens, Drafted: 312 tokens, Per-position acceptance rate: 0.788, 0.615, 0.500, 0.385, 0.231, 0.173, Avg Draft acceptance rate: 44.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:54 [loggers.py:257] Engine 000: Avg prompt throughput: 961.3 tokens/s, Avg generation throughput: 1125.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:34:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 817.73 tokens/s, Drafted throughput: 1849.79 tokens/s, Accepted: 8180 tokens, Drafted: 18504 tokens, Per-position acceptance rate: 0.770, 0.587, 0.461, 0.344, 0.273, 0.217, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:04 [loggers.py:257] Engine 000: Avg prompt throughput: 970.6 tokens/s, Avg generation throughput: 1217.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 53.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 889.03 tokens/s, Drafted throughput: 2016.81 tokens/s, Accepted: 8892 tokens, Drafted: 20172 tokens, Per-position acceptance rate: 0.758, 0.583, 0.455, 0.350, 0.277, 0.221, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:14 [loggers.py:257] Engine 000: Avg prompt throughput: 980.6 tokens/s, Avg generation throughput: 1230.2 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 44.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.72, Accepted throughput: 903.94 tokens/s, Drafted throughput: 1995.47 tokens/s, Accepted: 9040 tokens, Drafted: 19956 tokens, Per-position acceptance rate: 0.760, 0.591, 0.465, 0.363, 0.296, 0.244, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:24 [loggers.py:257] Engine 000: Avg prompt throughput: 922.4 tokens/s, Avg generation throughput: 1220.1 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 888.67 tokens/s, Drafted throughput: 2037.89 tokens/s, Accepted: 8888 tokens, Drafted: 20382 tokens, Per-position acceptance rate: 0.763, 0.570, 0.453, 0.348, 0.269, 0.213, Avg Draft acceptance rate: 43.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:34 [loggers.py:257] Engine 000: Avg prompt throughput: 933.3 tokens/s, Avg generation throughput: 1203.9 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 46.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 869.41 tokens/s, Drafted throughput: 2020.22 tokens/s, Accepted: 8694 tokens, Drafted: 20202 tokens, Per-position acceptance rate: 0.754, 0.571, 0.440, 0.336, 0.267, 0.214, Avg Draft acceptance rate: 43.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  55.97     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              5.72      
Output token throughput (tok/s):         1143.24   
Peak output token throughput (tok/s):    384.00    
Peak concurrent requests:                47.00     
Total token throughput (tok/s):          2005.54   
---------------Time to First Token----------------
Mean TTFT (ms):                          196.31    
Median TTFT (ms):                        190.30    
P99 TTFT (ms):                           280.21    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          25.37     
Median TPOT (ms):                        25.49     
P99 TPOT (ms):                           32.47     
---------------Inter-token Latency----------------
Mean ITL (ms):                           90.85     
Median ITL (ms):                         85.69     
P99 ITL (ms):                            141.39    
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.63     
Acceptance length:                       3.62      
Drafts:                                  17780     
Draft tokens:                            106680    
Accepted tokens:                         46540     
Per-position acceptance (%):
  Position 0:                            75.70     
  Position 1:                            57.62     
  Position 2:                            44.89     
  Position 3:                            34.35     
  Position 4:                            27.29     
  Position 5:                            21.90     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:44 [loggers.py:257] Engine 000: Avg prompt throughput: 57.6 tokens/s, Avg generation throughput: 402.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 285.46 tokens/s, Drafted throughput: 748.10 tokens/s, Accepted: 2855 tokens, Drafted: 7482 tokens, Per-position acceptance rate: 0.706, 0.521, 0.370, 0.283, 0.229, 0.180, Avg Draft acceptance rate: 38.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:35:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fab6426afc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0452a10f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:04 [loggers.py:257] Engine 000: Avg prompt throughput: 948.1 tokens/s, Avg generation throughput: 177.2 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 63.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.67, Accepted throughput: 67.28 tokens/s, Drafted throughput: 110.07 tokens/s, Accepted: 1346 tokens, Drafted: 2202 tokens, Per-position acceptance rate: 0.899, 0.787, 0.714, 0.510, 0.414, 0.343, Avg Draft acceptance rate: 61.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:14 [loggers.py:257] Engine 000: Avg prompt throughput: 995.1 tokens/s, Avg generation throughput: 1780.2 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 94.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 1297.37 tokens/s, Drafted throughput: 2930.93 tokens/s, Accepted: 12974 tokens, Drafted: 29310 tokens, Per-position acceptance rate: 0.771, 0.585, 0.454, 0.349, 0.278, 0.219, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1081.7 tokens/s, Avg generation throughput: 1661.9 tokens/s, Running: 56 reqs, Waiting: 0 reqs, GPU KV cache usage: 89.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 1199.57 tokens/s, Drafted throughput: 2811.89 tokens/s, Accepted: 11997 tokens, Drafted: 28122 tokens, Per-position acceptance rate: 0.752, 0.564, 0.433, 0.333, 0.265, 0.213, Avg Draft acceptance rate: 42.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1535.7 tokens/s, Avg generation throughput: 1618.4 tokens/s, Running: 57 reqs, Waiting: 0 reqs, GPU KV cache usage: 70.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 1180.99 tokens/s, Drafted throughput: 2676.95 tokens/s, Accepted: 11811 tokens, Drafted: 26772 tokens, Per-position acceptance rate: 0.755, 0.578, 0.454, 0.350, 0.280, 0.230, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1084.5 tokens/s, Avg generation throughput: 1758.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 94.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 1273.01 tokens/s, Drafted throughput: 2930.79 tokens/s, Accepted: 12731 tokens, Drafted: 29310 tokens, Per-position acceptance rate: 0.749, 0.574, 0.445, 0.346, 0.274, 0.218, Avg Draft acceptance rate: 43.4%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1231.8 tokens/s, Avg generation throughput: 1679.8 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:36:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1209.94 tokens/s, Drafted throughput: 2872.07 tokens/s, Accepted: 12100 tokens, Drafted: 28722 tokens, Per-position acceptance rate: 0.734, 0.564, 0.435, 0.332, 0.261, 0.203, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1372.5 tokens/s, Avg generation throughput: 1741.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 79.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 1268.08 tokens/s, Drafted throughput: 2911.81 tokens/s, Accepted: 12686 tokens, Drafted: 29130 tokens, Per-position acceptance rate: 0.752, 0.577, 0.451, 0.349, 0.269, 0.215, Avg Draft acceptance rate: 43.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1126.3 tokens/s, Avg generation throughput: 1728.6 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 1247.11 tokens/s, Drafted throughput: 2916.74 tokens/s, Accepted: 12473 tokens, Drafted: 29172 tokens, Per-position acceptance rate: 0.747, 0.568, 0.440, 0.341, 0.264, 0.206, Avg Draft acceptance rate: 42.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  78.78     
Total input tokens:                      94775     
Total generated tokens:                  127934    
Request throughput (req/s):              8.12      
Output token throughput (tok/s):         1624.01   
Peak output token throughput (tok/s):    621.00    
Peak concurrent requests:                89.00     
Total token throughput (tok/s):          2827.10   
---------------Time to First Token----------------
Mean TTFT (ms):                          378.88    
Median TTFT (ms):                        317.09    
P99 TTFT (ms):                           1374.17   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          35.64     
Median TPOT (ms):                        35.60     
P99 TPOT (ms):                           47.42     
---------------Inter-token Latency----------------
Mean ITL (ms):                           126.62    
Median ITL (ms):                         110.63    
P99 ITL (ms):                            219.86    
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.20     
Acceptance length:                       3.59      
Drafts:                                  35765     
Draft tokens:                            214590    
Accepted tokens:                         92701     
Per-position acceptance (%):
  Position 0:                            75.13     
  Position 1:                            57.24     
  Position 2:                            44.41     
  Position 3:                            34.22     
  Position 4:                            26.87     
  Position 5:                            21.33     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:24 [loggers.py:257] Engine 000: Avg prompt throughput: 118.1 tokens/s, Avg generation throughput: 665.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 473.19 tokens/s, Drafted throughput: 1217.98 tokens/s, Accepted: 4732 tokens, Drafted: 12180 tokens, Per-position acceptance rate: 0.725, 0.529, 0.393, 0.300, 0.222, 0.164, Avg Draft acceptance rate: 38.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff53d38afc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-018eef84-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:44 [loggers.py:257] Engine 000: Avg prompt throughput: 370.0 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 25 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 9.30 tokens/s, Drafted throughput: 19.20 tokens/s, Accepted: 186 tokens, Drafted: 384 tokens, Per-position acceptance rate: 0.828, 0.672, 0.562, 0.391, 0.250, 0.203, Avg Draft acceptance rate: 48.4%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1449.5 tokens/s, Avg generation throughput: 1546.7 tokens/s, Running: 71 reqs, Waiting: 49 reqs, GPU KV cache usage: 90.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:37:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 1122.60 tokens/s, Drafted throughput: 2520.98 tokens/s, Accepted: 11227 tokens, Drafted: 25212 tokens, Per-position acceptance rate: 0.772, 0.591, 0.466, 0.348, 0.273, 0.221, Avg Draft acceptance rate: 44.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1328.9 tokens/s, Avg generation throughput: 1735.5 tokens/s, Running: 72 reqs, Waiting: 56 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 1264.60 tokens/s, Drafted throughput: 2856.22 tokens/s, Accepted: 12653 tokens, Drafted: 28578 tokens, Per-position acceptance rate: 0.765, 0.586, 0.449, 0.349, 0.282, 0.225, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:14 [loggers.py:257] Engine 000: Avg prompt throughput: 971.4 tokens/s, Avg generation throughput: 1676.2 tokens/s, Running: 60 reqs, Waiting: 67 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1204.92 tokens/s, Drafted throughput: 2863.01 tokens/s, Accepted: 12050 tokens, Drafted: 28632 tokens, Per-position acceptance rate: 0.744, 0.556, 0.432, 0.329, 0.258, 0.206, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1246.6 tokens/s, Avg generation throughput: 1547.1 tokens/s, Running: 63 reqs, Waiting: 58 reqs, GPU KV cache usage: 92.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 1109.45 tokens/s, Drafted throughput: 2636.65 tokens/s, Accepted: 11096 tokens, Drafted: 26370 tokens, Per-position acceptance rate: 0.739, 0.563, 0.428, 0.327, 0.263, 0.205, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1471.8 tokens/s, Avg generation throughput: 1618.5 tokens/s, Running: 79 reqs, Waiting: 46 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 1176.44 tokens/s, Drafted throughput: 2679.09 tokens/s, Accepted: 11764 tokens, Drafted: 26790 tokens, Per-position acceptance rate: 0.749, 0.580, 0.453, 0.348, 0.282, 0.222, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1072.4 tokens/s, Avg generation throughput: 1707.3 tokens/s, Running: 74 reqs, Waiting: 54 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 1225.09 tokens/s, Drafted throughput: 2916.58 tokens/s, Accepted: 12251 tokens, Drafted: 29166 tokens, Per-position acceptance rate: 0.731, 0.558, 0.437, 0.330, 0.257, 0.207, Avg Draft acceptance rate: 42.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1071.3 tokens/s, Avg generation throughput: 1683.7 tokens/s, Running: 57 reqs, Waiting: 69 reqs, GPU KV cache usage: 96.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:38:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1209.59 tokens/s, Drafted throughput: 2865.34 tokens/s, Accepted: 12097 tokens, Drafted: 28656 tokens, Per-position acceptance rate: 0.741, 0.566, 0.436, 0.337, 0.255, 0.198, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:04 [loggers.py:257] Engine 000: Avg prompt throughput: 988.1 tokens/s, Avg generation throughput: 1543.0 tokens/s, Running: 56 reqs, Waiting: 68 reqs, GPU KV cache usage: 91.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 1098.75 tokens/s, Drafted throughput: 2685.48 tokens/s, Accepted: 10988 tokens, Drafted: 26856 tokens, Per-position acceptance rate: 0.730, 0.553, 0.416, 0.317, 0.248, 0.190, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1638.3 tokens/s, Avg generation throughput: 1604.8 tokens/s, Running: 83 reqs, Waiting: 39 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 1155.14 tokens/s, Drafted throughput: 2696.02 tokens/s, Accepted: 11553 tokens, Drafted: 26964 tokens, Per-position acceptance rate: 0.737, 0.569, 0.440, 0.343, 0.267, 0.215, Avg Draft acceptance rate: 42.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1069.7 tokens/s, Avg generation throughput: 1750.6 tokens/s, Running: 73 reqs, Waiting: 54 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 1262.47 tokens/s, Drafted throughput: 2961.53 tokens/s, Accepted: 12625 tokens, Drafted: 29616 tokens, Per-position acceptance rate: 0.737, 0.558, 0.437, 0.337, 0.270, 0.219, Avg Draft acceptance rate: 42.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1088.4 tokens/s, Avg generation throughput: 1681.8 tokens/s, Running: 61 reqs, Waiting: 67 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 1195.83 tokens/s, Drafted throughput: 2928.05 tokens/s, Accepted: 11963 tokens, Drafted: 29292 tokens, Per-position acceptance rate: 0.736, 0.543, 0.415, 0.314, 0.248, 0.194, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1135.8 tokens/s, Avg generation throughput: 1518.7 tokens/s, Running: 61 reqs, Waiting: 63 reqs, GPU KV cache usage: 95.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 1085.96 tokens/s, Drafted throughput: 2626.70 tokens/s, Accepted: 10860 tokens, Drafted: 26268 tokens, Per-position acceptance rate: 0.738, 0.549, 0.419, 0.316, 0.254, 0.205, Avg Draft acceptance rate: 41.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1562.2 tokens/s, Avg generation throughput: 1601.1 tokens/s, Running: 82 reqs, Waiting: 40 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:39:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 1156.01 tokens/s, Drafted throughput: 2684.79 tokens/s, Accepted: 11561 tokens, Drafted: 26850 tokens, Per-position acceptance rate: 0.753, 0.570, 0.441, 0.335, 0.269, 0.215, Avg Draft acceptance rate: 43.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1187.5 tokens/s, Avg generation throughput: 1699.3 tokens/s, Running: 75 reqs, Waiting: 53 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1220.29 tokens/s, Drafted throughput: 2892.73 tokens/s, Accepted: 12210 tokens, Drafted: 28944 tokens, Per-position acceptance rate: 0.743, 0.559, 0.430, 0.335, 0.256, 0.209, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:14 [loggers.py:257] Engine 000: Avg prompt throughput: 968.8 tokens/s, Avg generation throughput: 1644.6 tokens/s, Running: 60 reqs, Waiting: 34 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 1170.36 tokens/s, Drafted throughput: 2866.45 tokens/s, Accepted: 11705 tokens, Drafted: 28668 tokens, Per-position acceptance rate: 0.730, 0.549, 0.413, 0.314, 0.247, 0.196, Avg Draft acceptance rate: 40.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  158.85    
Total input tokens:                      189093    
Total generated tokens:                  255974    
Request throughput (req/s):              8.06      
Output token throughput (tok/s):         1611.45   
Peak output token throughput (tok/s):    697.00    
Peak concurrent requests:                146.00    
Total token throughput (tok/s):          2801.86   
---------------Time to First Token----------------
Mean TTFT (ms):                          6396.09   
Median TTFT (ms):                        7354.71   
P99 TTFT (ms):                           9560.03   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          44.43     
Median TPOT (ms):                        42.28     
P99 TPOT (ms):                           78.01     
---------------Inter-token Latency----------------
Mean ITL (ms):                           154.66    
Median ITL (ms):                         121.15    
P99 ITL (ms):                            349.71    
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.21     
Acceptance length:                       3.53      
Drafts:                                  72619     
Draft tokens:                            435714    
Accepted tokens:                         183910    
Per-position acceptance (%):
  Position 0:                            74.19     
  Position 1:                            56.18     
  Position 2:                            43.19     
  Position 3:                            33.02     
  Position 4:                            26.00     
  Position 5:                            20.68     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:24 [loggers.py:257] Engine 000: Avg prompt throughput: 303.5 tokens/s, Avg generation throughput: 1027.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 727.01 tokens/s, Drafted throughput: 1879.82 tokens/s, Accepted: 7270 tokens, Drafted: 18798 tokens, Per-position acceptance rate: 0.721, 0.533, 0.386, 0.290, 0.220, 0.170, Avg Draft acceptance rate: 38.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9c2fd8afc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d166e576-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:44 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.76, Accepted throughput: 3.45 tokens/s, Drafted throughput: 7.50 tokens/s, Accepted: 69 tokens, Drafted: 150 tokens, Per-position acceptance rate: 0.800, 0.640, 0.480, 0.400, 0.240, 0.200, Avg Draft acceptance rate: 46.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1666.1 tokens/s, Avg generation throughput: 1347.1 tokens/s, Running: 54 reqs, Waiting: 201 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:40:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 980.09 tokens/s, Drafted throughput: 2141.38 tokens/s, Accepted: 9801 tokens, Drafted: 21414 tokens, Per-position acceptance rate: 0.781, 0.604, 0.484, 0.363, 0.283, 0.231, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1044.3 tokens/s, Avg generation throughput: 1443.9 tokens/s, Running: 67 reqs, Waiting: 181 reqs, GPU KV cache usage: 91.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1039.28 tokens/s, Drafted throughput: 2464.50 tokens/s, Accepted: 10394 tokens, Drafted: 24648 tokens, Per-position acceptance rate: 0.747, 0.562, 0.427, 0.328, 0.260, 0.205, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1370.2 tokens/s, Avg generation throughput: 1688.1 tokens/s, Running: 74 reqs, Waiting: 180 reqs, GPU KV cache usage: 97.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 1224.54 tokens/s, Drafted throughput: 2791.67 tokens/s, Accepted: 12246 tokens, Drafted: 27918 tokens, Per-position acceptance rate: 0.757, 0.575, 0.450, 0.348, 0.279, 0.224, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1004.5 tokens/s, Avg generation throughput: 1676.7 tokens/s, Running: 61 reqs, Waiting: 195 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 1193.43 tokens/s, Drafted throughput: 2912.83 tokens/s, Accepted: 11935 tokens, Drafted: 29130 tokens, Per-position acceptance rate: 0.726, 0.542, 0.419, 0.322, 0.252, 0.197, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:34 [loggers.py:257] Engine 000: Avg prompt throughput: 929.3 tokens/s, Avg generation throughput: 1650.8 tokens/s, Running: 53 reqs, Waiting: 197 reqs, GPU KV cache usage: 91.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 1181.61 tokens/s, Drafted throughput: 2849.18 tokens/s, Accepted: 11822 tokens, Drafted: 28506 tokens, Per-position acceptance rate: 0.736, 0.559, 0.421, 0.320, 0.253, 0.199, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1579.1 tokens/s, Avg generation throughput: 1573.7 tokens/s, Running: 74 reqs, Waiting: 178 reqs, GPU KV cache usage: 96.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 1138.05 tokens/s, Drafted throughput: 2655.25 tokens/s, Accepted: 11382 tokens, Drafted: 26556 tokens, Per-position acceptance rate: 0.744, 0.569, 0.449, 0.333, 0.263, 0.213, Avg Draft acceptance rate: 42.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1359.6 tokens/s, Avg generation throughput: 1765.9 tokens/s, Running: 72 reqs, Waiting: 184 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:41:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 1285.16 tokens/s, Drafted throughput: 2915.14 tokens/s, Accepted: 12858 tokens, Drafted: 29166 tokens, Per-position acceptance rate: 0.758, 0.584, 0.457, 0.357, 0.274, 0.216, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:04 [loggers.py:257] Engine 000: Avg prompt throughput: 991.5 tokens/s, Avg generation throughput: 1697.0 tokens/s, Running: 63 reqs, Waiting: 192 reqs, GPU KV cache usage: 97.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 1217.23 tokens/s, Drafted throughput: 2901.42 tokens/s, Accepted: 12173 tokens, Drafted: 29016 tokens, Per-position acceptance rate: 0.738, 0.562, 0.430, 0.331, 0.258, 0.198, Avg Draft acceptance rate: 42.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:14 [loggers.py:257] Engine 000: Avg prompt throughput: 980.8 tokens/s, Avg generation throughput: 1625.7 tokens/s, Running: 58 reqs, Waiting: 195 reqs, GPU KV cache usage: 95.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 1163.57 tokens/s, Drafted throughput: 2805.65 tokens/s, Accepted: 11638 tokens, Drafted: 28062 tokens, Per-position acceptance rate: 0.730, 0.557, 0.422, 0.328, 0.253, 0.199, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1558.3 tokens/s, Avg generation throughput: 1630.2 tokens/s, Running: 75 reqs, Waiting: 177 reqs, GPU KV cache usage: 95.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 1182.35 tokens/s, Drafted throughput: 2718.85 tokens/s, Accepted: 11825 tokens, Drafted: 27192 tokens, Per-position acceptance rate: 0.746, 0.569, 0.447, 0.345, 0.280, 0.222, Avg Draft acceptance rate: 43.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1330.7 tokens/s, Avg generation throughput: 1691.1 tokens/s, Running: 72 reqs, Waiting: 184 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1215.56 tokens/s, Drafted throughput: 2887.48 tokens/s, Accepted: 12157 tokens, Drafted: 28878 tokens, Per-position acceptance rate: 0.733, 0.561, 0.433, 0.334, 0.261, 0.204, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:44 [loggers.py:257] Engine 000: Avg prompt throughput: 870.2 tokens/s, Avg generation throughput: 1628.9 tokens/s, Running: 59 reqs, Waiting: 195 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 1160.71 tokens/s, Drafted throughput: 2828.17 tokens/s, Accepted: 11608 tokens, Drafted: 28284 tokens, Per-position acceptance rate: 0.736, 0.546, 0.421, 0.317, 0.248, 0.195, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1328.3 tokens/s, Avg generation throughput: 1611.9 tokens/s, Running: 70 reqs, Waiting: 180 reqs, GPU KV cache usage: 94.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:42:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 1154.08 tokens/s, Drafted throughput: 2771.72 tokens/s, Accepted: 11542 tokens, Drafted: 27720 tokens, Per-position acceptance rate: 0.742, 0.553, 0.418, 0.324, 0.258, 0.204, Avg Draft acceptance rate: 41.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1426.0 tokens/s, Avg generation throughput: 1649.1 tokens/s, Running: 75 reqs, Waiting: 177 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 1195.52 tokens/s, Drafted throughput: 2744.81 tokens/s, Accepted: 11956 tokens, Drafted: 27450 tokens, Per-position acceptance rate: 0.750, 0.575, 0.447, 0.348, 0.273, 0.221, Avg Draft acceptance rate: 43.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1137.0 tokens/s, Avg generation throughput: 1725.6 tokens/s, Running: 67 reqs, Waiting: 187 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 1245.63 tokens/s, Drafted throughput: 2904.43 tokens/s, Accepted: 12457 tokens, Drafted: 29046 tokens, Per-position acceptance rate: 0.744, 0.566, 0.435, 0.341, 0.271, 0.216, Avg Draft acceptance rate: 42.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1019.5 tokens/s, Avg generation throughput: 1673.4 tokens/s, Running: 55 reqs, Waiting: 198 reqs, GPU KV cache usage: 94.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 1187.63 tokens/s, Drafted throughput: 2915.84 tokens/s, Accepted: 11877 tokens, Drafted: 29160 tokens, Per-position acceptance rate: 0.729, 0.553, 0.416, 0.315, 0.241, 0.190, Avg Draft acceptance rate: 40.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1282.7 tokens/s, Avg generation throughput: 1542.9 tokens/s, Running: 66 reqs, Waiting: 185 reqs, GPU KV cache usage: 93.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 1105.17 tokens/s, Drafted throughput: 2657.10 tokens/s, Accepted: 11053 tokens, Drafted: 26574 tokens, Per-position acceptance rate: 0.738, 0.556, 0.430, 0.319, 0.248, 0.205, Avg Draft acceptance rate: 41.6%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1518.4 tokens/s, Avg generation throughput: 1617.0 tokens/s, Running: 79 reqs, Waiting: 174 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.62, Accepted throughput: 1173.11 tokens/s, Drafted throughput: 2681.79 tokens/s, Accepted: 11732 tokens, Drafted: 26820 tokens, Per-position acceptance rate: 0.753, 0.582, 0.454, 0.347, 0.273, 0.215, Avg Draft acceptance rate: 43.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1018.7 tokens/s, Avg generation throughput: 1743.1 tokens/s, Running: 65 reqs, Waiting: 190 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:43:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 1252.28 tokens/s, Drafted throughput: 2971.16 tokens/s, Accepted: 12523 tokens, Drafted: 29712 tokens, Per-position acceptance rate: 0.743, 0.564, 0.431, 0.334, 0.257, 0.200, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:04 [loggers.py:257] Engine 000: Avg prompt throughput: 988.4 tokens/s, Avg generation throughput: 1655.7 tokens/s, Running: 53 reqs, Waiting: 199 reqs, GPU KV cache usage: 92.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 1182.70 tokens/s, Drafted throughput: 2856.71 tokens/s, Accepted: 11829 tokens, Drafted: 28572 tokens, Per-position acceptance rate: 0.735, 0.555, 0.419, 0.318, 0.252, 0.205, Avg Draft acceptance rate: 41.4%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:14 [loggers.py:257] Engine 000: Avg prompt throughput: 1510.3 tokens/s, Avg generation throughput: 1571.3 tokens/s, Running: 76 reqs, Waiting: 176 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 1135.48 tokens/s, Drafted throughput: 2632.51 tokens/s, Accepted: 11356 tokens, Drafted: 26328 tokens, Per-position acceptance rate: 0.750, 0.570, 0.445, 0.339, 0.270, 0.214, Avg Draft acceptance rate: 43.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1390.6 tokens/s, Avg generation throughput: 1774.4 tokens/s, Running: 74 reqs, Waiting: 182 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 1289.27 tokens/s, Drafted throughput: 2934.14 tokens/s, Accepted: 12900 tokens, Drafted: 29358 tokens, Per-position acceptance rate: 0.764, 0.580, 0.456, 0.348, 0.273, 0.216, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:34 [loggers.py:257] Engine 000: Avg prompt throughput: 938.7 tokens/s, Avg generation throughput: 1688.1 tokens/s, Running: 63 reqs, Waiting: 193 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 1204.10 tokens/s, Drafted throughput: 2921.38 tokens/s, Accepted: 12046 tokens, Drafted: 29226 tokens, Per-position acceptance rate: 0.733, 0.555, 0.416, 0.321, 0.251, 0.197, Avg Draft acceptance rate: 41.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1004.2 tokens/s, Avg generation throughput: 1628.8 tokens/s, Running: 53 reqs, Waiting: 198 reqs, GPU KV cache usage: 93.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 1160.28 tokens/s, Drafted throughput: 2826.31 tokens/s, Accepted: 11604 tokens, Drafted: 28266 tokens, Per-position acceptance rate: 0.736, 0.547, 0.421, 0.317, 0.247, 0.195, Avg Draft acceptance rate: 41.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:55 [loggers.py:257] Engine 000: Avg prompt throughput: 1495.8 tokens/s, Avg generation throughput: 1594.2 tokens/s, Running: 79 reqs, Waiting: 173 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:44:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 1151.90 tokens/s, Drafted throughput: 2672.78 tokens/s, Accepted: 11520 tokens, Drafted: 26730 tokens, Per-position acceptance rate: 0.752, 0.570, 0.442, 0.333, 0.272, 0.217, Avg Draft acceptance rate: 43.1%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:05 [loggers.py:257] Engine 000: Avg prompt throughput: 1275.2 tokens/s, Avg generation throughput: 1739.4 tokens/s, Running: 73 reqs, Waiting: 183 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 1267.40 tokens/s, Drafted throughput: 2859.23 tokens/s, Accepted: 12681 tokens, Drafted: 28608 tokens, Per-position acceptance rate: 0.766, 0.585, 0.457, 0.353, 0.277, 0.221, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:15 [loggers.py:257] Engine 000: Avg prompt throughput: 992.1 tokens/s, Avg generation throughput: 1703.8 tokens/s, Running: 61 reqs, Waiting: 195 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 1219.66 tokens/s, Drafted throughput: 2923.11 tokens/s, Accepted: 12202 tokens, Drafted: 29244 tokens, Per-position acceptance rate: 0.727, 0.557, 0.431, 0.333, 0.256, 0.200, Avg Draft acceptance rate: 41.7%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:25 [loggers.py:257] Engine 000: Avg prompt throughput: 1109.6 tokens/s, Avg generation throughput: 1582.0 tokens/s, Running: 68 reqs, Waiting: 182 reqs, GPU KV cache usage: 95.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 1129.47 tokens/s, Drafted throughput: 2724.88 tokens/s, Accepted: 11296 tokens, Drafted: 27252 tokens, Per-position acceptance rate: 0.740, 0.562, 0.423, 0.321, 0.247, 0.194, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:35 [loggers.py:257] Engine 000: Avg prompt throughput: 1578.5 tokens/s, Avg generation throughput: 1648.2 tokens/s, Running: 82 reqs, Waiting: 126 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 1191.84 tokens/s, Drafted throughput: 2755.84 tokens/s, Accepted: 11926 tokens, Drafted: 27576 tokens, Per-position acceptance rate: 0.743, 0.568, 0.452, 0.347, 0.270, 0.215, Avg Draft acceptance rate: 43.2%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:45 [loggers.py:257] Engine 000: Avg prompt throughput: 998.6 tokens/s, Avg generation throughput: 1785.1 tokens/s, Running: 74 reqs, Waiting: 58 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 1292.68 tokens/s, Drafted throughput: 2969.54 tokens/s, Accepted: 12934 tokens, Drafted: 29712 tokens, Per-position acceptance rate: 0.745, 0.566, 0.450, 0.348, 0.280, 0.222, Avg Draft acceptance rate: 43.5%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:55 [loggers.py:257] Engine 000: Avg prompt throughput: 617.6 tokens/s, Avg generation throughput: 1746.0 tokens/s, Running: 34 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 1262.15 tokens/s, Drafted throughput: 2965.09 tokens/s, Accepted: 12622 tokens, Drafted: 29652 tokens, Per-position acceptance rate: 0.742, 0.568, 0.439, 0.338, 0.262, 0.207, Avg Draft acceptance rate: 42.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  311.03    
Total input tokens:                      373233    
Total generated tokens:                  511925    
Request throughput (req/s):              8.23      
Output token throughput (tok/s):         1645.92   
Peak output token throughput (tok/s):    707.00    
Peak concurrent requests:                278.00    
Total token throughput (tok/s):          2845.92   
---------------Time to First Token----------------
Mean TTFT (ms):                          20980.96  
Median TTFT (ms):                        22719.78  
P99 TTFT (ms):                           24513.55  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          44.02     
Median TPOT (ms):                        41.80     
P99 TPOT (ms):                           75.00     
---------------Inter-token Latency----------------
Mean ITL (ms):                           153.95    
Median ITL (ms):                         121.83    
P99 ITL (ms):                            339.75    
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.48     
Acceptance length:                       3.55      
Drafts:                                  144588    
Draft tokens:                            867528    
Accepted tokens:                         368496    
Per-position acceptance (%):
  Position 0:                            74.35     
  Position 1:                            56.45     
  Position 2:                            43.63     
  Position 3:                            33.41     
  Position 4:                            26.23     
  Position 5:                            20.79     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-draft_model-Qwen3-4B-k6-t0.0-tp1...
[0;36m(APIServer pid=247151)[0;0m INFO 01-23 19:45:58 [launcher.py:110] Shutting down FastAPI HTTP server.
