Removing any existing container named vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k1-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k1-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 842087
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:10 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:10 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15025, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 1, 'max_model_len': 5000}}
[0;36m(APIServer pid=842087)[0;0m WARNING 01-22 22:01:10 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:11 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:11 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:13 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:13 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:13 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:13 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:01:13 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fcf39326fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-08c00ef7-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 22:01:17 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:01:22 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:01:24 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=1), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:01:26 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.63:52547 backend=nccl
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:01:26 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=842369)[0;0m WARNING 01-22 22:01:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:01:27 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-22 22:01:27 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:01:27 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 22:01:32 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:01:37 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:01:42 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:01:47 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:01:52 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:01:57 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:02 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:12 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:17 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:20 [default_loader.py:291] Loading weights took 51.13 seconds
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:20 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:21 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-22 22:02:22 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:23 [default_loader.py:291] Loading weights took 2.57 seconds
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:25 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:27 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
WARNING 01-22 22:02:27 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:28 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 59.778944 seconds
WARNING 01-22 22:02:32 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:37 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:40 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:40 [backends.py:704] Dynamo bytecode transform time: 11.71 s
WARNING 01-22 22:02:42 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:47 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:52 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:02:57 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:57 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.535 s
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:57 [monitor.py:34] torch.compile takes 16.24 s in total
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:57 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:57 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:58 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.116 s
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:58 [monitor.py:34] torch.compile takes 16.82 s in total
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:59 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:59 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:02:59 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-22 22:03:02 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
WARNING 01-22 22:03:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:03:10 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.61 GiB
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:03:10 [core.py:272] init engine (profile, create kv cache, warmup model) took 42.51 seconds
[0;36m(EngineCore_DP0 pid=842369)[0;0m INFO 01-22 22:03:12 [vllm.py:618] Asynchronous scheduling is enabled.
WARNING 01-22 22:03:12 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15025)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15025 ssl:default [Connect call failed (\'127.0.0.1\', 15025)]\n''
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:12 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=842087)[0;0m WARNING 01-22 22:03:12 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:12 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:12 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:12 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [serving.py:221] Chat template warmup completed in 1739.1ms
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15025
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:14 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:25 [loggers.py:257] Engine 000: Avg prompt throughput: 28.3 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 9.20 tokens/s, Drafted throughput: 14.54 tokens/s, Accepted: 119 tokens, Drafted: 188 tokens, Per-position acceptance rate: 0.633, Avg Draft acceptance rate: 63.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:35 [loggers.py:257] Engine 000: Avg prompt throughput: 32.8 tokens/s, Avg generation throughput: 46.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 18.50 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 185 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.670, Avg Draft acceptance rate: 67.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:45 [loggers.py:257] Engine 000: Avg prompt throughput: 41.5 tokens/s, Avg generation throughput: 46.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.10 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 191 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.695, Avg Draft acceptance rate: 69.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:55 [loggers.py:257] Engine 000: Avg prompt throughput: 25.9 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:03:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 18.00 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 180 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.652, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:05 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 17.00 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 170 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.616, Avg Draft acceptance rate: 61.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:15 [loggers.py:257] Engine 000: Avg prompt throughput: 30.3 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.61, Accepted throughput: 16.80 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 168 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.606, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:25 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 46.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 18.70 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 187 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.680, Avg Draft acceptance rate: 68.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:35 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.54, Accepted throughput: 14.90 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 149 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.542, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:45 [loggers.py:257] Engine 000: Avg prompt throughput: 30.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 18.20 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 182 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.659, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:55 [loggers.py:257] Engine 000: Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:04:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 18.60 tokens/s, Drafted throughput: 27.30 tokens/s, Accepted: 186 tokens, Drafted: 273 tokens, Per-position acceptance rate: 0.681, Avg Draft acceptance rate: 68.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:05 [loggers.py:257] Engine 000: Avg prompt throughput: 21.4 tokens/s, Avg generation throughput: 47.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 19.30 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 193 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.697, Avg Draft acceptance rate: 69.7%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:15 [loggers.py:257] Engine 000: Avg prompt throughput: 24.2 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.57, Accepted throughput: 15.70 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 157 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.569, Avg Draft acceptance rate: 56.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:25 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.58, Accepted throughput: 16.10 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 161 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.583, Avg Draft acceptance rate: 58.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:35 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 45.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.70 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 177 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.644, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:45 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 46.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.10 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 191 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.692, Avg Draft acceptance rate: 69.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:55 [loggers.py:257] Engine 000: Avg prompt throughput: 40.0 tokens/s, Avg generation throughput: 44.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:05:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 17.20 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 172 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.621, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:05 [loggers.py:257] Engine 000: Avg prompt throughput: 14.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 18.30 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 183 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.663, Avg Draft acceptance rate: 66.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:15 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 46.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 18.70 tokens/s, Drafted throughput: 27.30 tokens/s, Accepted: 187 tokens, Drafted: 273 tokens, Per-position acceptance rate: 0.685, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:25 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 16.40 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 164 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.596, Avg Draft acceptance rate: 59.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:35 [loggers.py:257] Engine 000: Avg prompt throughput: 17.4 tokens/s, Avg generation throughput: 42.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.54, Accepted throughput: 15.00 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 150 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.543, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:45 [loggers.py:257] Engine 000: Avg prompt throughput: 38.5 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.50 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 175 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.636, Avg Draft acceptance rate: 63.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:55 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:06:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 20.10 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 201 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.731, Avg Draft acceptance rate: 73.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  221.09    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.23      
Output token throughput (tok/s):         45.23     
Peak output token throughput (tok/s):    29.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          77.92     
---------------Time to First Token----------------
Mean TTFT (ms):                          62.76     
Median TTFT (ms):                        53.37     
P99 TTFT (ms):                           81.86     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.83     
Median TPOT (ms):                        21.85     
P99 TPOT (ms):                           23.66     
---------------Inter-token Latency----------------
Mean ITL (ms):                           35.77     
Median ITL (ms):                         35.78     
P99 ITL (ms):                            36.14     
---------------Speculative Decoding---------------
Acceptance rate (%):                     64.14     
Acceptance length:                       1.64      
Drafts:                                  6071      
Draft tokens:                            6071      
Accepted tokens:                         3894      
Per-position acceptance (%):
  Position 0:                            64.14     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:05 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 36.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 14.40 tokens/s, Drafted throughput: 21.80 tokens/s, Accepted: 144 tokens, Drafted: 218 tokens, Per-position acceptance rate: 0.661, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2d08a52fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-48904e6a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:25 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 3.90 tokens/s, Drafted throughput: 6.05 tokens/s, Accepted: 78 tokens, Drafted: 121 tokens, Per-position acceptance rate: 0.645, Avg Draft acceptance rate: 64.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:35 [loggers.py:257] Engine 000: Avg prompt throughput: 92.6 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 34.60 tokens/s, Drafted throughput: 50.79 tokens/s, Accepted: 346 tokens, Drafted: 508 tokens, Per-position acceptance rate: 0.681, Avg Draft acceptance rate: 68.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:45 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 35.30 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 353 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.651, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:55 [loggers.py:257] Engine 000: Avg prompt throughput: 51.3 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:07:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 34.60 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 346 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.636, Avg Draft acceptance rate: 63.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:05 [loggers.py:257] Engine 000: Avg prompt throughput: 66.5 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 32.60 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 326 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.601, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:15 [loggers.py:257] Engine 000: Avg prompt throughput: 84.2 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 38.30 tokens/s, Drafted throughput: 53.60 tokens/s, Accepted: 383 tokens, Drafted: 536 tokens, Per-position acceptance rate: 0.715, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:25 [loggers.py:257] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.61, Accepted throughput: 32.90 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 329 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.607, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:35 [loggers.py:257] Engine 000: Avg prompt throughput: 64.7 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 35.10 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 351 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.645, Avg Draft acceptance rate: 64.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:45 [loggers.py:257] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 36.00 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 360 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.664, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:55 [loggers.py:257] Engine 000: Avg prompt throughput: 82.4 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:08:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 32.50 tokens/s, Drafted throughput: 53.90 tokens/s, Accepted: 325 tokens, Drafted: 539 tokens, Per-position acceptance rate: 0.603, Avg Draft acceptance rate: 60.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:05 [loggers.py:257] Engine 000: Avg prompt throughput: 54.2 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.58, Accepted throughput: 31.60 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 316 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.583, Avg Draft acceptance rate: 58.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:15 [loggers.py:257] Engine 000: Avg prompt throughput: 63.4 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 36.39 tokens/s, Drafted throughput: 54.09 tokens/s, Accepted: 364 tokens, Drafted: 541 tokens, Per-position acceptance rate: 0.673, Avg Draft acceptance rate: 67.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  113.75    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.44      
Output token throughput (tok/s):         87.92     
Peak output token throughput (tok/s):    56.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          151.45    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.70    
Median TTFT (ms):                        112.20    
P99 TTFT (ms):                           129.47    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.99     
Median TPOT (ms):                        22.00     
P99 TPOT (ms):                           23.72     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.03     
Median ITL (ms):                         35.98     
P99 ITL (ms):                            37.29     
---------------Speculative Decoding---------------
Acceptance rate (%):                     64.15     
Acceptance length:                       1.64      
Drafts:                                  6073      
Draft tokens:                            6073      
Accepted tokens:                         3896      
Per-position acceptance (%):
  Position 0:                            64.15     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 9.70 tokens/s, Drafted throughput: 15.10 tokens/s, Accepted: 97 tokens, Drafted: 151 tokens, Per-position acceptance rate: 0.642, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd2c7f46fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1669ca08-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:45 [loggers.py:257] Engine 000: Avg prompt throughput: 93.5 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.50 tokens/s, Drafted throughput: 28.35 tokens/s, Accepted: 390 tokens, Drafted: 567 tokens, Per-position acceptance rate: 0.688, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:55 [loggers.py:257] Engine 000: Avg prompt throughput: 140.1 tokens/s, Avg generation throughput: 174.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:09:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 67.79 tokens/s, Drafted throughput: 106.29 tokens/s, Accepted: 678 tokens, Drafted: 1063 tokens, Per-position acceptance rate: 0.638, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:05 [loggers.py:257] Engine 000: Avg prompt throughput: 123.9 tokens/s, Avg generation throughput: 177.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 69.80 tokens/s, Drafted throughput: 107.20 tokens/s, Accepted: 698 tokens, Drafted: 1072 tokens, Per-position acceptance rate: 0.651, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:15 [loggers.py:257] Engine 000: Avg prompt throughput: 129.7 tokens/s, Avg generation throughput: 177.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 69.39 tokens/s, Drafted throughput: 107.59 tokens/s, Accepted: 694 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.645, Avg Draft acceptance rate: 64.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:25 [loggers.py:257] Engine 000: Avg prompt throughput: 136.2 tokens/s, Avg generation throughput: 175.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 68.10 tokens/s, Drafted throughput: 106.90 tokens/s, Accepted: 681 tokens, Drafted: 1069 tokens, Per-position acceptance rate: 0.637, Avg Draft acceptance rate: 63.7%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:35 [loggers.py:257] Engine 000: Avg prompt throughput: 108.8 tokens/s, Avg generation throughput: 175.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 66.79 tokens/s, Drafted throughput: 107.59 tokens/s, Accepted: 668 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.621, Avg Draft acceptance rate: 62.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  59.61     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.84      
Output token throughput (tok/s):         167.74    
Peak output token throughput (tok/s):    112.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          288.97    
---------------Time to First Token----------------
Mean TTFT (ms):                          113.25    
Median TTFT (ms):                        113.46    
P99 TTFT (ms):                           125.19    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.18     
Median TPOT (ms):                        22.05     
P99 TPOT (ms):                           24.06     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.38     
Median ITL (ms):                         36.25     
P99 ITL (ms):                            44.94     
---------------Speculative Decoding---------------
Acceptance rate (%):                     64.31     
Acceptance length:                       1.64      
Drafts:                                  6066      
Draft tokens:                            6066      
Accepted tokens:                         3901      
Per-position acceptance (%):
  Position 0:                            64.31     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:45 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.00 tokens/s, Drafted throughput: 26.40 tokens/s, Accepted: 170 tokens, Drafted: 264 tokens, Per-position acceptance rate: 0.644, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:10:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8147c72fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9377ff67-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:05 [loggers.py:257] Engine 000: Avg prompt throughput: 136.8 tokens/s, Avg generation throughput: 131.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 26.40 tokens/s, Drafted throughput: 39.05 tokens/s, Accepted: 528 tokens, Drafted: 781 tokens, Per-position acceptance rate: 0.676, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:15 [loggers.py:257] Engine 000: Avg prompt throughput: 220.7 tokens/s, Avg generation throughput: 348.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 136.39 tokens/s, Drafted throughput: 211.18 tokens/s, Accepted: 1364 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.646, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:25 [loggers.py:257] Engine 000: Avg prompt throughput: 339.7 tokens/s, Avg generation throughput: 345.7 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 136.18 tokens/s, Drafted throughput: 208.37 tokens/s, Accepted: 1362 tokens, Drafted: 2084 tokens, Per-position acceptance rate: 0.654, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:35 [loggers.py:257] Engine 000: Avg prompt throughput: 251.3 tokens/s, Avg generation throughput: 346.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 134.48 tokens/s, Drafted throughput: 210.67 tokens/s, Accepted: 1345 tokens, Drafted: 2107 tokens, Per-position acceptance rate: 0.638, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:45 [loggers.py:257] Engine 000: Avg prompt throughput: 238.7 tokens/s, Avg generation throughput: 348.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 135.97 tokens/s, Drafted throughput: 211.16 tokens/s, Accepted: 1360 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.644, Avg Draft acceptance rate: 64.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  47.03     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.70      
Output token throughput (tok/s):         340.21    
Peak output token throughput (tok/s):    224.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          588.78    
---------------Time to First Token----------------
Mean TTFT (ms):                          117.45    
Median TTFT (ms):                        116.53    
P99 TTFT (ms):                           143.64    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.51     
Median TPOT (ms):                        22.51     
P99 TPOT (ms):                           24.12     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.96     
Median ITL (ms):                         36.66     
P99 ITL (ms):                            46.46     
---------------Speculative Decoding---------------
Acceptance rate (%):                     64.48     
Acceptance length:                       1.64      
Drafts:                                  9694      
Draft tokens:                            9694      
Accepted tokens:                         6251      
Per-position acceptance (%):
  Position 0:                            64.48     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:11:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 37.00 tokens/s, Drafted throughput: 61.90 tokens/s, Accepted: 370 tokens, Drafted: 619 tokens, Per-position acceptance rate: 0.598, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f01af95afc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-39ee5d2f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:15 [loggers.py:257] Engine 000: Avg prompt throughput: 302.1 tokens/s, Avg generation throughput: 323.8 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 63.99 tokens/s, Drafted throughput: 96.94 tokens/s, Accepted: 1280 tokens, Drafted: 1939 tokens, Per-position acceptance rate: 0.660, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:25 [loggers.py:257] Engine 000: Avg prompt throughput: 528.0 tokens/s, Avg generation throughput: 664.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 258.44 tokens/s, Drafted throughput: 404.01 tokens/s, Accepted: 2585 tokens, Drafted: 4041 tokens, Per-position acceptance rate: 0.640, Avg Draft acceptance rate: 64.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:35 [loggers.py:257] Engine 000: Avg prompt throughput: 522.8 tokens/s, Avg generation throughput: 664.7 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 257.26 tokens/s, Drafted throughput: 404.84 tokens/s, Accepted: 2573 tokens, Drafted: 4049 tokens, Per-position acceptance rate: 0.635, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:45 [loggers.py:257] Engine 000: Avg prompt throughput: 572.3 tokens/s, Avg generation throughput: 662.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 257.87 tokens/s, Drafted throughput: 402.55 tokens/s, Accepted: 2579 tokens, Drafted: 4026 tokens, Per-position acceptance rate: 0.641, Avg Draft acceptance rate: 64.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:55 [loggers.py:257] Engine 000: Avg prompt throughput: 528.8 tokens/s, Avg generation throughput: 669.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:12:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 262.76 tokens/s, Drafted throughput: 404.44 tokens/s, Accepted: 2628 tokens, Drafted: 4045 tokens, Per-position acceptance rate: 0.650, Avg Draft acceptance rate: 65.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  49.56     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.23      
Output token throughput (tok/s):         645.48    
Peak output token throughput (tok/s):    432.00    
Peak concurrent requests:                27.00     
Total token throughput (tok/s):          1137.01   
---------------Time to First Token----------------
Mean TTFT (ms):                          123.18    
Median TTFT (ms):                        119.78    
P99 TTFT (ms):                           170.06    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.44     
Median TPOT (ms):                        23.50     
P99 TPOT (ms):                           25.31     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.46     
Median ITL (ms):                         37.82     
P99 ITL (ms):                            48.56     
---------------Speculative Decoding---------------
Acceptance rate (%):                     64.39     
Acceptance length:                       1.64      
Drafts:                                  19399     
Draft tokens:                            19399     
Accepted tokens:                         12491     
Per-position acceptance (%):
  Position 0:                            64.39     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 233.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 92.40 tokens/s, Drafted throughput: 142.00 tokens/s, Accepted: 924 tokens, Drafted: 1420 tokens, Per-position acceptance rate: 0.651, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9858126fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7ef4dbea-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:25 [loggers.py:257] Engine 000: Avg prompt throughput: 487.2 tokens/s, Avg generation throughput: 426.2 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 84.90 tokens/s, Drafted throughput: 126.55 tokens/s, Accepted: 1698 tokens, Drafted: 2531 tokens, Per-position acceptance rate: 0.671, Avg Draft acceptance rate: 67.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:35 [loggers.py:257] Engine 000: Avg prompt throughput: 915.4 tokens/s, Avg generation throughput: 1257.3 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 488.60 tokens/s, Drafted throughput: 764.64 tokens/s, Accepted: 4887 tokens, Drafted: 7648 tokens, Per-position acceptance rate: 0.639, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:45 [loggers.py:257] Engine 000: Avg prompt throughput: 1051.3 tokens/s, Avg generation throughput: 1247.4 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 485.38 tokens/s, Drafted throughput: 758.21 tokens/s, Accepted: 4855 tokens, Drafted: 7584 tokens, Per-position acceptance rate: 0.640, Avg Draft acceptance rate: 64.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:55 [loggers.py:257] Engine 000: Avg prompt throughput: 917.5 tokens/s, Avg generation throughput: 1268.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:13:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 493.14 tokens/s, Drafted throughput: 770.96 tokens/s, Accepted: 4933 tokens, Drafted: 7712 tokens, Per-position acceptance rate: 0.640, Avg Draft acceptance rate: 64.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:05 [loggers.py:257] Engine 000: Avg prompt throughput: 980.3 tokens/s, Avg generation throughput: 1257.6 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 489.78 tokens/s, Drafted throughput: 764.76 tokens/s, Accepted: 4898 tokens, Drafted: 7648 tokens, Per-position acceptance rate: 0.640, Avg Draft acceptance rate: 64.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  52.63     
Total input tokens:                      48266     
Total generated tokens:                  63975     
Request throughput (req/s):              6.08      
Output token throughput (tok/s):         1215.57   
Peak output token throughput (tok/s):    832.00    
Peak concurrent requests:                55.00     
Total token throughput (tok/s):          2132.65   
---------------Time to First Token----------------
Mean TTFT (ms):                          136.59    
Median TTFT (ms):                        131.51    
P99 TTFT (ms):                           192.67    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.78     
Median TPOT (ms):                        24.74     
P99 TPOT (ms):                           26.76     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.55     
Median ITL (ms):                         39.04     
P99 ITL (ms):                            77.49     
---------------Speculative Decoding---------------
Acceptance rate (%):                     63.92     
Acceptance length:                       1.64      
Drafts:                                  38906     
Draft tokens:                            38906     
Accepted tokens:                         24868     
Per-position acceptance (%):
  Position 0:                            63.92     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:15 [loggers.py:257] Engine 000: Avg prompt throughput: 492.2 tokens/s, Avg generation throughput: 959.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 367.46 tokens/s, Drafted throughput: 590.34 tokens/s, Accepted: 3675 tokens, Drafted: 5904 tokens, Per-position acceptance rate: 0.622, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3dd2796fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-36ed3d73-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 2.60 tokens/s, Drafted throughput: 4.30 tokens/s, Accepted: 52 tokens, Drafted: 86 tokens, Per-position acceptance rate: 0.605, Avg Draft acceptance rate: 60.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:45 [loggers.py:257] Engine 000: Avg prompt throughput: 1924.9 tokens/s, Avg generation throughput: 1693.3 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 668.92 tokens/s, Drafted throughput: 1014.12 tokens/s, Accepted: 6691 tokens, Drafted: 10144 tokens, Per-position acceptance rate: 0.660, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:55 [loggers.py:257] Engine 000: Avg prompt throughput: 1366.6 tokens/s, Avg generation throughput: 2149.6 tokens/s, Running: 59 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:14:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 827.99 tokens/s, Drafted throughput: 1315.83 tokens/s, Accepted: 8281 tokens, Drafted: 13160 tokens, Per-position acceptance rate: 0.629, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:05 [loggers.py:257] Engine 000: Avg prompt throughput: 1533.6 tokens/s, Avg generation throughput: 2190.3 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 857.97 tokens/s, Drafted throughput: 1326.68 tokens/s, Accepted: 8583 tokens, Drafted: 13272 tokens, Per-position acceptance rate: 0.647, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:15 [loggers.py:257] Engine 000: Avg prompt throughput: 1841.5 tokens/s, Avg generation throughput: 2124.4 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 830.58 tokens/s, Drafted throughput: 1285.90 tokens/s, Accepted: 8309 tokens, Drafted: 12864 tokens, Per-position acceptance rate: 0.646, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:25 [loggers.py:257] Engine 000: Avg prompt throughput: 1536.8 tokens/s, Avg generation throughput: 2159.4 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 829.06 tokens/s, Drafted throughput: 1323.93 tokens/s, Accepted: 8291 tokens, Drafted: 13240 tokens, Per-position acceptance rate: 0.626, Avg Draft acceptance rate: 62.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:35 [loggers.py:257] Engine 000: Avg prompt throughput: 1271.3 tokens/s, Avg generation throughput: 2225.6 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 865.02 tokens/s, Drafted throughput: 1355.35 tokens/s, Accepted: 8653 tokens, Drafted: 13558 tokens, Per-position acceptance rate: 0.638, Avg Draft acceptance rate: 63.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  61.03     
Total input tokens:                      94775     
Total generated tokens:                  127930    
Request throughput (req/s):              10.49     
Output token throughput (tok/s):         2096.18   
Peak output token throughput (tok/s):    1536.00   
Peak concurrent requests:                108.00    
Total token throughput (tok/s):          3649.10   
---------------Time to First Token----------------
Mean TTFT (ms):                          218.64    
Median TTFT (ms):                        201.58    
P99 TTFT (ms):                           550.58    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          28.67     
Median TPOT (ms):                        28.70     
P99 TPOT (ms):                           31.82     
---------------Inter-token Latency----------------
Mean ITL (ms):                           46.91     
Median ITL (ms):                         42.64     
P99 ITL (ms):                            103.70    
---------------Speculative Decoding---------------
Acceptance rate (%):                     63.94     
Acceptance length:                       1.64      
Drafts:                                  77796     
Draft tokens:                            77796     
Accepted tokens:                         49745     
Per-position acceptance (%):
  Position 0:                            63.94     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 253.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 96.29 tokens/s, Drafted throughput: 159.29 tokens/s, Accepted: 963 tokens, Drafted: 1593 tokens, Per-position acceptance rate: 0.605, Avg Draft acceptance rate: 60.5%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:15:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f788e0c2fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e718d719-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:05 [loggers.py:257] Engine 000: Avg prompt throughput: 1943.8 tokens/s, Avg generation throughput: 1285.5 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 70.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 256.53 tokens/s, Drafted throughput: 379.73 tokens/s, Accepted: 5131 tokens, Drafted: 7595 tokens, Per-position acceptance rate: 0.676, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:15 [loggers.py:257] Engine 000: Avg prompt throughput: 1922.0 tokens/s, Avg generation throughput: 3399.4 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 87.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1318.39 tokens/s, Drafted throughput: 2073.58 tokens/s, Accepted: 13184 tokens, Drafted: 20736 tokens, Per-position acceptance rate: 0.636, Avg Draft acceptance rate: 63.6%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:25 [loggers.py:257] Engine 000: Avg prompt throughput: 2700.3 tokens/s, Avg generation throughput: 3021.1 tokens/s, Running: 113 reqs, Waiting: 0 reqs, GPU KV cache usage: 56.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1175.59 tokens/s, Drafted throughput: 1836.03 tokens/s, Accepted: 11757 tokens, Drafted: 18362 tokens, Per-position acceptance rate: 0.640, Avg Draft acceptance rate: 64.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:35 [loggers.py:257] Engine 000: Avg prompt throughput: 2929.0 tokens/s, Avg generation throughput: 3104.3 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 1216.10 tokens/s, Drafted throughput: 1875.64 tokens/s, Accepted: 12162 tokens, Drafted: 18758 tokens, Per-position acceptance rate: 0.648, Avg Draft acceptance rate: 64.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:45 [loggers.py:257] Engine 000: Avg prompt throughput: 1802.9 tokens/s, Avg generation throughput: 3495.5 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 72.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 1351.80 tokens/s, Drafted throughput: 2136.02 tokens/s, Accepted: 13528 tokens, Drafted: 21376 tokens, Per-position acceptance rate: 0.633, Avg Draft acceptance rate: 63.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:55 [loggers.py:257] Engine 000: Avg prompt throughput: 2002.4 tokens/s, Avg generation throughput: 3347.7 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 85.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:16:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 1281.84 tokens/s, Drafted throughput: 2058.04 tokens/s, Accepted: 12820 tokens, Drafted: 20583 tokens, Per-position acceptance rate: 0.623, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:05 [loggers.py:257] Engine 000: Avg prompt throughput: 3077.0 tokens/s, Avg generation throughput: 2931.1 tokens/s, Running: 122 reqs, Waiting: 0 reqs, GPU KV cache usage: 56.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1137.40 tokens/s, Drafted throughput: 1780.95 tokens/s, Accepted: 11375 tokens, Drafted: 17811 tokens, Per-position acceptance rate: 0.639, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:15 [loggers.py:257] Engine 000: Avg prompt throughput: 2546.1 tokens/s, Avg generation throughput: 3244.2 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1263.02 tokens/s, Drafted throughput: 1970.73 tokens/s, Accepted: 12637 tokens, Drafted: 19718 tokens, Per-position acceptance rate: 0.641, Avg Draft acceptance rate: 64.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  80.48     
Total input tokens:                      189093    
Total generated tokens:                  255929    
Request throughput (req/s):              15.90     
Output token throughput (tok/s):         3179.87   
Peak output token throughput (tok/s):    2688.00   
Peak concurrent requests:                190.00    
Total token throughput (tok/s):          5529.31   
---------------Time to First Token----------------
Mean TTFT (ms):                          417.49    
Median TTFT (ms):                        352.23    
P99 TTFT (ms):                           1650.29   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          37.46     
Median TPOT (ms):                        37.69     
P99 TPOT (ms):                           44.39     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.21     
Median ITL (ms):                         49.26     
P99 ITL (ms):                            195.87    
---------------Speculative Decoding---------------
Acceptance rate (%):                     63.74     
Acceptance length:                       1.64      
Drafts:                                  155840    
Draft tokens:                            155840    
Accepted tokens:                         99330     
Per-position acceptance (%):
  Position 0:                            63.74     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1777.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 681.32 tokens/s, Drafted throughput: 1102.07 tokens/s, Accepted: 6814 tokens, Drafted: 11022 tokens, Per-position acceptance rate: 0.618, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb01d342fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15025, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b7d6d912-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:45 [loggers.py:257] Engine 000: Avg prompt throughput: 302.1 tokens/s, Avg generation throughput: 23.8 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 4.35 tokens/s, Drafted throughput: 6.50 tokens/s, Accepted: 87 tokens, Drafted: 130 tokens, Per-position acceptance rate: 0.669, Avg Draft acceptance rate: 66.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:55 [loggers.py:257] Engine 000: Avg prompt throughput: 3563.7 tokens/s, Avg generation throughput: 3279.6 tokens/s, Running: 145 reqs, Waiting: 111 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:17:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 1292.96 tokens/s, Drafted throughput: 1963.04 tokens/s, Accepted: 12930 tokens, Drafted: 19631 tokens, Per-position acceptance rate: 0.659, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:05 [loggers.py:257] Engine 000: Avg prompt throughput: 1528.1 tokens/s, Avg generation throughput: 2347.3 tokens/s, Running: 167 reqs, Waiting: 88 reqs, GPU KV cache usage: 99.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 901.33 tokens/s, Drafted throughput: 1426.59 tokens/s, Accepted: 9014 tokens, Drafted: 14267 tokens, Per-position acceptance rate: 0.632, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:15 [loggers.py:257] Engine 000: Avg prompt throughput: 3210.9 tokens/s, Avg generation throughput: 3061.6 tokens/s, Running: 200 reqs, Waiting: 56 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 1198.48 tokens/s, Drafted throughput: 1842.28 tokens/s, Accepted: 11985 tokens, Drafted: 18423 tokens, Per-position acceptance rate: 0.651, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:25 [loggers.py:257] Engine 000: Avg prompt throughput: 2613.8 tokens/s, Avg generation throughput: 2679.9 tokens/s, Running: 242 reqs, Waiting: 12 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 1015.40 tokens/s, Drafted throughput: 1644.30 tokens/s, Accepted: 10154 tokens, Drafted: 16443 tokens, Per-position acceptance rate: 0.618, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:35 [loggers.py:257] Engine 000: Avg prompt throughput: 1671.8 tokens/s, Avg generation throughput: 3259.1 tokens/s, Running: 207 reqs, Waiting: 33 reqs, GPU KV cache usage: 91.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 1252.53 tokens/s, Drafted throughput: 1993.09 tokens/s, Accepted: 12526 tokens, Drafted: 19932 tokens, Per-position acceptance rate: 0.628, Avg Draft acceptance rate: 62.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:45 [loggers.py:257] Engine 000: Avg prompt throughput: 1362.4 tokens/s, Avg generation throughput: 3547.5 tokens/s, Running: 157 reqs, Waiting: 81 reqs, GPU KV cache usage: 95.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 1366.80 tokens/s, Drafted throughput: 2170.14 tokens/s, Accepted: 13669 tokens, Drafted: 21703 tokens, Per-position acceptance rate: 0.630, Avg Draft acceptance rate: 63.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:55 [loggers.py:257] Engine 000: Avg prompt throughput: 2323.1 tokens/s, Avg generation throughput: 3091.5 tokens/s, Running: 152 reqs, Waiting: 101 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:18:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 1192.40 tokens/s, Drafted throughput: 1881.80 tokens/s, Accepted: 11924 tokens, Drafted: 18818 tokens, Per-position acceptance rate: 0.634, Avg Draft acceptance rate: 63.4%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:05 [loggers.py:257] Engine 000: Avg prompt throughput: 2909.2 tokens/s, Avg generation throughput: 3208.9 tokens/s, Running: 155 reqs, Waiting: 100 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 1255.60 tokens/s, Drafted throughput: 1935.25 tokens/s, Accepted: 12566 tokens, Drafted: 19368 tokens, Per-position acceptance rate: 0.649, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:15 [loggers.py:257] Engine 000: Avg prompt throughput: 2700.0 tokens/s, Avg generation throughput: 2782.3 tokens/s, Running: 179 reqs, Waiting: 76 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1075.92 tokens/s, Drafted throughput: 1685.61 tokens/s, Accepted: 10763 tokens, Drafted: 16862 tokens, Per-position acceptance rate: 0.638, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:25 [loggers.py:257] Engine 000: Avg prompt throughput: 2533.5 tokens/s, Avg generation throughput: 2691.4 tokens/s, Running: 213 reqs, Waiting: 42 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1041.12 tokens/s, Drafted throughput: 1631.32 tokens/s, Accepted: 10413 tokens, Drafted: 16316 tokens, Per-position acceptance rate: 0.638, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:35 [loggers.py:257] Engine 000: Avg prompt throughput: 2035.7 tokens/s, Avg generation throughput: 2759.1 tokens/s, Running: 231 reqs, Waiting: 7 reqs, GPU KV cache usage: 92.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 1052.82 tokens/s, Drafted throughput: 1690.07 tokens/s, Accepted: 10529 tokens, Drafted: 16902 tokens, Per-position acceptance rate: 0.623, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:45 [loggers.py:257] Engine 000: Avg prompt throughput: 1030.2 tokens/s, Avg generation throughput: 3488.7 tokens/s, Running: 177 reqs, Waiting: 60 reqs, GPU KV cache usage: 93.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1354.06 tokens/s, Drafted throughput: 2123.37 tokens/s, Accepted: 13542 tokens, Drafted: 21236 tokens, Per-position acceptance rate: 0.638, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:55 [loggers.py:257] Engine 000: Avg prompt throughput: 2263.7 tokens/s, Avg generation throughput: 3454.1 tokens/s, Running: 144 reqs, Waiting: 110 reqs, GPU KV cache usage: 98.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:19:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 1330.98 tokens/s, Drafted throughput: 2107.32 tokens/s, Accepted: 13316 tokens, Drafted: 21083 tokens, Per-position acceptance rate: 0.632, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:05 [loggers.py:257] Engine 000: Avg prompt throughput: 2661.3 tokens/s, Avg generation throughput: 3136.4 tokens/s, Running: 149 reqs, Waiting: 105 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1221.04 tokens/s, Drafted throughput: 1896.66 tokens/s, Accepted: 12212 tokens, Drafted: 18969 tokens, Per-position acceptance rate: 0.644, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:15 [loggers.py:257] Engine 000: Avg prompt throughput: 2640.7 tokens/s, Avg generation throughput: 2904.7 tokens/s, Running: 176 reqs, Waiting: 80 reqs, GPU KV cache usage: 99.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 1134.11 tokens/s, Drafted throughput: 1750.28 tokens/s, Accepted: 11349 tokens, Drafted: 17515 tokens, Per-position acceptance rate: 0.648, Avg Draft acceptance rate: 64.8%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:25 [loggers.py:257] Engine 000: Avg prompt throughput: 1983.0 tokens/s, Avg generation throughput: 3067.2 tokens/s, Running: 201 reqs, Waiting: 8 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 1190.14 tokens/s, Drafted throughput: 1860.81 tokens/s, Accepted: 11902 tokens, Drafted: 18609 tokens, Per-position acceptance rate: 0.640, Avg Draft acceptance rate: 64.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  169.61    
Total input tokens:                      373233    
Total generated tokens:                  511955    
Request throughput (req/s):              15.09     
Output token throughput (tok/s):         3018.49   
Peak output token throughput (tok/s):    3661.00   
Peak concurrent requests:                298.00    
Total token throughput (tok/s):          5219.07   
---------------Time to First Token----------------
Mean TTFT (ms):                          3053.39   
Median TTFT (ms):                        2467.27   
P99 TTFT (ms):                           8850.70   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          67.03     
Median TPOT (ms):                        61.74     
P99 TPOT (ms):                           99.95     
---------------Inter-token Latency----------------
Mean ITL (ms):                           109.31    
Median ITL (ms):                         66.31     
P99 ITL (ms):                            506.46    
---------------Speculative Decoding---------------
Acceptance rate (%):                     63.61     
Acceptance length:                       1.64      
Drafts:                                  311181    
Draft tokens:                            311181    
Accepted tokens:                         197943    
Per-position acceptance (%):
  Position 0:                            63.61     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k1-t0.0-tp1...
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:35 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2197.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=842087)[0;0m INFO 01-22 22:20:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.61, Accepted throughput: 829.52 tokens/s, Drafted throughput: 1369.98 tokens/s, Accepted: 9140 tokens, Drafted: 15095 tokens, Per-position acceptance rate: 0.605, Avg Draft acceptance rate: 60.5%
