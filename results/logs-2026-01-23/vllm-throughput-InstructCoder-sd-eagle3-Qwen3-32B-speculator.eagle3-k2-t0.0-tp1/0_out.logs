Removing any existing container named vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k2-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k2-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 848678
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:42 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:42 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15026, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 2, 'max_model_len': 5000}}
[0;36m(APIServer pid=848678)[0;0m WARNING 01-22 22:21:42 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:43 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:43 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:47 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:47 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:47 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:47 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:21:47 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f77f4b52fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0a9dfbe7-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-22 22:21:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:21:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:21:58 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=2), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-22 22:21:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:21:59 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.63:37425 backend=nccl
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:00 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=848786)[0;0m WARNING 01-22 22:22:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:00 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:01 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-22 22:22:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:22:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:54 [default_loader.py:291] Loading weights took 51.14 seconds
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:54 [gpu_model_runner.py:3851] Loading drafter model...
WARNING 01-22 22:22:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:54 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:57 [default_loader.py:291] Loading weights took 2.57 seconds
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:22:59 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
WARNING 01-22 22:22:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:00 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:01 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 59.725339 seconds
WARNING 01-22 22:23:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:23:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:14 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:14 [backends.py:704] Dynamo bytecode transform time: 12.01 s
WARNING 01-22 22:23:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:23:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:23:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:29 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 2.419 s
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:29 [monitor.py:34] torch.compile takes 14.43 s in total
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:29 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:29 [backends.py:704] Dynamo bytecode transform time: 0.47 s
WARNING 01-22 22:23:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:30 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.062 s
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:30 [monitor.py:34] torch.compile takes 14.96 s in total
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:31 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:31 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:31 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-22 22:23:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
WARNING 01-22 22:23:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:42 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.65 GiB
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:42 [core.py:272] init engine (profile, create kv cache, warmup model) took 40.46 seconds
[0;36m(EngineCore_DP0 pid=848786)[0;0m INFO 01-22 22:23:43 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:44 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=848678)[0;0m WARNING 01-22 22:23:44 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:44 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:44 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:44 [serving.py:185] Warming up chat template processing...
WARNING 01-22 22:23:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15026)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15026 ssl:default [Connect call failed (\'127.0.0.1\', 15026)]\n''
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [serving.py:221] Chat template warmup completed in 1893.8ms
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15026
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:46 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:57 [loggers.py:257] Engine 000: Avg prompt throughput: 27.9 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:23:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 13.25 tokens/s, Drafted throughput: 26.04 tokens/s, Accepted: 174 tokens, Drafted: 342 tokens, Per-position acceptance rate: 0.649, 0.368, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:07 [loggers.py:257] Engine 000: Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 27.00 tokens/s, Drafted throughput: 53.80 tokens/s, Accepted: 270 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.647, 0.357, Avg Draft acceptance rate: 50.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:17 [loggers.py:257] Engine 000: Avg prompt throughput: 45.5 tokens/s, Avg generation throughput: 53.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 26.60 tokens/s, Drafted throughput: 53.39 tokens/s, Accepted: 266 tokens, Drafted: 534 tokens, Per-position acceptance rate: 0.629, 0.367, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:27 [loggers.py:257] Engine 000: Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 26.00 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 260 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.616, 0.343, Avg Draft acceptance rate: 48.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:37 [loggers.py:257] Engine 000: Avg prompt throughput: 38.7 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 25.90 tokens/s, Drafted throughput: 53.60 tokens/s, Accepted: 259 tokens, Drafted: 536 tokens, Per-position acceptance rate: 0.631, 0.336, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:47 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.10, Accepted throughput: 29.60 tokens/s, Drafted throughput: 53.60 tokens/s, Accepted: 296 tokens, Drafted: 536 tokens, Per-position acceptance rate: 0.675, 0.429, Avg Draft acceptance rate: 55.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:57 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:24:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.94, Accepted throughput: 25.60 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 256 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.642, 0.303, Avg Draft acceptance rate: 47.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:07 [loggers.py:257] Engine 000: Avg prompt throughput: 43.1 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 29.40 tokens/s, Drafted throughput: 53.80 tokens/s, Accepted: 294 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.688, 0.405, Avg Draft acceptance rate: 54.6%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:17 [loggers.py:257] Engine 000: Avg prompt throughput: 44.8 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 31.20 tokens/s, Drafted throughput: 53.99 tokens/s, Accepted: 312 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.715, 0.441, Avg Draft acceptance rate: 57.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:27 [loggers.py:257] Engine 000: Avg prompt throughput: 34.8 tokens/s, Avg generation throughput: 52.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.95, Accepted throughput: 25.60 tokens/s, Drafted throughput: 53.80 tokens/s, Accepted: 256 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.621, 0.331, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:37 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 26.00 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 260 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.629, 0.327, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:47 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.07, Accepted throughput: 28.90 tokens/s, Drafted throughput: 53.80 tokens/s, Accepted: 289 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.669, 0.405, Avg Draft acceptance rate: 53.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:57 [loggers.py:257] Engine 000: Avg prompt throughput: 59.4 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:25:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 27.60 tokens/s, Drafted throughput: 53.80 tokens/s, Accepted: 276 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.643, 0.383, Avg Draft acceptance rate: 51.3%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:07 [loggers.py:257] Engine 000: Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.93, Accepted throughput: 25.10 tokens/s, Drafted throughput: 53.99 tokens/s, Accepted: 251 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.611, 0.319, Avg Draft acceptance rate: 46.5%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:17 [loggers.py:257] Engine 000: Avg prompt throughput: 36.1 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 27.80 tokens/s, Drafted throughput: 53.99 tokens/s, Accepted: 278 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.648, 0.381, Avg Draft acceptance rate: 51.5%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:27 [loggers.py:257] Engine 000: Avg prompt throughput: 54.9 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 27.80 tokens/s, Drafted throughput: 53.79 tokens/s, Accepted: 278 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.643, 0.390, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:37 [loggers.py:257] Engine 000: Avg prompt throughput: 36.2 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 26.20 tokens/s, Drafted throughput: 53.60 tokens/s, Accepted: 262 tokens, Drafted: 536 tokens, Per-position acceptance rate: 0.649, 0.328, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:47 [loggers.py:257] Engine 000: Avg prompt throughput: 29.6 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 29.40 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 294 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.685, 0.404, Avg Draft acceptance rate: 54.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:57 [loggers.py:257] Engine 000: Avg prompt throughput: 33.8 tokens/s, Avg generation throughput: 57.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:26:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.10, Accepted throughput: 29.80 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 298 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.681, 0.422, Avg Draft acceptance rate: 55.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  184.11    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.27      
Output token throughput (tok/s):         54.31     
Peak output token throughput (tok/s):    28.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          93.57     
---------------Time to First Token----------------
Mean TTFT (ms):                          70.52     
Median TTFT (ms):                        77.49     
P99 TTFT (ms):                           84.41     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.07     
Median TPOT (ms):                        18.09     
P99 TPOT (ms):                           20.41     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.41     
Median ITL (ms):                         36.41     
P99 ITL (ms):                            36.78     
---------------Speculative Decoding---------------
Acceptance rate (%):                     51.01     
Acceptance length:                       2.02      
Drafts:                                  4939      
Draft tokens:                            9878      
Accepted tokens:                         5039      
Per-position acceptance (%):
  Position 0:                            65.03     
  Position 1:                            36.99     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1.20 tokens/s, Drafted throughput: 3.40 tokens/s, Accepted: 12 tokens, Drafted: 34 tokens, Per-position acceptance rate: 0.471, 0.235, Avg Draft acceptance rate: 35.3%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7faacbfeafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f38e06fe-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:17 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 11.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 5.80 tokens/s, Drafted throughput: 11.20 tokens/s, Accepted: 58 tokens, Drafted: 112 tokens, Per-position acceptance rate: 0.643, 0.393, Avg Draft acceptance rate: 51.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:27 [loggers.py:257] Engine 000: Avg prompt throughput: 75.2 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 45.80 tokens/s, Drafted throughput: 90.39 tokens/s, Accepted: 458 tokens, Drafted: 904 tokens, Per-position acceptance rate: 0.646, 0.367, Avg Draft acceptance rate: 50.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:37 [loggers.py:257] Engine 000: Avg prompt throughput: 62.0 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 51.70 tokens/s, Drafted throughput: 105.99 tokens/s, Accepted: 517 tokens, Drafted: 1060 tokens, Per-position acceptance rate: 0.625, 0.351, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:47 [loggers.py:257] Engine 000: Avg prompt throughput: 78.1 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.06, Accepted throughput: 55.80 tokens/s, Drafted throughput: 105.20 tokens/s, Accepted: 558 tokens, Drafted: 1052 tokens, Per-position acceptance rate: 0.662, 0.399, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:57 [loggers.py:257] Engine 000: Avg prompt throughput: 102.5 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:27:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 53.49 tokens/s, Drafted throughput: 105.18 tokens/s, Accepted: 535 tokens, Drafted: 1052 tokens, Per-position acceptance rate: 0.660, 0.357, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:07 [loggers.py:257] Engine 000: Avg prompt throughput: 45.6 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 51.70 tokens/s, Drafted throughput: 106.79 tokens/s, Accepted: 517 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.625, 0.343, Avg Draft acceptance rate: 48.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:17 [loggers.py:257] Engine 000: Avg prompt throughput: 105.5 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 57.70 tokens/s, Drafted throughput: 105.60 tokens/s, Accepted: 577 tokens, Drafted: 1056 tokens, Per-position acceptance rate: 0.686, 0.407, Avg Draft acceptance rate: 54.6%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:27 [loggers.py:257] Engine 000: Avg prompt throughput: 63.2 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 51.09 tokens/s, Drafted throughput: 105.78 tokens/s, Accepted: 511 tokens, Drafted: 1058 tokens, Per-position acceptance rate: 0.631, 0.335, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:37 [loggers.py:257] Engine 000: Avg prompt throughput: 91.0 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 53.29 tokens/s, Drafted throughput: 105.79 tokens/s, Accepted: 533 tokens, Drafted: 1058 tokens, Per-position acceptance rate: 0.626, 0.382, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:47 [loggers.py:257] Engine 000: Avg prompt throughput: 81.1 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 53.10 tokens/s, Drafted throughput: 105.59 tokens/s, Accepted: 531 tokens, Drafted: 1056 tokens, Per-position acceptance rate: 0.665, 0.341, Avg Draft acceptance rate: 50.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  95.27     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.52      
Output token throughput (tok/s):         104.97    
Peak output token throughput (tok/s):    57.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          180.83    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.36    
Median TTFT (ms):                        112.92    
P99 TTFT (ms):                           122.62    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.29     
Median TPOT (ms):                        18.24     
P99 TPOT (ms):                           20.56     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.69     
Median ITL (ms):                         36.65     
P99 ITL (ms):                            39.45     
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.59     
Acceptance length:                       2.01      
Drafts:                                  4959      
Draft tokens:                            9918      
Accepted tokens:                         5018      
Per-position acceptance (%):
  Position 0:                            64.67     
  Position 1:                            36.52     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:57 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:28:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 32.50 tokens/s, Drafted throughput: 63.80 tokens/s, Accepted: 325 tokens, Drafted: 638 tokens, Per-position acceptance rate: 0.646, 0.373, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f06264d2fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6bfc3df3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:17 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.07, Accepted throughput: 15.25 tokens/s, Drafted throughput: 28.50 tokens/s, Accepted: 305 tokens, Drafted: 570 tokens, Per-position acceptance rate: 0.660, 0.411, Avg Draft acceptance rate: 53.5%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:27 [loggers.py:257] Engine 000: Avg prompt throughput: 147.1 tokens/s, Avg generation throughput: 208.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 103.99 tokens/s, Drafted throughput: 207.77 tokens/s, Accepted: 1040 tokens, Drafted: 2078 tokens, Per-position acceptance rate: 0.644, 0.357, Avg Draft acceptance rate: 50.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:37 [loggers.py:257] Engine 000: Avg prompt throughput: 132.5 tokens/s, Avg generation throughput: 213.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 108.18 tokens/s, Drafted throughput: 209.76 tokens/s, Accepted: 1082 tokens, Drafted: 2098 tokens, Per-position acceptance rate: 0.654, 0.378, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:47 [loggers.py:257] Engine 000: Avg prompt throughput: 183.5 tokens/s, Avg generation throughput: 209.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 105.99 tokens/s, Drafted throughput: 207.17 tokens/s, Accepted: 1060 tokens, Drafted: 2072 tokens, Per-position acceptance rate: 0.656, 0.367, Avg Draft acceptance rate: 51.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:57 [loggers.py:257] Engine 000: Avg prompt throughput: 136.6 tokens/s, Avg generation throughput: 205.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:29:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 100.29 tokens/s, Drafted throughput: 208.98 tokens/s, Accepted: 1003 tokens, Drafted: 2090 tokens, Per-position acceptance rate: 0.623, 0.337, Avg Draft acceptance rate: 48.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  50.15     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.00      
Output token throughput (tok/s):         199.40    
Peak output token throughput (tok/s):    108.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          343.51    
---------------Time to First Token----------------
Mean TTFT (ms):                          114.31    
Median TTFT (ms):                        114.02    
P99 TTFT (ms):                           138.94    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.57     
Median TPOT (ms):                        18.62     
P99 TPOT (ms):                           20.85     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.19     
Median ITL (ms):                         37.11     
P99 ITL (ms):                            46.16     
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.45     
Acceptance length:                       2.01      
Drafts:                                  4967      
Draft tokens:                            9934      
Accepted tokens:                         5012      
Per-position acceptance (%):
  Position 0:                            64.47     
  Position 1:                            36.44     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:07 [loggers.py:257] Engine 000: Avg prompt throughput: 63.4 tokens/s, Avg generation throughput: 123.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 62.39 tokens/s, Drafted throughput: 122.19 tokens/s, Accepted: 624 tokens, Drafted: 1222 tokens, Per-position acceptance rate: 0.643, 0.378, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fbd5668efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1693d8ab-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:27 [loggers.py:257] Engine 000: Avg prompt throughput: 136.8 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 13.40 tokens/s, Drafted throughput: 23.00 tokens/s, Accepted: 268 tokens, Drafted: 460 tokens, Per-position acceptance rate: 0.704, 0.461, Avg Draft acceptance rate: 58.3%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:37 [loggers.py:257] Engine 000: Avg prompt throughput: 220.7 tokens/s, Avg generation throughput: 414.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 206.57 tokens/s, Drafted throughput: 414.35 tokens/s, Accepted: 2066 tokens, Drafted: 4144 tokens, Per-position acceptance rate: 0.636, 0.361, Avg Draft acceptance rate: 49.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:47 [loggers.py:257] Engine 000: Avg prompt throughput: 365.0 tokens/s, Avg generation throughput: 413.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 208.49 tokens/s, Drafted throughput: 407.98 tokens/s, Accepted: 2085 tokens, Drafted: 4080 tokens, Per-position acceptance rate: 0.652, 0.370, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:57 [loggers.py:257] Engine 000: Avg prompt throughput: 257.4 tokens/s, Avg generation throughput: 409.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:30:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 201.98 tokens/s, Drafted throughput: 413.15 tokens/s, Accepted: 2020 tokens, Drafted: 4132 tokens, Per-position acceptance rate: 0.620, 0.358, Avg Draft acceptance rate: 48.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  40.55     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              1.97      
Output token throughput (tok/s):         394.55    
Peak output token throughput (tok/s):    221.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          682.81    
---------------Time to First Token----------------
Mean TTFT (ms):                          117.33    
Median TTFT (ms):                        116.61    
P99 TTFT (ms):                           136.35    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.89     
Median TPOT (ms):                        18.96     
P99 TPOT (ms):                           21.03     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.77     
Median ITL (ms):                         37.47     
P99 ITL (ms):                            48.00     
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.26     
Acceptance length:                       2.01      
Drafts:                                  7962      
Draft tokens:                            15924     
Accepted tokens:                         8003      
Per-position acceptance (%):
  Position 0:                            63.90     
  Position 1:                            36.61     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:07 [loggers.py:257] Engine 000: Avg prompt throughput: 207.4 tokens/s, Avg generation throughput: 332.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 166.59 tokens/s, Drafted throughput: 330.38 tokens/s, Accepted: 1666 tokens, Drafted: 3304 tokens, Per-position acceptance rate: 0.644, 0.364, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd6e9d2efc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7d99a23d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:27 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 19.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 4.85 tokens/s, Drafted throughput: 9.40 tokens/s, Accepted: 97 tokens, Drafted: 188 tokens, Per-position acceptance rate: 0.660, 0.372, Avg Draft acceptance rate: 51.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:37 [loggers.py:257] Engine 000: Avg prompt throughput: 704.2 tokens/s, Avg generation throughput: 732.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 369.29 tokens/s, Drafted throughput: 720.97 tokens/s, Accepted: 3693 tokens, Drafted: 7210 tokens, Per-position acceptance rate: 0.650, 0.374, Avg Draft acceptance rate: 51.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:47 [loggers.py:257] Engine 000: Avg prompt throughput: 464.7 tokens/s, Avg generation throughput: 784.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 390.53 tokens/s, Drafted throughput: 786.66 tokens/s, Accepted: 3906 tokens, Drafted: 7868 tokens, Per-position acceptance rate: 0.625, 0.368, Avg Draft acceptance rate: 49.6%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:57 [loggers.py:257] Engine 000: Avg prompt throughput: 681.3 tokens/s, Avg generation throughput: 768.2 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:31:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 379.49 tokens/s, Drafted throughput: 776.78 tokens/s, Accepted: 3795 tokens, Drafted: 7768 tokens, Per-position acceptance rate: 0.627, 0.350, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:07 [loggers.py:257] Engine 000: Avg prompt throughput: 585.7 tokens/s, Avg generation throughput: 792.9 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 398.05 tokens/s, Drafted throughput: 787.30 tokens/s, Accepted: 3981 tokens, Drafted: 7874 tokens, Per-position acceptance rate: 0.633, 0.378, Avg Draft acceptance rate: 50.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  42.68     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.75      
Output token throughput (tok/s):         749.59    
Peak output token throughput (tok/s):    416.00    
Peak concurrent requests:                27.00     
Total token throughput (tok/s):          1320.39   
---------------Time to First Token----------------
Mean TTFT (ms):                          123.67    
Median TTFT (ms):                        122.37    
P99 TTFT (ms):                           154.10    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.87     
Median TPOT (ms):                        19.92     
P99 TPOT (ms):                           22.27     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.58     
Median ITL (ms):                         38.94     
P99 ITL (ms):                            50.13     
---------------Speculative Decoding---------------
Acceptance rate (%):                     49.94     
Acceptance length:                       2.00      
Drafts:                                  15982     
Draft tokens:                            31964     
Accepted tokens:                         15962     
Per-position acceptance (%):
  Position 0:                            63.22     
  Position 1:                            36.65     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.95, Accepted throughput: 59.20 tokens/s, Drafted throughput: 125.19 tokens/s, Accepted: 592 tokens, Drafted: 1252 tokens, Per-position acceptance rate: 0.605, 0.340, Avg Draft acceptance rate: 47.3%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe5ecadafc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-55e95894-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:37 [loggers.py:257] Engine 000: Avg prompt throughput: 948.3 tokens/s, Avg generation throughput: 801.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 201.86 tokens/s, Drafted throughput: 392.63 tokens/s, Accepted: 4038 tokens, Drafted: 7854 tokens, Per-position acceptance rate: 0.648, 0.380, Avg Draft acceptance rate: 51.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:47 [loggers.py:257] Engine 000: Avg prompt throughput: 995.1 tokens/s, Avg generation throughput: 1453.5 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 724.83 tokens/s, Drafted throughput: 1452.66 tokens/s, Accepted: 7249 tokens, Drafted: 14528 tokens, Per-position acceptance rate: 0.632, 0.366, Avg Draft acceptance rate: 49.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:57 [loggers.py:257] Engine 000: Avg prompt throughput: 975.2 tokens/s, Avg generation throughput: 1466.0 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:32:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 731.16 tokens/s, Drafted throughput: 1464.73 tokens/s, Accepted: 7313 tokens, Drafted: 14650 tokens, Per-position acceptance rate: 0.630, 0.369, Avg Draft acceptance rate: 49.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1179.9 tokens/s, Avg generation throughput: 1418.9 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 700.24 tokens/s, Drafted throughput: 1431.67 tokens/s, Accepted: 7003 tokens, Drafted: 14318 tokens, Per-position acceptance rate: 0.628, 0.350, Avg Draft acceptance rate: 48.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  45.93     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.97      
Output token throughput (tok/s):         1393.34   
Peak output token throughput (tok/s):    800.00    
Peak concurrent requests:                56.00     
Total token throughput (tok/s):          2444.29   
---------------Time to First Token----------------
Mean TTFT (ms):                          142.69    
Median TTFT (ms):                        138.10    
P99 TTFT (ms):                           188.39    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.49     
Median TPOT (ms):                        21.46     
P99 TPOT (ms):                           24.16     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.71     
Median ITL (ms):                         41.07     
P99 ITL (ms):                            78.90     
---------------Speculative Decoding---------------
Acceptance rate (%):                     49.69     
Acceptance length:                       1.99      
Drafts:                                  32035     
Draft tokens:                            64070     
Accepted tokens:                         31838     
Per-position acceptance (%):
  Position 0:                            63.12     
  Position 1:                            36.27     
==================================================
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:17 [loggers.py:257] Engine 000: Avg prompt throughput: 745.6 tokens/s, Avg generation throughput: 1278.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 633.64 tokens/s, Drafted throughput: 1291.47 tokens/s, Accepted: 6337 tokens, Drafted: 12916 tokens, Per-position acceptance rate: 0.625, 0.356, Avg Draft acceptance rate: 49.1%
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f04c7466fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-33d29f95-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:37 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 3.05 tokens/s, Drafted throughput: 6.20 tokens/s, Accepted: 61 tokens, Drafted: 124 tokens, Per-position acceptance rate: 0.613, 0.371, Avg Draft acceptance rate: 49.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1924.8 tokens/s, Avg generation throughput: 1935.5 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 36.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 973.18 tokens/s, Drafted throughput: 1905.97 tokens/s, Accepted: 9735 tokens, Drafted: 19066 tokens, Per-position acceptance rate: 0.646, 0.376, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1921.3 tokens/s, Avg generation throughput: 2352.8 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 31.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:33:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1165.33 tokens/s, Drafted throughput: 2367.04 tokens/s, Accepted: 11658 tokens, Drafted: 23680 tokens, Per-position acceptance rate: 0.626, 0.359, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1820.1 tokens/s, Avg generation throughput: 2410.1 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1194.64 tokens/s, Drafted throughput: 2420.68 tokens/s, Accepted: 11947 tokens, Drafted: 24208 tokens, Per-position acceptance rate: 0.630, 0.357, Avg Draft acceptance rate: 49.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1711.1 tokens/s, Avg generation throughput: 2454.8 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1212.75 tokens/s, Drafted throughput: 2475.48 tokens/s, Accepted: 12129 tokens, Drafted: 24758 tokens, Per-position acceptance rate: 0.627, 0.353, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1781.6 tokens/s, Avg generation throughput: 2473.7 tokens/s, Running: 60 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 1232.10 tokens/s, Drafted throughput: 2476.19 tokens/s, Accepted: 12321 tokens, Drafted: 24762 tokens, Per-position acceptance rate: 0.635, 0.360, Avg Draft acceptance rate: 49.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  55.12     
Total input tokens:                      94775     
Total generated tokens:                  127983    
Request throughput (req/s):              11.61     
Output token throughput (tok/s):         2321.93   
Peak output token throughput (tok/s):    1472.00   
Peak concurrent requests:                103.00    
Total token throughput (tok/s):          4041.38   
---------------Time to First Token----------------
Mean TTFT (ms):                          221.00    
Median TTFT (ms):                        194.95    
P99 TTFT (ms):                           609.23    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          25.25     
Median TPOT (ms):                        25.23     
P99 TPOT (ms):                           29.17     
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.08     
Median ITL (ms):                         44.45     
P99 ITL (ms):                            110.65    
---------------Speculative Decoding---------------
Acceptance rate (%):                     49.48     
Acceptance length:                       1.99      
Drafts:                                  64212     
Draft tokens:                            128424    
Accepted tokens:                         63542     
Per-position acceptance (%):
  Position 0:                            63.08     
  Position 1:                            35.88     
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:37 [loggers.py:257] Engine 000: Avg prompt throughput: 316.5 tokens/s, Avg generation throughput: 1176.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 579.27 tokens/s, Drafted throughput: 1202.13 tokens/s, Accepted: 5793 tokens, Drafted: 12022 tokens, Per-position acceptance rate: 0.619, 0.345, Avg Draft acceptance rate: 48.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fecc63defc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-483ceb82-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:57 [loggers.py:257] Engine 000: Avg prompt throughput: 767.6 tokens/s, Avg generation throughput: 44.0 tokens/s, Running: 86 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:34:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 10.65 tokens/s, Drafted throughput: 17.50 tokens/s, Accepted: 213 tokens, Drafted: 350 tokens, Per-position acceptance rate: 0.720, 0.497, Avg Draft acceptance rate: 60.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:07 [loggers.py:257] Engine 000: Avg prompt throughput: 3098.0 tokens/s, Avg generation throughput: 3317.8 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 60.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 1667.11 tokens/s, Drafted throughput: 3278.82 tokens/s, Accepted: 16671 tokens, Drafted: 32788 tokens, Per-position acceptance rate: 0.640, 0.376, Avg Draft acceptance rate: 50.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1876.4 tokens/s, Avg generation throughput: 3570.3 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 81.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1761.49 tokens/s, Drafted throughput: 3606.72 tokens/s, Accepted: 17629 tokens, Drafted: 36096 tokens, Per-position acceptance rate: 0.622, 0.355, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:27 [loggers.py:257] Engine 000: Avg prompt throughput: 2398.0 tokens/s, Avg generation throughput: 3263.4 tokens/s, Running: 115 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1608.81 tokens/s, Drafted throughput: 3299.58 tokens/s, Accepted: 16094 tokens, Drafted: 33008 tokens, Per-position acceptance rate: 0.622, 0.353, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:37 [loggers.py:257] Engine 000: Avg prompt throughput: 2981.2 tokens/s, Avg generation throughput: 3037.3 tokens/s, Running: 123 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 1513.63 tokens/s, Drafted throughput: 3028.86 tokens/s, Accepted: 15140 tokens, Drafted: 30296 tokens, Per-position acceptance rate: 0.632, 0.367, Avg Draft acceptance rate: 50.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:47 [loggers.py:257] Engine 000: Avg prompt throughput: 2069.7 tokens/s, Avg generation throughput: 3458.5 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 1704.94 tokens/s, Drafted throughput: 3497.82 tokens/s, Accepted: 17060 tokens, Drafted: 35000 tokens, Per-position acceptance rate: 0.619, 0.356, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:57 [loggers.py:257] Engine 000: Avg prompt throughput: 2106.8 tokens/s, Avg generation throughput: 3357.5 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 84.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:35:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 1641.38 tokens/s, Drafted throughput: 3424.15 tokens/s, Accepted: 16415 tokens, Drafted: 34244 tokens, Per-position acceptance rate: 0.613, 0.346, Avg Draft acceptance rate: 47.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:07 [loggers.py:257] Engine 000: Avg prompt throughput: 2736.7 tokens/s, Avg generation throughput: 3078.4 tokens/s, Running: 122 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1529.19 tokens/s, Drafted throughput: 3087.99 tokens/s, Accepted: 15293 tokens, Drafted: 30882 tokens, Per-position acceptance rate: 0.628, 0.362, Avg Draft acceptance rate: 49.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  79.47     
Total input tokens:                      189093    
Total generated tokens:                  255983    
Request throughput (req/s):              16.11     
Output token throughput (tok/s):         3221.17   
Peak output token throughput (tok/s):    2402.00   
Peak concurrent requests:                179.00    
Total token throughput (tok/s):          5600.63   
---------------Time to First Token----------------
Mean TTFT (ms):                          406.73    
Median TTFT (ms):                        344.25    
P99 TTFT (ms):                           1601.57   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          36.78     
Median TPOT (ms):                        37.03     
P99 TPOT (ms):                           44.90     
---------------Inter-token Latency----------------
Mean ITL (ms):                           72.72     
Median ITL (ms):                         56.20     
P99 ITL (ms):                            192.63    
---------------Speculative Decoding---------------
Acceptance rate (%):                     49.18     
Acceptance length:                       1.98      
Drafts:                                  128827    
Draft tokens:                            257654    
Accepted tokens:                         126712    
Per-position acceptance (%):
  Position 0:                            62.48     
  Position 1:                            35.88     
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:17 [loggers.py:257] Engine 000: Avg prompt throughput: 887.8 tokens/s, Avg generation throughput: 2482.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1229.83 tokens/s, Drafted throughput: 2518.47 tokens/s, Accepted: 12299 tokens, Drafted: 25186 tokens, Per-position acceptance rate: 0.622, 0.354, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc34da96fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15026, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9ce1592c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:37 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 12.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 3.05 tokens/s, Drafted throughput: 6.10 tokens/s, Accepted: 61 tokens, Drafted: 122 tokens, Per-position acceptance rate: 0.623, 0.377, Avg Draft acceptance rate: 50.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:47 [loggers.py:257] Engine 000: Avg prompt throughput: 3847.1 tokens/s, Avg generation throughput: 2273.2 tokens/s, Running: 176 reqs, Waiting: 80 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 1166.48 tokens/s, Drafted throughput: 2162.58 tokens/s, Accepted: 11666 tokens, Drafted: 21628 tokens, Per-position acceptance rate: 0.667, 0.411, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1582.7 tokens/s, Avg generation throughput: 2516.0 tokens/s, Running: 190 reqs, Waiting: 63 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:36:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 1231.72 tokens/s, Drafted throughput: 2541.83 tokens/s, Accepted: 12318 tokens, Drafted: 25420 tokens, Per-position acceptance rate: 0.619, 0.350, Avg Draft acceptance rate: 48.5%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:07 [loggers.py:257] Engine 000: Avg prompt throughput: 2609.6 tokens/s, Avg generation throughput: 2858.4 tokens/s, Running: 228 reqs, Waiting: 19 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1408.42 tokens/s, Drafted throughput: 2877.84 tokens/s, Accepted: 14085 tokens, Drafted: 28780 tokens, Per-position acceptance rate: 0.622, 0.356, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1376.2 tokens/s, Avg generation throughput: 3232.1 tokens/s, Running: 180 reqs, Waiting: 67 reqs, GPU KV cache usage: 95.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 1584.14 tokens/s, Drafted throughput: 3280.27 tokens/s, Accepted: 15842 tokens, Drafted: 32804 tokens, Per-position acceptance rate: 0.620, 0.346, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1587.7 tokens/s, Avg generation throughput: 3273.0 tokens/s, Running: 144 reqs, Waiting: 105 reqs, GPU KV cache usage: 96.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1616.99 tokens/s, Drafted throughput: 3294.37 tokens/s, Accepted: 16174 tokens, Drafted: 32952 tokens, Per-position acceptance rate: 0.625, 0.356, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:37 [loggers.py:257] Engine 000: Avg prompt throughput: 2948.6 tokens/s, Avg generation throughput: 3115.4 tokens/s, Running: 148 reqs, Waiting: 105 reqs, GPU KV cache usage: 97.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1546.09 tokens/s, Drafted throughput: 3112.18 tokens/s, Accepted: 15461 tokens, Drafted: 31122 tokens, Per-position acceptance rate: 0.629, 0.365, Avg Draft acceptance rate: 49.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:47 [loggers.py:257] Engine 000: Avg prompt throughput: 2331.9 tokens/s, Avg generation throughput: 2612.8 tokens/s, Running: 170 reqs, Waiting: 84 reqs, GPU KV cache usage: 98.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1291.46 tokens/s, Drafted throughput: 2615.70 tokens/s, Accepted: 12924 tokens, Drafted: 26176 tokens, Per-position acceptance rate: 0.624, 0.363, Avg Draft acceptance rate: 49.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:57 [loggers.py:257] Engine 000: Avg prompt throughput: 2855.5 tokens/s, Avg generation throughput: 2649.1 tokens/s, Running: 212 reqs, Waiting: 42 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:37:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 1319.49 tokens/s, Drafted throughput: 2630.19 tokens/s, Accepted: 13200 tokens, Drafted: 26312 tokens, Per-position acceptance rate: 0.637, 0.366, Avg Draft acceptance rate: 50.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:07 [loggers.py:257] Engine 000: Avg prompt throughput: 2181.7 tokens/s, Avg generation throughput: 2747.1 tokens/s, Running: 225 reqs, Waiting: 25 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 1337.60 tokens/s, Drafted throughput: 2794.95 tokens/s, Accepted: 13382 tokens, Drafted: 27962 tokens, Per-position acceptance rate: 0.609, 0.348, Avg Draft acceptance rate: 47.9%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1252.0 tokens/s, Avg generation throughput: 3304.6 tokens/s, Running: 188 reqs, Waiting: 48 reqs, GPU KV cache usage: 93.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 1623.77 tokens/s, Drafted throughput: 3351.94 tokens/s, Accepted: 16239 tokens, Drafted: 33522 tokens, Per-position acceptance rate: 0.618, 0.351, Avg Draft acceptance rate: 48.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1801.9 tokens/s, Avg generation throughput: 3367.9 tokens/s, Running: 144 reqs, Waiting: 102 reqs, GPU KV cache usage: 95.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1673.91 tokens/s, Drafted throughput: 3371.22 tokens/s, Accepted: 16743 tokens, Drafted: 33720 tokens, Per-position acceptance rate: 0.631, 0.362, Avg Draft acceptance rate: 49.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:37 [loggers.py:257] Engine 000: Avg prompt throughput: 2627.2 tokens/s, Avg generation throughput: 3081.6 tokens/s, Running: 150 reqs, Waiting: 103 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 1541.77 tokens/s, Drafted throughput: 3052.74 tokens/s, Accepted: 15420 tokens, Drafted: 30532 tokens, Per-position acceptance rate: 0.639, 0.371, Avg Draft acceptance rate: 50.5%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:47 [loggers.py:257] Engine 000: Avg prompt throughput: 2512.3 tokens/s, Avg generation throughput: 2799.1 tokens/s, Running: 168 reqs, Waiting: 87 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1388.87 tokens/s, Drafted throughput: 2792.75 tokens/s, Accepted: 13889 tokens, Drafted: 27928 tokens, Per-position acceptance rate: 0.628, 0.367, Avg Draft acceptance rate: 49.7%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:57 [loggers.py:257] Engine 000: Avg prompt throughput: 2546.2 tokens/s, Avg generation throughput: 2637.9 tokens/s, Running: 210 reqs, Waiting: 44 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:38:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 1302.02 tokens/s, Drafted throughput: 2644.21 tokens/s, Accepted: 13034 tokens, Drafted: 26470 tokens, Per-position acceptance rate: 0.626, 0.359, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:07 [loggers.py:257] Engine 000: Avg prompt throughput: 2168.9 tokens/s, Avg generation throughput: 2712.9 tokens/s, Running: 235 reqs, Waiting: 13 reqs, GPU KV cache usage: 97.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.96, Accepted throughput: 1323.74 tokens/s, Drafted throughput: 2753.48 tokens/s, Accepted: 13239 tokens, Drafted: 27538 tokens, Per-position acceptance rate: 0.616, 0.346, Avg Draft acceptance rate: 48.1%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1467.2 tokens/s, Avg generation throughput: 3360.1 tokens/s, Running: 201 reqs, Waiting: 35 reqs, GPU KV cache usage: 92.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1667.33 tokens/s, Drafted throughput: 3373.05 tokens/s, Accepted: 16674 tokens, Drafted: 33732 tokens, Per-position acceptance rate: 0.632, 0.356, Avg Draft acceptance rate: 49.4%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1590.1 tokens/s, Avg generation throughput: 3492.2 tokens/s, Running: 145 reqs, Waiting: 37 reqs, GPU KV cache usage: 95.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 1736.52 tokens/s, Drafted throughput: 3492.44 tokens/s, Accepted: 17366 tokens, Drafted: 34926 tokens, Per-position acceptance rate: 0.634, 0.360, Avg Draft acceptance rate: 49.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  174.06    
Total input tokens:                      373233    
Total generated tokens:                  511920    
Request throughput (req/s):              14.71     
Output token throughput (tok/s):         2941.10   
Peak output token throughput (tok/s):    2715.00   
Peak concurrent requests:                292.00    
Total token throughput (tok/s):          5085.41   
---------------Time to First Token----------------
Mean TTFT (ms):                          3582.37   
Median TTFT (ms):                        2848.62   
P99 TTFT (ms):                           9276.49   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          66.92     
Median TPOT (ms):                        62.53     
P99 TPOT (ms):                           104.09    
---------------Inter-token Latency----------------
Mean ITL (ms):                           132.26    
Median ITL (ms):                         86.28     
P99 ITL (ms):                            435.58    
---------------Speculative Decoding---------------
Acceptance rate (%):                     49.34     
Acceptance length:                       1.99      
Drafts:                                  256662    
Draft tokens:                            513324    
Accepted tokens:                         253288    
Per-position acceptance (%):
  Position 0:                            62.74     
  Position 1:                            35.95     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k2-t0.0-tp1...
[0;36m(APIServer pid=848678)[0;0m INFO 01-22 22:39:33 [launcher.py:110] Shutting down FastAPI HTTP server.
