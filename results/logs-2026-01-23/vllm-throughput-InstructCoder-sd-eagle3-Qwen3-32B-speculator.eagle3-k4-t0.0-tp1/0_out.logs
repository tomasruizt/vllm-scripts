Removing any existing container named vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k4-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k4-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 3326062
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:12 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:12 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15028, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 4, 'max_model_len': 5000}}
[0;36m(APIServer pid=3326062)[0;0m WARNING 01-23 00:40:12 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:13 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:13 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:14 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:14 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:14 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:14 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:40:14 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb1f30cafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f05fd4c5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 00:40:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:40:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:40:26 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=4), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:40:27 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.45:45989 backend=nccl
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:40:27 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=3326389)[0;0m WARNING 01-23 00:40:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:40:28 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-23 00:40:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:40:29 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 00:40:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:40:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:40:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:40:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:40:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:40:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:11 [default_loader.py:291] Loading weights took 40.41 seconds
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:11 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:12 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 00:41:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:14 [default_loader.py:291] Loading weights took 1.90 seconds
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:16 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:17 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:18 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 48.504035 seconds
WARNING 01-23 00:41:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:31 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:31 [backends.py:704] Dynamo bytecode transform time: 12.93 s
WARNING 01-23 00:41:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:49 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.596 s
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:49 [monitor.py:34] torch.compile takes 17.53 s in total
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:49 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:49 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:50 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.123 s
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:50 [monitor.py:34] torch.compile takes 18.12 s in total
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:51 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:51 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:41:51 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 00:41:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
WARNING 01-23 00:41:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:42:02 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.67 GiB
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:42:02 [core.py:272] init engine (profile, create kv cache, warmup model) took 44.25 seconds
WARNING 01-23 00:42:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15028)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15028 ssl:default [Connect call failed (\'127.0.0.1\', 15028)]\n''
[0;36m(EngineCore_DP0 pid=3326389)[0;0m INFO 01-23 00:42:04 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:04 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=3326062)[0;0m WARNING 01-23 00:42:04 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:04 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:04 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:04 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:06 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:06 [serving.py:221] Chat template warmup completed in 1763.6ms
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:06 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15028
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:07 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:17 [loggers.py:257] Engine 000: Avg prompt throughput: 40.8 tokens/s, Avg generation throughput: 33.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 19.37 tokens/s, Drafted throughput: 57.19 tokens/s, Accepted: 256 tokens, Drafted: 756 tokens, Per-position acceptance rate: 0.667, 0.381, 0.196, 0.111, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:27 [loggers.py:257] Engine 000: Avg prompt throughput: 39.6 tokens/s, Avg generation throughput: 58.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 32.40 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 324 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.625, 0.355, 0.188, 0.098, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:37 [loggers.py:257] Engine 000: Avg prompt throughput: 43.3 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 34.90 tokens/s, Drafted throughput: 101.99 tokens/s, Accepted: 349 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.663, 0.392, 0.200, 0.114, Avg Draft acceptance rate: 34.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:47 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 30.40 tokens/s, Drafted throughput: 102.80 tokens/s, Accepted: 304 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.615, 0.315, 0.175, 0.078, Avg Draft acceptance rate: 29.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:57 [loggers.py:257] Engine 000: Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:42:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 32.80 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 328 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.617, 0.348, 0.211, 0.105, Avg Draft acceptance rate: 32.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:07 [loggers.py:257] Engine 000: Avg prompt throughput: 37.5 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 33.70 tokens/s, Drafted throughput: 101.99 tokens/s, Accepted: 337 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.678, 0.380, 0.192, 0.071, Avg Draft acceptance rate: 33.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:17 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 33.10 tokens/s, Drafted throughput: 101.98 tokens/s, Accepted: 331 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.647, 0.353, 0.200, 0.098, Avg Draft acceptance rate: 32.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:27 [loggers.py:257] Engine 000: Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 37.80 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 378 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.715, 0.414, 0.219, 0.129, Avg Draft acceptance rate: 36.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:37 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 29.80 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 298 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.625, 0.316, 0.152, 0.070, Avg Draft acceptance rate: 29.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:47 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 32.40 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 324 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.629, 0.352, 0.180, 0.105, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:57 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 61.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:43:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 35.50 tokens/s, Drafted throughput: 102.40 tokens/s, Accepted: 355 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.660, 0.418, 0.207, 0.102, Avg Draft acceptance rate: 34.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:07 [loggers.py:257] Engine 000: Avg prompt throughput: 59.4 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 32.09 tokens/s, Drafted throughput: 101.58 tokens/s, Accepted: 321 tokens, Drafted: 1016 tokens, Per-position acceptance rate: 0.661, 0.331, 0.177, 0.094, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:17 [loggers.py:257] Engine 000: Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 56.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 31.20 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 312 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.638, 0.339, 0.179, 0.058, Avg Draft acceptance rate: 30.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:27 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 33.80 tokens/s, Drafted throughput: 101.60 tokens/s, Accepted: 338 tokens, Drafted: 1016 tokens, Per-position acceptance rate: 0.638, 0.394, 0.205, 0.094, Avg Draft acceptance rate: 33.3%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:37 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 29.30 tokens/s, Drafted throughput: 101.99 tokens/s, Accepted: 293 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.576, 0.345, 0.157, 0.071, Avg Draft acceptance rate: 28.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:47 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.10, Accepted throughput: 28.30 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 283 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.615, 0.315, 0.121, 0.051, Avg Draft acceptance rate: 27.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:57 [loggers.py:257] Engine 000: Avg prompt throughput: 44.9 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:44:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 34.50 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 345 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.648, 0.398, 0.227, 0.074, Avg Draft acceptance rate: 33.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  172.03    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.29      
Output token throughput (tok/s):         58.13     
Peak output token throughput (tok/s):    27.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          100.14    
---------------Time to First Token----------------
Mean TTFT (ms):                          75.61     
Median TTFT (ms):                        81.50     
P99 TTFT (ms):                           113.51    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.83     
Median TPOT (ms):                        16.73     
P99 TPOT (ms):                           20.09     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.26     
Median ITL (ms):                         38.27     
P99 ITL (ms):                            38.99     
---------------Speculative Decoding---------------
Acceptance rate (%):                     32.08     
Acceptance length:                       2.28      
Drafts:                                  4377      
Draft tokens:                            17508     
Accepted tokens:                         5616      
Per-position acceptance (%):
  Position 0:                            64.29     
  Position 1:                            36.17     
  Position 2:                            18.83     
  Position 3:                            9.02      
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:07 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 25.50 tokens/s, Drafted throughput: 73.59 tokens/s, Accepted: 255 tokens, Drafted: 736 tokens, Per-position acceptance rate: 0.674, 0.375, 0.212, 0.125, Avg Draft acceptance rate: 34.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff7788aafc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e2f9d9f2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:37 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 69.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 13.47 tokens/s, Drafted throughput: 38.40 tokens/s, Accepted: 404 tokens, Drafted: 1152 tokens, Per-position acceptance rate: 0.684, 0.396, 0.208, 0.115, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:47 [loggers.py:257] Engine 000: Avg prompt throughput: 69.2 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 63.60 tokens/s, Drafted throughput: 201.19 tokens/s, Accepted: 636 tokens, Drafted: 2012 tokens, Per-position acceptance rate: 0.630, 0.364, 0.181, 0.089, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:57 [loggers.py:257] Engine 000: Avg prompt throughput: 59.7 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:45:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 65.60 tokens/s, Drafted throughput: 201.19 tokens/s, Accepted: 656 tokens, Drafted: 2012 tokens, Per-position acceptance rate: 0.632, 0.362, 0.213, 0.097, Avg Draft acceptance rate: 32.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:07 [loggers.py:257] Engine 000: Avg prompt throughput: 95.3 tokens/s, Avg generation throughput: 115.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 65.39 tokens/s, Drafted throughput: 200.78 tokens/s, Accepted: 654 tokens, Drafted: 2008 tokens, Per-position acceptance rate: 0.665, 0.363, 0.187, 0.088, Avg Draft acceptance rate: 32.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:17 [loggers.py:257] Engine 000: Avg prompt throughput: 79.6 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 65.00 tokens/s, Drafted throughput: 199.99 tokens/s, Accepted: 650 tokens, Drafted: 2000 tokens, Per-position acceptance rate: 0.632, 0.364, 0.192, 0.112, Avg Draft acceptance rate: 32.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:27 [loggers.py:257] Engine 000: Avg prompt throughput: 105.5 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 67.19 tokens/s, Drafted throughput: 199.97 tokens/s, Accepted: 672 tokens, Drafted: 2000 tokens, Per-position acceptance rate: 0.656, 0.392, 0.194, 0.102, Avg Draft acceptance rate: 33.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:37 [loggers.py:257] Engine 000: Avg prompt throughput: 84.6 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 60.79 tokens/s, Drafted throughput: 199.97 tokens/s, Accepted: 608 tokens, Drafted: 2000 tokens, Per-position acceptance rate: 0.642, 0.330, 0.172, 0.072, Avg Draft acceptance rate: 30.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:47 [loggers.py:257] Engine 000: Avg prompt throughput: 87.0 tokens/s, Avg generation throughput: 114.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 64.09 tokens/s, Drafted throughput: 199.98 tokens/s, Accepted: 641 tokens, Drafted: 2000 tokens, Per-position acceptance rate: 0.644, 0.376, 0.176, 0.086, Avg Draft acceptance rate: 32.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:57 [loggers.py:257] Engine 000: Avg prompt throughput: 73.4 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:46:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 64.10 tokens/s, Drafted throughput: 201.19 tokens/s, Accepted: 641 tokens, Drafted: 2012 tokens, Per-position acceptance rate: 0.638, 0.374, 0.187, 0.076, Avg Draft acceptance rate: 31.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  88.92     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.56      
Output token throughput (tok/s):         112.46    
Peak output token throughput (tok/s):    52.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          193.73    
---------------Time to First Token----------------
Mean TTFT (ms):                          119.82    
Median TTFT (ms):                        118.38    
P99 TTFT (ms):                           163.78    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.89     
Median TPOT (ms):                        16.84     
P99 TPOT (ms):                           19.24     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.55     
Median ITL (ms):                         38.48     
P99 ITL (ms):                            45.25     
---------------Speculative Decoding---------------
Acceptance rate (%):                     32.30     
Acceptance length:                       2.29      
Drafts:                                  4361      
Draft tokens:                            17444     
Accepted tokens:                         5635      
Per-position acceptance (%):
  Position 0:                            64.53     
  Position 1:                            36.60     
  Position 2:                            18.89     
  Position 3:                            9.20      
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:07 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 33.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 18.80 tokens/s, Drafted throughput: 59.59 tokens/s, Accepted: 188 tokens, Drafted: 596 tokens, Per-position acceptance rate: 0.651, 0.322, 0.181, 0.107, Avg Draft acceptance rate: 31.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:17 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fac194e6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2b7f4a2c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:27 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 81.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 23.40 tokens/s, Drafted throughput: 68.59 tokens/s, Accepted: 468 tokens, Drafted: 1372 tokens, Per-position acceptance rate: 0.659, 0.388, 0.210, 0.108, Avg Draft acceptance rate: 34.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:37 [loggers.py:257] Engine 000: Avg prompt throughput: 155.7 tokens/s, Avg generation throughput: 226.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 128.29 tokens/s, Drafted throughput: 391.97 tokens/s, Accepted: 1283 tokens, Drafted: 3920 tokens, Per-position acceptance rate: 0.644, 0.372, 0.202, 0.091, Avg Draft acceptance rate: 32.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:47 [loggers.py:257] Engine 000: Avg prompt throughput: 188.9 tokens/s, Avg generation throughput: 224.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 126.38 tokens/s, Drafted throughput: 393.53 tokens/s, Accepted: 1264 tokens, Drafted: 3936 tokens, Per-position acceptance rate: 0.639, 0.354, 0.189, 0.103, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:57 [loggers.py:257] Engine 000: Avg prompt throughput: 164.0 tokens/s, Avg generation throughput: 225.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:47:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 126.48 tokens/s, Drafted throughput: 394.75 tokens/s, Accepted: 1265 tokens, Drafted: 3948 tokens, Per-position acceptance rate: 0.647, 0.359, 0.193, 0.083, Avg Draft acceptance rate: 32.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:07 [loggers.py:257] Engine 000: Avg prompt throughput: 145.7 tokens/s, Avg generation throughput: 221.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 122.99 tokens/s, Drafted throughput: 395.18 tokens/s, Accepted: 1230 tokens, Drafted: 3952 tokens, Per-position acceptance rate: 0.622, 0.367, 0.179, 0.076, Avg Draft acceptance rate: 31.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  46.44     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.08      
Output token throughput (tok/s):         215.34    
Peak output token throughput (tok/s):    104.00    
Peak concurrent requests:                6.00      
Total token throughput (tok/s):          370.96    
---------------Time to First Token----------------
Mean TTFT (ms):                          121.37    
Median TTFT (ms):                        120.45    
P99 TTFT (ms):                           159.33    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.24     
Median TPOT (ms):                        17.35     
P99 TPOT (ms):                           19.89     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.18     
Median ITL (ms):                         39.00     
P99 ITL (ms):                            49.21     
---------------Speculative Decoding---------------
Acceptance rate (%):                     32.05     
Acceptance length:                       2.28      
Drafts:                                  4380      
Draft tokens:                            17520     
Accepted tokens:                         5615      
Per-position acceptance (%):
  Position 0:                            63.88     
  Position 1:                            36.26     
  Position 2:                            19.13     
  Position 3:                            8.93      
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:17 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 40.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 22.00 tokens/s, Drafted throughput: 74.00 tokens/s, Accepted: 220 tokens, Drafted: 740 tokens, Per-position acceptance rate: 0.622, 0.308, 0.168, 0.092, Avg Draft acceptance rate: 29.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2f188aafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6db0148c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:37 [loggers.py:257] Engine 000: Avg prompt throughput: 147.1 tokens/s, Avg generation throughput: 170.5 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 48.30 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 966 tokens, Drafted: 2940 tokens, Per-position acceptance rate: 0.644, 0.373, 0.196, 0.102, Avg Draft acceptance rate: 32.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:47 [loggers.py:257] Engine 000: Avg prompt throughput: 340.1 tokens/s, Avg generation throughput: 429.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 239.80 tokens/s, Drafted throughput: 753.19 tokens/s, Accepted: 2398 tokens, Drafted: 7532 tokens, Per-position acceptance rate: 0.636, 0.360, 0.185, 0.093, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:57 [loggers.py:257] Engine 000: Avg prompt throughput: 342.9 tokens/s, Avg generation throughput: 424.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:48:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 235.86 tokens/s, Drafted throughput: 751.88 tokens/s, Accepted: 2359 tokens, Drafted: 7520 tokens, Per-position acceptance rate: 0.631, 0.352, 0.187, 0.085, Avg Draft acceptance rate: 31.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:07 [loggers.py:257] Engine 000: Avg prompt throughput: 237.0 tokens/s, Avg generation throughput: 424.8 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 233.56 tokens/s, Drafted throughput: 763.05 tokens/s, Accepted: 2336 tokens, Drafted: 7632 tokens, Per-position acceptance rate: 0.613, 0.344, 0.177, 0.091, Avg Draft acceptance rate: 30.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  39.17     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              2.04      
Output token throughput (tok/s):         408.44    
Peak output token throughput (tok/s):    200.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          706.86    
---------------Time to First Token----------------
Mean TTFT (ms):                          125.51    
Median TTFT (ms):                        124.85    
P99 TTFT (ms):                           155.31    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.15     
Median TPOT (ms):                        18.00     
P99 TPOT (ms):                           21.03     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.82     
Median ITL (ms):                         40.53     
P99 ITL (ms):                            51.76     
---------------Speculative Decoding---------------
Acceptance rate (%):                     31.45     
Acceptance length:                       2.26      
Drafts:                                  7077      
Draft tokens:                            28308     
Accepted tokens:                         8902      
Per-position acceptance (%):
  Position 0:                            62.94     
  Position 1:                            35.42     
  Position 2:                            18.41     
  Position 3:                            9.02      
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:17 [loggers.py:257] Engine 000: Avg prompt throughput: 120.1 tokens/s, Avg generation throughput: 171.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 95.79 tokens/s, Drafted throughput: 303.18 tokens/s, Accepted: 958 tokens, Drafted: 3032 tokens, Per-position acceptance rate: 0.640, 0.356, 0.182, 0.086, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f734d552fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b962ed65-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:37 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 229.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 65.84 tokens/s, Drafted throughput: 192.98 tokens/s, Accepted: 1317 tokens, Drafted: 3860 tokens, Per-position acceptance rate: 0.654, 0.387, 0.217, 0.108, Avg Draft acceptance rate: 34.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:47 [loggers.py:257] Engine 000: Avg prompt throughput: 652.2 tokens/s, Avg generation throughput: 794.6 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 441.80 tokens/s, Drafted throughput: 1406.40 tokens/s, Accepted: 4418 tokens, Drafted: 14064 tokens, Per-position acceptance rate: 0.636, 0.355, 0.184, 0.082, Avg Draft acceptance rate: 31.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:57 [loggers.py:257] Engine 000: Avg prompt throughput: 579.0 tokens/s, Avg generation throughput: 796.7 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:49:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 444.63 tokens/s, Drafted throughput: 1406.96 tokens/s, Accepted: 4447 tokens, Drafted: 14072 tokens, Per-position acceptance rate: 0.615, 0.358, 0.192, 0.099, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:07 [loggers.py:257] Engine 000: Avg prompt throughput: 585.0 tokens/s, Avg generation throughput: 793.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 439.93 tokens/s, Drafted throughput: 1412.18 tokens/s, Accepted: 4400 tokens, Drafted: 14124 tokens, Per-position acceptance rate: 0.624, 0.355, 0.182, 0.086, Avg Draft acceptance rate: 31.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  41.59     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.85      
Output token throughput (tok/s):         769.28    
Peak output token throughput (tok/s):    384.00    
Peak concurrent requests:                27.00     
Total token throughput (tok/s):          1355.09   
---------------Time to First Token----------------
Mean TTFT (ms):                          137.59    
Median TTFT (ms):                        134.84    
P99 TTFT (ms):                           197.01    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.37     
Median TPOT (ms):                        19.36     
P99 TPOT (ms):                           22.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.72     
Median ITL (ms):                         43.03     
P99 ITL (ms):                            67.85     
---------------Speculative Decoding---------------
Acceptance rate (%):                     31.66     
Acceptance length:                       2.27      
Drafts:                                  14105     
Draft tokens:                            56420     
Accepted tokens:                         17865     
Per-position acceptance (%):
  Position 0:                            62.67     
  Position 1:                            35.91     
  Position 2:                            18.89     
  Position 3:                            9.19      
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:17 [loggers.py:257] Engine 000: Avg prompt throughput: 404.3 tokens/s, Avg generation throughput: 604.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 339.75 tokens/s, Drafted throughput: 1064.65 tokens/s, Accepted: 3398 tokens, Drafted: 10648 tokens, Per-position acceptance rate: 0.625, 0.361, 0.191, 0.099, Avg Draft acceptance rate: 31.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:27 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7feac726efc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e0439d0a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:37 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 5.20 tokens/s, Drafted throughput: 15.60 tokens/s, Accepted: 104 tokens, Drafted: 312 tokens, Per-position acceptance rate: 0.667, 0.372, 0.179, 0.115, Avg Draft acceptance rate: 33.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1209.2 tokens/s, Avg generation throughput: 1327.4 tokens/s, Running: 27 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 740.54 tokens/s, Drafted throughput: 2334.62 tokens/s, Accepted: 7406 tokens, Drafted: 23348 tokens, Per-position acceptance rate: 0.629, 0.361, 0.190, 0.089, Avg Draft acceptance rate: 31.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1226.4 tokens/s, Avg generation throughput: 1485.1 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:50:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 839.06 tokens/s, Drafted throughput: 2583.27 tokens/s, Accepted: 8393 tokens, Drafted: 25840 tokens, Per-position acceptance rate: 0.643, 0.366, 0.195, 0.095, Avg Draft acceptance rate: 32.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:07 [loggers.py:257] Engine 000: Avg prompt throughput: 955.3 tokens/s, Avg generation throughput: 1467.6 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 811.40 tokens/s, Drafted throughput: 2620.80 tokens/s, Accepted: 8114 tokens, Drafted: 26208 tokens, Per-position acceptance rate: 0.625, 0.344, 0.183, 0.087, Avg Draft acceptance rate: 31.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1086.7 tokens/s, Avg generation throughput: 1444.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 794.50 tokens/s, Drafted throughput: 2601.74 tokens/s, Accepted: 7947 tokens, Drafted: 26024 tokens, Per-position acceptance rate: 0.615, 0.336, 0.182, 0.088, Avg Draft acceptance rate: 30.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  45.87     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.98      
Output token throughput (tok/s):         1395.05   
Peak output token throughput (tok/s):    719.00    
Peak concurrent requests:                50.00     
Total token throughput (tok/s):          2447.28   
---------------Time to First Token----------------
Mean TTFT (ms):                          156.13    
Median TTFT (ms):                        149.92    
P99 TTFT (ms):                           234.73    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.04     
Median TPOT (ms):                        20.95     
P99 TPOT (ms):                           24.90     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.27     
Median ITL (ms):                         45.28     
P99 ITL (ms):                            85.34     
---------------Speculative Decoding---------------
Acceptance rate (%):                     31.43     
Acceptance length:                       2.26      
Drafts:                                  28343     
Draft tokens:                            113372    
Accepted tokens:                         35631     
Per-position acceptance (%):
  Position 0:                            62.70     
  Position 1:                            35.25     
  Position 2:                            18.70     
  Position 3:                            9.07      
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:27 [loggers.py:257] Engine 000: Avg prompt throughput: 348.2 tokens/s, Avg generation throughput: 674.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 378.19 tokens/s, Drafted throughput: 1198.76 tokens/s, Accepted: 3782 tokens, Drafted: 11988 tokens, Per-position acceptance rate: 0.620, 0.359, 0.184, 0.099, Avg Draft acceptance rate: 31.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8a24666fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e218e996-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:47 [loggers.py:257] Engine 000: Avg prompt throughput: 948.6 tokens/s, Avg generation throughput: 178.1 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 55.35 tokens/s, Drafted throughput: 122.39 tokens/s, Accepted: 1107 tokens, Drafted: 2448 tokens, Per-position acceptance rate: 0.776, 0.529, 0.338, 0.165, Avg Draft acceptance rate: 45.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1142.7 tokens/s, Avg generation throughput: 2292.4 tokens/s, Running: 60 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:51:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1271.60 tokens/s, Drafted throughput: 4077.68 tokens/s, Accepted: 12722 tokens, Drafted: 40796 tokens, Per-position acceptance rate: 0.621, 0.353, 0.184, 0.089, Avg Draft acceptance rate: 31.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1773.2 tokens/s, Avg generation throughput: 2200.2 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1228.13 tokens/s, Drafted throughput: 3880.21 tokens/s, Accepted: 12287 tokens, Drafted: 38820 tokens, Per-position acceptance rate: 0.627, 0.354, 0.192, 0.092, Avg Draft acceptance rate: 31.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1868.9 tokens/s, Avg generation throughput: 2149.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1194.68 tokens/s, Drafted throughput: 3813.94 tokens/s, Accepted: 11952 tokens, Drafted: 38156 tokens, Per-position acceptance rate: 0.621, 0.352, 0.189, 0.091, Avg Draft acceptance rate: 31.3%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1491.9 tokens/s, Avg generation throughput: 2225.3 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 1231.83 tokens/s, Drafted throughput: 3973.77 tokens/s, Accepted: 12319 tokens, Drafted: 39740 tokens, Per-position acceptance rate: 0.623, 0.346, 0.182, 0.088, Avg Draft acceptance rate: 31.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1469.1 tokens/s, Avg generation throughput: 2273.7 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1261.49 tokens/s, Drafted throughput: 4052.38 tokens/s, Accepted: 12615 tokens, Drafted: 40524 tokens, Per-position acceptance rate: 0.621, 0.352, 0.179, 0.093, Avg Draft acceptance rate: 31.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  60.39     
Total input tokens:                      94775     
Total generated tokens:                  127968    
Request throughput (req/s):              10.60     
Output token throughput (tok/s):         2119.04   
Peak output token throughput (tok/s):    1216.00   
Peak concurrent requests:                92.00     
Total token throughput (tok/s):          3688.43   
---------------Time to First Token----------------
Mean TTFT (ms):                          247.35    
Median TTFT (ms):                        226.50    
P99 TTFT (ms):                           640.54    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          27.65     
Median TPOT (ms):                        27.60     
P99 TPOT (ms):                           33.42     
---------------Inter-token Latency----------------
Mean ITL (ms):                           62.03     
Median ITL (ms):                         55.22     
P99 ITL (ms):                            119.93    
---------------Speculative Decoding---------------
Acceptance rate (%):                     31.34     
Acceptance length:                       2.25      
Drafts:                                  56761     
Draft tokens:                            227044    
Accepted tokens:                         71159     
Per-position acceptance (%):
  Position 0:                            62.39     
  Position 1:                            35.25     
  Position 2:                            18.63     
  Position 3:                            9.10      
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:47 [loggers.py:257] Engine 000: Avg prompt throughput: 798.8 tokens/s, Avg generation throughput: 1493.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 827.15 tokens/s, Drafted throughput: 2690.65 tokens/s, Accepted: 8272 tokens, Drafted: 26908 tokens, Per-position acceptance rate: 0.619, 0.345, 0.179, 0.087, Avg Draft acceptance rate: 30.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:52:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9d4b802fc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-4b57ac64-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1942.1 tokens/s, Avg generation throughput: 398.7 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 53.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 83.17 tokens/s, Drafted throughput: 182.34 tokens/s, Accepted: 2496 tokens, Drafted: 5472 tokens, Per-position acceptance rate: 0.770, 0.531, 0.347, 0.176, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1886.0 tokens/s, Avg generation throughput: 2629.2 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1462.21 tokens/s, Drafted throughput: 4661.09 tokens/s, Accepted: 14625 tokens, Drafted: 46620 tokens, Per-position acceptance rate: 0.624, 0.353, 0.188, 0.089, Avg Draft acceptance rate: 31.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1845.9 tokens/s, Avg generation throughput: 2638.3 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 56.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1473.50 tokens/s, Drafted throughput: 4661.78 tokens/s, Accepted: 14737 tokens, Drafted: 46624 tokens, Per-position acceptance rate: 0.621, 0.358, 0.193, 0.092, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1839.2 tokens/s, Avg generation throughput: 2652.2 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 57.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1476.24 tokens/s, Drafted throughput: 4705.97 tokens/s, Accepted: 14775 tokens, Drafted: 47100 tokens, Per-position acceptance rate: 0.627, 0.354, 0.183, 0.091, Avg Draft acceptance rate: 31.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1866.1 tokens/s, Avg generation throughput: 2581.8 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 60.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:53:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 1427.98 tokens/s, Drafted throughput: 4612.02 tokens/s, Accepted: 14281 tokens, Drafted: 46124 tokens, Per-position acceptance rate: 0.616, 0.347, 0.184, 0.093, Avg Draft acceptance rate: 31.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1762.1 tokens/s, Avg generation throughput: 2672.8 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 63.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1482.06 tokens/s, Drafted throughput: 4760.80 tokens/s, Accepted: 14833 tokens, Drafted: 47648 tokens, Per-position acceptance rate: 0.624, 0.349, 0.184, 0.088, Avg Draft acceptance rate: 31.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:17 [loggers.py:257] Engine 000: Avg prompt throughput: 1938.5 tokens/s, Avg generation throughput: 2627.6 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1462.48 tokens/s, Drafted throughput: 4654.99 tokens/s, Accepted: 14628 tokens, Drafted: 46560 tokens, Per-position acceptance rate: 0.618, 0.351, 0.191, 0.097, Avg Draft acceptance rate: 31.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1939.4 tokens/s, Avg generation throughput: 2575.2 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1408.83 tokens/s, Drafted throughput: 4656.14 tokens/s, Accepted: 14100 tokens, Drafted: 46600 tokens, Per-position acceptance rate: 0.608, 0.338, 0.178, 0.086, Avg Draft acceptance rate: 30.3%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1877.3 tokens/s, Avg generation throughput: 2620.9 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 65.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1456.74 tokens/s, Drafted throughput: 4655.82 tokens/s, Accepted: 14568 tokens, Drafted: 46560 tokens, Per-position acceptance rate: 0.621, 0.351, 0.188, 0.091, Avg Draft acceptance rate: 31.3%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1956.8 tokens/s, Avg generation throughput: 2587.7 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1436.77 tokens/s, Drafted throughput: 4602.77 tokens/s, Accepted: 14379 tokens, Drafted: 46064 tokens, Per-position acceptance rate: 0.620, 0.353, 0.184, 0.092, Avg Draft acceptance rate: 31.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  99.84     
Total input tokens:                      189093    
Total generated tokens:                  255908    
Request throughput (req/s):              12.82     
Output token throughput (tok/s):         2563.23   
Peak output token throughput (tok/s):    1536.00   
Peak concurrent requests:                167.00    
Total token throughput (tok/s):          4457.23   
---------------Time to First Token----------------
Mean TTFT (ms):                          456.47    
Median TTFT (ms):                        379.84    
P99 TTFT (ms):                           1708.19   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          46.44     
Median TPOT (ms):                        46.85     
P99 TPOT (ms):                           57.61     
---------------Inter-token Latency----------------
Mean ITL (ms):                           103.92    
Median ITL (ms):                         89.51     
P99 ITL (ms):                            198.53    
---------------Speculative Decoding---------------
Acceptance rate (%):                     31.21     
Acceptance length:                       2.25      
Drafts:                                  113792    
Draft tokens:                            455168    
Accepted tokens:                         142039    
Per-position acceptance (%):
  Position 0:                            61.99     
  Position 1:                            35.08     
  Position 2:                            18.63     
  Position 3:                            9.13      
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:57 [loggers.py:257] Engine 000: Avg prompt throughput: 64.4 tokens/s, Avg generation throughput: 1614.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:54:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 873.11 tokens/s, Drafted throughput: 3014.08 tokens/s, Accepted: 8732 tokens, Drafted: 30144 tokens, Per-position acceptance rate: 0.592, 0.322, 0.163, 0.082, Avg Draft acceptance rate: 29.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7feed37fafc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15028, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d31700f1-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:17 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 3.55 tokens/s, Drafted throughput: 11.80 tokens/s, Accepted: 71 tokens, Drafted: 236 tokens, Per-position acceptance rate: 0.627, 0.356, 0.136, 0.085, Avg Draft acceptance rate: 30.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:27 [loggers.py:257] Engine 000: Avg prompt throughput: 3847.2 tokens/s, Avg generation throughput: 1834.9 tokens/s, Running: 188 reqs, Waiting: 68 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 1078.78 tokens/s, Drafted throughput: 2923.95 tokens/s, Accepted: 10788 tokens, Drafted: 29240 tokens, Per-position acceptance rate: 0.683, 0.424, 0.247, 0.122, Avg Draft acceptance rate: 36.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:37 [loggers.py:257] Engine 000: Avg prompt throughput: 876.5 tokens/s, Avg generation throughput: 2040.0 tokens/s, Running: 187 reqs, Waiting: 60 reqs, GPU KV cache usage: 94.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 1092.54 tokens/s, Drafted throughput: 3769.40 tokens/s, Accepted: 10926 tokens, Drafted: 37696 tokens, Per-position acceptance rate: 0.595, 0.323, 0.166, 0.075, Avg Draft acceptance rate: 29.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1026.0 tokens/s, Avg generation throughput: 2676.6 tokens/s, Running: 160 reqs, Waiting: 92 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1484.21 tokens/s, Drafted throughput: 4760.68 tokens/s, Accepted: 14855 tokens, Drafted: 47648 tokens, Per-position acceptance rate: 0.621, 0.353, 0.183, 0.089, Avg Draft acceptance rate: 31.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:57 [loggers.py:257] Engine 000: Avg prompt throughput: 2323.2 tokens/s, Avg generation throughput: 2506.3 tokens/s, Running: 167 reqs, Waiting: 87 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:55:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1406.89 tokens/s, Drafted throughput: 4378.06 tokens/s, Accepted: 14070 tokens, Drafted: 43784 tokens, Per-position acceptance rate: 0.630, 0.362, 0.195, 0.098, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:07 [loggers.py:257] Engine 000: Avg prompt throughput: 2332.9 tokens/s, Avg generation throughput: 2123.5 tokens/s, Running: 209 reqs, Waiting: 46 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1178.70 tokens/s, Drafted throughput: 3761.28 tokens/s, Accepted: 11788 tokens, Drafted: 37616 tokens, Per-position acceptance rate: 0.620, 0.350, 0.189, 0.094, Avg Draft acceptance rate: 31.3%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:17 [loggers.py:257] Engine 000: Avg prompt throughput: 744.3 tokens/s, Avg generation throughput: 2653.5 tokens/s, Running: 174 reqs, Waiting: 67 reqs, GPU KV cache usage: 95.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 1436.60 tokens/s, Drafted throughput: 4868.53 tokens/s, Accepted: 14368 tokens, Drafted: 48692 tokens, Per-position acceptance rate: 0.602, 0.331, 0.168, 0.079, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:27 [loggers.py:257] Engine 000: Avg prompt throughput: 2013.0 tokens/s, Avg generation throughput: 2634.3 tokens/s, Running: 154 reqs, Waiting: 102 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1468.41 tokens/s, Drafted throughput: 4634.92 tokens/s, Accepted: 14685 tokens, Drafted: 46352 tokens, Per-position acceptance rate: 0.625, 0.351, 0.192, 0.099, Avg Draft acceptance rate: 31.7%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:37 [loggers.py:257] Engine 000: Avg prompt throughput: 2509.5 tokens/s, Avg generation throughput: 2132.3 tokens/s, Running: 192 reqs, Waiting: 63 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1188.55 tokens/s, Drafted throughput: 3743.12 tokens/s, Accepted: 11887 tokens, Drafted: 37436 tokens, Per-position acceptance rate: 0.624, 0.359, 0.195, 0.093, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1424.5 tokens/s, Avg generation throughput: 2284.5 tokens/s, Running: 195 reqs, Waiting: 45 reqs, GPU KV cache usage: 93.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 1232.06 tokens/s, Drafted throughput: 4196.39 tokens/s, Accepted: 12323 tokens, Drafted: 41972 tokens, Per-position acceptance rate: 0.603, 0.327, 0.166, 0.078, Avg Draft acceptance rate: 29.4%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:57 [loggers.py:257] Engine 000: Avg prompt throughput: 1352.0 tokens/s, Avg generation throughput: 2910.5 tokens/s, Running: 139 reqs, Waiting: 113 reqs, GPU KV cache usage: 98.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:56:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1620.06 tokens/s, Drafted throughput: 5148.37 tokens/s, Accepted: 16202 tokens, Drafted: 51488 tokens, Per-position acceptance rate: 0.620, 0.357, 0.187, 0.094, Avg Draft acceptance rate: 31.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:07 [loggers.py:257] Engine 000: Avg prompt throughput: 2382.5 tokens/s, Avg generation throughput: 2153.1 tokens/s, Running: 175 reqs, Waiting: 81 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1219.32 tokens/s, Drafted throughput: 3707.75 tokens/s, Accepted: 12198 tokens, Drafted: 37092 tokens, Per-position acceptance rate: 0.640, 0.371, 0.206, 0.099, Avg Draft acceptance rate: 32.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:17 [loggers.py:257] Engine 000: Avg prompt throughput: 2096.9 tokens/s, Avg generation throughput: 2128.0 tokens/s, Running: 209 reqs, Waiting: 36 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1159.74 tokens/s, Drafted throughput: 3843.00 tokens/s, Accepted: 11598 tokens, Drafted: 38432 tokens, Per-position acceptance rate: 0.611, 0.338, 0.176, 0.082, Avg Draft acceptance rate: 30.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:27 [loggers.py:257] Engine 000: Avg prompt throughput: 486.0 tokens/s, Avg generation throughput: 2796.3 tokens/s, Running: 140 reqs, Waiting: 105 reqs, GPU KV cache usage: 93.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1540.14 tokens/s, Drafted throughput: 5029.79 tokens/s, Accepted: 15402 tokens, Drafted: 50300 tokens, Per-position acceptance rate: 0.617, 0.347, 0.175, 0.085, Avg Draft acceptance rate: 30.6%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:37 [loggers.py:257] Engine 000: Avg prompt throughput: 2473.3 tokens/s, Avg generation throughput: 2435.6 tokens/s, Running: 165 reqs, Waiting: 90 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1378.40 tokens/s, Drafted throughput: 4199.35 tokens/s, Accepted: 13794 tokens, Drafted: 42024 tokens, Per-position acceptance rate: 0.634, 0.370, 0.204, 0.106, Avg Draft acceptance rate: 32.8%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:47 [loggers.py:257] Engine 000: Avg prompt throughput: 2627.8 tokens/s, Avg generation throughput: 2107.8 tokens/s, Running: 225 reqs, Waiting: 26 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1179.92 tokens/s, Drafted throughput: 3668.17 tokens/s, Accepted: 11800 tokens, Drafted: 36684 tokens, Per-position acceptance rate: 0.626, 0.363, 0.199, 0.098, Avg Draft acceptance rate: 32.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:57 [loggers.py:257] Engine 000: Avg prompt throughput: 633.4 tokens/s, Avg generation throughput: 2601.5 tokens/s, Running: 175 reqs, Waiting: 69 reqs, GPU KV cache usage: 95.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:57:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 1394.30 tokens/s, Drafted throughput: 4818.49 tokens/s, Accepted: 13945 tokens, Drafted: 48192 tokens, Per-position acceptance rate: 0.598, 0.327, 0.159, 0.074, Avg Draft acceptance rate: 28.9%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:07 [loggers.py:257] Engine 000: Avg prompt throughput: 1850.6 tokens/s, Avg generation throughput: 2704.7 tokens/s, Running: 151 reqs, Waiting: 102 reqs, GPU KV cache usage: 97.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1516.59 tokens/s, Drafted throughput: 4731.65 tokens/s, Accepted: 15167 tokens, Drafted: 47320 tokens, Per-position acceptance rate: 0.627, 0.362, 0.195, 0.097, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:17 [loggers.py:257] Engine 000: Avg prompt throughput: 2588.9 tokens/s, Avg generation throughput: 2115.4 tokens/s, Running: 202 reqs, Waiting: 53 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1201.25 tokens/s, Drafted throughput: 3613.64 tokens/s, Accepted: 12015 tokens, Drafted: 36144 tokens, Per-position acceptance rate: 0.645, 0.374, 0.209, 0.102, Avg Draft acceptance rate: 33.2%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:27 [loggers.py:257] Engine 000: Avg prompt throughput: 1205.6 tokens/s, Avg generation throughput: 2369.4 tokens/s, Running: 194 reqs, Waiting: 46 reqs, GPU KV cache usage: 93.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 1281.80 tokens/s, Drafted throughput: 4342.45 tokens/s, Accepted: 12819 tokens, Drafted: 43428 tokens, Per-position acceptance rate: 0.608, 0.324, 0.168, 0.080, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:37 [loggers.py:257] Engine 000: Avg prompt throughput: 1262.2 tokens/s, Avg generation throughput: 2830.2 tokens/s, Running: 148 reqs, Waiting: 105 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1576.58 tokens/s, Drafted throughput: 4998.91 tokens/s, Accepted: 15768 tokens, Drafted: 49996 tokens, Per-position acceptance rate: 0.631, 0.351, 0.184, 0.096, Avg Draft acceptance rate: 31.5%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:47 [loggers.py:257] Engine 000: Avg prompt throughput: 1259.2 tokens/s, Avg generation throughput: 2674.6 tokens/s, Running: 116 reqs, Waiting: 0 reqs, GPU KV cache usage: 76.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 1479.75 tokens/s, Drafted throughput: 4786.24 tokens/s, Accepted: 14798 tokens, Drafted: 47864 tokens, Per-position acceptance rate: 0.626, 0.347, 0.177, 0.086, Avg Draft acceptance rate: 30.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  211.10    
Total input tokens:                      373233    
Total generated tokens:                  511918    
Request throughput (req/s):              12.13     
Output token throughput (tok/s):         2425.05   
Peak output token throughput (tok/s):    1734.00   
Peak concurrent requests:                289.00    
Total token throughput (tok/s):          4193.13   
---------------Time to First Token----------------
Mean TTFT (ms):                          5130.04   
Median TTFT (ms):                        3626.74   
P99 TTFT (ms):                           11696.54  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          77.83     
Median TPOT (ms):                        73.95     
P99 TPOT (ms):                           127.14    
---------------Inter-token Latency----------------
Mean ITL (ms):                           173.79    
Median ITL (ms):                         131.74    
P99 ITL (ms):                            441.96    
---------------Speculative Decoding---------------
Acceptance rate (%):                     31.22     
Acceptance length:                       2.25      
Drafts:                                  227159    
Draft tokens:                            908636    
Accepted tokens:                         283666    
Per-position acceptance (%):
  Position 0:                            62.14     
  Position 1:                            35.09     
  Position 2:                            18.56     
  Position 3:                            9.08      
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k4-t0.0-tp1...
[0;36m(APIServer pid=3326062)[0;0m INFO 01-23 00:58:50 [launcher.py:110] Shutting down FastAPI HTTP server.
