Removing any existing container named vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k5-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k5-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 3338824
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:45 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:45 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15029, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=3338824)[0;0m WARNING 01-23 00:59:45 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:46 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:46 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:48 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:48 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:48 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:48 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 00:59:48 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe4729a6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a71ea69d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 00:59:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 00:59:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:00 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=5), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:01 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.45:34957 backend=nccl
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:01 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=3339186)[0;0m WARNING 01-23 01:00:02 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:02 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-23 01:00:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:04 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 01:00:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:00:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:45 [default_loader.py:291] Loading weights took 40.55 seconds
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:45 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:47 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 01:00:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:49 [default_loader.py:291] Loading weights took 1.93 seconds
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:51 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:52 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
WARNING 01-23 01:00:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:00:53 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 49.551724 seconds
WARNING 01-23 01:00:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:01:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:07 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:07 [backends.py:704] Dynamo bytecode transform time: 13.52 s
WARNING 01-23 01:01:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:01:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:01:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:01:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:25 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.534 s
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:25 [monitor.py:34] torch.compile takes 18.05 s in total
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:25 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:25 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:26 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.128 s
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:26 [monitor.py:34] torch.compile takes 18.65 s in total
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:27 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:27 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:27 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 01:01:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:01:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 01:01:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:38 [gpu_model_runner.py:4880] Graph capturing finished in 10 secs, took -0.67 GiB
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:38 [core.py:272] init engine (profile, create kv cache, warmup model) took 44.79 seconds
[0;36m(EngineCore_DP0 pid=3339186)[0;0m INFO 01-23 01:01:39 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:40 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=3338824)[0;0m WARNING 01-23 01:01:40 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:40 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:40 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:40 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [serving.py:221] Chat template warmup completed in 1722.3ms
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15029
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:42 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:53 [loggers.py:257] Engine 000: Avg prompt throughput: 41.0 tokens/s, Avg generation throughput: 40.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:01:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 23.38 tokens/s, Drafted throughput: 85.28 tokens/s, Accepted: 307 tokens, Drafted: 1120 tokens, Per-position acceptance rate: 0.656, 0.371, 0.188, 0.094, 0.062, Avg Draft acceptance rate: 27.4%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:03 [loggers.py:257] Engine 000: Avg prompt throughput: 39.6 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 31.30 tokens/s, Drafted throughput: 124.99 tokens/s, Accepted: 313 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.592, 0.352, 0.180, 0.092, 0.036, Avg Draft acceptance rate: 25.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:13 [loggers.py:257] Engine 000: Avg prompt throughput: 43.3 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 34.70 tokens/s, Drafted throughput: 124.00 tokens/s, Accepted: 347 tokens, Drafted: 1240 tokens, Per-position acceptance rate: 0.645, 0.411, 0.198, 0.097, 0.048, Avg Draft acceptance rate: 28.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:23 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 55.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 30.30 tokens/s, Drafted throughput: 125.00 tokens/s, Accepted: 303 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.608, 0.300, 0.172, 0.084, 0.048, Avg Draft acceptance rate: 24.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:33 [loggers.py:257] Engine 000: Avg prompt throughput: 41.1 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 33.00 tokens/s, Drafted throughput: 123.99 tokens/s, Accepted: 330 tokens, Drafted: 1240 tokens, Per-position acceptance rate: 0.613, 0.355, 0.210, 0.101, 0.052, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:43 [loggers.py:257] Engine 000: Avg prompt throughput: 26.8 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 33.40 tokens/s, Drafted throughput: 125.49 tokens/s, Accepted: 334 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.693, 0.359, 0.175, 0.068, 0.036, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:53 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:02:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 33.80 tokens/s, Drafted throughput: 124.50 tokens/s, Accepted: 338 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.647, 0.369, 0.201, 0.092, 0.048, Avg Draft acceptance rate: 27.1%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:03 [loggers.py:257] Engine 000: Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 39.60 tokens/s, Drafted throughput: 124.49 tokens/s, Accepted: 396 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.735, 0.426, 0.225, 0.141, 0.064, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:13 [loggers.py:257] Engine 000: Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 29.60 tokens/s, Drafted throughput: 125.50 tokens/s, Accepted: 296 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.625, 0.311, 0.155, 0.056, 0.032, Avg Draft acceptance rate: 23.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:23 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 31.10 tokens/s, Drafted throughput: 124.48 tokens/s, Accepted: 311 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.606, 0.337, 0.173, 0.100, 0.032, Avg Draft acceptance rate: 25.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:33 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 35.20 tokens/s, Drafted throughput: 124.98 tokens/s, Accepted: 352 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.676, 0.404, 0.200, 0.088, 0.040, Avg Draft acceptance rate: 28.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:43 [loggers.py:257] Engine 000: Avg prompt throughput: 59.4 tokens/s, Avg generation throughput: 57.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 32.86 tokens/s, Drafted throughput: 124.86 tokens/s, Accepted: 329 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.632, 0.360, 0.196, 0.084, 0.044, Avg Draft acceptance rate: 26.3%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:53 [loggers.py:257] Engine 000: Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:03:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 31.20 tokens/s, Drafted throughput: 125.49 tokens/s, Accepted: 312 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.633, 0.335, 0.171, 0.068, 0.036, Avg Draft acceptance rate: 24.9%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:03 [loggers.py:257] Engine 000: Avg prompt throughput: 64.1 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 34.40 tokens/s, Drafted throughput: 124.00 tokens/s, Accepted: 344 tokens, Drafted: 1240 tokens, Per-position acceptance rate: 0.633, 0.399, 0.202, 0.089, 0.065, Avg Draft acceptance rate: 27.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:13 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 29.30 tokens/s, Drafted throughput: 123.99 tokens/s, Accepted: 293 tokens, Drafted: 1240 tokens, Per-position acceptance rate: 0.577, 0.343, 0.161, 0.069, 0.032, Avg Draft acceptance rate: 23.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:23 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 30.99 tokens/s, Drafted throughput: 125.47 tokens/s, Accepted: 310 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.677, 0.351, 0.131, 0.056, 0.020, Avg Draft acceptance rate: 24.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:33 [loggers.py:257] Engine 000: Avg prompt throughput: 44.9 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 35.50 tokens/s, Drafted throughput: 124.49 tokens/s, Accepted: 355 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.647, 0.410, 0.229, 0.092, 0.048, Avg Draft acceptance rate: 28.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  173.34    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.29      
Output token throughput (tok/s):         57.69     
Peak output token throughput (tok/s):    26.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          99.38     
---------------Time to First Token----------------
Mean TTFT (ms):                          74.74     
Median TTFT (ms):                        82.69     
P99 TTFT (ms):                           88.97     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.97     
Median TPOT (ms):                        16.96     
P99 TPOT (ms):                           20.12     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.22     
Median ITL (ms):                         39.23     
P99 ITL (ms):                            39.94     
---------------Speculative Decoding---------------
Acceptance rate (%):                     26.49     
Acceptance length:                       2.32      
Drafts:                                  4304      
Draft tokens:                            21520     
Accepted tokens:                         5701      
Per-position acceptance (%):
  Position 0:                            64.17     
  Position 1:                            36.45     
  Position 2:                            18.70     
  Position 3:                            8.71      
  Position 4:                            4.41      
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:43 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 42.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 24.70 tokens/s, Drafted throughput: 87.49 tokens/s, Accepted: 247 tokens, Drafted: 875 tokens, Per-position acceptance rate: 0.657, 0.371, 0.200, 0.114, 0.069, Avg Draft acceptance rate: 28.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:04:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd70b012fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3b41ab43-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:13 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 73.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 14.10 tokens/s, Drafted throughput: 51.33 tokens/s, Accepted: 423 tokens, Drafted: 1540 tokens, Per-position acceptance rate: 0.653, 0.373, 0.195, 0.094, 0.058, Avg Draft acceptance rate: 27.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:23 [loggers.py:257] Engine 000: Avg prompt throughput: 77.6 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 64.69 tokens/s, Drafted throughput: 243.98 tokens/s, Accepted: 647 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.662, 0.359, 0.184, 0.080, 0.041, Avg Draft acceptance rate: 26.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:33 [loggers.py:257] Engine 000: Avg prompt throughput: 78.1 tokens/s, Avg generation throughput: 115.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 66.99 tokens/s, Drafted throughput: 243.97 tokens/s, Accepted: 670 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.633, 0.363, 0.221, 0.102, 0.053, Avg Draft acceptance rate: 27.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:43 [loggers.py:257] Engine 000: Avg prompt throughput: 68.5 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 61.69 tokens/s, Drafted throughput: 246.97 tokens/s, Accepted: 617 tokens, Drafted: 2470 tokens, Per-position acceptance rate: 0.638, 0.340, 0.162, 0.077, 0.032, Avg Draft acceptance rate: 25.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:53 [loggers.py:257] Engine 000: Avg prompt throughput: 79.6 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:05:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 66.40 tokens/s, Drafted throughput: 243.99 tokens/s, Accepted: 664 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.656, 0.359, 0.189, 0.109, 0.049, Avg Draft acceptance rate: 27.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:03 [loggers.py:257] Engine 000: Avg prompt throughput: 105.5 tokens/s, Avg generation throughput: 114.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 65.20 tokens/s, Drafted throughput: 243.98 tokens/s, Accepted: 652 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.639, 0.369, 0.184, 0.094, 0.049, Avg Draft acceptance rate: 26.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:13 [loggers.py:257] Engine 000: Avg prompt throughput: 84.6 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 60.80 tokens/s, Drafted throughput: 243.98 tokens/s, Accepted: 608 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.623, 0.336, 0.180, 0.074, 0.033, Avg Draft acceptance rate: 24.9%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:23 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 61.09 tokens/s, Drafted throughput: 245.48 tokens/s, Accepted: 611 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.605, 0.356, 0.165, 0.079, 0.039, Avg Draft acceptance rate: 24.9%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:33 [loggers.py:257] Engine 000: Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 112.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 63.19 tokens/s, Drafted throughput: 243.96 tokens/s, Accepted: 632 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.635, 0.369, 0.180, 0.076, 0.035, Avg Draft acceptance rate: 25.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  89.90     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.56      
Output token throughput (tok/s):         111.24    
Peak output token throughput (tok/s):    52.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          191.63    
---------------Time to First Token----------------
Mean TTFT (ms):                          121.70    
Median TTFT (ms):                        121.21    
P99 TTFT (ms):                           157.33    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.18     
Median TPOT (ms):                        17.06     
P99 TPOT (ms):                           19.84     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.48     
Median ITL (ms):                         39.43     
P99 ITL (ms):                            46.28     
---------------Speculative Decoding---------------
Acceptance rate (%):                     26.18     
Acceptance length:                       2.31      
Drafts:                                  4329      
Draft tokens:                            21645     
Accepted tokens:                         5667      
Per-position acceptance (%):
  Position 0:                            63.78     
  Position 1:                            35.71     
  Position 2:                            18.46     
  Position 3:                            8.71      
  Position 4:                            4.25      
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:43 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 25.90 tokens/s, Drafted throughput: 97.49 tokens/s, Accepted: 259 tokens, Drafted: 975 tokens, Per-position acceptance rate: 0.641, 0.349, 0.190, 0.097, 0.051, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:06:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f11ed58efc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b1abd0a1-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:03 [loggers.py:257] Engine 000: Avg prompt throughput: 77.9 tokens/s, Avg generation throughput: 82.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 23.50 tokens/s, Drafted throughput: 89.25 tokens/s, Accepted: 470 tokens, Drafted: 1785 tokens, Per-position acceptance rate: 0.625, 0.370, 0.185, 0.095, 0.042, Avg Draft acceptance rate: 26.3%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:13 [loggers.py:257] Engine 000: Avg prompt throughput: 155.7 tokens/s, Avg generation throughput: 223.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 127.60 tokens/s, Drafted throughput: 480.00 tokens/s, Accepted: 1276 tokens, Drafted: 4800 tokens, Per-position acceptance rate: 0.644, 0.360, 0.192, 0.085, 0.048, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:23 [loggers.py:257] Engine 000: Avg prompt throughput: 188.9 tokens/s, Avg generation throughput: 221.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 124.98 tokens/s, Drafted throughput: 479.92 tokens/s, Accepted: 1250 tokens, Drafted: 4800 tokens, Per-position acceptance rate: 0.645, 0.349, 0.179, 0.089, 0.041, Avg Draft acceptance rate: 26.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:33 [loggers.py:257] Engine 000: Avg prompt throughput: 149.3 tokens/s, Avg generation throughput: 219.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 123.57 tokens/s, Drafted throughput: 482.89 tokens/s, Accepted: 1236 tokens, Drafted: 4830 tokens, Per-position acceptance rate: 0.642, 0.351, 0.177, 0.078, 0.032, Avg Draft acceptance rate: 25.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:43 [loggers.py:257] Engine 000: Avg prompt throughput: 160.4 tokens/s, Avg generation throughput: 218.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 122.88 tokens/s, Drafted throughput: 479.44 tokens/s, Accepted: 1229 tokens, Drafted: 4795 tokens, Per-position acceptance rate: 0.637, 0.367, 0.173, 0.070, 0.034, Avg Draft acceptance rate: 25.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  47.75     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.05      
Output token throughput (tok/s):         209.42    
Peak output token throughput (tok/s):    100.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          360.77    
---------------Time to First Token----------------
Mean TTFT (ms):                          122.49    
Median TTFT (ms):                        123.04    
P99 TTFT (ms):                           133.43    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.54     
Median TPOT (ms):                        17.46     
P99 TPOT (ms):                           20.37     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.10     
Median ITL (ms):                         39.99     
P99 ITL (ms):                            50.08     
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.95     
Acceptance length:                       2.30      
Drafts:                                  4353      
Draft tokens:                            21765     
Accepted tokens:                         5649      
Per-position acceptance (%):
  Position 0:                            63.98     
  Position 1:                            35.61     
  Position 2:                            18.08     
  Position 3:                            8.20      
  Position 4:                            3.91      
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:53 [loggers.py:257] Engine 000: Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 54.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:07:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 30.40 tokens/s, Drafted throughput: 118.99 tokens/s, Accepted: 304 tokens, Drafted: 1190 tokens, Per-position acceptance rate: 0.626, 0.324, 0.181, 0.097, 0.050, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f75a0df6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e185e6d0-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:13 [loggers.py:257] Engine 000: Avg prompt throughput: 136.8 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 44.25 tokens/s, Drafted throughput: 166.23 tokens/s, Accepted: 885 tokens, Drafted: 3325 tokens, Per-position acceptance rate: 0.632, 0.374, 0.197, 0.090, 0.038, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:23 [loggers.py:257] Engine 000: Avg prompt throughput: 350.4 tokens/s, Avg generation throughput: 426.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 239.58 tokens/s, Drafted throughput: 928.93 tokens/s, Accepted: 2396 tokens, Drafted: 9290 tokens, Per-position acceptance rate: 0.625, 0.343, 0.186, 0.092, 0.043, Avg Draft acceptance rate: 25.8%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:33 [loggers.py:257] Engine 000: Avg prompt throughput: 318.5 tokens/s, Avg generation throughput: 430.9 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 244.94 tokens/s, Drafted throughput: 929.28 tokens/s, Accepted: 2450 tokens, Drafted: 9295 tokens, Per-position acceptance rate: 0.635, 0.365, 0.190, 0.084, 0.044, Avg Draft acceptance rate: 26.4%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:43 [loggers.py:257] Engine 000: Avg prompt throughput: 283.6 tokens/s, Avg generation throughput: 426.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 237.48 tokens/s, Drafted throughput: 939.43 tokens/s, Accepted: 2375 tokens, Drafted: 9395 tokens, Per-position acceptance rate: 0.609, 0.343, 0.179, 0.087, 0.045, Avg Draft acceptance rate: 25.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  38.61     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              2.07      
Output token throughput (tok/s):         414.45    
Peak output token throughput (tok/s):    200.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          717.26    
---------------Time to First Token----------------
Mean TTFT (ms):                          127.55    
Median TTFT (ms):                        127.14    
P99 TTFT (ms):                           151.86    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.08     
Median TPOT (ms):                        18.16     
P99 TPOT (ms):                           21.05     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.39     
Median ITL (ms):                         41.15     
P99 ITL (ms):                            51.77     
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.96     
Acceptance length:                       2.30      
Drafts:                                  6952      
Draft tokens:                            34760     
Accepted tokens:                         9024      
Per-position acceptance (%):
  Position 0:                            62.63     
  Position 1:                            35.40     
  Position 2:                            18.61     
  Position 3:                            8.80      
  Position 4:                            4.36      
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:53 [loggers.py:257] Engine 000: Avg prompt throughput: 97.8 tokens/s, Avg generation throughput: 181.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:08:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 103.39 tokens/s, Drafted throughput: 388.96 tokens/s, Accepted: 1034 tokens, Drafted: 3890 tokens, Per-position acceptance rate: 0.645, 0.362, 0.183, 0.089, 0.050, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f171d50afc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-5cb38a12-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:13 [loggers.py:257] Engine 000: Avg prompt throughput: 233.6 tokens/s, Avg generation throughput: 240.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 68.70 tokens/s, Drafted throughput: 253.49 tokens/s, Accepted: 1374 tokens, Drafted: 5070 tokens, Per-position acceptance rate: 0.640, 0.373, 0.194, 0.098, 0.050, Avg Draft acceptance rate: 27.1%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:23 [loggers.py:257] Engine 000: Avg prompt throughput: 661.9 tokens/s, Avg generation throughput: 798.3 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 451.30 tokens/s, Drafted throughput: 1737.63 tokens/s, Accepted: 4514 tokens, Drafted: 17380 tokens, Per-position acceptance rate: 0.637, 0.361, 0.181, 0.081, 0.038, Avg Draft acceptance rate: 26.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:33 [loggers.py:257] Engine 000: Avg prompt throughput: 569.1 tokens/s, Avg generation throughput: 827.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 470.91 tokens/s, Drafted throughput: 1777.66 tokens/s, Accepted: 4710 tokens, Drafted: 17780 tokens, Per-position acceptance rate: 0.624, 0.363, 0.190, 0.098, 0.049, Avg Draft acceptance rate: 26.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:43 [loggers.py:257] Engine 000: Avg prompt throughput: 686.7 tokens/s, Avg generation throughput: 797.7 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 447.17 tokens/s, Drafted throughput: 1749.89 tokens/s, Accepted: 4472 tokens, Drafted: 17500 tokens, Per-position acceptance rate: 0.626, 0.348, 0.182, 0.082, 0.040, Avg Draft acceptance rate: 25.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  41.42     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.86      
Output token throughput (tok/s):         772.29    
Peak output token throughput (tok/s):    368.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          1360.39   
---------------Time to First Token----------------
Mean TTFT (ms):                          139.10    
Median TTFT (ms):                        135.18    
P99 TTFT (ms):                           196.13    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.06     
Median TPOT (ms):                        18.99     
P99 TPOT (ms):                           22.03     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.75     
Median ITL (ms):                         43.18     
P99 ITL (ms):                            55.22     
---------------Speculative Decoding---------------
Acceptance rate (%):                     26.11     
Acceptance length:                       2.31      
Drafts:                                  13869     
Draft tokens:                            69345     
Accepted tokens:                         18105     
Per-position acceptance (%):
  Position 0:                            62.79     
  Position 1:                            35.76     
  Position 2:                            18.59     
  Position 3:                            8.99      
  Position 4:                            4.42      
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:53 [loggers.py:257] Engine 000: Avg prompt throughput: 302.6 tokens/s, Avg generation throughput: 554.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:09:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 315.09 tokens/s, Drafted throughput: 1204.95 tokens/s, Accepted: 3151 tokens, Drafted: 12050 tokens, Per-position acceptance rate: 0.619, 0.352, 0.188, 0.100, 0.050, Avg Draft acceptance rate: 26.1%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f17915f2fc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-dc966e69-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:13 [loggers.py:257] Engine 000: Avg prompt throughput: 53.9 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 5.95 tokens/s, Drafted throughput: 22.25 tokens/s, Accepted: 119 tokens, Drafted: 445 tokens, Per-position acceptance rate: 0.629, 0.360, 0.180, 0.101, 0.067, Avg Draft acceptance rate: 26.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1323.6 tokens/s, Avg generation throughput: 1409.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 804.81 tokens/s, Drafted throughput: 3001.79 tokens/s, Accepted: 8050 tokens, Drafted: 30025 tokens, Per-position acceptance rate: 0.634, 0.368, 0.196, 0.095, 0.048, Avg Draft acceptance rate: 26.8%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1076.5 tokens/s, Avg generation throughput: 1458.3 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 822.25 tokens/s, Drafted throughput: 3180.81 tokens/s, Accepted: 8223 tokens, Drafted: 31810 tokens, Per-position acceptance rate: 0.627, 0.350, 0.181, 0.089, 0.046, Avg Draft acceptance rate: 25.9%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:43 [loggers.py:257] Engine 000: Avg prompt throughput: 964.7 tokens/s, Avg generation throughput: 1462.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 815.75 tokens/s, Drafted throughput: 3227.32 tokens/s, Accepted: 8158 tokens, Drafted: 32275 tokens, Per-position acceptance rate: 0.612, 0.343, 0.182, 0.084, 0.043, Avg Draft acceptance rate: 25.3%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1182.9 tokens/s, Avg generation throughput: 1442.4 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:10:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 811.08 tokens/s, Drafted throughput: 3162.64 tokens/s, Accepted: 8113 tokens, Drafted: 31635 tokens, Per-position acceptance rate: 0.621, 0.343, 0.185, 0.090, 0.043, Avg Draft acceptance rate: 25.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  46.19     
Total input tokens:                      48266     
Total generated tokens:                  63991     
Request throughput (req/s):              6.93      
Output token throughput (tok/s):         1385.42   
Peak output token throughput (tok/s):    704.00    
Peak concurrent requests:                50.00     
Total token throughput (tok/s):          2430.39   
---------------Time to First Token----------------
Mean TTFT (ms):                          159.22    
Median TTFT (ms):                        156.15    
P99 TTFT (ms):                           226.16    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.17     
Median TPOT (ms):                        21.06     
P99 TPOT (ms):                           25.24     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.22     
Median ITL (ms):                         45.58     
P99 ITL (ms):                            87.30     
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.76     
Acceptance length:                       2.29      
Drafts:                                  27954     
Draft tokens:                            139770    
Accepted tokens:                         36004     
Per-position acceptance (%):
  Position 0:                            62.20     
  Position 1:                            34.98     
  Position 2:                            18.37     
  Position 3:                            8.83      
  Position 4:                            4.42      
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:03 [loggers.py:257] Engine 000: Avg prompt throughput: 242.4 tokens/s, Avg generation throughput: 624.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 345.66 tokens/s, Drafted throughput: 1401.35 tokens/s, Accepted: 3457 tokens, Drafted: 14015 tokens, Per-position acceptance rate: 0.610, 0.341, 0.164, 0.079, 0.040, Avg Draft acceptance rate: 24.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7bf9726fc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-5809de99-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:23 [loggers.py:257] Engine 000: Avg prompt throughput: 948.2 tokens/s, Avg generation throughput: 309.0 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.69, Accepted throughput: 95.23 tokens/s, Drafted throughput: 281.18 tokens/s, Accepted: 1905 tokens, Drafted: 5625 tokens, Per-position acceptance rate: 0.737, 0.468, 0.276, 0.142, 0.070, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1228.5 tokens/s, Avg generation throughput: 2212.8 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 40.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1240.78 tokens/s, Drafted throughput: 4869.93 tokens/s, Accepted: 12408 tokens, Drafted: 48700 tokens, Per-position acceptance rate: 0.620, 0.348, 0.180, 0.084, 0.041, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1688.0 tokens/s, Avg generation throughput: 2108.8 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1193.57 tokens/s, Drafted throughput: 4567.97 tokens/s, Accepted: 11941 tokens, Drafted: 45700 tokens, Per-position acceptance rate: 0.621, 0.354, 0.192, 0.093, 0.047, Avg Draft acceptance rate: 26.1%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1737.6 tokens/s, Avg generation throughput: 2076.3 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:11:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1167.03 tokens/s, Drafted throughput: 4552.79 tokens/s, Accepted: 11676 tokens, Drafted: 45550 tokens, Per-position acceptance rate: 0.619, 0.347, 0.183, 0.088, 0.045, Avg Draft acceptance rate: 25.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1351.5 tokens/s, Avg generation throughput: 2196.8 tokens/s, Running: 58 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1225.36 tokens/s, Drafted throughput: 4854.65 tokens/s, Accepted: 12257 tokens, Drafted: 48560 tokens, Per-position acceptance rate: 0.617, 0.342, 0.178, 0.085, 0.039, Avg Draft acceptance rate: 25.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1648.0 tokens/s, Avg generation throughput: 2108.8 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1186.70 tokens/s, Drafted throughput: 4600.04 tokens/s, Accepted: 11872 tokens, Drafted: 46020 tokens, Per-position acceptance rate: 0.619, 0.350, 0.183, 0.093, 0.045, Avg Draft acceptance rate: 25.8%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:23 [loggers.py:257] Engine 000: Avg prompt throughput: 890.5 tokens/s, Avg generation throughput: 1799.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1006.53 tokens/s, Drafted throughput: 4000.74 tokens/s, Accepted: 10066 tokens, Drafted: 40010 tokens, Per-position acceptance rate: 0.616, 0.339, 0.174, 0.085, 0.044, Avg Draft acceptance rate: 25.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  62.55     
Total input tokens:                      94775     
Total generated tokens:                  127983    
Request throughput (req/s):              10.23     
Output token throughput (tok/s):         2046.02   
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                91.00     
Total token throughput (tok/s):          3561.15   
---------------Time to First Token----------------
Mean TTFT (ms):                          261.65    
Median TTFT (ms):                        238.78    
P99 TTFT (ms):                           670.24    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          28.56     
Median TPOT (ms):                        28.58     
P99 TPOT (ms):                           35.34     
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.00     
Median ITL (ms):                         56.06     
P99 ITL (ms):                            137.95    
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.74     
Acceptance length:                       2.29      
Drafts:                                  55957     
Draft tokens:                            279785    
Accepted tokens:                         72022     
Per-position acceptance (%):
  Position 0:                            62.13     
  Position 1:                            34.90     
  Position 2:                            18.38     
  Position 3:                            8.91      
  Position 4:                            4.40      
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 1.30 tokens/s, Drafted throughput: 5.50 tokens/s, Accepted: 13 tokens, Drafted: 55 tokens, Per-position acceptance rate: 0.636, 0.273, 0.182, 0.091, 0.000, Avg Draft acceptance rate: 23.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f571c46afc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-99aadc34-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1942.1 tokens/s, Avg generation throughput: 1308.3 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 73.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:12:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 383.71 tokens/s, Drafted throughput: 1322.36 tokens/s, Accepted: 7678 tokens, Drafted: 26460 tokens, Per-position acceptance rate: 0.666, 0.402, 0.219, 0.109, 0.053, Avg Draft acceptance rate: 29.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1922.0 tokens/s, Avg generation throughput: 2309.4 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1297.75 tokens/s, Drafted throughput: 5055.80 tokens/s, Accepted: 12978 tokens, Drafted: 50560 tokens, Per-position acceptance rate: 0.618, 0.348, 0.188, 0.087, 0.043, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1869.1 tokens/s, Avg generation throughput: 2338.0 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 62.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1328.68 tokens/s, Drafted throughput: 5055.41 tokens/s, Accepted: 13291 tokens, Drafted: 50570 tokens, Per-position acceptance rate: 0.623, 0.357, 0.192, 0.093, 0.049, Avg Draft acceptance rate: 26.3%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1713.3 tokens/s, Avg generation throughput: 2330.8 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1307.33 tokens/s, Drafted throughput: 5132.74 tokens/s, Accepted: 13074 tokens, Drafted: 51330 tokens, Per-position acceptance rate: 0.622, 0.347, 0.181, 0.085, 0.040, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1626.5 tokens/s, Avg generation throughput: 2354.0 tokens/s, Running: 122 reqs, Waiting: 0 reqs, GPU KV cache usage: 62.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1313.16 tokens/s, Drafted throughput: 5212.45 tokens/s, Accepted: 13133 tokens, Drafted: 52130 tokens, Per-position acceptance rate: 0.608, 0.338, 0.180, 0.090, 0.043, Avg Draft acceptance rate: 25.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1538.0 tokens/s, Avg generation throughput: 2370.7 tokens/s, Running: 122 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1328.84 tokens/s, Drafted throughput: 5210.59 tokens/s, Accepted: 13292 tokens, Drafted: 52120 tokens, Per-position acceptance rate: 0.617, 0.347, 0.180, 0.086, 0.045, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1465.8 tokens/s, Avg generation throughput: 2372.9 tokens/s, Running: 123 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:13:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1330.21 tokens/s, Drafted throughput: 5215.14 tokens/s, Accepted: 13303 tokens, Drafted: 52155 tokens, Per-position acceptance rate: 0.611, 0.345, 0.183, 0.091, 0.045, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:03 [loggers.py:257] Engine 000: Avg prompt throughput: 1672.2 tokens/s, Avg generation throughput: 2317.9 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 77.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1285.62 tokens/s, Drafted throughput: 5152.18 tokens/s, Accepted: 12857 tokens, Drafted: 51525 tokens, Per-position acceptance rate: 0.606, 0.336, 0.178, 0.086, 0.042, Avg Draft acceptance rate: 25.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1670.5 tokens/s, Avg generation throughput: 2311.1 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 80.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1294.87 tokens/s, Drafted throughput: 5079.03 tokens/s, Accepted: 12955 tokens, Drafted: 50815 tokens, Per-position acceptance rate: 0.616, 0.350, 0.179, 0.086, 0.044, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1747.7 tokens/s, Avg generation throughput: 2283.3 tokens/s, Running: 126 reqs, Waiting: 0 reqs, GPU KV cache usage: 77.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1281.56 tokens/s, Drafted throughput: 4997.77 tokens/s, Accepted: 12838 tokens, Drafted: 50065 tokens, Per-position acceptance rate: 0.620, 0.348, 0.184, 0.087, 0.043, Avg Draft acceptance rate: 25.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1751.4 tokens/s, Avg generation throughput: 2324.6 tokens/s, Running: 123 reqs, Waiting: 0 reqs, GPU KV cache usage: 76.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 1318.29 tokens/s, Drafted throughput: 5052.15 tokens/s, Accepted: 13193 tokens, Drafted: 50560 tokens, Per-position acceptance rate: 0.615, 0.356, 0.192, 0.093, 0.049, Avg Draft acceptance rate: 26.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  111.32    
Total input tokens:                      189093    
Total generated tokens:                  255968    
Request throughput (req/s):              11.50     
Output token throughput (tok/s):         2299.39   
Peak output token throughput (tok/s):    1408.00   
Peak concurrent requests:                163.00    
Total token throughput (tok/s):          3998.03   
---------------Time to First Token----------------
Mean TTFT (ms):                          484.41    
Median TTFT (ms):                        418.70    
P99 TTFT (ms):                           1624.79   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          52.02     
Median TPOT (ms):                        52.35     
P99 TPOT (ms):                           65.47     
---------------Inter-token Latency----------------
Mean ITL (ms):                           118.09    
Median ITL (ms):                         107.93    
P99 ITL (ms):                            218.24    
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.63     
Acceptance length:                       2.28      
Drafts:                                  112199    
Draft tokens:                            560995    
Accepted tokens:                         143809    
Per-position acceptance (%):
  Position 0:                            61.70     
  Position 1:                            34.79     
  Position 2:                            18.39     
  Position 3:                            8.86      
  Position 4:                            4.43      
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 984.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 533.27 tokens/s, Drafted throughput: 2313.89 tokens/s, Accepted: 5333 tokens, Drafted: 23140 tokens, Per-position acceptance rate: 0.594, 0.306, 0.148, 0.071, 0.034, Avg Draft acceptance rate: 23.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:14:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc9d42c6fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-baf03121-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:03 [loggers.py:257] Engine 000: Avg prompt throughput: 468.4 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 67 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 7.75 tokens/s, Drafted throughput: 26.25 tokens/s, Accepted: 155 tokens, Drafted: 525 tokens, Per-position acceptance rate: 0.648, 0.419, 0.267, 0.086, 0.057, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:13 [loggers.py:257] Engine 000: Avg prompt throughput: 3397.3 tokens/s, Avg generation throughput: 2092.1 tokens/s, Running: 173 reqs, Waiting: 83 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 1231.22 tokens/s, Drafted throughput: 4191.73 tokens/s, Accepted: 12313 tokens, Drafted: 41920 tokens, Per-position acceptance rate: 0.666, 0.406, 0.229, 0.112, 0.056, Avg Draft acceptance rate: 29.4%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:23 [loggers.py:257] Engine 000: Avg prompt throughput: 782.6 tokens/s, Avg generation throughput: 1741.8 tokens/s, Running: 188 reqs, Waiting: 58 reqs, GPU KV cache usage: 95.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 946.54 tokens/s, Drafted throughput: 3959.24 tokens/s, Accepted: 9466 tokens, Drafted: 39595 tokens, Per-position acceptance rate: 0.597, 0.320, 0.167, 0.075, 0.036, Avg Draft acceptance rate: 23.9%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:33 [loggers.py:257] Engine 000: Avg prompt throughput: 925.5 tokens/s, Avg generation throughput: 2442.3 tokens/s, Running: 159 reqs, Waiting: 94 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1372.27 tokens/s, Drafted throughput: 5343.10 tokens/s, Accepted: 13725 tokens, Drafted: 53440 tokens, Per-position acceptance rate: 0.619, 0.352, 0.180, 0.087, 0.045, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:43 [loggers.py:257] Engine 000: Avg prompt throughput: 2455.2 tokens/s, Avg generation throughput: 2285.6 tokens/s, Running: 180 reqs, Waiting: 75 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 1315.34 tokens/s, Drafted throughput: 4830.79 tokens/s, Accepted: 13154 tokens, Drafted: 48310 tokens, Per-position acceptance rate: 0.640, 0.368, 0.202, 0.102, 0.051, Avg Draft acceptance rate: 27.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1347.8 tokens/s, Avg generation throughput: 2015.1 tokens/s, Running: 186 reqs, Waiting: 59 reqs, GPU KV cache usage: 96.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:15:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1103.73 tokens/s, Drafted throughput: 4556.21 tokens/s, Accepted: 11038 tokens, Drafted: 45565 tokens, Per-position acceptance rate: 0.598, 0.324, 0.169, 0.083, 0.037, Avg Draft acceptance rate: 24.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:03 [loggers.py:257] Engine 000: Avg prompt throughput: 994.0 tokens/s, Avg generation throughput: 2481.2 tokens/s, Running: 144 reqs, Waiting: 106 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1382.92 tokens/s, Drafted throughput: 5486.18 tokens/s, Accepted: 13830 tokens, Drafted: 54865 tokens, Per-position acceptance rate: 0.614, 0.343, 0.176, 0.085, 0.042, Avg Draft acceptance rate: 25.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:13 [loggers.py:257] Engine 000: Avg prompt throughput: 2650.7 tokens/s, Avg generation throughput: 2045.9 tokens/s, Running: 195 reqs, Waiting: 61 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 1181.52 tokens/s, Drafted throughput: 4288.32 tokens/s, Accepted: 11824 tokens, Drafted: 42915 tokens, Per-position acceptance rate: 0.633, 0.373, 0.213, 0.104, 0.055, Avg Draft acceptance rate: 27.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:23 [loggers.py:257] Engine 000: Avg prompt throughput: 970.1 tokens/s, Avg generation throughput: 2095.3 tokens/s, Running: 163 reqs, Waiting: 78 reqs, GPU KV cache usage: 90.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 1124.94 tokens/s, Drafted throughput: 4840.24 tokens/s, Accepted: 11250 tokens, Drafted: 48405 tokens, Per-position acceptance rate: 0.585, 0.312, 0.156, 0.076, 0.033, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:33 [loggers.py:257] Engine 000: Avg prompt throughput: 1520.5 tokens/s, Avg generation throughput: 2399.8 tokens/s, Running: 149 reqs, Waiting: 106 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 1354.97 tokens/s, Drafted throughput: 5208.02 tokens/s, Accepted: 13551 tokens, Drafted: 52085 tokens, Per-position acceptance rate: 0.624, 0.355, 0.187, 0.089, 0.046, Avg Draft acceptance rate: 26.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:43 [loggers.py:257] Engine 000: Avg prompt throughput: 2591.7 tokens/s, Avg generation throughput: 1902.3 tokens/s, Running: 204 reqs, Waiting: 52 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1088.48 tokens/s, Drafted throughput: 4029.06 tokens/s, Accepted: 10886 tokens, Drafted: 40295 tokens, Per-position acceptance rate: 0.634, 0.366, 0.203, 0.098, 0.050, Avg Draft acceptance rate: 27.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:53 [loggers.py:257] Engine 000: Avg prompt throughput: 363.0 tokens/s, Avg generation throughput: 2280.7 tokens/s, Running: 163 reqs, Waiting: 81 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:16:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 1240.08 tokens/s, Drafted throughput: 5208.91 tokens/s, Accepted: 12401 tokens, Drafted: 52090 tokens, Per-position acceptance rate: 0.594, 0.326, 0.157, 0.075, 0.038, Avg Draft acceptance rate: 23.8%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:03 [loggers.py:257] Engine 000: Avg prompt throughput: 2068.8 tokens/s, Avg generation throughput: 2297.8 tokens/s, Running: 156 reqs, Waiting: 95 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1317.78 tokens/s, Drafted throughput: 4880.67 tokens/s, Accepted: 13180 tokens, Drafted: 48815 tokens, Per-position acceptance rate: 0.635, 0.365, 0.202, 0.098, 0.049, Avg Draft acceptance rate: 27.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:13 [loggers.py:257] Engine 000: Avg prompt throughput: 2392.9 tokens/s, Avg generation throughput: 1841.8 tokens/s, Running: 215 reqs, Waiting: 33 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1031.00 tokens/s, Drafted throughput: 4016.99 tokens/s, Accepted: 10310 tokens, Drafted: 40170 tokens, Per-position acceptance rate: 0.618, 0.348, 0.186, 0.086, 0.044, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:23 [loggers.py:257] Engine 000: Avg prompt throughput: 252.1 tokens/s, Avg generation throughput: 2525.7 tokens/s, Running: 133 reqs, Waiting: 115 reqs, GPU KV cache usage: 94.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 1393.13 tokens/s, Drafted throughput: 5674.32 tokens/s, Accepted: 13933 tokens, Drafted: 56750 tokens, Per-position acceptance rate: 0.609, 0.336, 0.165, 0.080, 0.038, Avg Draft acceptance rate: 24.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:33 [loggers.py:257] Engine 000: Avg prompt throughput: 2422.8 tokens/s, Avg generation throughput: 2133.6 tokens/s, Running: 166 reqs, Waiting: 88 reqs, GPU KV cache usage: 98.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1231.45 tokens/s, Drafted throughput: 4482.45 tokens/s, Accepted: 12316 tokens, Drafted: 44830 tokens, Per-position acceptance rate: 0.637, 0.374, 0.206, 0.105, 0.052, Avg Draft acceptance rate: 27.5%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:43 [loggers.py:257] Engine 000: Avg prompt throughput: 1880.9 tokens/s, Avg generation throughput: 1880.3 tokens/s, Running: 207 reqs, Waiting: 41 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1042.88 tokens/s, Drafted throughput: 4153.04 tokens/s, Accepted: 10430 tokens, Drafted: 41535 tokens, Per-position acceptance rate: 0.604, 0.340, 0.180, 0.089, 0.042, Avg Draft acceptance rate: 25.1%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:53 [loggers.py:257] Engine 000: Avg prompt throughput: 651.2 tokens/s, Avg generation throughput: 2637.5 tokens/s, Running: 140 reqs, Waiting: 112 reqs, GPU KV cache usage: 98.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:17:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1476.33 tokens/s, Drafted throughput: 5807.72 tokens/s, Accepted: 14764 tokens, Drafted: 58080 tokens, Per-position acceptance rate: 0.617, 0.348, 0.178, 0.088, 0.041, Avg Draft acceptance rate: 25.4%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:03 [loggers.py:257] Engine 000: Avg prompt throughput: 2507.2 tokens/s, Avg generation throughput: 2028.9 tokens/s, Running: 187 reqs, Waiting: 69 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1155.90 tokens/s, Drafted throughput: 4316.12 tokens/s, Accepted: 11568 tokens, Drafted: 43195 tokens, Per-position acceptance rate: 0.635, 0.368, 0.199, 0.092, 0.045, Avg Draft acceptance rate: 26.8%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:13 [loggers.py:257] Engine 000: Avg prompt throughput: 1202.1 tokens/s, Avg generation throughput: 2024.9 tokens/s, Running: 187 reqs, Waiting: 58 reqs, GPU KV cache usage: 96.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1110.31 tokens/s, Drafted throughput: 4551.02 tokens/s, Accepted: 11103 tokens, Drafted: 45510 tokens, Per-position acceptance rate: 0.606, 0.327, 0.169, 0.079, 0.038, Avg Draft acceptance rate: 24.4%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:23 [loggers.py:257] Engine 000: Avg prompt throughput: 1280.9 tokens/s, Avg generation throughput: 2542.0 tokens/s, Running: 147 reqs, Waiting: 109 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1449.40 tokens/s, Drafted throughput: 5454.61 tokens/s, Accepted: 14495 tokens, Drafted: 54550 tokens, Per-position acceptance rate: 0.626, 0.362, 0.196, 0.096, 0.048, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:33 [loggers.py:257] Engine 000: Avg prompt throughput: 2529.9 tokens/s, Avg generation throughput: 1865.1 tokens/s, Running: 208 reqs, Waiting: 48 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1061.84 tokens/s, Drafted throughput: 3973.78 tokens/s, Accepted: 10619 tokens, Drafted: 39740 tokens, Per-position acceptance rate: 0.637, 0.353, 0.199, 0.099, 0.048, Avg Draft acceptance rate: 26.7%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:43 [loggers.py:257] Engine 000: Avg prompt throughput: 164.0 tokens/s, Avg generation throughput: 2322.6 tokens/s, Running: 160 reqs, Waiting: 80 reqs, GPU KV cache usage: 93.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 1270.70 tokens/s, Drafted throughput: 5269.19 tokens/s, Accepted: 12709 tokens, Drafted: 52700 tokens, Per-position acceptance rate: 0.610, 0.326, 0.157, 0.077, 0.035, Avg Draft acceptance rate: 24.1%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:53 [loggers.py:257] Engine 000: Avg prompt throughput: 1514.2 tokens/s, Avg generation throughput: 2521.6 tokens/s, Running: 150 reqs, Waiting: 0 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1430.33 tokens/s, Drafted throughput: 5444.09 tokens/s, Accepted: 14307 tokens, Drafted: 54455 tokens, Per-position acceptance rate: 0.631, 0.355, 0.189, 0.093, 0.047, Avg Draft acceptance rate: 26.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  234.16    
Total input tokens:                      373233    
Total generated tokens:                  511918    
Request throughput (req/s):              10.93     
Output token throughput (tok/s):         2186.14   
Peak output token throughput (tok/s):    1492.00   
Peak concurrent requests:                288.00    
Total token throughput (tok/s):          3780.03   
---------------Time to First Token----------------
Mean TTFT (ms):                          5851.31   
Median TTFT (ms):                        3923.65   
P99 TTFT (ms):                           13259.45  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          85.63     
Median TPOT (ms):                        81.13     
P99 TPOT (ms):                           140.04    
---------------Inter-token Latency----------------
Mean ITL (ms):                           194.23    
Median ITL (ms):                         150.24    
P99 ITL (ms):                            415.54    
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.69     
Acceptance length:                       2.28      
Drafts:                                  223697    
Draft tokens:                            1118485   
Accepted tokens:                         287329    
Per-position acceptance (%):
  Position 0:                            61.93     
  Position 1:                            34.85     
  Position 2:                            18.37     
  Position 3:                            8.90      
  Position 4:                            4.39      
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k5-t0.0-tp1...
[0;36m(APIServer pid=3338824)[0;0m INFO 01-23 01:18:57 [launcher.py:110] Shutting down FastAPI HTTP server.
