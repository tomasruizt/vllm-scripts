Removing any existing container named vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k6-t0.0-tp1...
Creating new container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k6-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 254728
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:12 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:12 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15035, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=254728)[0;0m WARNING 01-23 19:47:12 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:13 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:13 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:14 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:14 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:14 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:14 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:47:14 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f51f1caefc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9169ebd0-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 19:47:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:47:26 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=6), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:47:26 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.68:53289 backend=nccl
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:47:26 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=254841)[0;0m WARNING 01-23 19:47:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:47:27 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:47:28 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 19:47:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:47:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:22 [default_loader.py:291] Loading weights took 52.45 seconds
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:22 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:23 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 19:48:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:26 [default_loader.py:291] Loading weights took 2.65 seconds
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:28 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:29 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
WARNING 01-23 19:48:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:30 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 61.229478 seconds
WARNING 01-23 19:48:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:42 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:48:42 [backends.py:704] Dynamo bytecode transform time: 12.50 s
WARNING 01-23 19:48:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:48:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.533 s
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:00 [monitor.py:34] torch.compile takes 17.03 s in total
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:00 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:00 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:01 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.130 s
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:01 [monitor.py:34] torch.compile takes 17.64 s in total
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:02 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:02 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:02 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 19:49:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
WARNING 01-23 19:49:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:13 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.67 GiB
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:13 [core.py:272] init engine (profile, create kv cache, warmup model) took 43.33 seconds
WARNING 01-23 19:49:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15035)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15035 ssl:default [Connect call failed (\'127.0.0.1\', 15035)]\n''
[0;36m(EngineCore_DP0 pid=254841)[0;0m INFO 01-23 19:49:14 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:15 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=254728)[0;0m WARNING 01-23 19:49:15 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:15 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:15 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:15 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [serving.py:221] Chat template warmup completed in 1751.4ms
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15035
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:17 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:28 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 32.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 19.44 tokens/s, Drafted throughput: 83.90 tokens/s, Accepted: 253 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.637, 0.374, 0.170, 0.099, 0.066, 0.044, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:38 [loggers.py:257] Engine 000: Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 55.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 30.50 tokens/s, Drafted throughput: 147.58 tokens/s, Accepted: 305 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.618, 0.309, 0.159, 0.081, 0.041, 0.033, Avg Draft acceptance rate: 20.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:48 [loggers.py:257] Engine 000: Avg prompt throughput: 45.5 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 34.90 tokens/s, Drafted throughput: 145.79 tokens/s, Accepted: 349 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.658, 0.383, 0.206, 0.111, 0.058, 0.021, Avg Draft acceptance rate: 23.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:58 [loggers.py:257] Engine 000: Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:49:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 28.60 tokens/s, Drafted throughput: 145.79 tokens/s, Accepted: 286 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.609, 0.305, 0.169, 0.066, 0.025, 0.004, Avg Draft acceptance rate: 19.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:08 [loggers.py:257] Engine 000: Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 34.70 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 347 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.615, 0.357, 0.213, 0.123, 0.074, 0.041, Avg Draft acceptance rate: 23.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:18 [loggers.py:257] Engine 000: Avg prompt throughput: 37.5 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 33.60 tokens/s, Drafted throughput: 145.78 tokens/s, Accepted: 336 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.621, 0.379, 0.218, 0.099, 0.045, 0.021, Avg Draft acceptance rate: 23.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:28 [loggers.py:257] Engine 000: Avg prompt throughput: 55.4 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 32.00 tokens/s, Drafted throughput: 145.79 tokens/s, Accepted: 320 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.646, 0.333, 0.177, 0.091, 0.041, 0.029, Avg Draft acceptance rate: 21.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:38 [loggers.py:257] Engine 000: Avg prompt throughput: 47.1 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 37.70 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 377 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.735, 0.420, 0.220, 0.106, 0.041, 0.016, Avg Draft acceptance rate: 25.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:48 [loggers.py:257] Engine 000: Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 31.00 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 310 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.615, 0.307, 0.180, 0.086, 0.053, 0.029, Avg Draft acceptance rate: 21.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:58 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:50:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 30.50 tokens/s, Drafted throughput: 147.60 tokens/s, Accepted: 305 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.577, 0.313, 0.175, 0.110, 0.045, 0.020, Avg Draft acceptance rate: 20.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:08 [loggers.py:257] Engine 000: Avg prompt throughput: 53.6 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 34.70 tokens/s, Drafted throughput: 146.38 tokens/s, Accepted: 347 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.672, 0.414, 0.201, 0.094, 0.029, 0.012, Avg Draft acceptance rate: 23.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:18 [loggers.py:257] Engine 000: Avg prompt throughput: 44.1 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 31.40 tokens/s, Drafted throughput: 146.38 tokens/s, Accepted: 314 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.631, 0.344, 0.180, 0.078, 0.041, 0.012, Avg Draft acceptance rate: 21.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:28 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 30.00 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 300 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.602, 0.336, 0.160, 0.082, 0.037, 0.012, Avg Draft acceptance rate: 20.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:38 [loggers.py:257] Engine 000: Avg prompt throughput: 30.8 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 30.10 tokens/s, Drafted throughput: 147.58 tokens/s, Accepted: 301 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.622, 0.341, 0.179, 0.049, 0.028, 0.004, Avg Draft acceptance rate: 20.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:48 [loggers.py:257] Engine 000: Avg prompt throughput: 69.6 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 37.20 tokens/s, Drafted throughput: 144.00 tokens/s, Accepted: 372 tokens, Drafted: 1440 tokens, Per-position acceptance rate: 0.679, 0.442, 0.225, 0.096, 0.058, 0.050, Avg Draft acceptance rate: 25.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:58 [loggers.py:257] Engine 000: Avg prompt throughput: 17.4 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:51:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 24.10 tokens/s, Drafted throughput: 147.58 tokens/s, Accepted: 241 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.537, 0.260, 0.093, 0.049, 0.028, 0.012, Avg Draft acceptance rate: 16.3%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:08 [loggers.py:257] Engine 000: Avg prompt throughput: 48.4 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 33.10 tokens/s, Drafted throughput: 146.38 tokens/s, Accepted: 331 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.631, 0.377, 0.197, 0.086, 0.037, 0.029, Avg Draft acceptance rate: 22.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:18 [loggers.py:257] Engine 000: Avg prompt throughput: 33.8 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 35.10 tokens/s, Drafted throughput: 145.78 tokens/s, Accepted: 351 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.663, 0.387, 0.214, 0.095, 0.053, 0.033, Avg Draft acceptance rate: 24.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  177.02    
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.28      
Output token throughput (tok/s):         56.49     
Peak output token throughput (tok/s):    25.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          97.32     
---------------Time to First Token----------------
Mean TTFT (ms):                          75.82     
Median TTFT (ms):                        84.28     
P99 TTFT (ms):                           91.92     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.33     
Median TPOT (ms):                        17.23     
P99 TPOT (ms):                           20.65     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.10     
Median ITL (ms):                         40.11     
P99 ITL (ms):                            40.63     
---------------Speculative Decoding---------------
Acceptance rate (%):                     22.11     
Acceptance length:                       2.33      
Drafts:                                  4300      
Draft tokens:                            25800     
Accepted tokens:                         5704      
Per-position acceptance (%):
  Position 0:                            63.19     
  Position 1:                            35.40     
  Position 2:                            18.60     
  Position 3:                            8.84      
  Position 4:                            4.37      
  Position 5:                            2.26      
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 7.70 tokens/s, Drafted throughput: 33.60 tokens/s, Accepted: 77 tokens, Drafted: 336 tokens, Per-position acceptance rate: 0.661, 0.357, 0.196, 0.089, 0.054, 0.018, Avg Draft acceptance rate: 22.9%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f956bd12fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8efce7cc-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:38 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 2.40 tokens/s, Drafted throughput: 9.00 tokens/s, Accepted: 24 tokens, Drafted: 90 tokens, Per-position acceptance rate: 0.667, 0.467, 0.200, 0.133, 0.067, 0.067, Avg Draft acceptance rate: 26.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:48 [loggers.py:257] Engine 000: Avg prompt throughput: 59.6 tokens/s, Avg generation throughput: 88.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 50.09 tokens/s, Drafted throughput: 228.57 tokens/s, Accepted: 501 tokens, Drafted: 2286 tokens, Per-position acceptance rate: 0.622, 0.344, 0.189, 0.089, 0.045, 0.026, Avg Draft acceptance rate: 21.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:58 [loggers.py:257] Engine 000: Avg prompt throughput: 77.6 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:52:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 61.10 tokens/s, Drafted throughput: 285.60 tokens/s, Accepted: 611 tokens, Drafted: 2856 tokens, Per-position acceptance rate: 0.626, 0.351, 0.181, 0.080, 0.036, 0.011, Avg Draft acceptance rate: 21.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:08 [loggers.py:257] Engine 000: Avg prompt throughput: 78.1 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 66.40 tokens/s, Drafted throughput: 286.78 tokens/s, Accepted: 664 tokens, Drafted: 2868 tokens, Per-position acceptance rate: 0.646, 0.366, 0.199, 0.092, 0.054, 0.031, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:18 [loggers.py:257] Engine 000: Avg prompt throughput: 102.5 tokens/s, Avg generation throughput: 111.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 64.19 tokens/s, Drafted throughput: 285.57 tokens/s, Accepted: 642 tokens, Drafted: 2856 tokens, Per-position acceptance rate: 0.676, 0.351, 0.183, 0.084, 0.036, 0.019, Avg Draft acceptance rate: 22.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:28 [loggers.py:257] Engine 000: Avg prompt throughput: 45.6 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 63.39 tokens/s, Drafted throughput: 292.75 tokens/s, Accepted: 634 tokens, Drafted: 2928 tokens, Per-position acceptance rate: 0.615, 0.344, 0.168, 0.098, 0.049, 0.025, Avg Draft acceptance rate: 21.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:38 [loggers.py:257] Engine 000: Avg prompt throughput: 105.5 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 63.89 tokens/s, Drafted throughput: 284.36 tokens/s, Accepted: 639 tokens, Drafted: 2844 tokens, Per-position acceptance rate: 0.648, 0.371, 0.179, 0.084, 0.042, 0.023, Avg Draft acceptance rate: 22.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:48 [loggers.py:257] Engine 000: Avg prompt throughput: 84.6 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 59.49 tokens/s, Drafted throughput: 286.77 tokens/s, Accepted: 595 tokens, Drafted: 2868 tokens, Per-position acceptance rate: 0.617, 0.328, 0.176, 0.082, 0.036, 0.006, Avg Draft acceptance rate: 20.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:58 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:53:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 58.99 tokens/s, Drafted throughput: 287.37 tokens/s, Accepted: 590 tokens, Drafted: 2874 tokens, Per-position acceptance rate: 0.628, 0.336, 0.157, 0.063, 0.033, 0.015, Avg Draft acceptance rate: 20.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:08 [loggers.py:257] Engine 000: Avg prompt throughput: 76.3 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 59.69 tokens/s, Drafted throughput: 287.37 tokens/s, Accepted: 597 tokens, Drafted: 2874 tokens, Per-position acceptance rate: 0.618, 0.351, 0.163, 0.065, 0.031, 0.019, Avg Draft acceptance rate: 20.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  91.60     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              0.55      
Output token throughput (tok/s):         109.17    
Peak output token throughput (tok/s):    51.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          188.07    
---------------Time to First Token----------------
Mean TTFT (ms):                          122.02    
Median TTFT (ms):                        123.32    
P99 TTFT (ms):                           155.43    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.59     
Median TPOT (ms):                        17.44     
P99 TPOT (ms):                           20.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.37     
Median ITL (ms):                         40.33     
P99 ITL (ms):                            47.18     
---------------Speculative Decoding---------------
Acceptance rate (%):                     21.78     
Acceptance length:                       2.31      
Drafts:                                  4335      
Draft tokens:                            26010     
Accepted tokens:                         5664      
Per-position acceptance (%):
  Position 0:                            63.48     
  Position 1:                            34.99     
  Position 2:                            17.88     
  Position 3:                            8.28      
  Position 4:                            4.04      
  Position 5:                            1.98      
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 50...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:18 [loggers.py:257] Engine 000: Avg prompt throughput: 18.5 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 28.50 tokens/s, Drafted throughput: 118.19 tokens/s, Accepted: 285 tokens, Drafted: 1182 tokens, Per-position acceptance rate: 0.665, 0.360, 0.213, 0.112, 0.056, 0.041, Avg Draft acceptance rate: 24.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f62734b6fc0>, seed=0, num_prompts=50, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-31b2b72c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:38 [loggers.py:257] Engine 000: Avg prompt throughput: 123.4 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 32.15 tokens/s, Drafted throughput: 138.30 tokens/s, Accepted: 643 tokens, Drafted: 2766 tokens, Per-position acceptance rate: 0.646, 0.371, 0.197, 0.100, 0.052, 0.028, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:48 [loggers.py:257] Engine 000: Avg prompt throughput: 135.6 tokens/s, Avg generation throughput: 220.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 125.89 tokens/s, Drafted throughput: 565.76 tokens/s, Accepted: 1259 tokens, Drafted: 5658 tokens, Per-position acceptance rate: 0.632, 0.365, 0.192, 0.081, 0.042, 0.023, Avg Draft acceptance rate: 22.3%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:58 [loggers.py:257] Engine 000: Avg prompt throughput: 163.5 tokens/s, Avg generation throughput: 218.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:54:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 125.09 tokens/s, Drafted throughput: 563.97 tokens/s, Accepted: 1251 tokens, Drafted: 5640 tokens, Per-position acceptance rate: 0.640, 0.343, 0.181, 0.098, 0.043, 0.027, Avg Draft acceptance rate: 22.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:08 [loggers.py:257] Engine 000: Avg prompt throughput: 164.0 tokens/s, Avg generation throughput: 217.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 124.29 tokens/s, Drafted throughput: 560.95 tokens/s, Accepted: 1243 tokens, Drafted: 5610 tokens, Per-position acceptance rate: 0.644, 0.361, 0.186, 0.084, 0.039, 0.015, Avg Draft acceptance rate: 22.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:18 [loggers.py:257] Engine 000: Avg prompt throughput: 154.5 tokens/s, Avg generation throughput: 218.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 126.38 tokens/s, Drafted throughput: 553.73 tokens/s, Accepted: 1264 tokens, Drafted: 5538 tokens, Per-position acceptance rate: 0.644, 0.371, 0.191, 0.086, 0.046, 0.034, Avg Draft acceptance rate: 22.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     50        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  47.19     
Total input tokens:                      7227      
Total generated tokens:                  10000     
Request throughput (req/s):              1.06      
Output token throughput (tok/s):         211.90    
Peak output token throughput (tok/s):    100.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          365.04    
---------------Time to First Token----------------
Mean TTFT (ms):                          127.45    
Median TTFT (ms):                        125.66    
P99 TTFT (ms):                           163.02    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.63     
Median TPOT (ms):                        17.78     
P99 TPOT (ms):                           20.37     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.13     
Median ITL (ms):                         41.01     
P99 ITL (ms):                            51.45     
---------------Speculative Decoding---------------
Acceptance rate (%):                     22.44     
Acceptance length:                       2.35      
Drafts:                                  4265      
Draft tokens:                            25590     
Accepted tokens:                         5742      
Per-position acceptance (%):
  Position 0:                            64.01     
  Position 1:                            36.01     
  Position 2:                            18.92     
  Position 3:                            8.89      
  Position 4:                            4.31      
  Position 5:                            2.49      
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 20.00 tokens/s, Drafted throughput: 89.39 tokens/s, Accepted: 200 tokens, Drafted: 894 tokens, Per-position acceptance rate: 0.617, 0.336, 0.195, 0.107, 0.054, 0.034, Avg Draft acceptance rate: 22.4%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f068f4cafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3ee0d2c6-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:38 [loggers.py:257] Engine 000: Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 0.30 tokens/s, Drafted throughput: 0.60 tokens/s, Accepted: 3 tokens, Drafted: 6 tokens, Per-position acceptance rate: 1.000, 1.000, 1.000, 0.000, 0.000, 0.000, Avg Draft acceptance rate: 50.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:48 [loggers.py:257] Engine 000: Avg prompt throughput: 215.3 tokens/s, Avg generation throughput: 285.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 164.78 tokens/s, Drafted throughput: 721.70 tokens/s, Accepted: 1648 tokens, Drafted: 7218 tokens, Per-position acceptance rate: 0.643, 0.367, 0.190, 0.092, 0.048, 0.030, Avg Draft acceptance rate: 22.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:58 [loggers.py:257] Engine 000: Avg prompt throughput: 338.1 tokens/s, Avg generation throughput: 415.2 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:55:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 233.06 tokens/s, Drafted throughput: 1092.43 tokens/s, Accepted: 2331 tokens, Drafted: 10926 tokens, Per-position acceptance rate: 0.619, 0.349, 0.173, 0.083, 0.037, 0.020, Avg Draft acceptance rate: 21.3%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:08 [loggers.py:257] Engine 000: Avg prompt throughput: 270.3 tokens/s, Avg generation throughput: 425.3 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 242.16 tokens/s, Drafted throughput: 1103.20 tokens/s, Accepted: 2422 tokens, Drafted: 11034 tokens, Per-position acceptance rate: 0.619, 0.346, 0.181, 0.089, 0.051, 0.031, Avg Draft acceptance rate: 22.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:18 [loggers.py:257] Engine 000: Avg prompt throughput: 345.0 tokens/s, Avg generation throughput: 427.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 245.07 tokens/s, Drafted throughput: 1096.67 tokens/s, Accepted: 2451 tokens, Drafted: 10968 tokens, Per-position acceptance rate: 0.620, 0.368, 0.189, 0.090, 0.045, 0.028, Avg Draft acceptance rate: 22.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  39.64     
Total input tokens:                      11690     
Total generated tokens:                  16000     
Request throughput (req/s):              2.02      
Output token throughput (tok/s):         403.64    
Peak output token throughput (tok/s):    192.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          698.55    
---------------Time to First Token----------------
Mean TTFT (ms):                          131.45    
Median TTFT (ms):                        129.47    
P99 TTFT (ms):                           162.72    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.25     
Median TPOT (ms):                        18.38     
P99 TPOT (ms):                           21.35     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.06     
Median ITL (ms):                         41.64     
P99 ITL (ms):                            53.10     
---------------Speculative Decoding---------------
Acceptance rate (%):                     21.97     
Acceptance length:                       2.32      
Drafts:                                  6907      
Draft tokens:                            41442     
Accepted tokens:                         9105      
Per-position acceptance (%):
  Position 0:                            62.31     
  Position 1:                            35.51     
  Position 2:                            18.20     
  Position 3:                            8.73      
  Position 4:                            4.44      
  Position 5:                            2.62      
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 160...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 36.80 tokens/s, Drafted throughput: 180.59 tokens/s, Accepted: 368 tokens, Drafted: 1806 tokens, Per-position acceptance rate: 0.611, 0.322, 0.163, 0.070, 0.040, 0.017, Avg Draft acceptance rate: 20.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f94d1c06fc0>, seed=0, num_prompts=160, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d50ff78c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:48 [loggers.py:257] Engine 000: Avg prompt throughput: 487.1 tokens/s, Avg generation throughput: 485.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 140.64 tokens/s, Drafted throughput: 608.35 tokens/s, Accepted: 2813 tokens, Drafted: 12168 tokens, Per-position acceptance rate: 0.642, 0.378, 0.200, 0.094, 0.047, 0.026, Avg Draft acceptance rate: 23.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:58 [loggers.py:257] Engine 000: Avg prompt throughput: 547.2 tokens/s, Avg generation throughput: 804.1 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:56:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 455.75 tokens/s, Drafted throughput: 2091.37 tokens/s, Accepted: 4558 tokens, Drafted: 20916 tokens, Per-position acceptance rate: 0.627, 0.347, 0.180, 0.085, 0.045, 0.024, Avg Draft acceptance rate: 21.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:08 [loggers.py:257] Engine 000: Avg prompt throughput: 649.9 tokens/s, Avg generation throughput: 792.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 446.33 tokens/s, Drafted throughput: 2074.46 tokens/s, Accepted: 4464 tokens, Drafted: 20748 tokens, Per-position acceptance rate: 0.615, 0.344, 0.181, 0.087, 0.042, 0.022, Avg Draft acceptance rate: 21.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:18 [loggers.py:257] Engine 000: Avg prompt throughput: 567.5 tokens/s, Avg generation throughput: 805.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 458.11 tokens/s, Drafted throughput: 2092.97 tokens/s, Accepted: 4582 tokens, Drafted: 20934 tokens, Per-position acceptance rate: 0.619, 0.352, 0.184, 0.090, 0.044, 0.025, Avg Draft acceptance rate: 21.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     160       
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  41.81     
Total input tokens:                      24361     
Total generated tokens:                  31991     
Request throughput (req/s):              3.83      
Output token throughput (tok/s):         765.19    
Peak output token throughput (tok/s):    368.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          1347.87   
---------------Time to First Token----------------
Mean TTFT (ms):                          139.51    
Median TTFT (ms):                        138.41    
P99 TTFT (ms):                           176.14    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.20     
Median TPOT (ms):                        19.28     
P99 TPOT (ms):                           22.48     
---------------Inter-token Latency----------------
Mean ITL (ms):                           44.40     
Median ITL (ms):                         43.59     
P99 ITL (ms):                            86.08     
---------------Speculative Decoding---------------
Acceptance rate (%):                     22.08     
Acceptance length:                       2.32      
Drafts:                                  13765     
Draft tokens:                            82590     
Accepted tokens:                         18235     
Per-position acceptance (%):
  Position 0:                            62.40     
  Position 1:                            35.37     
  Position 2:                            18.68     
  Position 3:                            9.10      
  Position 4:                            4.50      
  Position 5:                            2.43      
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 320...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:28 [loggers.py:257] Engine 000: Avg prompt throughput: 202.3 tokens/s, Avg generation throughput: 331.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 193.59 tokens/s, Drafted throughput: 833.97 tokens/s, Accepted: 1936 tokens, Drafted: 8340 tokens, Per-position acceptance rate: 0.629, 0.362, 0.205, 0.114, 0.055, 0.028, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd2622defc0>, seed=0, num_prompts=320, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9c6fec90-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:48 [loggers.py:257] Engine 000: Avg prompt throughput: 487.1 tokens/s, Avg generation throughput: 450.2 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 131.69 tokens/s, Drafted throughput: 551.95 tokens/s, Accepted: 2634 tokens, Drafted: 11040 tokens, Per-position acceptance rate: 0.655, 0.383, 0.204, 0.105, 0.055, 0.030, Avg Draft acceptance rate: 23.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1042.1 tokens/s, Avg generation throughput: 1409.8 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:57:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 805.51 tokens/s, Drafted throughput: 3614.13 tokens/s, Accepted: 8057 tokens, Drafted: 36150 tokens, Per-position acceptance rate: 0.628, 0.361, 0.189, 0.090, 0.045, 0.025, Avg Draft acceptance rate: 22.3%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1146.8 tokens/s, Avg generation throughput: 1378.7 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 786.62 tokens/s, Drafted throughput: 3555.26 tokens/s, Accepted: 7867 tokens, Drafted: 35556 tokens, Per-position acceptance rate: 0.627, 0.351, 0.192, 0.091, 0.044, 0.023, Avg Draft acceptance rate: 22.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1092.6 tokens/s, Avg generation throughput: 1389.1 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 792.51 tokens/s, Drafted throughput: 3592.87 tokens/s, Accepted: 7925 tokens, Drafted: 35928 tokens, Per-position acceptance rate: 0.624, 0.351, 0.190, 0.090, 0.046, 0.022, Avg Draft acceptance rate: 22.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1059.0 tokens/s, Avg generation throughput: 1396.8 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 791.97 tokens/s, Drafted throughput: 3620.27 tokens/s, Accepted: 7920 tokens, Drafted: 36204 tokens, Per-position acceptance rate: 0.624, 0.348, 0.183, 0.088, 0.046, 0.024, Avg Draft acceptance rate: 21.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     320       
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  47.87     
Total input tokens:                      48266     
Total generated tokens:                  63976     
Request throughput (req/s):              6.68      
Output token throughput (tok/s):         1336.35   
Peak output token throughput (tok/s):    672.00    
Peak concurrent requests:                47.00     
Total token throughput (tok/s):          2344.55   
---------------Time to First Token----------------
Mean TTFT (ms):                          168.36    
Median TTFT (ms):                        159.22    
P99 TTFT (ms):                           261.41    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.04     
Median TPOT (ms):                        22.01     
P99 TPOT (ms):                           26.45     
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.92     
Median ITL (ms):                         48.30     
P99 ITL (ms):                            91.51     
---------------Speculative Decoding---------------
Acceptance rate (%):                     22.04     
Acceptance length:                       2.32      
Drafts:                                  27550     
Draft tokens:                            165300    
Accepted tokens:                         36435     
Per-position acceptance (%):
  Position 0:                            62.59     
  Position 1:                            35.21     
  Position 2:                            18.65     
  Position 3:                            8.95      
  Position 4:                            4.51      
  Position 5:                            2.35      
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 640...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:38 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 392.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 214.98 tokens/s, Drafted throughput: 1093.72 tokens/s, Accepted: 2150 tokens, Drafted: 10938 tokens, Per-position acceptance rate: 0.598, 0.312, 0.145, 0.071, 0.035, 0.018, Avg Draft acceptance rate: 19.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0baa71afc0>, seed=0, num_prompts=640, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3d96d968-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:58 [loggers.py:257] Engine 000: Avg prompt throughput: 948.3 tokens/s, Avg generation throughput: 705.1 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:58:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 206.82 tokens/s, Drafted throughput: 856.66 tokens/s, Accepted: 4137 tokens, Drafted: 17136 tokens, Per-position acceptance rate: 0.661, 0.390, 0.208, 0.105, 0.052, 0.032, Avg Draft acceptance rate: 24.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1302.9 tokens/s, Avg generation throughput: 1854.4 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1044.36 tokens/s, Drafted throughput: 4871.73 tokens/s, Accepted: 10448 tokens, Drafted: 48738 tokens, Per-position acceptance rate: 0.617, 0.344, 0.182, 0.084, 0.039, 0.020, Avg Draft acceptance rate: 21.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1613.0 tokens/s, Avg generation throughput: 1880.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 1080.29 tokens/s, Drafted throughput: 4800.74 tokens/s, Accepted: 10808 tokens, Drafted: 48030 tokens, Per-position acceptance rate: 0.622, 0.357, 0.200, 0.096, 0.049, 0.026, Avg Draft acceptance rate: 22.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1221.2 tokens/s, Avg generation throughput: 1887.6 tokens/s, Running: 59 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 1058.17 tokens/s, Drafted throughput: 4992.04 tokens/s, Accepted: 10588 tokens, Drafted: 49950 tokens, Per-position acceptance rate: 0.610, 0.341, 0.175, 0.083, 0.041, 0.022, Avg Draft acceptance rate: 21.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1566.0 tokens/s, Avg generation throughput: 1874.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1075.99 tokens/s, Drafted throughput: 4800.12 tokens/s, Accepted: 10761 tokens, Drafted: 48006 tokens, Per-position acceptance rate: 0.639, 0.360, 0.191, 0.091, 0.043, 0.022, Avg Draft acceptance rate: 22.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1106.8 tokens/s, Avg generation throughput: 1912.9 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 38.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1068.14 tokens/s, Drafted throughput: 5072.63 tokens/s, Accepted: 10686 tokens, Drafted: 50748 tokens, Per-position acceptance rate: 0.605, 0.334, 0.173, 0.085, 0.043, 0.023, Avg Draft acceptance rate: 21.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1660.7 tokens/s, Avg generation throughput: 1870.7 tokens/s, Running: 63 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 19:59:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1069.55 tokens/s, Drafted throughput: 4798.59 tokens/s, Accepted: 10700 tokens, Drafted: 48006 tokens, Per-position acceptance rate: 0.629, 0.355, 0.186, 0.094, 0.047, 0.025, Avg Draft acceptance rate: 22.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     640       
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  70.39     
Total input tokens:                      94775     
Total generated tokens:                  127975    
Request throughput (req/s):              9.09      
Output token throughput (tok/s):         1818.01   
Peak output token throughput (tok/s):    965.00    
Peak concurrent requests:                90.00     
Total token throughput (tok/s):          3164.38   
---------------Time to First Token----------------
Mean TTFT (ms):                          283.07    
Median TTFT (ms):                        261.50    
P99 TTFT (ms):                           673.19    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          32.33     
Median TPOT (ms):                        32.42     
P99 TPOT (ms):                           39.85     
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.19     
Median ITL (ms):                         67.08     
P99 ITL (ms):                            141.14    
---------------Speculative Decoding---------------
Acceptance rate (%):                     21.79     
Acceptance length:                       2.31      
Drafts:                                  55478     
Draft tokens:                            332868    
Accepted tokens:                         72537     
Per-position acceptance (%):
  Position 0:                            62.06     
  Position 1:                            34.81     
  Position 2:                            18.38     
  Position 3:                            8.86      
  Position 4:                            4.33      
  Position 5:                            2.31      
==================================================
Starting benchmark with MAX_CONCURRENCY = 128 and NUM_PROMPTS = 1280...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:08 [loggers.py:257] Engine 000: Avg prompt throughput: 72.9 tokens/s, Avg generation throughput: 826.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 452.66 tokens/s, Drafted throughput: 2276.79 tokens/s, Accepted: 4527 tokens, Drafted: 22770 tokens, Per-position acceptance rate: 0.596, 0.314, 0.160, 0.074, 0.034, 0.015, Avg Draft acceptance rate: 19.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff8a798efc0>, seed=0, num_prompts=1280, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=128, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=128.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6e67689f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 128.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:28 [loggers.py:257] Engine 000: Avg prompt throughput: 861.9 tokens/s, Avg generation throughput: 52.5 tokens/s, Running: 103 reqs, Waiting: 0 reqs, GPU KV cache usage: 35.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 15.05 tokens/s, Drafted throughput: 51.00 tokens/s, Accepted: 301 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.724, 0.512, 0.347, 0.112, 0.047, 0.029, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1149.6 tokens/s, Avg generation throughput: 2291.8 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 88.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1298.77 tokens/s, Drafted throughput: 5918.39 tokens/s, Accepted: 12989 tokens, Drafted: 59190 tokens, Per-position acceptance rate: 0.624, 0.354, 0.184, 0.089, 0.042, 0.023, Avg Draft acceptance rate: 21.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1854.2 tokens/s, Avg generation throughput: 2004.8 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 84.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1158.14 tokens/s, Drafted throughput: 5082.93 tokens/s, Accepted: 11582 tokens, Drafted: 50832 tokens, Per-position acceptance rate: 0.630, 0.364, 0.202, 0.096, 0.049, 0.025, Avg Draft acceptance rate: 22.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1877.7 tokens/s, Avg generation throughput: 1978.8 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 71.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:00:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1135.60 tokens/s, Drafted throughput: 5068.37 tokens/s, Accepted: 11357 tokens, Drafted: 50688 tokens, Per-position acceptance rate: 0.630, 0.360, 0.191, 0.090, 0.048, 0.026, Avg Draft acceptance rate: 22.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1705.2 tokens/s, Avg generation throughput: 1973.0 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 60.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 1116.00 tokens/s, Drafted throughput: 5162.54 tokens/s, Accepted: 11161 tokens, Drafted: 51630 tokens, Per-position acceptance rate: 0.622, 0.347, 0.182, 0.085, 0.039, 0.021, Avg Draft acceptance rate: 21.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1138.9 tokens/s, Avg generation throughput: 2077.7 tokens/s, Running: 119 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1156.16 tokens/s, Drafted throughput: 5538.42 tokens/s, Accepted: 11562 tokens, Drafted: 55386 tokens, Per-position acceptance rate: 0.601, 0.328, 0.174, 0.086, 0.041, 0.023, Avg Draft acceptance rate: 20.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1320.6 tokens/s, Avg generation throughput: 2097.6 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 77.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1191.87 tokens/s, Drafted throughput: 5441.37 tokens/s, Accepted: 11928 tokens, Drafted: 54456 tokens, Per-position acceptance rate: 0.623, 0.351, 0.182, 0.089, 0.045, 0.024, Avg Draft acceptance rate: 21.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:38 [loggers.py:257] Engine 000: Avg prompt throughput: 1465.4 tokens/s, Avg generation throughput: 2039.2 tokens/s, Running: 127 reqs, Waiting: 0 reqs, GPU KV cache usage: 77.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1148.62 tokens/s, Drafted throughput: 5341.91 tokens/s, Accepted: 11495 tokens, Drafted: 53460 tokens, Per-position acceptance rate: 0.607, 0.343, 0.185, 0.089, 0.043, 0.024, Avg Draft acceptance rate: 21.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:48 [loggers.py:257] Engine 000: Avg prompt throughput: 1780.6 tokens/s, Avg generation throughput: 1998.7 tokens/s, Running: 128 reqs, Waiting: 0 reqs, GPU KV cache usage: 72.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 1137.07 tokens/s, Drafted throughput: 5157.57 tokens/s, Accepted: 11380 tokens, Drafted: 51618 tokens, Per-position acceptance rate: 0.622, 0.349, 0.190, 0.092, 0.045, 0.025, Avg Draft acceptance rate: 22.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:58 [loggers.py:257] Engine 000: Avg prompt throughput: 1589.9 tokens/s, Avg generation throughput: 1941.1 tokens/s, Running: 124 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:01:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1078.05 tokens/s, Drafted throughput: 5189.26 tokens/s, Accepted: 10782 tokens, Drafted: 51900 tokens, Per-position acceptance rate: 0.603, 0.335, 0.169, 0.079, 0.039, 0.021, Avg Draft acceptance rate: 20.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1261.0 tokens/s, Avg generation throughput: 2100.8 tokens/s, Running: 120 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1181.46 tokens/s, Drafted throughput: 5519.80 tokens/s, Accepted: 11815 tokens, Drafted: 55200 tokens, Per-position acceptance rate: 0.614, 0.341, 0.178, 0.086, 0.045, 0.022, Avg Draft acceptance rate: 21.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:18 [loggers.py:257] Engine 000: Avg prompt throughput: 1389.9 tokens/s, Avg generation throughput: 2054.5 tokens/s, Running: 125 reqs, Waiting: 0 reqs, GPU KV cache usage: 77.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 1160.05 tokens/s, Drafted throughput: 5363.17 tokens/s, Accepted: 11601 tokens, Drafted: 53634 tokens, Per-position acceptance rate: 0.618, 0.348, 0.181, 0.086, 0.042, 0.023, Avg Draft acceptance rate: 21.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1527.7 tokens/s, Avg generation throughput: 2060.1 tokens/s, Running: 120 reqs, Waiting: 0 reqs, GPU KV cache usage: 75.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 1175.88 tokens/s, Drafted throughput: 5320.13 tokens/s, Accepted: 11759 tokens, Drafted: 53202 tokens, Per-position acceptance rate: 0.614, 0.355, 0.191, 0.091, 0.047, 0.029, Avg Draft acceptance rate: 22.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     1280      
Failed requests:                         0         
Maximum request concurrency:             128       
Request rate configured (RPS):           128.00    
Benchmark duration (s):                  127.31    
Total input tokens:                      189093    
Total generated tokens:                  255968    
Request throughput (req/s):              10.05     
Output token throughput (tok/s):         2010.59   
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                168.00    
Total token throughput (tok/s):          3495.89   
---------------Time to First Token----------------
Mean TTFT (ms):                          523.31    
Median TTFT (ms):                        456.40    
P99 TTFT (ms):                           1524.90   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          59.73     
Median TPOT (ms):                        59.99     
P99 TPOT (ms):                           75.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           136.67    
Median ITL (ms):                         128.91    
P99 ITL (ms):                            221.10    
---------------Speculative Decoding---------------
Acceptance rate (%):                     21.68     
Acceptance length:                       2.30      
Drafts:                                  111303    
Draft tokens:                            667818    
Accepted tokens:                         144767    
Per-position acceptance (%):
  Position 0:                            61.65     
  Position 1:                            34.65     
  Position 2:                            18.29     
  Position 3:                            8.77      
  Position 4:                            4.34      
  Position 5:                            2.37      
==================================================
Starting benchmark with MAX_CONCURRENCY = 256 and NUM_PROMPTS = 2560...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 939.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 517.24 tokens/s, Drafted throughput: 2611.48 tokens/s, Accepted: 5173 tokens, Drafted: 26118 tokens, Per-position acceptance rate: 0.597, 0.310, 0.152, 0.074, 0.038, 0.017, Avg Draft acceptance rate: 19.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9ca24d2fc0>, seed=0, num_prompts=2560, dataset_name='hf', no_stream=False, dataset_path='likaixin/InstructCoder', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15035, endpoint='/v1/completions', header=None, max_concurrency=256, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=256.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9d32fc79-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 256.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 256
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:58 [loggers.py:257] Engine 000: Avg prompt throughput: 775.4 tokens/s, Avg generation throughput: 38.4 tokens/s, Running: 104 reqs, Waiting: 32 reqs, GPU KV cache usage: 36.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:02:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.71, Accepted throughput: 10.60 tokens/s, Drafted throughput: 37.20 tokens/s, Accepted: 212 tokens, Drafted: 744 tokens, Per-position acceptance rate: 0.694, 0.476, 0.331, 0.113, 0.056, 0.040, Avg Draft acceptance rate: 28.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:08 [loggers.py:257] Engine 000: Avg prompt throughput: 3090.2 tokens/s, Avg generation throughput: 2065.2 tokens/s, Running: 170 reqs, Waiting: 86 reqs, GPU KV cache usage: 99.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 1225.46 tokens/s, Drafted throughput: 4916.25 tokens/s, Accepted: 12255 tokens, Drafted: 49164 tokens, Per-position acceptance rate: 0.665, 0.403, 0.228, 0.113, 0.055, 0.032, Avg Draft acceptance rate: 24.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:18 [loggers.py:257] Engine 000: Avg prompt throughput: 114.2 tokens/s, Avg generation throughput: 1607.1 tokens/s, Running: 176 reqs, Waiting: 60 reqs, GPU KV cache usage: 94.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 871.40 tokens/s, Drafted throughput: 4414.88 tokens/s, Accepted: 8715 tokens, Drafted: 44154 tokens, Per-position acceptance rate: 0.590, 0.312, 0.158, 0.072, 0.035, 0.017, Avg Draft acceptance rate: 19.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:28 [loggers.py:257] Engine 000: Avg prompt throughput: 1214.9 tokens/s, Avg generation throughput: 2250.6 tokens/s, Running: 150 reqs, Waiting: 103 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1274.00 tokens/s, Drafted throughput: 5836.35 tokens/s, Accepted: 12741 tokens, Drafted: 58368 tokens, Per-position acceptance rate: 0.619, 0.354, 0.181, 0.086, 0.045, 0.023, Avg Draft acceptance rate: 21.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:38 [loggers.py:257] Engine 000: Avg prompt throughput: 2690.6 tokens/s, Avg generation throughput: 1833.3 tokens/s, Running: 205 reqs, Waiting: 47 reqs, GPU KV cache usage: 97.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 1069.63 tokens/s, Drafted throughput: 4552.49 tokens/s, Accepted: 10697 tokens, Drafted: 45528 tokens, Per-position acceptance rate: 0.647, 0.374, 0.207, 0.102, 0.050, 0.030, Avg Draft acceptance rate: 23.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:48 [loggers.py:257] Engine 000: Avg prompt throughput: 104.4 tokens/s, Avg generation throughput: 2132.5 tokens/s, Running: 150 reqs, Waiting: 96 reqs, GPU KV cache usage: 95.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 1164.75 tokens/s, Drafted throughput: 5818.67 tokens/s, Accepted: 11649 tokens, Drafted: 58194 tokens, Per-position acceptance rate: 0.590, 0.319, 0.161, 0.079, 0.035, 0.017, Avg Draft acceptance rate: 20.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:58 [loggers.py:257] Engine 000: Avg prompt throughput: 2216.5 tokens/s, Avg generation throughput: 1998.6 tokens/s, Running: 170 reqs, Waiting: 85 reqs, GPU KV cache usage: 98.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:03:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1152.41 tokens/s, Drafted throughput: 5061.79 tokens/s, Accepted: 11525 tokens, Drafted: 50622 tokens, Per-position acceptance rate: 0.635, 0.363, 0.197, 0.096, 0.048, 0.027, Avg Draft acceptance rate: 22.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1634.7 tokens/s, Avg generation throughput: 1832.4 tokens/s, Running: 199 reqs, Waiting: 49 reqs, GPU KV cache usage: 96.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 1014.63 tokens/s, Drafted throughput: 4886.67 tokens/s, Accepted: 10147 tokens, Drafted: 48870 tokens, Per-position acceptance rate: 0.602, 0.332, 0.174, 0.079, 0.038, 0.021, Avg Draft acceptance rate: 20.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:18 [loggers.py:257] Engine 000: Avg prompt throughput: 944.8 tokens/s, Avg generation throughput: 2310.3 tokens/s, Running: 154 reqs, Waiting: 102 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1310.56 tokens/s, Drafted throughput: 5992.75 tokens/s, Accepted: 13107 tokens, Drafted: 59934 tokens, Per-position acceptance rate: 0.617, 0.345, 0.186, 0.093, 0.046, 0.025, Avg Draft acceptance rate: 21.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:28 [loggers.py:257] Engine 000: Avg prompt throughput: 2412.7 tokens/s, Avg generation throughput: 1630.9 tokens/s, Running: 203 reqs, Waiting: 47 reqs, GPU KV cache usage: 99.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 919.84 tokens/s, Drafted throughput: 4224.69 tokens/s, Accepted: 9206 tokens, Drafted: 42282 tokens, Per-position acceptance rate: 0.617, 0.352, 0.186, 0.087, 0.040, 0.024, Avg Draft acceptance rate: 21.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:38 [loggers.py:257] Engine 000: Avg prompt throughput: 101.3 tokens/s, Avg generation throughput: 2214.8 tokens/s, Running: 133 reqs, Waiting: 115 reqs, GPU KV cache usage: 96.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 1219.45 tokens/s, Drafted throughput: 5987.87 tokens/s, Accepted: 12196 tokens, Drafted: 59886 tokens, Per-position acceptance rate: 0.598, 0.325, 0.161, 0.078, 0.040, 0.019, Avg Draft acceptance rate: 20.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:48 [loggers.py:257] Engine 000: Avg prompt throughput: 2634.5 tokens/s, Avg generation throughput: 1851.8 tokens/s, Running: 185 reqs, Waiting: 71 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 1078.62 tokens/s, Drafted throughput: 4587.84 tokens/s, Accepted: 10787 tokens, Drafted: 45882 tokens, Per-position acceptance rate: 0.646, 0.375, 0.210, 0.100, 0.051, 0.028, Avg Draft acceptance rate: 23.5%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:58 [loggers.py:257] Engine 000: Avg prompt throughput: 795.5 tokens/s, Avg generation throughput: 1916.0 tokens/s, Running: 170 reqs, Waiting: 74 reqs, GPU KV cache usage: 94.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:04:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 1046.54 tokens/s, Drafted throughput: 5227.41 tokens/s, Accepted: 10465 tokens, Drafted: 52272 tokens, Per-position acceptance rate: 0.592, 0.325, 0.155, 0.073, 0.036, 0.019, Avg Draft acceptance rate: 20.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1600.9 tokens/s, Avg generation throughput: 2091.4 tokens/s, Running: 160 reqs, Waiting: 96 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1206.35 tokens/s, Drafted throughput: 5285.76 tokens/s, Accepted: 12064 tokens, Drafted: 52860 tokens, Per-position acceptance rate: 0.629, 0.363, 0.199, 0.099, 0.050, 0.028, Avg Draft acceptance rate: 22.8%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:18 [loggers.py:257] Engine 000: Avg prompt throughput: 2195.3 tokens/s, Avg generation throughput: 1645.5 tokens/s, Running: 206 reqs, Waiting: 44 reqs, GPU KV cache usage: 96.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 916.92 tokens/s, Drafted throughput: 4331.63 tokens/s, Accepted: 9170 tokens, Drafted: 43320 tokens, Per-position acceptance rate: 0.614, 0.341, 0.176, 0.081, 0.038, 0.020, Avg Draft acceptance rate: 21.2%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:28 [loggers.py:257] Engine 000: Avg prompt throughput: 380.2 tokens/s, Avg generation throughput: 2318.7 tokens/s, Running: 143 reqs, Waiting: 112 reqs, GPU KV cache usage: 99.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 1303.25 tokens/s, Drafted throughput: 6104.17 tokens/s, Accepted: 13033 tokens, Drafted: 61044 tokens, Per-position acceptance rate: 0.615, 0.344, 0.174, 0.083, 0.043, 0.022, Avg Draft acceptance rate: 21.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:38 [loggers.py:257] Engine 000: Avg prompt throughput: 2522.8 tokens/s, Avg generation throughput: 1689.8 tokens/s, Running: 201 reqs, Waiting: 55 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 986.67 tokens/s, Drafted throughput: 4174.25 tokens/s, Accepted: 9868 tokens, Drafted: 41748 tokens, Per-position acceptance rate: 0.647, 0.374, 0.213, 0.106, 0.049, 0.029, Avg Draft acceptance rate: 23.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:48 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2139.3 tokens/s, Running: 153 reqs, Waiting: 87 reqs, GPU KV cache usage: 95.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 1170.78 tokens/s, Drafted throughput: 5831.82 tokens/s, Accepted: 11719 tokens, Drafted: 58374 tokens, Per-position acceptance rate: 0.590, 0.325, 0.157, 0.077, 0.036, 0.019, Avg Draft acceptance rate: 20.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:58 [loggers.py:257] Engine 000: Avg prompt throughput: 2177.4 tokens/s, Avg generation throughput: 2005.6 tokens/s, Running: 172 reqs, Waiting: 83 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:05:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 1166.04 tokens/s, Drafted throughput: 4980.86 tokens/s, Accepted: 11664 tokens, Drafted: 49824 tokens, Per-position acceptance rate: 0.643, 0.375, 0.208, 0.105, 0.049, 0.026, Avg Draft acceptance rate: 23.4%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1721.5 tokens/s, Avg generation throughput: 1791.7 tokens/s, Running: 202 reqs, Waiting: 38 reqs, GPU KV cache usage: 95.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 990.24 tokens/s, Drafted throughput: 4786.49 tokens/s, Accepted: 9903 tokens, Drafted: 47868 tokens, Per-position acceptance rate: 0.601, 0.332, 0.172, 0.078, 0.038, 0.021, Avg Draft acceptance rate: 20.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:18 [loggers.py:257] Engine 000: Avg prompt throughput: 955.4 tokens/s, Avg generation throughput: 2331.4 tokens/s, Running: 150 reqs, Waiting: 104 reqs, GPU KV cache usage: 99.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 1314.85 tokens/s, Drafted throughput: 6083.92 tokens/s, Accepted: 13150 tokens, Drafted: 60846 tokens, Per-position acceptance rate: 0.622, 0.350, 0.181, 0.086, 0.038, 0.019, Avg Draft acceptance rate: 21.6%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:28 [loggers.py:257] Engine 000: Avg prompt throughput: 2291.9 tokens/s, Avg generation throughput: 1611.9 tokens/s, Running: 207 reqs, Waiting: 46 reqs, GPU KV cache usage: 98.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 918.17 tokens/s, Drafted throughput: 4123.81 tokens/s, Accepted: 9183 tokens, Drafted: 41244 tokens, Per-position acceptance rate: 0.624, 0.352, 0.193, 0.092, 0.047, 0.028, Avg Draft acceptance rate: 22.3%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:38 [loggers.py:257] Engine 000: Avg prompt throughput: 22.3 tokens/s, Avg generation throughput: 2303.8 tokens/s, Running: 136 reqs, Waiting: 112 reqs, GPU KV cache usage: 96.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 1288.04 tokens/s, Drafted throughput: 6111.31 tokens/s, Accepted: 12881 tokens, Drafted: 61116 tokens, Per-position acceptance rate: 0.614, 0.336, 0.171, 0.082, 0.040, 0.021, Avg Draft acceptance rate: 21.1%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:48 [loggers.py:257] Engine 000: Avg prompt throughput: 2713.4 tokens/s, Avg generation throughput: 1841.4 tokens/s, Running: 192 reqs, Waiting: 64 reqs, GPU KV cache usage: 99.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 1074.99 tokens/s, Drafted throughput: 4542.25 tokens/s, Accepted: 10755 tokens, Drafted: 45444 tokens, Per-position acceptance rate: 0.644, 0.373, 0.212, 0.108, 0.053, 0.030, Avg Draft acceptance rate: 23.7%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:58 [loggers.py:257] Engine 000: Avg prompt throughput: 407.0 tokens/s, Avg generation throughput: 1953.3 tokens/s, Running: 172 reqs, Waiting: 64 reqs, GPU KV cache usage: 92.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:06:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 1062.00 tokens/s, Drafted throughput: 5348.69 tokens/s, Accepted: 10623 tokens, Drafted: 53502 tokens, Per-position acceptance rate: 0.599, 0.315, 0.152, 0.075, 0.034, 0.016, Avg Draft acceptance rate: 19.9%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:07:08 [loggers.py:257] Engine 000: Avg prompt throughput: 1616.6 tokens/s, Avg generation throughput: 2214.1 tokens/s, Running: 162 reqs, Waiting: 29 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:07:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 1274.88 tokens/s, Drafted throughput: 5603.47 tokens/s, Accepted: 12750 tokens, Drafted: 56040 tokens, Per-position acceptance rate: 0.639, 0.363, 0.194, 0.094, 0.047, 0.028, Avg Draft acceptance rate: 22.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     2560      
Failed requests:                         0         
Maximum request concurrency:             256       
Request rate configured (RPS):           256.00    
Benchmark duration (s):                  261.16    
Total input tokens:                      373233    
Total generated tokens:                  511918    
Request throughput (req/s):              9.80      
Output token throughput (tok/s):         1960.16   
Peak output token throughput (tok/s):    1268.00   
Peak concurrent requests:                285.00    
Total token throughput (tok/s):          3389.30   
---------------Time to First Token----------------
Mean TTFT (ms):                          6696.84   
Median TTFT (ms):                        4396.21   
P99 TTFT (ms):                           14681.13  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          93.89     
Median TPOT (ms):                        88.63     
P99 TPOT (ms):                           153.48    
---------------Inter-token Latency----------------
Mean ITL (ms):                           214.47    
Median ITL (ms):                         170.61    
P99 ITL (ms):                            442.89    
---------------Speculative Decoding---------------
Acceptance rate (%):                     21.69     
Acceptance length:                       2.30      
Drafts:                                  222088    
Draft tokens:                            1332528   
Accepted tokens:                         289004    
Per-position acceptance (%):
  Position 0:                            61.83     
  Position 1:                            34.69     
  Position 2:                            18.21     
  Position 3:                            8.79      
  Position 4:                            4.28      
  Position 5:                            2.33      
==================================================
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:07:18 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1581.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:07:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 865.61 tokens/s, Drafted throughput: 4390.93 tokens/s, Accepted: 8657 tokens, Drafted: 43914 tokens, Per-position acceptance rate: 0.595, 0.313, 0.149, 0.071, 0.037, 0.019, Avg Draft acceptance rate: 19.7%
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-InstructCoder-sd-eagle3-Qwen3-32B-speculator.eagle3-k6-t0.0-tp1...
[0;36m(APIServer pid=254728)[0;0m INFO 01-23 20:07:18 [launcher.py:110] Shutting down FastAPI HTTP server.
