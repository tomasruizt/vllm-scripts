Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k2-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k2-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 130035
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:27 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:27 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15001, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 2, 'max_model_len': 5000}}
[0;36m(APIServer pid=130035)[0;0m WARNING 01-23 11:47:27 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:28 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:28 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:30 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:30 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:30 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=130035)[0;0m WARNING 01-23 11:47:30 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:47:30 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb4e2aaafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a52c1de5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 11:47:32 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:47:37 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:47:42 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=2), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 11:47:42 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:47:44 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.67:33765 backend=nccl
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:47:44 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=130614)[0;0m WARNING 01-23 11:47:47 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:47:47 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-23 11:47:47 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:47:49 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 11:47:52 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:47:57 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:02 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:12 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:17 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:22 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:27 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:32 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:37 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:42 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:47 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:48:52 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:48:53 [default_loader.py:291] Loading weights took 63.13 seconds
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:48:53 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:48:53 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:48:53 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-23 11:48:57 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:01 [default_loader.py:291] Loading weights took 6.80 seconds
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:02 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 74.083163 seconds
WARNING 01-23 11:49:02 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:49:07 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:49:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:49:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:18 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:18 [backends.py:704] Dynamo bytecode transform time: 15.62 s
WARNING 01-23 11:49:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:49:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:49:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:35 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.644 s
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:35 [monitor.py:34] torch.compile takes 20.26 s in total
WARNING 01-23 11:49:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:40 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:40 [backends.py:704] Dynamo bytecode transform time: 4.86 s
WARNING 01-23 11:49:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:47 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.748 s
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:47 [monitor.py:34] torch.compile takes 26.87 s in total
WARNING 01-23 11:49:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:49 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:49 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:49:49 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-23 11:49:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:49:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
WARNING 01-23 11:50:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:50:04 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took -0.03 GiB
[0;36m(EngineCore_DP0 pid=130614)[0;0m INFO 01-23 11:50:04 [core.py:272] init engine (profile, create kv cache, warmup model) took 61.76 seconds
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:06 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=130035)[0;0m WARNING 01-23 11:50:06 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:06 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:06 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:06 [serving.py:185] Warming up chat template processing...
WARNING 01-23 11:50:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15001)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15001 ssl:default [Connect call failed (\'127.0.0.1\', 15001)]\n''
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:08 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:08 [serving.py:221] Chat template warmup completed in 1768.3ms
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:08 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15001
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:09 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:19 [loggers.py:257] Engine 000: Avg prompt throughput: 26.2 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 16.73 tokens/s, Drafted throughput: 19.21 tokens/s, Accepted: 230 tokens, Drafted: 264 tokens, Per-position acceptance rate: 0.917, 0.826, Avg Draft acceptance rate: 87.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:29 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 30.50 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 305 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.726, 0.561, Avg Draft acceptance rate: 64.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:39 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 27.60 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 276 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.684, 0.481, Avg Draft acceptance rate: 58.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:49 [loggers.py:257] Engine 000: Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 60.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.57, Accepted throughput: 36.99 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 370 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.856, 0.712, Avg Draft acceptance rate: 78.4%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:59 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:50:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 35.50 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 355 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.818, 0.686, Avg Draft acceptance rate: 75.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:09 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 50.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 27.20 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 272 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.712, 0.441, Avg Draft acceptance rate: 57.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:19 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 35.00 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 350 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.819, 0.658, Avg Draft acceptance rate: 73.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:29 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 32.60 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 326 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.785, 0.591, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:39 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 52.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 28.60 tokens/s, Drafted throughput: 47.59 tokens/s, Accepted: 286 tokens, Drafted: 476 tokens, Per-position acceptance rate: 0.706, 0.496, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:49 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 28.00 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 280 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.705, 0.477, Avg Draft acceptance rate: 59.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:59 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:51:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 29.50 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 295 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.717, 0.527, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:09 [loggers.py:257] Engine 000: Avg prompt throughput: 25.6 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 32.20 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 322 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.746, 0.619, Avg Draft acceptance rate: 68.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:19 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 33.10 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 331 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.759, 0.637, Avg Draft acceptance rate: 69.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:29 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 30.20 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 302 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.730, 0.544, Avg Draft acceptance rate: 63.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:39 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 35.00 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 350 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.793, 0.684, Avg Draft acceptance rate: 73.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:49 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 62.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.64, Accepted throughput: 38.60 tokens/s, Drafted throughput: 47.00 tokens/s, Accepted: 386 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.872, 0.770, Avg Draft acceptance rate: 82.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:59 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:52:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 30.60 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 306 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.743, 0.549, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:09 [loggers.py:257] Engine 000: Avg prompt throughput: 25.5 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 31.00 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 310 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.738, 0.570, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:19 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 32.70 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 327 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.764, 0.616, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:29 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 27.40 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 274 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.650, 0.506, Avg Draft acceptance rate: 57.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:39 [loggers.py:257] Engine 000: Avg prompt throughput: 25.2 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 33.90 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 339 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.810, 0.620, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:49 [loggers.py:257] Engine 000: Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 27.90 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 279 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.688, 0.489, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:59 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 49.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:53:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 25.90 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 259 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.624, 0.468, Avg Draft acceptance rate: 54.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:09 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 35.50 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 355 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.831, 0.674, Avg Draft acceptance rate: 75.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:19 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 30.70 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 307 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.722, 0.574, Avg Draft acceptance rate: 64.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:29 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 55.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 31.70 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 317 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.747, 0.591, Avg Draft acceptance rate: 66.9%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:39 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 52.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 28.80 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 288 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.705, 0.511, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:49 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 52.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 28.60 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 286 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.679, 0.527, Avg Draft acceptance rate: 60.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:59 [loggers.py:257] Engine 000: Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 57.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:54:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 33.70 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 337 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.797, 0.624, Avg Draft acceptance rate: 71.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 32.30 tokens/s, Drafted throughput: 47.60 tokens/s, Accepted: 323 tokens, Drafted: 476 tokens, Per-position acceptance rate: 0.756, 0.601, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:19 [loggers.py:257] Engine 000: Avg prompt throughput: 33.0 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 31.70 tokens/s, Drafted throughput: 46.99 tokens/s, Accepted: 317 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.770, 0.579, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:29 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 35.80 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 358 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.835, 0.675, Avg Draft acceptance rate: 75.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:39 [loggers.py:257] Engine 000: Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 33.80 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 338 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.806, 0.620, Avg Draft acceptance rate: 71.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:49 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 27.80 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 278 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.705, 0.468, Avg Draft acceptance rate: 58.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:59 [loggers.py:257] Engine 000: Avg prompt throughput: 37.8 tokens/s, Avg generation throughput: 57.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:55:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 33.40 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 334 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.793, 0.616, Avg Draft acceptance rate: 70.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:09 [loggers.py:257] Engine 000: Avg prompt throughput: 33.1 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 34.40 tokens/s, Drafted throughput: 47.00 tokens/s, Accepted: 344 tokens, Drafted: 470 tokens, Per-position acceptance rate: 0.826, 0.638, Avg Draft acceptance rate: 73.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:19 [loggers.py:257] Engine 000: Avg prompt throughput: 24.4 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.61, Accepted throughput: 38.10 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 381 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.861, 0.747, Avg Draft acceptance rate: 80.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  369.48    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         55.43     
Peak output token throughput (tok/s):    25.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          71.88     
---------------Time to First Token----------------
Mean TTFT (ms):                          54.49     
Median TTFT (ms):                        53.20     
P99 TTFT (ms):                           71.57     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.86     
Median TPOT (ms):                        18.02     
P99 TPOT (ms):                           20.90     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.74     
Median ITL (ms):                         41.76     
P99 ITL (ms):                            42.25     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.27     
Acceptance length:                       2.35      
Drafts:                                  8727      
Draft tokens:                            17454     
Accepted tokens:                         11741     
Per-position acceptance (%):
  Position 0:                            75.74     
  Position 1:                            58.79     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:29 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 37.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 21.10 tokens/s, Drafted throughput: 33.20 tokens/s, Accepted: 211 tokens, Drafted: 332 tokens, Per-position acceptance rate: 0.705, 0.566, Avg Draft acceptance rate: 63.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3b20fe6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-fbaf2153-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:49 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.86, Accepted throughput: 2.60 tokens/s, Drafted throughput: 2.80 tokens/s, Accepted: 52 tokens, Drafted: 56 tokens, Per-position acceptance rate: 0.964, 0.893, Avg Draft acceptance rate: 92.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:59 [loggers.py:257] Engine 000: Avg prompt throughput: 34.7 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:56:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 52.80 tokens/s, Drafted throughput: 73.20 tokens/s, Accepted: 528 tokens, Drafted: 732 tokens, Per-position acceptance rate: 0.784, 0.658, Avg Draft acceptance rate: 72.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:09 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 65.49 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 655 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.796, 0.609, Avg Draft acceptance rate: 70.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:19 [loggers.py:257] Engine 000: Avg prompt throughput: 56.7 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 61.40 tokens/s, Drafted throughput: 92.80 tokens/s, Accepted: 614 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.765, 0.558, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:29 [loggers.py:257] Engine 000: Avg prompt throughput: 23.5 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 65.59 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 656 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.793, 0.621, Avg Draft acceptance rate: 70.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:39 [loggers.py:257] Engine 000: Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 55.99 tokens/s, Drafted throughput: 93.59 tokens/s, Accepted: 560 tokens, Drafted: 936 tokens, Per-position acceptance rate: 0.705, 0.491, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:49 [loggers.py:257] Engine 000: Avg prompt throughput: 27.6 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 63.40 tokens/s, Drafted throughput: 93.20 tokens/s, Accepted: 634 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.758, 0.603, Avg Draft acceptance rate: 68.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:59 [loggers.py:257] Engine 000: Avg prompt throughput: 45.4 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:57:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 60.19 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 602 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.726, 0.571, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:09 [loggers.py:257] Engine 000: Avg prompt throughput: 28.4 tokens/s, Avg generation throughput: 118.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 72.09 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 721 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.832, 0.722, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:19 [loggers.py:257] Engine 000: Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 63.70 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 637 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.762, 0.605, Avg Draft acceptance rate: 68.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:29 [loggers.py:257] Engine 000: Avg prompt throughput: 39.4 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 59.60 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 596 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.718, 0.567, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:39 [loggers.py:257] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 63.49 tokens/s, Drafted throughput: 93.59 tokens/s, Accepted: 635 tokens, Drafted: 936 tokens, Per-position acceptance rate: 0.774, 0.583, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:49 [loggers.py:257] Engine 000: Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 60.70 tokens/s, Drafted throughput: 93.20 tokens/s, Accepted: 607 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.725, 0.577, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:59 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:58:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 61.99 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 620 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.736, 0.594, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:09 [loggers.py:257] Engine 000: Avg prompt throughput: 20.8 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 59.50 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 595 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.732, 0.545, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:19 [loggers.py:257] Engine 000: Avg prompt throughput: 21.1 tokens/s, Avg generation throughput: 112.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 65.59 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 656 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.781, 0.627, Avg Draft acceptance rate: 70.4%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:29 [loggers.py:257] Engine 000: Avg prompt throughput: 42.4 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 67.49 tokens/s, Drafted throughput: 92.79 tokens/s, Accepted: 675 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.812, 0.642, Avg Draft acceptance rate: 72.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:39 [loggers.py:257] Engine 000: Avg prompt throughput: 12.8 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 60.49 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 605 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.736, 0.562, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:49 [loggers.py:257] Engine 000: Avg prompt throughput: 68.7 tokens/s, Avg generation throughput: 111.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 64.39 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 644 tokens, Drafted: 932 tokens, Per-position acceptance rate: 0.781, 0.601, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:59 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 11:59:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 64.19 tokens/s, Drafted throughput: 87.19 tokens/s, Accepted: 642 tokens, Drafted: 872 tokens, Per-position acceptance rate: 0.807, 0.665, Avg Draft acceptance rate: 73.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  188.21    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.43      
Output token throughput (tok/s):         108.82    
Peak output token throughput (tok/s):    48.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          141.11    
---------------Time to First Token----------------
Mean TTFT (ms):                          87.19     
Median TTFT (ms):                        86.28     
P99 TTFT (ms):                           100.71    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.93     
Median TPOT (ms):                        18.11     
P99 TPOT (ms):                           20.42     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.17     
Median ITL (ms):                         42.14     
P99 ITL (ms):                            43.48     
---------------Speculative Decoding---------------
Acceptance rate (%):                     68.00     
Acceptance length:                       2.36      
Drafts:                                  8673      
Draft tokens:                            17346     
Accepted tokens:                         11795     
Per-position acceptance (%):
  Position 0:                            76.28     
  Position 1:                            59.71     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 2.40 tokens/s, Drafted throughput: 4.60 tokens/s, Accepted: 24 tokens, Drafted: 46 tokens, Per-position acceptance rate: 0.652, 0.391, Avg Draft acceptance rate: 52.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff1eabc2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f8d9df99-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:19 [loggers.py:257] Engine 000: Avg prompt throughput: 36.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.73, Accepted throughput: 16.30 tokens/s, Drafted throughput: 18.80 tokens/s, Accepted: 163 tokens, Drafted: 188 tokens, Per-position acceptance rate: 0.904, 0.830, Avg Draft acceptance rate: 86.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:29 [loggers.py:257] Engine 000: Avg prompt throughput: 62.7 tokens/s, Avg generation throughput: 210.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 121.99 tokens/s, Drafted throughput: 176.18 tokens/s, Accepted: 1220 tokens, Drafted: 1762 tokens, Per-position acceptance rate: 0.781, 0.604, Avg Draft acceptance rate: 69.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:39 [loggers.py:257] Engine 000: Avg prompt throughput: 78.6 tokens/s, Avg generation throughput: 211.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 119.39 tokens/s, Drafted throughput: 182.98 tokens/s, Accepted: 1194 tokens, Drafted: 1830 tokens, Per-position acceptance rate: 0.752, 0.553, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:49 [loggers.py:257] Engine 000: Avg prompt throughput: 42.4 tokens/s, Avg generation throughput: 209.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 116.89 tokens/s, Drafted throughput: 182.98 tokens/s, Accepted: 1169 tokens, Drafted: 1830 tokens, Per-position acceptance rate: 0.728, 0.550, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:59 [loggers.py:257] Engine 000: Avg prompt throughput: 78.8 tokens/s, Avg generation throughput: 222.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:00:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 130.69 tokens/s, Drafted throughput: 182.58 tokens/s, Accepted: 1307 tokens, Drafted: 1826 tokens, Per-position acceptance rate: 0.783, 0.648, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:09 [loggers.py:257] Engine 000: Avg prompt throughput: 72.3 tokens/s, Avg generation throughput: 212.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 121.20 tokens/s, Drafted throughput: 181.99 tokens/s, Accepted: 1212 tokens, Drafted: 1820 tokens, Per-position acceptance rate: 0.756, 0.576, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:19 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 120.58 tokens/s, Drafted throughput: 183.98 tokens/s, Accepted: 1206 tokens, Drafted: 1840 tokens, Per-position acceptance rate: 0.743, 0.567, Avg Draft acceptance rate: 65.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:29 [loggers.py:257] Engine 000: Avg prompt throughput: 64.0 tokens/s, Avg generation throughput: 211.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 120.59 tokens/s, Drafted throughput: 182.39 tokens/s, Accepted: 1206 tokens, Drafted: 1824 tokens, Per-position acceptance rate: 0.745, 0.578, Avg Draft acceptance rate: 66.1%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:39 [loggers.py:257] Engine 000: Avg prompt throughput: 55.7 tokens/s, Avg generation throughput: 218.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 126.99 tokens/s, Drafted throughput: 183.18 tokens/s, Accepted: 1270 tokens, Drafted: 1832 tokens, Per-position acceptance rate: 0.781, 0.606, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:49 [loggers.py:257] Engine 000: Avg prompt throughput: 81.5 tokens/s, Avg generation throughput: 216.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 124.79 tokens/s, Drafted throughput: 182.38 tokens/s, Accepted: 1248 tokens, Drafted: 1824 tokens, Per-position acceptance rate: 0.772, 0.596, Avg Draft acceptance rate: 68.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  97.95     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.82      
Output token throughput (tok/s):         209.08    
Peak output token throughput (tok/s):    96.00     
Peak concurrent requests:                8.00      
Total token throughput (tok/s):          271.13    
---------------Time to First Token----------------
Mean TTFT (ms):                          87.71     
Median TTFT (ms):                        87.06     
P99 TTFT (ms):                           97.74     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.30     
Median TPOT (ms):                        18.53     
P99 TPOT (ms):                           21.31     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.92     
Median ITL (ms):                         42.85     
P99 ITL (ms):                            46.01     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.67     
Acceptance length:                       2.35      
Drafts:                                  8699      
Draft tokens:                            17398     
Accepted tokens:                         11774     
Per-position acceptance (%):
  Position 0:                            76.28     
  Position 1:                            59.06     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:59 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 124.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:01:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 74.20 tokens/s, Drafted throughput: 101.00 tokens/s, Accepted: 742 tokens, Drafted: 1010 tokens, Per-position acceptance rate: 0.810, 0.659, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f534fa06fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8e83e95f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:19 [loggers.py:257] Engine 000: Avg prompt throughput: 94.1 tokens/s, Avg generation throughput: 154.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 46.24 tokens/s, Drafted throughput: 61.19 tokens/s, Accepted: 925 tokens, Drafted: 1224 tokens, Per-position acceptance rate: 0.832, 0.680, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:29 [loggers.py:257] Engine 000: Avg prompt throughput: 116.9 tokens/s, Avg generation throughput: 413.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 232.79 tokens/s, Drafted throughput: 359.98 tokens/s, Accepted: 2328 tokens, Drafted: 3600 tokens, Per-position acceptance rate: 0.741, 0.552, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:39 [loggers.py:257] Engine 000: Avg prompt throughput: 128.1 tokens/s, Avg generation throughput: 429.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 249.06 tokens/s, Drafted throughput: 359.55 tokens/s, Accepted: 2491 tokens, Drafted: 3596 tokens, Per-position acceptance rate: 0.770, 0.615, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:49 [loggers.py:257] Engine 000: Avg prompt throughput: 110.0 tokens/s, Avg generation throughput: 424.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 243.67 tokens/s, Drafted throughput: 359.56 tokens/s, Accepted: 2437 tokens, Drafted: 3596 tokens, Per-position acceptance rate: 0.760, 0.595, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:59 [loggers.py:257] Engine 000: Avg prompt throughput: 133.3 tokens/s, Avg generation throughput: 424.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:02:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 245.26 tokens/s, Drafted throughput: 357.94 tokens/s, Accepted: 2453 tokens, Drafted: 3580 tokens, Per-position acceptance rate: 0.774, 0.596, Avg Draft acceptance rate: 68.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  50.79     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.58      
Output token throughput (tok/s):         403.20    
Peak output token throughput (tok/s):    192.00    
Peak concurrent requests:                13.00     
Total token throughput (tok/s):          522.87    
---------------Time to First Token----------------
Mean TTFT (ms):                          88.78     
Median TTFT (ms):                        88.86     
P99 TTFT (ms):                           102.89    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.54     
Median TPOT (ms):                        18.78     
P99 TPOT (ms):                           21.71     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.63     
Median ITL (ms):                         43.44     
P99 ITL (ms):                            48.07     
---------------Speculative Decoding---------------
Acceptance rate (%):                     68.04     
Acceptance length:                       2.36      
Drafts:                                  8668      
Draft tokens:                            17336     
Accepted tokens:                         11796     
Per-position acceptance (%):
  Position 0:                            76.67     
  Position 1:                            59.41     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:09 [loggers.py:257] Engine 000: Avg prompt throughput: 43.4 tokens/s, Avg generation throughput: 228.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 132.48 tokens/s, Drafted throughput: 192.77 tokens/s, Accepted: 1325 tokens, Drafted: 1928 tokens, Per-position acceptance rate: 0.778, 0.596, Avg Draft acceptance rate: 68.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3a7c7befc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0b7161c4-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:29 [loggers.py:257] Engine 000: Avg prompt throughput: 171.4 tokens/s, Avg generation throughput: 256.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 76.24 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 1525 tokens, Drafted: 2048 tokens, Per-position acceptance rate: 0.836, 0.653, Avg Draft acceptance rate: 74.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:39 [loggers.py:257] Engine 000: Avg prompt throughput: 210.0 tokens/s, Avg generation throughput: 805.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 462.22 tokens/s, Drafted throughput: 684.68 tokens/s, Accepted: 4623 tokens, Drafted: 6848 tokens, Per-position acceptance rate: 0.763, 0.587, Avg Draft acceptance rate: 67.5%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:49 [loggers.py:257] Engine 000: Avg prompt throughput: 244.4 tokens/s, Avg generation throughput: 806.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 464.14 tokens/s, Drafted throughput: 681.51 tokens/s, Accepted: 4642 tokens, Drafted: 6816 tokens, Per-position acceptance rate: 0.767, 0.595, Avg Draft acceptance rate: 68.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  27.78     
Total input tokens:                      6078      
Total generated tokens:                  20450     
Request throughput (req/s):              2.88      
Output token throughput (tok/s):         736.24    
Peak output token throughput (tok/s):    352.00    
Peak concurrent requests:                25.00     
Total token throughput (tok/s):          955.06    
---------------Time to First Token----------------
Mean TTFT (ms):                          97.18     
Median TTFT (ms):                        93.18     
P99 TTFT (ms):                           194.19    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.53     
Median TPOT (ms):                        19.60     
P99 TPOT (ms):                           22.60     
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.97     
Median ITL (ms):                         45.51     
P99 ITL (ms):                            53.05     
---------------Speculative Decoding---------------
Acceptance rate (%):                     68.05     
Acceptance length:                       2.36      
Drafts:                                  8652      
Draft tokens:                            17304     
Accepted tokens:                         11775     
Per-position acceptance (%):
  Position 0:                            76.96     
  Position 1:                            59.13     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 202.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:03:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 114.79 tokens/s, Drafted throughput: 177.98 tokens/s, Accepted: 1148 tokens, Drafted: 1780 tokens, Per-position acceptance rate: 0.744, 0.546, Avg Draft acceptance rate: 64.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9a2077efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f424131c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:09 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 3.40 tokens/s, Drafted throughput: 3.80 tokens/s, Accepted: 34 tokens, Drafted: 38 tokens, Per-position acceptance rate: 0.947, 0.842, Avg Draft acceptance rate: 89.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:19 [loggers.py:257] Engine 000: Avg prompt throughput: 433.6 tokens/s, Avg generation throughput: 968.6 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 563.44 tokens/s, Drafted throughput: 804.72 tokens/s, Accepted: 5635 tokens, Drafted: 8048 tokens, Per-position acceptance rate: 0.779, 0.621, Avg Draft acceptance rate: 70.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:29 [loggers.py:257] Engine 000: Avg prompt throughput: 174.1 tokens/s, Avg generation throughput: 1099.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:29 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 625.70 tokens/s, Drafted throughput: 951.00 tokens/s, Accepted: 6257 tokens, Drafted: 9510 tokens, Per-position acceptance rate: 0.750, 0.566, Avg Draft acceptance rate: 65.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  16.93     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.73      
Output token throughput (tok/s):         1209.71   
Peak output token throughput (tok/s):    672.00    
Peak concurrent requests:                48.00     
Total token throughput (tok/s):          1568.73   
---------------Time to First Token----------------
Mean TTFT (ms):                          94.22     
Median TTFT (ms):                        97.87     
P99 TTFT (ms):                           119.31    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.63     
Median TPOT (ms):                        20.83     
P99 TPOT (ms):                           23.99     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.34     
Median ITL (ms):                         48.13     
P99 ITL (ms):                            62.65     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.57     
Acceptance length:                       2.35      
Drafts:                                  8706      
Draft tokens:                            17412     
Accepted tokens:                         11766     
Per-position acceptance (%):
  Position 0:                            76.21     
  Position 1:                            58.94     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:39 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 0.30 tokens/s, Drafted throughput: 0.40 tokens/s, Accepted: 3 tokens, Drafted: 4 tokens, Per-position acceptance rate: 1.000, 0.500, Avg Draft acceptance rate: 75.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1ae83bafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-365ecd3a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:49 [loggers.py:257] Engine 000: Avg prompt throughput: 397.7 tokens/s, Avg generation throughput: 119.8 tokens/s, Running: 53 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:49 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.75, Accepted throughput: 73.00 tokens/s, Drafted throughput: 83.20 tokens/s, Accepted: 730 tokens, Drafted: 832 tokens, Per-position acceptance rate: 0.916, 0.839, Avg Draft acceptance rate: 87.7%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:59 [loggers.py:257] Engine 000: Avg prompt throughput: 228.1 tokens/s, Avg generation throughput: 1941.3 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:04:59 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 1112.61 tokens/s, Drafted throughput: 1664.07 tokens/s, Accepted: 11127 tokens, Drafted: 16642 tokens, Per-position acceptance rate: 0.755, 0.582, Avg Draft acceptance rate: 66.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  11.89     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.73      
Output token throughput (tok/s):         1722.15   
Peak output token throughput (tok/s):    1272.00   
Peak concurrent requests:                75.00     
Total token throughput (tok/s):          2233.25   
---------------Time to First Token----------------
Mean TTFT (ms):                          124.37    
Median TTFT (ms):                        116.27    
P99 TTFT (ms):                           215.17    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.73     
Median TPOT (ms):                        22.84     
P99 TPOT (ms):                           26.83     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.26     
Median ITL (ms):                         52.32     
P99 ITL (ms):                            114.28    
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.54     
Acceptance length:                       2.35      
Drafts:                                  8705      
Draft tokens:                            17410     
Accepted tokens:                         11759     
Per-position acceptance (%):
  Position 0:                            76.01     
  Position 1:                            59.07     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:05:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:05:09 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.05, Accepted throughput: 6.50 tokens/s, Drafted throughput: 12.40 tokens/s, Accepted: 65 tokens, Drafted: 124 tokens, Per-position acceptance rate: 0.629, 0.419, Avg Draft acceptance rate: 52.4%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f505b0defc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15001, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8dd50c21-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:05:19 [loggers.py:257] Engine 000: Avg prompt throughput: 183.0 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 20 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:05:19 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 27.00 tokens/s, Drafted throughput: 30.00 tokens/s, Accepted: 270 tokens, Drafted: 300 tokens, Per-position acceptance rate: 0.933, 0.867, Avg Draft acceptance rate: 90.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  7.82      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              10.23     
Output token throughput (tok/s):         2619.01   
Peak output token throughput (tok/s):    1520.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          3396.28   
---------------Time to First Token----------------
Mean TTFT (ms):                          170.75    
Median TTFT (ms):                        169.58    
P99 TTFT (ms):                           310.70    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.71     
Median TPOT (ms):                        24.91     
P99 TPOT (ms):                           28.49     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.84     
Median ITL (ms):                         55.89     
P99 ITL (ms):                            138.27    
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.41     
Acceptance length:                       2.35      
Drafts:                                  8716      
Draft tokens:                            17432     
Accepted tokens:                         11751     
Per-position acceptance (%):
  Position 0:                            76.20     
  Position 1:                            58.62     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k2-t0.0-tp1...
[0;36m(APIServer pid=130035)[0;0m INFO 01-23 12:05:27 [launcher.py:110] Shutting down FastAPI HTTP server.
