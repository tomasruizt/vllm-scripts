Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k5-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k5-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 3695774
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:15 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:15 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15004, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=3695774)[0;0m WARNING 01-23 12:10:15 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:17 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:17 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:18 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:18 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:18 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=3695774)[0;0m WARNING 01-23 12:10:18 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:10:18 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff3322cefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-58e107cb-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:10:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:10:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:10:30 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=5), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:10:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:10:31 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.45:58833 backend=nccl
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:10:31 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=3696180)[0;0m WARNING 01-23 12:10:33 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:10:33 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:10:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:10:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:10:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:10:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:10:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:10:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:26 [default_loader.py:291] Loading weights took 50.23 seconds
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:26 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:26 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:26 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-23 12:11:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:34 [default_loader.py:291] Loading weights took 6.80 seconds
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:35 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 60.374660 seconds
WARNING 01-23 12:11:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:11:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:51 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:11:51 [backends.py:704] Dynamo bytecode transform time: 15.89 s
WARNING 01-23 12:11:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:12:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:12:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:08 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.349 s
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:08 [monitor.py:34] torch.compile takes 20.24 s in total
WARNING 01-23 12:12:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:13 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:13 [backends.py:704] Dynamo bytecode transform time: 4.87 s
WARNING 01-23 12:12:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:19 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.540 s
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:19 [monitor.py:34] torch.compile takes 26.65 s in total
WARNING 01-23 12:12:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:21 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:21 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:21 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-23 12:12:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:12:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
WARNING 01-23 12:12:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:39 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took -0.06 GiB
[0;36m(EngineCore_DP0 pid=3696180)[0;0m INFO 01-23 12:12:40 [core.py:272] init engine (profile, create kv cache, warmup model) took 65.08 seconds
WARNING 01-23 12:12:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15004)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15004 ssl:default [Connect call failed (\'127.0.0.1\', 15004)]\n''
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:42 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=3695774)[0;0m WARNING 01-23 12:12:42 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:42 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:42 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:42 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [serving.py:221] Chat template warmup completed in 1919.0ms
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15004
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:44 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:55 [loggers.py:257] Engine 000: Avg prompt throughput: 29.4 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:12:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.08, Accepted throughput: 33.02 tokens/s, Drafted throughput: 53.56 tokens/s, Accepted: 447 tokens, Drafted: 725 tokens, Per-position acceptance rate: 0.855, 0.669, 0.566, 0.517, 0.476, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:05 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 36.70 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 367 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.706, 0.464, 0.335, 0.211, 0.175, Avg Draft acceptance rate: 37.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:15 [loggers.py:257] Engine 000: Avg prompt throughput: 38.6 tokens/s, Avg generation throughput: 67.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 48.60 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 486 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.776, 0.589, 0.484, 0.380, 0.302, Avg Draft acceptance rate: 50.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:25 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.99, Accepted throughput: 57.70 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 577 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.813, 0.663, 0.565, 0.503, 0.446, Avg Draft acceptance rate: 59.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:35 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 54.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 34.40 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 344 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.696, 0.438, 0.320, 0.206, 0.113, Avg Draft acceptance rate: 35.5%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:45 [loggers.py:257] Engine 000: Avg prompt throughput: 17.6 tokens/s, Avg generation throughput: 74.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.87, Accepted throughput: 55.29 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 553 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.824, 0.653, 0.544, 0.466, 0.378, Avg Draft acceptance rate: 57.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:55 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:13:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 44.20 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 442 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.784, 0.577, 0.433, 0.289, 0.196, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:05 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.73, Accepted throughput: 33.50 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 335 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.660, 0.433, 0.309, 0.175, 0.149, Avg Draft acceptance rate: 34.5%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:15 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 43.99 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 440 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.767, 0.570, 0.404, 0.316, 0.223, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:25 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 45.80 tokens/s, Drafted throughput: 97.49 tokens/s, Accepted: 458 tokens, Drafted: 975 tokens, Per-position acceptance rate: 0.749, 0.590, 0.467, 0.308, 0.236, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:35 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 48.80 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 488 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.715, 0.596, 0.492, 0.399, 0.326, Avg Draft acceptance rate: 50.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:45 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 66.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 46.99 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 470 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.756, 0.575, 0.446, 0.358, 0.301, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:55 [loggers.py:257] Engine 000: Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 79.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:14:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.17, Accepted throughput: 60.90 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 609 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.833, 0.719, 0.620, 0.531, 0.469, Avg Draft acceptance rate: 63.4%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:05 [loggers.py:257] Engine 000: Avg prompt throughput: 8.2 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 43.99 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 440 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.720, 0.508, 0.430, 0.332, 0.290, Avg Draft acceptance rate: 45.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:15 [loggers.py:257] Engine 000: Avg prompt throughput: 31.5 tokens/s, Avg generation throughput: 69.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 50.60 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 506 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.776, 0.620, 0.500, 0.411, 0.328, Avg Draft acceptance rate: 52.7%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:25 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 46.59 tokens/s, Drafted throughput: 97.49 tokens/s, Accepted: 466 tokens, Drafted: 975 tokens, Per-position acceptance rate: 0.754, 0.569, 0.441, 0.328, 0.297, Avg Draft acceptance rate: 47.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:35 [loggers.py:257] Engine 000: Avg prompt throughput: 29.7 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 49.10 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 491 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.760, 0.635, 0.500, 0.370, 0.292, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:45 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 45.60 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 456 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.772, 0.565, 0.435, 0.337, 0.254, Avg Draft acceptance rate: 47.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:55 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:15:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.71, Accepted throughput: 33.40 tokens/s, Drafted throughput: 97.50 tokens/s, Accepted: 334 tokens, Drafted: 975 tokens, Per-position acceptance rate: 0.615, 0.472, 0.277, 0.195, 0.154, Avg Draft acceptance rate: 34.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:05 [loggers.py:257] Engine 000: Avg prompt throughput: 6.9 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 44.40 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 444 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.747, 0.526, 0.397, 0.330, 0.289, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:15 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 50.10 tokens/s, Drafted throughput: 96.50 tokens/s, Accepted: 501 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.782, 0.611, 0.497, 0.373, 0.332, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:25 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 44.40 tokens/s, Drafted throughput: 97.00 tokens/s, Accepted: 444 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.722, 0.577, 0.423, 0.320, 0.247, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.2 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 43.30 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 433 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.751, 0.596, 0.425, 0.264, 0.207, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:45 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 60.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.14, Accepted throughput: 41.50 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 415 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.691, 0.531, 0.376, 0.309, 0.232, Avg Draft acceptance rate: 42.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:55 [loggers.py:257] Engine 000: Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:16:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 49.49 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 495 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.751, 0.585, 0.477, 0.409, 0.342, Avg Draft acceptance rate: 51.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:05 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 41.00 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 410 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.737, 0.536, 0.340, 0.268, 0.232, Avg Draft acceptance rate: 42.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:15 [loggers.py:257] Engine 000: Avg prompt throughput: 17.0 tokens/s, Avg generation throughput: 70.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.68, Accepted throughput: 51.40 tokens/s, Drafted throughput: 96.00 tokens/s, Accepted: 514 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.807, 0.609, 0.500, 0.422, 0.339, Avg Draft acceptance rate: 53.5%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:25 [loggers.py:257] Engine 000: Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 66.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 46.90 tokens/s, Drafted throughput: 97.49 tokens/s, Accepted: 469 tokens, Drafted: 975 tokens, Per-position acceptance rate: 0.774, 0.549, 0.441, 0.359, 0.282, Avg Draft acceptance rate: 48.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:35 [loggers.py:257] Engine 000: Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 38.70 tokens/s, Drafted throughput: 96.49 tokens/s, Accepted: 387 tokens, Drafted: 965 tokens, Per-position acceptance rate: 0.694, 0.477, 0.337, 0.285, 0.212, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:45 [loggers.py:257] Engine 000: Avg prompt throughput: 65.9 tokens/s, Avg generation throughput: 75.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.94, Accepted throughput: 56.49 tokens/s, Drafted throughput: 95.99 tokens/s, Accepted: 565 tokens, Drafted: 960 tokens, Per-position acceptance rate: 0.807, 0.667, 0.568, 0.484, 0.417, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:55 [loggers.py:257] Engine 000: Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:17:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 49.30 tokens/s, Drafted throughput: 96.99 tokens/s, Accepted: 493 tokens, Drafted: 970 tokens, Per-position acceptance rate: 0.799, 0.577, 0.448, 0.381, 0.335, Avg Draft acceptance rate: 50.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  313.42    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.26      
Output token throughput (tok/s):         65.34     
Peak output token throughput (tok/s):    20.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          84.74     
---------------Time to First Token----------------
Mean TTFT (ms):                          63.96     
Median TTFT (ms):                        61.54     
P99 TTFT (ms):                           92.23     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.07     
Median TPOT (ms):                        15.49     
P99 TPOT (ms):                           20.31     
---------------Inter-token Latency----------------
Mean ITL (ms):                           50.88     
Median ITL (ms):                         50.91     
P99 ITL (ms):                            51.43     
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.91     
Acceptance length:                       3.40      
Drafts:                                  6043      
Draft tokens:                            30215     
Accepted tokens:                         14477     
Per-position acceptance (%):
  Position 0:                            75.21     
  Position 1:                            57.09     
  Position 2:                            44.28     
  Position 3:                            34.65     
  Position 4:                            28.33     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:05 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 39.60 tokens/s, Drafted throughput: 79.00 tokens/s, Accepted: 396 tokens, Drafted: 790 tokens, Per-position acceptance rate: 0.741, 0.595, 0.462, 0.373, 0.335, Avg Draft acceptance rate: 50.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3fdaf52fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-858aaa9c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:25 [loggers.py:257] Engine 000: Avg prompt throughput: 45.7 tokens/s, Avg generation throughput: 66.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.01, Accepted throughput: 24.95 tokens/s, Drafted throughput: 41.49 tokens/s, Accepted: 499 tokens, Drafted: 830 tokens, Per-position acceptance rate: 0.807, 0.663, 0.566, 0.500, 0.470, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:35 [loggers.py:257] Engine 000: Avg prompt throughput: 45.6 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 89.59 tokens/s, Drafted throughput: 189.97 tokens/s, Accepted: 896 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.771, 0.558, 0.442, 0.329, 0.258, Avg Draft acceptance rate: 47.2%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:45 [loggers.py:257] Engine 000: Avg prompt throughput: 64.4 tokens/s, Avg generation throughput: 130.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 92.50 tokens/s, Drafted throughput: 188.99 tokens/s, Accepted: 925 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.775, 0.566, 0.450, 0.370, 0.286, Avg Draft acceptance rate: 48.9%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:55 [loggers.py:257] Engine 000: Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 136.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:18:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 98.29 tokens/s, Drafted throughput: 188.97 tokens/s, Accepted: 983 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.815, 0.627, 0.497, 0.373, 0.288, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:05 [loggers.py:257] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 76.59 tokens/s, Drafted throughput: 189.98 tokens/s, Accepted: 766 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.713, 0.474, 0.361, 0.263, 0.205, Avg Draft acceptance rate: 40.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:15 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 125.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 87.58 tokens/s, Drafted throughput: 189.96 tokens/s, Accepted: 876 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.737, 0.550, 0.437, 0.326, 0.255, Avg Draft acceptance rate: 46.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:25 [loggers.py:257] Engine 000: Avg prompt throughput: 60.6 tokens/s, Avg generation throughput: 141.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.76, Accepted throughput: 103.69 tokens/s, Drafted throughput: 187.99 tokens/s, Accepted: 1037 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.779, 0.646, 0.524, 0.434, 0.375, Avg Draft acceptance rate: 55.2%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:35 [loggers.py:257] Engine 000: Avg prompt throughput: 39.7 tokens/s, Avg generation throughput: 134.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 96.09 tokens/s, Drafted throughput: 188.98 tokens/s, Accepted: 961 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.778, 0.622, 0.474, 0.376, 0.294, Avg Draft acceptance rate: 50.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:45 [loggers.py:257] Engine 000: Avg prompt throughput: 39.4 tokens/s, Avg generation throughput: 133.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 95.60 tokens/s, Drafted throughput: 189.99 tokens/s, Accepted: 956 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.768, 0.605, 0.476, 0.358, 0.308, Avg Draft acceptance rate: 50.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:55 [loggers.py:257] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:19:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 76.09 tokens/s, Drafted throughput: 189.98 tokens/s, Accepted: 761 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.697, 0.516, 0.355, 0.247, 0.187, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:05 [loggers.py:257] Engine 000: Avg prompt throughput: 17.3 tokens/s, Avg generation throughput: 125.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.33, Accepted throughput: 88.40 tokens/s, Drafted throughput: 189.99 tokens/s, Accepted: 884 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.739, 0.545, 0.411, 0.332, 0.300, Avg Draft acceptance rate: 46.5%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:15 [loggers.py:257] Engine 000: Avg prompt throughput: 53.4 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 83.89 tokens/s, Drafted throughput: 187.98 tokens/s, Accepted: 839 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.718, 0.532, 0.423, 0.314, 0.245, Avg Draft acceptance rate: 44.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:25 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 124.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 86.09 tokens/s, Drafted throughput: 190.98 tokens/s, Accepted: 861 tokens, Drafted: 1910 tokens, Per-position acceptance rate: 0.717, 0.524, 0.416, 0.330, 0.267, Avg Draft acceptance rate: 45.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:35 [loggers.py:257] Engine 000: Avg prompt throughput: 45.5 tokens/s, Avg generation throughput: 140.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.72, Accepted throughput: 102.89 tokens/s, Drafted throughput: 188.99 tokens/s, Accepted: 1029 tokens, Drafted: 1890 tokens, Per-position acceptance rate: 0.812, 0.635, 0.508, 0.418, 0.349, Avg Draft acceptance rate: 54.4%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:45 [loggers.py:257] Engine 000: Avg prompt throughput: 12.8 tokens/s, Avg generation throughput: 123.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 86.00 tokens/s, Drafted throughput: 189.99 tokens/s, Accepted: 860 tokens, Drafted: 1900 tokens, Per-position acceptance rate: 0.716, 0.534, 0.403, 0.339, 0.271, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:55 [loggers.py:257] Engine 000: Avg prompt throughput: 92.3 tokens/s, Avg generation throughput: 134.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:20:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 97.09 tokens/s, Drafted throughput: 187.98 tokens/s, Accepted: 971 tokens, Drafted: 1880 tokens, Per-position acceptance rate: 0.787, 0.604, 0.476, 0.391, 0.324, Avg Draft acceptance rate: 51.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  161.57    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.50      
Output token throughput (tok/s):         126.75    
Peak output token throughput (tok/s):    40.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          164.37    
---------------Time to First Token----------------
Mean TTFT (ms):                          104.97    
Median TTFT (ms):                        104.24    
P99 TTFT (ms):                           119.11    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.16     
Median TPOT (ms):                        15.26     
P99 TPOT (ms):                           20.27     
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.42     
Median ITL (ms):                         51.38     
P99 ITL (ms):                            53.78     
---------------Speculative Decoding---------------
Acceptance rate (%):                     48.26     
Acceptance length:                       3.41      
Drafts:                                  6014      
Draft tokens:                            30070     
Accepted tokens:                         14513     
Per-position acceptance (%):
  Position 0:                            75.54     
  Position 1:                            57.28     
  Position 2:                            44.75     
  Position 3:                            35.10     
  Position 4:                            28.65     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:05 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 82.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 60.70 tokens/s, Drafted throughput: 113.50 tokens/s, Accepted: 607 tokens, Drafted: 1135 tokens, Per-position acceptance rate: 0.767, 0.634, 0.502, 0.410, 0.361, Avg Draft acceptance rate: 53.5%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f89ea61afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-dca16cce-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:25 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 39.85 tokens/s, Drafted throughput: 74.50 tokens/s, Accepted: 797 tokens, Drafted: 1490 tokens, Per-position acceptance rate: 0.802, 0.597, 0.493, 0.423, 0.359, Avg Draft acceptance rate: 53.5%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:35 [loggers.py:257] Engine 000: Avg prompt throughput: 97.0 tokens/s, Avg generation throughput: 265.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 191.30 tokens/s, Drafted throughput: 370.99 tokens/s, Accepted: 1913 tokens, Drafted: 3710 tokens, Per-position acceptance rate: 0.788, 0.612, 0.481, 0.387, 0.310, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:45 [loggers.py:257] Engine 000: Avg prompt throughput: 43.9 tokens/s, Avg generation throughput: 247.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 171.68 tokens/s, Drafted throughput: 374.96 tokens/s, Accepted: 1717 tokens, Drafted: 3750 tokens, Per-position acceptance rate: 0.748, 0.565, 0.432, 0.312, 0.232, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:55 [loggers.py:257] Engine 000: Avg prompt throughput: 83.2 tokens/s, Avg generation throughput: 267.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:21:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 193.38 tokens/s, Drafted throughput: 370.96 tokens/s, Accepted: 1934 tokens, Drafted: 3710 tokens, Per-position acceptance rate: 0.757, 0.620, 0.495, 0.398, 0.337, Avg Draft acceptance rate: 52.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:05 [loggers.py:257] Engine 000: Avg prompt throughput: 79.9 tokens/s, Avg generation throughput: 263.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 189.57 tokens/s, Drafted throughput: 372.94 tokens/s, Accepted: 1896 tokens, Drafted: 3730 tokens, Per-position acceptance rate: 0.765, 0.599, 0.479, 0.381, 0.318, Avg Draft acceptance rate: 50.8%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:15 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 240.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 165.89 tokens/s, Drafted throughput: 373.97 tokens/s, Accepted: 1659 tokens, Drafted: 3740 tokens, Per-position acceptance rate: 0.722, 0.540, 0.393, 0.302, 0.261, Avg Draft acceptance rate: 44.4%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:25 [loggers.py:257] Engine 000: Avg prompt throughput: 57.2 tokens/s, Avg generation throughput: 247.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.29, Accepted throughput: 172.09 tokens/s, Drafted throughput: 374.99 tokens/s, Accepted: 1721 tokens, Drafted: 3750 tokens, Per-position acceptance rate: 0.728, 0.548, 0.433, 0.335, 0.251, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:35 [loggers.py:257] Engine 000: Avg prompt throughput: 94.3 tokens/s, Avg generation throughput: 254.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 181.59 tokens/s, Drafted throughput: 370.97 tokens/s, Accepted: 1816 tokens, Drafted: 3710 tokens, Per-position acceptance rate: 0.763, 0.567, 0.447, 0.375, 0.295, Avg Draft acceptance rate: 48.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  83.21     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.96      
Output token throughput (tok/s):         246.13    
Peak output token throughput (tok/s):    80.00     
Peak concurrent requests:                8.00      
Total token throughput (tok/s):          319.18    
---------------Time to First Token----------------
Mean TTFT (ms):                          105.70    
Median TTFT (ms):                        105.76    
P99 TTFT (ms):                           116.49    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.24     
Median TPOT (ms):                        15.18     
P99 TPOT (ms):                           20.55     
---------------Inter-token Latency----------------
Mean ITL (ms):                           52.20     
Median ITL (ms):                         52.09     
P99 ITL (ms):                            56.09     
---------------Speculative Decoding---------------
Acceptance rate (%):                     48.87     
Acceptance length:                       3.44      
Drafts:                                  5955      
Draft tokens:                            29775     
Accepted tokens:                         14552     
Per-position acceptance (%):
  Position 0:                            75.68     
  Position 1:                            58.15     
  Position 2:                            45.32     
  Position 3:                            36.02     
  Position 4:                            29.19     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:45 [loggers.py:257] Engine 000: Avg prompt throughput: 43.4 tokens/s, Avg generation throughput: 178.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 129.70 tokens/s, Drafted throughput: 248.99 tokens/s, Accepted: 1297 tokens, Drafted: 2490 tokens, Per-position acceptance rate: 0.785, 0.614, 0.466, 0.398, 0.341, Avg Draft acceptance rate: 52.1%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:22:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f60501aafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d28bdfac-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:05 [loggers.py:257] Engine 000: Avg prompt throughput: 94.1 tokens/s, Avg generation throughput: 139.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 51.69 tokens/s, Drafted throughput: 88.49 tokens/s, Accepted: 1034 tokens, Drafted: 1770 tokens, Per-position acceptance rate: 0.811, 0.641, 0.554, 0.486, 0.429, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:15 [loggers.py:257] Engine 000: Avg prompt throughput: 121.2 tokens/s, Avg generation throughput: 479.7 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.35, Accepted throughput: 337.35 tokens/s, Drafted throughput: 718.88 tokens/s, Accepted: 3374 tokens, Drafted: 7190 tokens, Per-position acceptance rate: 0.750, 0.572, 0.433, 0.333, 0.259, Avg Draft acceptance rate: 46.9%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:25 [loggers.py:257] Engine 000: Avg prompt throughput: 163.1 tokens/s, Avg generation throughput: 500.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 359.05 tokens/s, Drafted throughput: 713.90 tokens/s, Accepted: 3591 tokens, Drafted: 7140 tokens, Per-position acceptance rate: 0.764, 0.592, 0.476, 0.372, 0.311, Avg Draft acceptance rate: 50.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:35 [loggers.py:257] Engine 000: Avg prompt throughput: 119.8 tokens/s, Avg generation throughput: 460.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 318.16 tokens/s, Drafted throughput: 717.92 tokens/s, Accepted: 3182 tokens, Drafted: 7180 tokens, Per-position acceptance rate: 0.720, 0.533, 0.405, 0.307, 0.251, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:45 [loggers.py:257] Engine 000: Avg prompt throughput: 127.6 tokens/s, Avg generation throughput: 463.9 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 330.57 tokens/s, Drafted throughput: 673.95 tokens/s, Accepted: 3306 tokens, Drafted: 6740 tokens, Per-position acceptance rate: 0.763, 0.576, 0.450, 0.366, 0.297, Avg Draft acceptance rate: 49.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  44.60     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              1.79      
Output token throughput (tok/s):         458.15    
Peak output token throughput (tok/s):    152.00    
Peak concurrent requests:                12.00     
Total token throughput (tok/s):          594.41    
---------------Time to First Token----------------
Mean TTFT (ms):                          108.61    
Median TTFT (ms):                        109.69    
P99 TTFT (ms):                           121.01    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.07     
Median TPOT (ms):                        16.09     
P99 TPOT (ms):                           21.97     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.26     
Median ITL (ms):                         54.02     
P99 ITL (ms):                            60.33     
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.97     
Acceptance length:                       3.40      
Drafts:                                  6029      
Draft tokens:                            30145     
Accepted tokens:                         14460     
Per-position acceptance (%):
  Position 0:                            75.04     
  Position 1:                            56.99     
  Position 2:                            44.44     
  Position 3:                            34.96     
  Position 4:                            28.41     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:23:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 17.10 tokens/s, Drafted throughput: 43.00 tokens/s, Accepted: 171 tokens, Drafted: 430 tokens, Per-position acceptance rate: 0.686, 0.500, 0.326, 0.302, 0.174, Avg Draft acceptance rate: 39.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0ce753efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-28f1dfb1-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:05 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.16, Accepted throughput: 18.30 tokens/s, Drafted throughput: 29.00 tokens/s, Accepted: 183 tokens, Drafted: 290 tokens, Per-position acceptance rate: 0.879, 0.672, 0.569, 0.534, 0.500, Avg Draft acceptance rate: 63.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:15 [loggers.py:257] Engine 000: Avg prompt throughput: 324.6 tokens/s, Avg generation throughput: 857.0 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 610.77 tokens/s, Drafted throughput: 1235.95 tokens/s, Accepted: 6108 tokens, Drafted: 12360 tokens, Per-position acceptance rate: 0.765, 0.585, 0.459, 0.369, 0.294, Avg Draft acceptance rate: 49.4%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:25 [loggers.py:257] Engine 000: Avg prompt throughput: 254.1 tokens/s, Avg generation throughput: 907.9 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 636.11 tokens/s, Drafted throughput: 1374.02 tokens/s, Accepted: 6361 tokens, Drafted: 13740 tokens, Per-position acceptance rate: 0.743, 0.563, 0.421, 0.326, 0.262, Avg Draft acceptance rate: 46.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  25.14     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              3.18      
Output token throughput (tok/s):         814.50    
Peak output token throughput (tok/s):    288.00    
Peak concurrent requests:                23.00     
Total token throughput (tok/s):          1056.22   
---------------Time to First Token----------------
Mean TTFT (ms):                          112.48    
Median TTFT (ms):                        113.81    
P99 TTFT (ms):                           130.50    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.04     
Median TPOT (ms):                        17.15     
P99 TPOT (ms):                           23.52     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.13     
Median ITL (ms):                         56.92     
P99 ITL (ms):                            66.75     
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.53     
Acceptance length:                       3.38      
Drafts:                                  6085      
Draft tokens:                            30425     
Accepted tokens:                         14460     
Per-position acceptance (%):
  Position 0:                            75.10     
  Position 1:                            57.06     
  Position 2:                            43.52     
  Position 3:                            34.43     
  Position 4:                            27.53     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:35 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 284.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 200.59 tokens/s, Drafted throughput: 433.98 tokens/s, Accepted: 2006 tokens, Drafted: 4340 tokens, Per-position acceptance rate: 0.737, 0.556, 0.415, 0.336, 0.266, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8cde42efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9eee6b36-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:55 [loggers.py:257] Engine 000: Avg prompt throughput: 449.0 tokens/s, Avg generation throughput: 1043.8 tokens/s, Running: 30 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:24:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 375.58 tokens/s, Drafted throughput: 728.96 tokens/s, Accepted: 7512 tokens, Drafted: 14580 tokens, Per-position acceptance rate: 0.771, 0.606, 0.484, 0.394, 0.321, Avg Draft acceptance rate: 51.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  15.58     
Total input tokens:                      6078      
Total generated tokens:                  20471     
Request throughput (req/s):              5.14      
Output token throughput (tok/s):         1314.31   
Peak output token throughput (tok/s):    544.00    
Peak concurrent requests:                46.00     
Total token throughput (tok/s):          1704.54   
---------------Time to First Token----------------
Mean TTFT (ms):                          130.13    
Median TTFT (ms):                        125.87    
P99 TTFT (ms):                           185.80    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.27     
Median TPOT (ms):                        18.09     
P99 TPOT (ms):                           24.75     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.53     
Median ITL (ms):                         60.22     
P99 ITL (ms):                            116.54    
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.77     
Acceptance length:                       3.39      
Drafts:                                  6054      
Draft tokens:                            30270     
Accepted tokens:                         14461     
Per-position acceptance (%):
  Position 0:                            74.60     
  Position 1:                            56.85     
  Position 2:                            43.92     
  Position 3:                            35.13     
  Position 4:                            28.36     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:05 [loggers.py:257] Engine 000: Avg prompt throughput: 176.7 tokens/s, Avg generation throughput: 1028.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 714.66 tokens/s, Drafted throughput: 1599.40 tokens/s, Accepted: 7147 tokens, Drafted: 15995 tokens, Per-position acceptance rate: 0.726, 0.537, 0.401, 0.316, 0.254, Avg Draft acceptance rate: 44.7%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efcfba7afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0134a9ab-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:35 [loggers.py:257] Engine 000: Avg prompt throughput: 623.1 tokens/s, Avg generation throughput: 1499.8 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 50.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 357.87 tokens/s, Drafted throughput: 704.04 tokens/s, Accepted: 10738 tokens, Drafted: 21125 tokens, Per-position acceptance rate: 0.770, 0.594, 0.472, 0.385, 0.321, Avg Draft acceptance rate: 50.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  11.50     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.96      
Output token throughput (tok/s):         1781.47   
Peak output token throughput (tok/s):    896.00    
Peak concurrent requests:                73.00     
Total token throughput (tok/s):          2310.17   
---------------Time to First Token----------------
Mean TTFT (ms):                          167.08    
Median TTFT (ms):                        156.00    
P99 TTFT (ms):                           299.14    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.13     
Median TPOT (ms):                        22.61     
P99 TPOT (ms):                           29.73     
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.33     
Median ITL (ms):                         73.50     
P99 ITL (ms):                            132.38    
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.61     
Acceptance length:                       3.38      
Drafts:                                  6073      
Draft tokens:                            30365     
Accepted tokens:                         14458     
Per-position acceptance (%):
  Position 0:                            74.79     
  Position 1:                            56.43     
  Position 2:                            43.54     
  Position 3:                            34.93     
  Position 4:                            28.39     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:45 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 573.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.05, Accepted throughput: 391.79 tokens/s, Drafted throughput: 954.47 tokens/s, Accepted: 3918 tokens, Drafted: 9545 tokens, Per-position acceptance rate: 0.704, 0.503, 0.358, 0.278, 0.210, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:25:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fabfa5bafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15004, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-af2eb0ee-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:26:05 [loggers.py:257] Engine 000: Avg prompt throughput: 625.8 tokens/s, Avg generation throughput: 1706.7 tokens/s, Running: 67 reqs, Waiting: 0 reqs, GPU KV cache usage: 59.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:26:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 611.53 tokens/s, Drafted throughput: 1196.21 tokens/s, Accepted: 12231 tokens, Drafted: 23925 tokens, Per-position acceptance rate: 0.778, 0.598, 0.476, 0.385, 0.318, Avg Draft acceptance rate: 51.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  8.86      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              9.03      
Output token throughput (tok/s):         2312.11   
Peak output token throughput (tok/s):    960.00    
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2998.29   
---------------Time to First Token----------------
Mean TTFT (ms):                          213.40    
Median TTFT (ms):                        205.52    
P99 TTFT (ms):                           348.43    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.57     
Median TPOT (ms):                        25.09     
P99 TPOT (ms):                           33.59     
---------------Inter-token Latency----------------
Mean ITL (ms):                           82.82     
Median ITL (ms):                         81.71     
P99 ITL (ms):                            189.20    
---------------Speculative Decoding---------------
Acceptance rate (%):                     47.87     
Acceptance length:                       3.39      
Drafts:                                  6053      
Draft tokens:                            30265     
Accepted tokens:                         14489     
Per-position acceptance (%):
  Position 0:                            75.52     
  Position 1:                            56.70     
  Position 2:                            44.09     
  Position 3:                            34.88     
  Position 4:                            28.18     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k5-t0.0-tp1...
[0;36m(APIServer pid=3695774)[0;0m INFO 01-23 12:26:08 [launcher.py:110] Shutting down FastAPI HTTP server.
