Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k6-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k6-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 261674
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:23 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:23 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15005, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-1.7B', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=261674)[0;0m WARNING 01-23 20:08:23 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:24 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:24 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:25 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:25 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:25 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=261674)[0;0m WARNING 01-23 20:08:25 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:08:25 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1c6b2b6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-913343e3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 20:08:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:08:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:08:36 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-1.7B', num_spec_tokens=6), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:08:37 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.68:33177 backend=nccl
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:08:37 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
WARNING 01-23 20:08:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m WARNING 01-23 20:08:38 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:08:38 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:08:39 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 20:08:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:08:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:08:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:08:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:31 [default_loader.py:291] Loading weights took 50.70 seconds
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:31 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:31 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:31 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-1.7B. TP=1, rank=0
WARNING 01-23 20:09:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:34 [default_loader.py:291] Loading weights took 2.11 seconds
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:35 [gpu_model_runner.py:3921] Model loading took 64.24 GiB memory and 55.776388 seconds
WARNING 01-23 20:09:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:48 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:09:48 [backends.py:704] Dynamo bytecode transform time: 12.33 s
WARNING 01-23 20:09:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:09:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:10:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:05 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.361 s
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:05 [monitor.py:34] torch.compile takes 16.69 s in total
WARNING 01-23 20:10:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:10 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/b331b0176a/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:10 [backends.py:704] Dynamo bytecode transform time: 4.85 s
WARNING 01-23 20:10:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:16 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.591 s
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:16 [monitor.py:34] torch.compile takes 23.13 s in total
WARNING 01-23 20:10:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:18 [gpu_worker.py:355] Available KV cache memory: 11.42 GiB
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:18 [kv_cache_utils.py:1307] GPU KV cache size: 32,528 tokens
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:18 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 6.50x
WARNING 01-23 20:10:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:10:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:10:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
WARNING 01-23 20:10:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15005)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15005 ssl:default [Connect call failed (\'127.0.0.1\', 15005)]\n''
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:38 [gpu_model_runner.py:4880] Graph capturing finished in 19 secs, took -0.06 GiB
[0;36m(EngineCore_DP0 pid=261771)[0;0m INFO 01-23 20:10:38 [core.py:272] init engine (profile, create kv cache, warmup model) took 63.13 seconds
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:40 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=261674)[0;0m WARNING 01-23 20:10:40 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:40 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:40 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:40 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [serving.py:221] Chat template warmup completed in 1704.3ms
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15005
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:42 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:53 [loggers.py:257] Engine 000: Avg prompt throughput: 30.8 tokens/s, Avg generation throughput: 51.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:10:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.11, Accepted throughput: 38.67 tokens/s, Drafted throughput: 74.70 tokens/s, Accepted: 500 tokens, Drafted: 966 tokens, Per-position acceptance rate: 0.776, 0.590, 0.503, 0.460, 0.416, 0.360, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:03 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 34.20 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 342 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.672, 0.437, 0.301, 0.191, 0.148, 0.120, Avg Draft acceptance rate: 31.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:13 [loggers.py:257] Engine 000: Avg prompt throughput: 38.6 tokens/s, Avg generation throughput: 70.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 52.79 tokens/s, Drafted throughput: 108.58 tokens/s, Accepted: 528 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.801, 0.586, 0.497, 0.414, 0.343, 0.276, Avg Draft acceptance rate: 48.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:23 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 76.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.25, Accepted throughput: 59.09 tokens/s, Drafted throughput: 109.18 tokens/s, Accepted: 591 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.830, 0.654, 0.538, 0.467, 0.401, 0.357, Avg Draft acceptance rate: 54.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:33 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 41.00 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 410 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.765, 0.503, 0.372, 0.273, 0.191, 0.137, Avg Draft acceptance rate: 37.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:43 [loggers.py:257] Engine 000: Avg prompt throughput: 17.6 tokens/s, Avg generation throughput: 72.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 54.69 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 547 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.746, 0.646, 0.564, 0.425, 0.354, 0.287, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:53 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:11:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 40.30 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 403 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.760, 0.536, 0.377, 0.251, 0.169, 0.109, Avg Draft acceptance rate: 36.7%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:03 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 51.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 32.70 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 327 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.650, 0.421, 0.295, 0.169, 0.148, 0.104, Avg Draft acceptance rate: 29.8%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:13 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.52, Accepted throughput: 45.80 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 458 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.725, 0.571, 0.440, 0.324, 0.253, 0.203, Avg Draft acceptance rate: 41.9%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:23 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 62.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 44.40 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 444 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.705, 0.557, 0.421, 0.311, 0.235, 0.197, Avg Draft acceptance rate: 40.4%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:33 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 69.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.85, Accepted throughput: 51.89 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 519 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.742, 0.604, 0.489, 0.385, 0.335, 0.297, Avg Draft acceptance rate: 47.5%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:43 [loggers.py:257] Engine 000: Avg prompt throughput: 43.5 tokens/s, Avg generation throughput: 67.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 50.10 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 501 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.746, 0.597, 0.470, 0.376, 0.326, 0.254, Avg Draft acceptance rate: 46.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:53 [loggers.py:257] Engine 000: Avg prompt throughput: 17.1 tokens/s, Avg generation throughput: 79.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:12:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.40, Accepted throughput: 61.79 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 618 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.797, 0.687, 0.588, 0.495, 0.429, 0.401, Avg Draft acceptance rate: 56.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:03 [loggers.py:257] Engine 000: Avg prompt throughput: 8.2 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 48.50 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 485 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.781, 0.536, 0.443, 0.350, 0.301, 0.240, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:13 [loggers.py:257] Engine 000: Avg prompt throughput: 31.5 tokens/s, Avg generation throughput: 65.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 47.90 tokens/s, Drafted throughput: 108.60 tokens/s, Accepted: 479 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.718, 0.569, 0.436, 0.370, 0.298, 0.254, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:23 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 66.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 48.19 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 482 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.743, 0.552, 0.421, 0.344, 0.306, 0.268, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:33 [loggers.py:257] Engine 000: Avg prompt throughput: 29.7 tokens/s, Avg generation throughput: 72.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.98, Accepted throughput: 54.20 tokens/s, Drafted throughput: 109.20 tokens/s, Accepted: 542 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.764, 0.626, 0.500, 0.396, 0.368, 0.324, Avg Draft acceptance rate: 49.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:43 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 40.49 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 405 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.692, 0.500, 0.368, 0.286, 0.214, 0.165, Avg Draft acceptance rate: 37.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:53 [loggers.py:257] Engine 000: Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 49.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:13:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 31.60 tokens/s, Drafted throughput: 110.38 tokens/s, Accepted: 316 tokens, Drafted: 1104 tokens, Per-position acceptance rate: 0.587, 0.429, 0.261, 0.179, 0.147, 0.114, Avg Draft acceptance rate: 28.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:03 [loggers.py:257] Engine 000: Avg prompt throughput: 9.9 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.26, Accepted throughput: 41.19 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 412 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.725, 0.478, 0.341, 0.280, 0.242, 0.198, Avg Draft acceptance rate: 37.7%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:13 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 72.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.01, Accepted throughput: 54.80 tokens/s, Drafted throughput: 109.20 tokens/s, Accepted: 548 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.775, 0.632, 0.538, 0.418, 0.363, 0.286, Avg Draft acceptance rate: 50.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:23 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 44.59 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 446 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.687, 0.571, 0.418, 0.319, 0.258, 0.198, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:33 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.21, Accepted throughput: 40.50 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 405 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.738, 0.574, 0.383, 0.224, 0.175, 0.120, Avg Draft acceptance rate: 36.9%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:43 [loggers.py:257] Engine 000: Avg prompt throughput: 17.1 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 40.70 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 407 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.698, 0.495, 0.385, 0.280, 0.220, 0.159, Avg Draft acceptance rate: 37.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:53 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:14:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 48.50 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 485 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.727, 0.546, 0.454, 0.366, 0.295, 0.262, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:03 [loggers.py:257] Engine 000: Avg prompt throughput: 28.5 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 45.40 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 454 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.742, 0.555, 0.379, 0.313, 0.280, 0.225, Avg Draft acceptance rate: 41.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:13 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.73, Accepted throughput: 50.00 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 500 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.781, 0.585, 0.443, 0.372, 0.295, 0.257, Avg Draft acceptance rate: 45.5%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:23 [loggers.py:257] Engine 000: Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 69.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.86, Accepted throughput: 51.80 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 518 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.785, 0.586, 0.470, 0.414, 0.326, 0.282, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:33 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 57.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 38.80 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 388 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.699, 0.470, 0.344, 0.268, 0.197, 0.142, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:43 [loggers.py:257] Engine 000: Avg prompt throughput: 54.4 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.25, Accepted throughput: 59.10 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 591 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.797, 0.637, 0.560, 0.489, 0.418, 0.346, Avg Draft acceptance rate: 54.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:53 [loggers.py:257] Engine 000: Avg prompt throughput: 37.9 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:15:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 50.29 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 503 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.779, 0.580, 0.470, 0.392, 0.309, 0.249, Avg Draft acceptance rate: 46.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  315.82    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.25      
Output token throughput (tok/s):         64.85     
Peak output token throughput (tok/s):    19.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          84.09     
---------------Time to First Token----------------
Mean TTFT (ms):                          66.04     
Median TTFT (ms):                        64.64     
P99 TTFT (ms):                           77.96     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.18     
Median TPOT (ms):                        15.55     
P99 TPOT (ms):                           21.38     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.96     
Median ITL (ms):                         53.99     
P99 ITL (ms):                            54.54     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.05     
Acceptance length:                       3.58      
Drafts:                                  5739      
Draft tokens:                            34434     
Accepted tokens:                         14824     
Per-position acceptance (%):
  Position 0:                            73.97     
  Position 1:                            55.88     
  Position 2:                            43.34     
  Position 3:                            33.98     
  Position 4:                            28.09     
  Position 5:                            23.05     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:03 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.82, Accepted throughput: 47.09 tokens/s, Drafted throughput: 100.19 tokens/s, Accepted: 471 tokens, Drafted: 1002 tokens, Per-position acceptance rate: 0.778, 0.611, 0.461, 0.359, 0.329, 0.281, Avg Draft acceptance rate: 47.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f58d6352fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a0c0d7aa-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:33 [loggers.py:257] Engine 000: Avg prompt throughput: 52.7 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 24.60 tokens/s, Drafted throughput: 54.79 tokens/s, Accepted: 738 tokens, Drafted: 1644 tokens, Per-position acceptance rate: 0.763, 0.551, 0.445, 0.365, 0.303, 0.266, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:43 [loggers.py:257] Engine 000: Avg prompt throughput: 46.0 tokens/s, Avg generation throughput: 138.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 103.80 tokens/s, Drafted throughput: 214.80 tokens/s, Accepted: 1038 tokens, Drafted: 2148 tokens, Per-position acceptance rate: 0.777, 0.581, 0.492, 0.411, 0.344, 0.296, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:53 [loggers.py:257] Engine 000: Avg prompt throughput: 61.6 tokens/s, Avg generation throughput: 127.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:16:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 91.79 tokens/s, Drafted throughput: 213.57 tokens/s, Accepted: 918 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.778, 0.565, 0.449, 0.334, 0.253, 0.199, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:03 [loggers.py:257] Engine 000: Avg prompt throughput: 20.0 tokens/s, Avg generation throughput: 118.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 83.29 tokens/s, Drafted throughput: 213.58 tokens/s, Accepted: 833 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.739, 0.539, 0.410, 0.284, 0.213, 0.154, Avg Draft acceptance rate: 39.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:13 [loggers.py:257] Engine 000: Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 78.79 tokens/s, Drafted throughput: 215.98 tokens/s, Accepted: 788 tokens, Drafted: 2160 tokens, Per-position acceptance rate: 0.667, 0.497, 0.383, 0.289, 0.200, 0.153, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:23 [loggers.py:257] Engine 000: Avg prompt throughput: 65.5 tokens/s, Avg generation throughput: 125.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 90.10 tokens/s, Drafted throughput: 212.99 tokens/s, Accepted: 901 tokens, Drafted: 2130 tokens, Per-position acceptance rate: 0.755, 0.549, 0.417, 0.315, 0.268, 0.234, Avg Draft acceptance rate: 42.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:33 [loggers.py:257] Engine 000: Avg prompt throughput: 31.3 tokens/s, Avg generation throughput: 149.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.21, Accepted throughput: 114.08 tokens/s, Drafted throughput: 212.97 tokens/s, Accepted: 1141 tokens, Drafted: 2130 tokens, Per-position acceptance rate: 0.792, 0.659, 0.561, 0.459, 0.394, 0.349, Avg Draft acceptance rate: 53.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:43 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 93.00 tokens/s, Drafted throughput: 213.59 tokens/s, Accepted: 930 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.753, 0.565, 0.435, 0.348, 0.278, 0.233, Avg Draft acceptance rate: 43.5%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:53 [loggers.py:257] Engine 000: Avg prompt throughput: 38.8 tokens/s, Avg generation throughput: 122.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:17:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 87.99 tokens/s, Drafted throughput: 213.58 tokens/s, Accepted: 880 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.736, 0.576, 0.393, 0.289, 0.256, 0.222, Avg Draft acceptance rate: 41.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:03 [loggers.py:257] Engine 000: Avg prompt throughput: 16.7 tokens/s, Avg generation throughput: 113.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 77.99 tokens/s, Drafted throughput: 214.77 tokens/s, Accepted: 780 tokens, Drafted: 2148 tokens, Per-position acceptance rate: 0.684, 0.500, 0.360, 0.257, 0.204, 0.173, Avg Draft acceptance rate: 36.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:13 [loggers.py:257] Engine 000: Avg prompt throughput: 40.1 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 96.60 tokens/s, Drafted throughput: 213.60 tokens/s, Accepted: 966 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.750, 0.570, 0.441, 0.371, 0.320, 0.261, Avg Draft acceptance rate: 45.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:23 [loggers.py:257] Engine 000: Avg prompt throughput: 24.5 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 83.40 tokens/s, Drafted throughput: 215.99 tokens/s, Accepted: 834 tokens, Drafted: 2160 tokens, Per-position acceptance rate: 0.700, 0.533, 0.403, 0.294, 0.219, 0.167, Avg Draft acceptance rate: 38.6%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:33 [loggers.py:257] Engine 000: Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 121.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 85.89 tokens/s, Drafted throughput: 213.57 tokens/s, Accepted: 859 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.713, 0.525, 0.399, 0.303, 0.261, 0.211, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:43 [loggers.py:257] Engine 000: Avg prompt throughput: 42.4 tokens/s, Avg generation throughput: 139.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 104.09 tokens/s, Drafted throughput: 213.57 tokens/s, Accepted: 1041 tokens, Drafted: 2136 tokens, Per-position acceptance rate: 0.801, 0.607, 0.483, 0.407, 0.329, 0.298, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:53 [loggers.py:257] Engine 000: Avg prompt throughput: 12.8 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:18:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.27, Accepted throughput: 81.59 tokens/s, Drafted throughput: 215.98 tokens/s, Accepted: 816 tokens, Drafted: 2160 tokens, Per-position acceptance rate: 0.700, 0.511, 0.372, 0.286, 0.225, 0.172, Avg Draft acceptance rate: 37.8%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:03 [loggers.py:257] Engine 000: Avg prompt throughput: 95.3 tokens/s, Avg generation throughput: 138.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.94, Accepted throughput: 103.39 tokens/s, Drafted throughput: 211.17 tokens/s, Accepted: 1034 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.781, 0.614, 0.494, 0.418, 0.344, 0.287, Avg Draft acceptance rate: 49.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  164.26    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.49      
Output token throughput (tok/s):         124.68    
Peak output token throughput (tok/s):    38.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          161.68    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.15    
Median TTFT (ms):                        110.52    
P99 TTFT (ms):                           126.79    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.41     
Median TPOT (ms):                        15.83     
P99 TPOT (ms):                           21.25     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.52     
Median ITL (ms):                         54.49     
P99 ITL (ms):                            56.70     
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.77     
Acceptance length:                       3.57      
Drafts:                                  5765      
Draft tokens:                            34590     
Accepted tokens:                         14795     
Per-position acceptance (%):
  Position 0:                            74.05     
  Position 1:                            55.66     
  Position 2:                            43.10     
  Position 3:                            33.67     
  Position 4:                            27.37     
  Position 5:                            22.78     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:13 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 69.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 49.90 tokens/s, Drafted throughput: 118.19 tokens/s, Accepted: 499 tokens, Drafted: 1182 tokens, Per-position acceptance rate: 0.711, 0.518, 0.406, 0.335, 0.305, 0.259, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0d983cefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ffec0c84-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:33 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 116.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 43.10 tokens/s, Drafted throughput: 90.90 tokens/s, Accepted: 862 tokens, Drafted: 1818 tokens, Per-position acceptance rate: 0.772, 0.564, 0.469, 0.413, 0.343, 0.284, Avg Draft acceptance rate: 47.4%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:43 [loggers.py:257] Engine 000: Avg prompt throughput: 97.0 tokens/s, Avg generation throughput: 261.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 191.78 tokens/s, Drafted throughput: 418.76 tokens/s, Accepted: 1918 tokens, Drafted: 4188 tokens, Per-position acceptance rate: 0.788, 0.592, 0.464, 0.368, 0.295, 0.241, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:53 [loggers.py:257] Engine 000: Avg prompt throughput: 43.9 tokens/s, Avg generation throughput: 237.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:19:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 167.18 tokens/s, Drafted throughput: 421.15 tokens/s, Accepted: 1672 tokens, Drafted: 4212 tokens, Per-position acceptance rate: 0.722, 0.537, 0.415, 0.291, 0.231, 0.187, Avg Draft acceptance rate: 39.7%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:03 [loggers.py:257] Engine 000: Avg prompt throughput: 83.2 tokens/s, Avg generation throughput: 264.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 196.28 tokens/s, Drafted throughput: 418.75 tokens/s, Accepted: 1963 tokens, Drafted: 4188 tokens, Per-position acceptance rate: 0.755, 0.600, 0.480, 0.383, 0.318, 0.277, Avg Draft acceptance rate: 46.9%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:13 [loggers.py:257] Engine 000: Avg prompt throughput: 69.3 tokens/s, Avg generation throughput: 248.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 179.88 tokens/s, Drafted throughput: 421.76 tokens/s, Accepted: 1799 tokens, Drafted: 4218 tokens, Per-position acceptance rate: 0.745, 0.573, 0.430, 0.326, 0.267, 0.218, Avg Draft acceptance rate: 42.7%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:23 [loggers.py:257] Engine 000: Avg prompt throughput: 56.8 tokens/s, Avg generation throughput: 231.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.30, Accepted throughput: 161.48 tokens/s, Drafted throughput: 420.56 tokens/s, Accepted: 1615 tokens, Drafted: 4206 tokens, Per-position acceptance rate: 0.710, 0.512, 0.377, 0.291, 0.231, 0.183, Avg Draft acceptance rate: 38.4%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:33 [loggers.py:257] Engine 000: Avg prompt throughput: 48.2 tokens/s, Avg generation throughput: 244.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 174.68 tokens/s, Drafted throughput: 423.54 tokens/s, Accepted: 1747 tokens, Drafted: 4236 tokens, Per-position acceptance rate: 0.741, 0.545, 0.419, 0.314, 0.259, 0.195, Avg Draft acceptance rate: 41.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:43 [loggers.py:257] Engine 000: Avg prompt throughput: 58.0 tokens/s, Avg generation throughput: 250.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 181.19 tokens/s, Drafted throughput: 419.99 tokens/s, Accepted: 1812 tokens, Drafted: 4200 tokens, Per-position acceptance rate: 0.731, 0.566, 0.429, 0.349, 0.280, 0.234, Avg Draft acceptance rate: 43.1%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:53 [loggers.py:257] Engine 000: Avg prompt throughput: 95.0 tokens/s, Avg generation throughput: 213.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:20:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 157.78 tokens/s, Drafted throughput: 338.36 tokens/s, Accepted: 1578 tokens, Drafted: 3384 tokens, Per-position acceptance rate: 0.770, 0.590, 0.463, 0.379, 0.326, 0.270, Avg Draft acceptance rate: 46.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  85.49     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.94      
Output token throughput (tok/s):         239.56    
Peak output token throughput (tok/s):    76.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          310.66    
---------------Time to First Token----------------
Mean TTFT (ms):                          112.23    
Median TTFT (ms):                        113.31    
P99 TTFT (ms):                           123.56    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.61     
Median TPOT (ms):                        15.46     
P99 TPOT (ms):                           21.81     
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.46     
Median ITL (ms):                         55.33     
P99 ITL (ms):                            59.41     
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.97     
Acceptance length:                       3.58      
Drafts:                                  5743      
Draft tokens:                            34458     
Accepted tokens:                         14805     
Per-position acceptance (%):
  Position 0:                            74.42     
  Position 1:                            56.22     
  Position 2:                            43.34     
  Position 3:                            33.78     
  Position 4:                            27.56     
  Position 5:                            22.46     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.67, Accepted throughput: 4.00 tokens/s, Drafted throughput: 14.40 tokens/s, Accepted: 40 tokens, Drafted: 144 tokens, Per-position acceptance rate: 0.500, 0.375, 0.250, 0.208, 0.208, 0.125, Avg Draft acceptance rate: 27.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fefebc7efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f4b011e2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:13 [loggers.py:257] Engine 000: Avg prompt throughput: 84.8 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.93, Accepted throughput: 34.60 tokens/s, Drafted throughput: 52.80 tokens/s, Accepted: 346 tokens, Drafted: 528 tokens, Per-position acceptance rate: 0.864, 0.716, 0.636, 0.625, 0.580, 0.511, Avg Draft acceptance rate: 65.5%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:23 [loggers.py:257] Engine 000: Avg prompt throughput: 107.5 tokens/s, Avg generation throughput: 486.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 351.88 tokens/s, Drafted throughput: 813.55 tokens/s, Accepted: 3519 tokens, Drafted: 8136 tokens, Per-position acceptance rate: 0.757, 0.558, 0.439, 0.350, 0.277, 0.216, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:33 [loggers.py:257] Engine 000: Avg prompt throughput: 146.8 tokens/s, Avg generation throughput: 495.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 360.46 tokens/s, Drafted throughput: 818.31 tokens/s, Accepted: 3605 tokens, Drafted: 8184 tokens, Per-position acceptance rate: 0.735, 0.582, 0.454, 0.346, 0.284, 0.242, Avg Draft acceptance rate: 44.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:43 [loggers.py:257] Engine 000: Avg prompt throughput: 122.6 tokens/s, Avg generation throughput: 466.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 333.63 tokens/s, Drafted throughput: 815.82 tokens/s, Accepted: 3337 tokens, Drafted: 8160 tokens, Per-position acceptance rate: 0.730, 0.548, 0.411, 0.312, 0.251, 0.201, Avg Draft acceptance rate: 40.9%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:53 [loggers.py:257] Engine 000: Avg prompt throughput: 164.1 tokens/s, Avg generation throughput: 505.6 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:21:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 371.58 tokens/s, Drafted throughput: 811.15 tokens/s, Accepted: 3716 tokens, Drafted: 8112 tokens, Per-position acceptance rate: 0.778, 0.583, 0.453, 0.371, 0.306, 0.258, Avg Draft acceptance rate: 45.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  45.01     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.78      
Output token throughput (tok/s):         454.97    
Peak output token throughput (tok/s):    144.00    
Peak concurrent requests:                13.00     
Total token throughput (tok/s):          589.99    
---------------Time to First Token----------------
Mean TTFT (ms):                          113.64    
Median TTFT (ms):                        114.92    
P99 TTFT (ms):                           135.40    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.93     
Median TPOT (ms):                        16.26     
P99 TPOT (ms):                           22.23     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.16     
Median ITL (ms):                         56.91     
P99 ITL (ms):                            66.72     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.57     
Acceptance length:                       3.61      
Drafts:                                  5685      
Draft tokens:                            34110     
Accepted tokens:                         14863     
Per-position acceptance (%):
  Position 0:                            74.93     
  Position 1:                            56.73     
  Position 2:                            43.91     
  Position 3:                            34.58     
  Position 4:                            28.21     
  Position 5:                            23.08     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 75.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 54.09 tokens/s, Drafted throughput: 132.59 tokens/s, Accepted: 541 tokens, Drafted: 1326 tokens, Per-position acceptance rate: 0.715, 0.520, 0.394, 0.321, 0.285, 0.213, Avg Draft acceptance rate: 40.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fae8c5f6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8a60d481-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:13 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.94, Accepted throughput: 7.90 tokens/s, Drafted throughput: 9.60 tokens/s, Accepted: 79 tokens, Drafted: 96 tokens, Per-position acceptance rate: 1.000, 0.938, 0.812, 0.812, 0.750, 0.625, Avg Draft acceptance rate: 82.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:23 [loggers.py:257] Engine 000: Avg prompt throughput: 257.9 tokens/s, Avg generation throughput: 673.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 485.23 tokens/s, Drafted throughput: 1120.03 tokens/s, Accepted: 4853 tokens, Drafted: 11202 tokens, Per-position acceptance rate: 0.746, 0.557, 0.442, 0.344, 0.277, 0.232, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:33 [loggers.py:257] Engine 000: Avg prompt throughput: 233.4 tokens/s, Avg generation throughput: 909.6 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 655.74 tokens/s, Drafted throughput: 1541.02 tokens/s, Accepted: 6559 tokens, Drafted: 15414 tokens, Per-position acceptance rate: 0.736, 0.558, 0.427, 0.331, 0.276, 0.225, Avg Draft acceptance rate: 42.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  25.38     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              3.15      
Output token throughput (tok/s):         806.81    
Peak output token throughput (tok/s):    270.00    
Peak concurrent requests:                23.00     
Total token throughput (tok/s):          1046.26   
---------------Time to First Token----------------
Mean TTFT (ms):                          121.29    
Median TTFT (ms):                        121.70    
P99 TTFT (ms):                           151.84    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.10     
Median TPOT (ms):                        17.06     
P99 TPOT (ms):                           24.64     
---------------Inter-token Latency----------------
Mean ITL (ms):                           60.50     
Median ITL (ms):                         59.94     
P99 ITL (ms):                            75.02     
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.66     
Acceptance length:                       3.56      
Drafts:                                  5768      
Draft tokens:                            34608     
Accepted tokens:                         14763     
Per-position acceptance (%):
  Position 0:                            73.82     
  Position 1:                            55.32     
  Position 2:                            43.00     
  Position 3:                            33.76     
  Position 4:                            27.48     
  Position 5:                            22.57     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:43 [loggers.py:257] Engine 000: Avg prompt throughput: 116.4 tokens/s, Avg generation throughput: 480.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 347.26 tokens/s, Drafted throughput: 823.10 tokens/s, Accepted: 3473 tokens, Drafted: 8232 tokens, Per-position acceptance rate: 0.732, 0.538, 0.421, 0.344, 0.274, 0.223, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:22:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f62b9eaefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ebf70d43-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:03 [loggers.py:257] Engine 000: Avg prompt throughput: 328.9 tokens/s, Avg generation throughput: 728.9 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.93, Accepted throughput: 271.07 tokens/s, Drafted throughput: 555.24 tokens/s, Accepted: 5422 tokens, Drafted: 11106 tokens, Per-position acceptance rate: 0.782, 0.613, 0.504, 0.410, 0.338, 0.283, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:13 [loggers.py:257] Engine 000: Avg prompt throughput: 296.8 tokens/s, Avg generation throughput: 1326.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 948.05 tokens/s, Drafted throughput: 2321.88 tokens/s, Accepted: 9481 tokens, Drafted: 23220 tokens, Per-position acceptance rate: 0.736, 0.541, 0.404, 0.316, 0.252, 0.202, Avg Draft acceptance rate: 40.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  16.15     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.95      
Output token throughput (tok/s):         1267.83   
Peak output token throughput (tok/s):    480.00    
Peak concurrent requests:                42.00     
Total token throughput (tok/s):          1644.09   
---------------Time to First Token----------------
Mean TTFT (ms):                          136.21    
Median TTFT (ms):                        134.75    
P99 TTFT (ms):                           196.88    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.85     
Median TPOT (ms):                        18.84     
P99 TPOT (ms):                           26.37     
---------------Inter-token Latency----------------
Mean ITL (ms):                           67.07     
Median ITL (ms):                         65.70     
P99 ITL (ms):                            113.95    
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.06     
Acceptance length:                       3.58      
Drafts:                                  5733      
Draft tokens:                            34398     
Accepted tokens:                         14813     
Per-position acceptance (%):
  Position 0:                            74.80     
  Position 1:                            56.10     
  Position 2:                            43.28     
  Position 3:                            34.26     
  Position 4:                            27.59     
  Position 5:                            22.36     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:23 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.63, Accepted throughput: 11.10 tokens/s, Drafted throughput: 40.80 tokens/s, Accepted: 111 tokens, Drafted: 408 tokens, Per-position acceptance rate: 0.544, 0.368, 0.265, 0.221, 0.162, 0.074, Avg Draft acceptance rate: 27.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7ff162232fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-4b4122bc-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:33 [loggers.py:257] Engine 000: Avg prompt throughput: 104.5 tokens/s, Avg generation throughput: 35.2 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.90, Accepted throughput: 27.30 tokens/s, Drafted throughput: 42.00 tokens/s, Accepted: 273 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.857, 0.686, 0.629, 0.614, 0.586, 0.529, Avg Draft acceptance rate: 65.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:43 [loggers.py:257] Engine 000: Avg prompt throughput: 521.2 tokens/s, Avg generation throughput: 2003.0 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.58, Accepted throughput: 1448.14 tokens/s, Drafted throughput: 3371.62 tokens/s, Accepted: 14483 tokens, Drafted: 33720 tokens, Per-position acceptance rate: 0.742, 0.557, 0.431, 0.342, 0.277, 0.227, Avg Draft acceptance rate: 43.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  11.98     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.68      
Output token throughput (tok/s):         1709.35   
Peak output token throughput (tok/s):    768.00    
Peak concurrent requests:                72.00     
Total token throughput (tok/s):          2216.64   
---------------Time to First Token----------------
Mean TTFT (ms):                          193.62    
Median TTFT (ms):                        191.85    
P99 TTFT (ms):                           341.12    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.03     
Median TPOT (ms):                        24.05     
P99 TPOT (ms):                           34.54     
---------------Inter-token Latency----------------
Mean ITL (ms):                           85.00     
Median ITL (ms):                         85.54     
P99 ITL (ms):                            162.95    
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.77     
Acceptance length:                       3.57      
Drafts:                                  5767      
Draft tokens:                            34602     
Accepted tokens:                         14801     
Per-position acceptance (%):
  Position 0:                            74.11     
  Position 1:                            55.45     
  Position 2:                            42.92     
  Position 3:                            34.14     
  Position 4:                            27.48     
  Position 5:                            22.54     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:23:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 24.60 tokens/s, Drafted throughput: 79.79 tokens/s, Accepted: 246 tokens, Drafted: 798 tokens, Per-position acceptance rate: 0.677, 0.429, 0.293, 0.248, 0.120, 0.083, Avg Draft acceptance rate: 30.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f6c973e6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15005, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-37835008-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:24:03 [loggers.py:257] Engine 000: Avg prompt throughput: 223.7 tokens/s, Avg generation throughput: 66.3 tokens/s, Running: 27 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:24:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.98, Accepted throughput: 50.89 tokens/s, Drafted throughput: 76.79 tokens/s, Accepted: 509 tokens, Drafted: 768 tokens, Per-position acceptance rate: 0.875, 0.727, 0.672, 0.625, 0.578, 0.500, Avg Draft acceptance rate: 66.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  9.85      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              8.12      
Output token throughput (tok/s):         2078.86   
Peak output token throughput (tok/s):    800.00    
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2695.82   
---------------Time to First Token----------------
Mean TTFT (ms):                          235.60    
Median TTFT (ms):                        241.17    
P99 TTFT (ms):                           419.13    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          26.98     
Median TPOT (ms):                        27.62     
P99 TPOT (ms):                           35.24     
---------------Inter-token Latency----------------
Mean ITL (ms):                           95.98     
Median ITL (ms):                         97.27     
P99 ITL (ms):                            213.59    
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.10     
Acceptance length:                       3.59      
Drafts:                                  5735      
Draft tokens:                            34410     
Accepted tokens:                         14832     
Per-position acceptance (%):
  Position 0:                            74.59     
  Position 1:                            56.11     
  Position 2:                            43.19     
  Position 3:                            34.18     
  Position 4:                            27.81     
  Position 5:                            22.74     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-1.7B-k6-t0.0-tp1...
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:24:13 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:24:14 [loggers.py:257] Engine 000: Avg prompt throughput: 368.1 tokens/s, Avg generation throughput: 1837.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=261674)[0;0m INFO 01-23 20:24:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 1329.54 tokens/s, Drafted throughput: 3110.37 tokens/s, Accepted: 14524 tokens, Drafted: 33978 tokens, Per-position acceptance rate: 0.744, 0.558, 0.428, 0.337, 0.274, 0.224, Avg Draft acceptance rate: 42.7%
