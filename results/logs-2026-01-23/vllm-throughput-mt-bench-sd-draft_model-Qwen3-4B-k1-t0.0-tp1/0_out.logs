Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k1-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k1-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2755523
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:50 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:50 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15010, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 1, 'max_model_len': 5000}}
[0;36m(APIServer pid=2755523)[0;0m WARNING 01-23 12:09:50 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:51 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:51 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:52 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:52 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:52 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2755523)[0;0m WARNING 01-23 12:09:52 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:09:52 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f78d61a6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1f3c4c84-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:09:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:09:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:03 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=1), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:10:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:05 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:39623 backend=nccl
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:05 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2755678)[0;0m WARNING 01-23 12:10:05 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:06 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:06 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:10:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:10:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:50 [default_loader.py:291] Loading weights took 42.07 seconds
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:50 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:50 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:10:50 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 12:10:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:06 [default_loader.py:291] Loading weights took 15.72 seconds
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:07 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 60.618130 seconds
WARNING 01-23 12:11:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:19 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:19 [backends.py:704] Dynamo bytecode transform time: 11.70 s
WARNING 01-23 12:11:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:37 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.589 s
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:37 [monitor.py:34] torch.compile takes 16.29 s in total
WARNING 01-23 12:11:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:43 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:43 [backends.py:704] Dynamo bytecode transform time: 6.04 s
WARNING 01-23 12:11:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:11:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:51 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.218 s
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:51 [monitor.py:34] torch.compile takes 24.54 s in total
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:53 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:53 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:11:53 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 12:11:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:12:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
WARNING 01-23 12:12:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:12:07 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took 0.10 GiB
[0;36m(EngineCore_DP0 pid=2755678)[0;0m INFO 01-23 12:12:07 [core.py:272] init engine (profile, create kv cache, warmup model) took 59.97 seconds
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:09 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2755523)[0;0m WARNING 01-23 12:12:09 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:09 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:10 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:10 [serving.py:185] Warming up chat template processing...
WARNING 01-23 12:12:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15010 ssl:default [Connect call failed (\'127.0.0.1\', 15010)]\n''
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:11 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:11 [serving.py:221] Chat template warmup completed in 1707.9ms
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:11 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15010
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:12 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:22 [loggers.py:257] Engine 000: Avg prompt throughput: 27.6 tokens/s, Avg generation throughput: 22.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.91, Accepted throughput: 10.50 tokens/s, Drafted throughput: 11.57 tokens/s, Accepted: 137 tokens, Drafted: 151 tokens, Per-position acceptance rate: 0.907, Avg Draft acceptance rate: 90.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:32 [loggers.py:257] Engine 000: Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 20.10 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 201 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.841, Avg Draft acceptance rate: 84.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:42 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 41.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 17.80 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 178 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.748, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:52 [loggers.py:257] Engine 000: Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 44.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:12:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 20.30 tokens/s, Drafted throughput: 23.80 tokens/s, Accepted: 203 tokens, Drafted: 238 tokens, Per-position acceptance rate: 0.853, Avg Draft acceptance rate: 85.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:02 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.88, Accepted throughput: 21.00 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 210 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.875, Avg Draft acceptance rate: 87.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:12 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 45.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.88, Accepted throughput: 21.20 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 212 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.883, Avg Draft acceptance rate: 88.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:22 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 41.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 17.30 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 173 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.721, Avg Draft acceptance rate: 72.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:32 [loggers.py:257] Engine 000: Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 43.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 19.00 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 190 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.795, Avg Draft acceptance rate: 79.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:42 [loggers.py:257] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 43.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 19.00 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 190 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.788, Avg Draft acceptance rate: 78.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:52 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 44.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:13:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 20.30 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 203 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.846, Avg Draft acceptance rate: 84.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:02 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 42.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 18.00 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 180 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.750, Avg Draft acceptance rate: 75.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:12 [loggers.py:257] Engine 000: Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 41.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 16.80 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 168 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.697, Avg Draft acceptance rate: 69.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:22 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 44.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 19.80 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 198 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.825, Avg Draft acceptance rate: 82.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:32 [loggers.py:257] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 42.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 18.00 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 180 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.750, Avg Draft acceptance rate: 75.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:42 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 20.50 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 205 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.851, Avg Draft acceptance rate: 85.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:52 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:14:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.86, Accepted throughput: 20.70 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 207 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.863, Avg Draft acceptance rate: 86.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:02 [loggers.py:257] Engine 000: Avg prompt throughput: 38.1 tokens/s, Avg generation throughput: 41.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 17.70 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 177 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.741, Avg Draft acceptance rate: 74.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:12 [loggers.py:257] Engine 000: Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 45.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.90, Accepted throughput: 21.70 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 217 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.900, Avg Draft acceptance rate: 90.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:22 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 44.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 20.20 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 202 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.842, Avg Draft acceptance rate: 84.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:32 [loggers.py:257] Engine 000: Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.94, Accepted throughput: 22.50 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 225 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.938, Avg Draft acceptance rate: 93.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:42 [loggers.py:257] Engine 000: Avg prompt throughput: 8.2 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 18.80 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 188 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.787, Avg Draft acceptance rate: 78.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:52 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:15:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 20.30 tokens/s, Drafted throughput: 24.20 tokens/s, Accepted: 203 tokens, Drafted: 242 tokens, Per-position acceptance rate: 0.839, Avg Draft acceptance rate: 83.9%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:02 [loggers.py:257] Engine 000: Avg prompt throughput: 25.5 tokens/s, Avg generation throughput: 45.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.88, Accepted throughput: 21.00 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 210 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.879, Avg Draft acceptance rate: 87.9%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:12 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 19.30 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 193 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.804, Avg Draft acceptance rate: 80.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:22 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 18.10 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 181 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.754, Avg Draft acceptance rate: 75.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:32 [loggers.py:257] Engine 000: Avg prompt throughput: 20.8 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.87, Accepted throughput: 21.00 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 210 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.871, Avg Draft acceptance rate: 87.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:42 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 19.30 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 193 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.804, Avg Draft acceptance rate: 80.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:52 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 42.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:16:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 18.50 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 185 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:02 [loggers.py:257] Engine 000: Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 41.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 17.50 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 175 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.726, Avg Draft acceptance rate: 72.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:12 [loggers.py:257] Engine 000: Avg prompt throughput: 6.9 tokens/s, Avg generation throughput: 44.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 20.30 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 203 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.846, Avg Draft acceptance rate: 84.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:22 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 44.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 20.20 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 202 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.842, Avg Draft acceptance rate: 84.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:32 [loggers.py:257] Engine 000: Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 18.70 tokens/s, Drafted throughput: 24.20 tokens/s, Accepted: 187 tokens, Drafted: 242 tokens, Per-position acceptance rate: 0.773, Avg Draft acceptance rate: 77.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:42 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 19.80 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 198 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.828, Avg Draft acceptance rate: 82.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:52 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 41.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:17:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 17.80 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 178 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.742, Avg Draft acceptance rate: 74.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:02 [loggers.py:257] Engine 000: Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 42.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 18.50 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 185 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:12 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 43.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 19.00 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 190 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.792, Avg Draft acceptance rate: 79.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:22 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.86, Accepted throughput: 20.60 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 206 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.858, Avg Draft acceptance rate: 85.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:32 [loggers.py:257] Engine 000: Avg prompt throughput: 18.4 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 19.40 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 194 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.808, Avg Draft acceptance rate: 80.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:42 [loggers.py:257] Engine 000: Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 19.30 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 193 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.801, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:52 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:18:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.92, Accepted throughput: 22.00 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 220 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.917, Avg Draft acceptance rate: 91.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:02 [loggers.py:257] Engine 000: Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 20.40 tokens/s, Drafted throughput: 24.00 tokens/s, Accepted: 204 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.850, Avg Draft acceptance rate: 85.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:12 [loggers.py:257] Engine 000: Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 41.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 17.30 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 173 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.718, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:22 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 42.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 18.50 tokens/s, Drafted throughput: 24.10 tokens/s, Accepted: 185 tokens, Drafted: 241 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:32 [loggers.py:257] Engine 000: Avg prompt throughput: 37.8 tokens/s, Avg generation throughput: 43.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 19.70 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 197 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.824, Avg Draft acceptance rate: 82.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:42 [loggers.py:257] Engine 000: Avg prompt throughput: 30.9 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.93, Accepted throughput: 22.20 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 222 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.929, Avg Draft acceptance rate: 92.9%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:52 [loggers.py:257] Engine 000: Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 44.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:19:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 20.10 tokens/s, Drafted throughput: 24.20 tokens/s, Accepted: 201 tokens, Drafted: 242 tokens, Per-position acceptance rate: 0.831, Avg Draft acceptance rate: 83.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:02 [loggers.py:257] Engine 000: Avg prompt throughput: 24.4 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.87, Accepted throughput: 20.90 tokens/s, Drafted throughput: 23.90 tokens/s, Accepted: 209 tokens, Drafted: 239 tokens, Per-position acceptance rate: 0.874, Avg Draft acceptance rate: 87.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  469.79    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.17      
Output token throughput (tok/s):         43.59     
Peak output token throughput (tok/s):    25.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          56.53     
---------------Time to First Token----------------
Mean TTFT (ms):                          54.01     
Median TTFT (ms):                        52.73     
P99 TTFT (ms):                           66.57     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.78     
Median TPOT (ms):                        22.80     
P99 TPOT (ms):                           24.44     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.27     
Median ITL (ms):                         41.28     
P99 ITL (ms):                            41.77     
---------------Speculative Decoding---------------
Acceptance rate (%):                     81.44     
Acceptance length:                       1.81      
Drafts:                                  11258     
Draft tokens:                            11258     
Accepted tokens:                         9169      
Per-position acceptance (%):
  Position 0:                            81.44     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:12 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 34.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 14.90 tokens/s, Drafted throughput: 19.70 tokens/s, Accepted: 149 tokens, Drafted: 197 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fa75c0cefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-632a1c98-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:32 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.91, Accepted throughput: 7.25 tokens/s, Drafted throughput: 8.00 tokens/s, Accepted: 145 tokens, Drafted: 160 tokens, Per-position acceptance rate: 0.906, Avg Draft acceptance rate: 90.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:42 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 37.50 tokens/s, Drafted throughput: 47.80 tokens/s, Accepted: 375 tokens, Drafted: 478 tokens, Per-position acceptance rate: 0.785, Avg Draft acceptance rate: 78.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:52 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 87.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:20:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 40.30 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 403 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.854, Avg Draft acceptance rate: 85.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:02 [loggers.py:257] Engine 000: Avg prompt throughput: 56.7 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 38.40 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 384 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.814, Avg Draft acceptance rate: 81.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:12 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 37.90 tokens/s, Drafted throughput: 47.80 tokens/s, Accepted: 379 tokens, Drafted: 478 tokens, Per-position acceptance rate: 0.793, Avg Draft acceptance rate: 79.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:22 [loggers.py:257] Engine 000: Avg prompt throughput: 17.0 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 38.30 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 383 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.808, Avg Draft acceptance rate: 80.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:32 [loggers.py:257] Engine 000: Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 83.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 35.50 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 355 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.752, Avg Draft acceptance rate: 75.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:42 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 38.50 tokens/s, Drafted throughput: 47.80 tokens/s, Accepted: 385 tokens, Drafted: 478 tokens, Per-position acceptance rate: 0.805, Avg Draft acceptance rate: 80.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:52 [loggers.py:257] Engine 000: Avg prompt throughput: 46.8 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:21:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 38.60 tokens/s, Drafted throughput: 47.19 tokens/s, Accepted: 386 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.818, Avg Draft acceptance rate: 81.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:02 [loggers.py:257] Engine 000: Avg prompt throughput: 18.1 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 39.80 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 398 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.843, Avg Draft acceptance rate: 84.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:12 [loggers.py:257] Engine 000: Avg prompt throughput: 16.6 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.88, Accepted throughput: 41.89 tokens/s, Drafted throughput: 47.59 tokens/s, Accepted: 419 tokens, Drafted: 476 tokens, Per-position acceptance rate: 0.880, Avg Draft acceptance rate: 88.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:22 [loggers.py:257] Engine 000: Avg prompt throughput: 31.5 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 40.50 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 405 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.854, Avg Draft acceptance rate: 85.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:32 [loggers.py:257] Engine 000: Avg prompt throughput: 18.6 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 38.70 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 387 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.816, Avg Draft acceptance rate: 81.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:42 [loggers.py:257] Engine 000: Avg prompt throughput: 28.2 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 39.90 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 399 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.842, Avg Draft acceptance rate: 84.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:52 [loggers.py:257] Engine 000: Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:22:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 33.80 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 338 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.716, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:02 [loggers.py:257] Engine 000: Avg prompt throughput: 14.3 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 39.29 tokens/s, Drafted throughput: 47.39 tokens/s, Accepted: 393 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.829, Avg Draft acceptance rate: 82.9%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:12 [loggers.py:257] Engine 000: Avg prompt throughput: 35.2 tokens/s, Avg generation throughput: 84.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 37.10 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 371 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.783, Avg Draft acceptance rate: 78.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:22 [loggers.py:257] Engine 000: Avg prompt throughput: 18.2 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 36.70 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 367 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.774, Avg Draft acceptance rate: 77.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:32 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 39.30 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 393 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.833, Avg Draft acceptance rate: 83.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:42 [loggers.py:257] Engine 000: Avg prompt throughput: 28.5 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 37.90 tokens/s, Drafted throughput: 47.59 tokens/s, Accepted: 379 tokens, Drafted: 476 tokens, Per-position acceptance rate: 0.796, Avg Draft acceptance rate: 79.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:52 [loggers.py:257] Engine 000: Avg prompt throughput: 17.0 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:23:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.87, Accepted throughput: 41.40 tokens/s, Drafted throughput: 47.60 tokens/s, Accepted: 414 tokens, Drafted: 476 tokens, Per-position acceptance rate: 0.870, Avg Draft acceptance rate: 87.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:02 [loggers.py:257] Engine 000: Avg prompt throughput: 12.8 tokens/s, Avg generation throughput: 84.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 36.90 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 369 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.778, Avg Draft acceptance rate: 77.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:12 [loggers.py:257] Engine 000: Avg prompt throughput: 68.7 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 39.30 tokens/s, Drafted throughput: 47.20 tokens/s, Accepted: 393 tokens, Drafted: 472 tokens, Per-position acceptance rate: 0.833, Avg Draft acceptance rate: 83.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:22 [loggers.py:257] Engine 000: Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.86, Accepted throughput: 40.90 tokens/s, Drafted throughput: 47.60 tokens/s, Accepted: 409 tokens, Drafted: 476 tokens, Per-position acceptance rate: 0.859, Avg Draft acceptance rate: 85.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  238.98    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.33      
Output token throughput (tok/s):         85.70     
Peak output token throughput (tok/s):    50.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          111.13    
---------------Time to First Token----------------
Mean TTFT (ms):                          86.33     
Median TTFT (ms):                        85.69     
P99 TTFT (ms):                           96.90     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.95     
Median TPOT (ms):                        22.96     
P99 TPOT (ms):                           24.84     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.57     
Median ITL (ms):                         41.54     
P99 ITL (ms):                            42.37     
---------------Speculative Decoding---------------
Acceptance rate (%):                     81.47     
Acceptance length:                       1.81      
Drafts:                                  11261     
Draft tokens:                            11261     
Accepted tokens:                         9174      
Per-position acceptance (%):
  Position 0:                            81.47     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:32 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 26.60 tokens/s, Drafted throughput: 32.90 tokens/s, Accepted: 266 tokens, Drafted: 329 tokens, Per-position acceptance rate: 0.809, Avg Draft acceptance rate: 80.9%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:42 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f846146afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7fce96ca-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:52 [loggers.py:257] Engine 000: Avg prompt throughput: 52.7 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:24:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.90, Accepted throughput: 11.25 tokens/s, Drafted throughput: 12.50 tokens/s, Accepted: 225 tokens, Drafted: 250 tokens, Per-position acceptance rate: 0.900, Avg Draft acceptance rate: 90.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:02 [loggers.py:257] Engine 000: Avg prompt throughput: 46.0 tokens/s, Avg generation throughput: 172.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 78.09 tokens/s, Drafted throughput: 94.19 tokens/s, Accepted: 781 tokens, Drafted: 942 tokens, Per-position acceptance rate: 0.829, Avg Draft acceptance rate: 82.9%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:12 [loggers.py:257] Engine 000: Avg prompt throughput: 72.7 tokens/s, Avg generation throughput: 169.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 75.49 tokens/s, Drafted throughput: 93.79 tokens/s, Accepted: 755 tokens, Drafted: 938 tokens, Per-position acceptance rate: 0.805, Avg Draft acceptance rate: 80.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:22 [loggers.py:257] Engine 000: Avg prompt throughput: 39.6 tokens/s, Avg generation throughput: 167.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 72.79 tokens/s, Drafted throughput: 93.59 tokens/s, Accepted: 728 tokens, Drafted: 936 tokens, Per-position acceptance rate: 0.778, Avg Draft acceptance rate: 77.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:32 [loggers.py:257] Engine 000: Avg prompt throughput: 46.8 tokens/s, Avg generation throughput: 169.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 75.29 tokens/s, Drafted throughput: 93.89 tokens/s, Accepted: 753 tokens, Drafted: 939 tokens, Per-position acceptance rate: 0.802, Avg Draft acceptance rate: 80.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:42 [loggers.py:257] Engine 000: Avg prompt throughput: 40.7 tokens/s, Avg generation throughput: 174.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.86, Accepted throughput: 80.69 tokens/s, Drafted throughput: 93.29 tokens/s, Accepted: 807 tokens, Drafted: 933 tokens, Per-position acceptance rate: 0.865, Avg Draft acceptance rate: 86.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:52 [loggers.py:257] Engine 000: Avg prompt throughput: 69.3 tokens/s, Avg generation throughput: 171.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:25:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 77.40 tokens/s, Drafted throughput: 93.60 tokens/s, Accepted: 774 tokens, Drafted: 936 tokens, Per-position acceptance rate: 0.827, Avg Draft acceptance rate: 82.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:02 [loggers.py:257] Engine 000: Avg prompt throughput: 16.7 tokens/s, Avg generation throughput: 166.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 72.29 tokens/s, Drafted throughput: 94.19 tokens/s, Accepted: 723 tokens, Drafted: 942 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:12 [loggers.py:257] Engine 000: Avg prompt throughput: 49.8 tokens/s, Avg generation throughput: 169.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 75.79 tokens/s, Drafted throughput: 93.79 tokens/s, Accepted: 758 tokens, Drafted: 938 tokens, Per-position acceptance rate: 0.808, Avg Draft acceptance rate: 80.8%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:22 [loggers.py:257] Engine 000: Avg prompt throughput: 35.4 tokens/s, Avg generation throughput: 169.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 75.19 tokens/s, Drafted throughput: 93.89 tokens/s, Accepted: 752 tokens, Drafted: 939 tokens, Per-position acceptance rate: 0.801, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:32 [loggers.py:257] Engine 000: Avg prompt throughput: 47.7 tokens/s, Avg generation throughput: 173.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 79.00 tokens/s, Drafted throughput: 93.89 tokens/s, Accepted: 790 tokens, Drafted: 939 tokens, Per-position acceptance rate: 0.841, Avg Draft acceptance rate: 84.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:42 [loggers.py:257] Engine 000: Avg prompt throughput: 79.3 tokens/s, Avg generation throughput: 169.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 75.30 tokens/s, Drafted throughput: 93.39 tokens/s, Accepted: 753 tokens, Drafted: 934 tokens, Per-position acceptance rate: 0.806, Avg Draft acceptance rate: 80.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:52 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 149.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:26:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 67.80 tokens/s, Drafted throughput: 81.69 tokens/s, Accepted: 678 tokens, Drafted: 817 tokens, Per-position acceptance rate: 0.830, Avg Draft acceptance rate: 83.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  122.64    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.65      
Output token throughput (tok/s):         167.00    
Peak output token throughput (tok/s):    96.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          216.56    
---------------Time to First Token----------------
Mean TTFT (ms):                          86.97     
Median TTFT (ms):                        86.73     
P99 TTFT (ms):                           99.97     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.21     
Median TPOT (ms):                        23.23     
P99 TPOT (ms):                           25.26     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.03     
Median ITL (ms):                         41.94     
P99 ITL (ms):                            45.28     
---------------Speculative Decoding---------------
Acceptance rate (%):                     81.38     
Acceptance length:                       1.81      
Drafts:                                  11266     
Draft tokens:                            11266     
Accepted tokens:                         9168      
Per-position acceptance (%):
  Position 0:                            81.38     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:02 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1.20 tokens/s, Drafted throughput: 1.70 tokens/s, Accepted: 12 tokens, Drafted: 17 tokens, Per-position acceptance rate: 0.706, Avg Draft acceptance rate: 70.6%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f713d56afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a17158ee-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:12 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 21.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.91, Accepted throughput: 10.30 tokens/s, Drafted throughput: 11.30 tokens/s, Accepted: 103 tokens, Drafted: 113 tokens, Per-position acceptance rate: 0.912, Avg Draft acceptance rate: 91.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:22 [loggers.py:257] Engine 000: Avg prompt throughput: 153.4 tokens/s, Avg generation throughput: 294.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 133.28 tokens/s, Drafted throughput: 159.48 tokens/s, Accepted: 1333 tokens, Drafted: 1595 tokens, Per-position acceptance rate: 0.836, Avg Draft acceptance rate: 83.6%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:32 [loggers.py:257] Engine 000: Avg prompt throughput: 89.3 tokens/s, Avg generation throughput: 328.5 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 143.10 tokens/s, Drafted throughput: 184.60 tokens/s, Accepted: 1431 tokens, Drafted: 1846 tokens, Per-position acceptance rate: 0.775, Avg Draft acceptance rate: 77.5%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:42 [loggers.py:257] Engine 000: Avg prompt throughput: 73.0 tokens/s, Avg generation throughput: 340.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 154.87 tokens/s, Drafted throughput: 184.97 tokens/s, Accepted: 1549 tokens, Drafted: 1850 tokens, Per-position acceptance rate: 0.837, Avg Draft acceptance rate: 83.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:52 [loggers.py:257] Engine 000: Avg prompt throughput: 100.6 tokens/s, Avg generation throughput: 332.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:27:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 147.88 tokens/s, Drafted throughput: 183.98 tokens/s, Accepted: 1479 tokens, Drafted: 1840 tokens, Per-position acceptance rate: 0.804, Avg Draft acceptance rate: 80.4%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:02 [loggers.py:257] Engine 000: Avg prompt throughput: 75.1 tokens/s, Avg generation throughput: 337.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 151.89 tokens/s, Drafted throughput: 184.98 tokens/s, Accepted: 1519 tokens, Drafted: 1850 tokens, Per-position acceptance rate: 0.821, Avg Draft acceptance rate: 82.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:12 [loggers.py:257] Engine 000: Avg prompt throughput: 116.4 tokens/s, Avg generation throughput: 340.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 154.78 tokens/s, Drafted throughput: 184.58 tokens/s, Accepted: 1548 tokens, Drafted: 1846 tokens, Per-position acceptance rate: 0.839, Avg Draft acceptance rate: 83.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  62.91     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.27      
Output token throughput (tok/s):         325.57    
Peak output token throughput (tok/s):    192.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          422.19    
---------------Time to First Token----------------
Mean TTFT (ms):                          88.70     
Median TTFT (ms):                        87.88     
P99 TTFT (ms):                           120.82    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.54     
Median TPOT (ms):                        23.55     
P99 TPOT (ms):                           25.52     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.69     
Median ITL (ms):                         42.50     
P99 ITL (ms):                            47.53     
---------------Speculative Decoding---------------
Acceptance rate (%):                     81.68     
Acceptance length:                       1.82      
Drafts:                                  11247     
Draft tokens:                            11247     
Accepted tokens:                         9187      
Per-position acceptance (%):
  Position 0:                            81.68     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.78, Accepted throughput: 34.60 tokens/s, Drafted throughput: 44.10 tokens/s, Accepted: 346 tokens, Drafted: 441 tokens, Per-position acceptance rate: 0.785, Avg Draft acceptance rate: 78.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fa0513defc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1ee7c83b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:32 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.91, Accepted throughput: 3.10 tokens/s, Drafted throughput: 3.40 tokens/s, Accepted: 31 tokens, Drafted: 34 tokens, Per-position acceptance rate: 0.912, Avg Draft acceptance rate: 91.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:42 [loggers.py:257] Engine 000: Avg prompt throughput: 153.4 tokens/s, Avg generation throughput: 357.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 161.77 tokens/s, Drafted throughput: 194.26 tokens/s, Accepted: 1618 tokens, Drafted: 1943 tokens, Per-position acceptance rate: 0.833, Avg Draft acceptance rate: 83.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:52 [loggers.py:257] Engine 000: Avg prompt throughput: 210.0 tokens/s, Avg generation throughput: 637.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:28:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 284.96 tokens/s, Drafted throughput: 350.35 tokens/s, Accepted: 2850 tokens, Drafted: 3504 tokens, Per-position acceptance rate: 0.813, Avg Draft acceptance rate: 81.3%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:02 [loggers.py:257] Engine 000: Avg prompt throughput: 133.8 tokens/s, Avg generation throughput: 635.3 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:02 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 281.38 tokens/s, Drafted throughput: 352.77 tokens/s, Accepted: 2814 tokens, Drafted: 3528 tokens, Per-position acceptance rate: 0.798, Avg Draft acceptance rate: 79.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  33.93     
Total input tokens:                      6078      
Total generated tokens:                  20464     
Request throughput (req/s):              2.36      
Output token throughput (tok/s):         603.13    
Peak output token throughput (tok/s):    368.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          782.27    
---------------Time to First Token----------------
Mean TTFT (ms):                          92.96     
Median TTFT (ms):                        91.91     
P99 TTFT (ms):                           124.90    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.71     
Median TPOT (ms):                        24.63     
P99 TPOT (ms):                           27.14     
---------------Inter-token Latency----------------
Mean ITL (ms):                           44.73     
Median ITL (ms):                         44.40     
P99 ITL (ms):                            51.47     
---------------Speculative Decoding---------------
Acceptance rate (%):                     81.28     
Acceptance length:                       1.81      
Drafts:                                  11262     
Draft tokens:                            11262     
Accepted tokens:                         9154      
Per-position acceptance (%):
  Position 0:                            81.28     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:12 [loggers.py:257] Engine 000: Avg prompt throughput: 110.6 tokens/s, Avg generation throughput: 435.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 196.20 tokens/s, Drafted throughput: 238.70 tokens/s, Accepted: 1962 tokens, Drafted: 2387 tokens, Per-position acceptance rate: 0.822, Avg Draft acceptance rate: 82.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:22 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efda33dafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8e0a0ebd-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:32 [loggers.py:257] Engine 000: Avg prompt throughput: 275.8 tokens/s, Avg generation throughput: 137.4 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.90, Accepted throughput: 31.69 tokens/s, Drafted throughput: 35.34 tokens/s, Accepted: 634 tokens, Drafted: 707 tokens, Per-position acceptance rate: 0.897, Avg Draft acceptance rate: 89.7%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:42 [loggers.py:257] Engine 000: Avg prompt throughput: 212.2 tokens/s, Avg generation throughput: 1246.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 560.53 tokens/s, Drafted throughput: 684.71 tokens/s, Accepted: 5606 tokens, Drafted: 6848 tokens, Per-position acceptance rate: 0.819, Avg Draft acceptance rate: 81.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  20.65     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              3.87      
Output token throughput (tok/s):         991.71    
Peak output token throughput (tok/s):    704.00    
Peak concurrent requests:                53.00     
Total token throughput (tok/s):          1286.03   
---------------Time to First Token----------------
Mean TTFT (ms):                          98.04     
Median TTFT (ms):                        96.49     
P99 TTFT (ms):                           164.60    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          25.44     
Median TPOT (ms):                        25.31     
P99 TPOT (ms):                           27.85     
---------------Inter-token Latency----------------
Mean ITL (ms):                           46.21     
Median ITL (ms):                         45.59     
P99 ITL (ms):                            59.56     
---------------Speculative Decoding---------------
Acceptance rate (%):                     82.03     
Acceptance length:                       1.82      
Drafts:                                  11229     
Draft tokens:                            11229     
Accepted tokens:                         9211      
Per-position acceptance (%):
  Position 0:                            82.03     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:52 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 689.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:29:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.81, Accepted throughput: 309.20 tokens/s, Drafted throughput: 380.80 tokens/s, Accepted: 3092 tokens, Drafted: 3808 tokens, Per-position acceptance rate: 0.812, Avg Draft acceptance rate: 81.2%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:02 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f93cbb66fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-21d24475-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:12 [loggers.py:257] Engine 000: Avg prompt throughput: 183.0 tokens/s, Avg generation throughput: 41.1 tokens/s, Running: 20 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:12 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.92, Accepted throughput: 9.35 tokens/s, Drafted throughput: 10.15 tokens/s, Accepted: 187 tokens, Drafted: 203 tokens, Per-position acceptance rate: 0.921, Avg Draft acceptance rate: 92.1%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:22 [loggers.py:257] Engine 000: Avg prompt throughput: 442.7 tokens/s, Avg generation throughput: 1805.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:22 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.82, Accepted throughput: 810.58 tokens/s, Drafted throughput: 991.16 tokens/s, Accepted: 8107 tokens, Drafted: 9913 tokens, Per-position acceptance rate: 0.818, Avg Draft acceptance rate: 81.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  14.68     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              5.45      
Output token throughput (tok/s):         1395.00   
Peak output token throughput (tok/s):    1280.00   
Peak concurrent requests:                79.00     
Total token throughput (tok/s):          1809.00   
---------------Time to First Token----------------
Mean TTFT (ms):                          126.70    
Median TTFT (ms):                        112.34    
P99 TTFT (ms):                           464.65    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          28.23     
Median TPOT (ms):                        28.69     
P99 TPOT (ms):                           31.26     
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.16     
Median ITL (ms):                         50.22     
P99 ITL (ms):                            109.95    
---------------Speculative Decoding---------------
Acceptance rate (%):                     81.54     
Acceptance length:                       1.82      
Drafts:                                  11252     
Draft tokens:                            11252     
Accepted tokens:                         9175      
Per-position acceptance (%):
  Position 0:                            81.54     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:32 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 226.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:32 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.79, Accepted throughput: 100.19 tokens/s, Drafted throughput: 126.99 tokens/s, Accepted: 1002 tokens, Drafted: 1270 tokens, Per-position acceptance rate: 0.789, Avg Draft acceptance rate: 78.9%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efecae56fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15010, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-bb6b5fe2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:42 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 3.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:42 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.95, Accepted throughput: 1.80 tokens/s, Drafted throughput: 1.90 tokens/s, Accepted: 18 tokens, Drafted: 19 tokens, Per-position acceptance rate: 0.947, Avg Draft acceptance rate: 94.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:52 [loggers.py:257] Engine 000: Avg prompt throughput: 607.8 tokens/s, Avg generation throughput: 1212.2 tokens/s, Running: 79 reqs, Waiting: 1 reqs, GPU KV cache usage: 99.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:52 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 547.61 tokens/s, Drafted throughput: 656.62 tokens/s, Accepted: 5476 tokens, Drafted: 6566 tokens, Per-position acceptance rate: 0.834, Avg Draft acceptance rate: 83.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  11.92     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.71      
Output token throughput (tok/s):         1718.46   
Peak output token throughput (tok/s):    1520.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2228.46   
---------------Time to First Token----------------
Mean TTFT (ms):                          163.25    
Median TTFT (ms):                        156.87    
P99 TTFT (ms):                           263.97    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          33.20     
Median TPOT (ms):                        32.42     
P99 TPOT (ms):                           40.67     
---------------Inter-token Latency----------------
Mean ITL (ms):                           59.87     
Median ITL (ms):                         52.42     
P99 ITL (ms):                            175.00    
---------------Speculative Decoding---------------
Acceptance rate (%):                     80.81     
Acceptance length:                       1.81      
Drafts:                                  11292     
Draft tokens:                            11292     
Accepted tokens:                         9125      
Per-position acceptance (%):
  Position 0:                            80.81     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k1-t0.0-tp1...
[0;36m(APIServer pid=2755523)[0;0m INFO 01-23 12:30:59 [launcher.py:110] Shutting down FastAPI HTTP server.
