Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k2-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k2-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2757613
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:13:59 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:13:59 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15011, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 2, 'max_model_len': 5000}}
[0;36m(APIServer pid=2757613)[0;0m WARNING 01-23 12:13:59 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:14:00 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:14:00 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:14:01 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:14:01 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:14:01 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2757613)[0;0m WARNING 01-23 12:14:01 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:14:01 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f334d6d2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ff050958-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:14:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:14:12 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=2), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:14:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:14:14 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:50543 backend=nccl
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:14:14 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2757708)[0;0m WARNING 01-23 12:14:14 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:14:15 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:14:15 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:14:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:14:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:03 [default_loader.py:291] Loading weights took 46.11 seconds
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:03 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:03 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:03 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 12:15:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:15:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:10 [default_loader.py:291] Loading weights took 6.12 seconds
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:11 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 55.120221 seconds
WARNING 01-23 12:15:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:15:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:23 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:23 [backends.py:704] Dynamo bytecode transform time: 11.79 s
WARNING 01-23 12:15:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:15:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:15:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:38 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.405 s
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:38 [monitor.py:34] torch.compile takes 14.19 s in total
WARNING 01-23 12:15:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:15:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:44 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:44 [backends.py:704] Dynamo bytecode transform time: 6.04 s
WARNING 01-23 12:15:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:52 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 0.924 s
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:52 [monitor.py:34] torch.compile takes 21.16 s in total
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:53 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:53 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:15:53 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 12:15:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:15:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
WARNING 01-23 12:16:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:16:08 [gpu_model_runner.py:4880] Graph capturing finished in 14 secs, took 0.05 GiB
WARNING 01-23 12:16:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15011)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15011 ssl:default [Connect call failed (\'127.0.0.1\', 15011)]\n''
[0;36m(EngineCore_DP0 pid=2757708)[0;0m INFO 01-23 12:16:08 [core.py:272] init engine (profile, create kv cache, warmup model) took 57.37 seconds
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:10 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2757613)[0;0m WARNING 01-23 12:16:10 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:10 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:10 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:10 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [serving.py:221] Chat template warmup completed in 1700.4ms
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15011
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:12 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:23 [loggers.py:257] Engine 000: Avg prompt throughput: 30.7 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 25.28 tokens/s, Drafted throughput: 28.06 tokens/s, Accepted: 328 tokens, Drafted: 364 tokens, Per-position acceptance rate: 0.923, 0.879, Avg Draft acceptance rate: 90.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:33 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 48.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 27.80 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 278 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.751, 0.579, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:43 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 31.40 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 314 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.817, 0.692, Avg Draft acceptance rate: 75.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:53 [loggers.py:257] Engine 000: Avg prompt throughput: 19.7 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:16:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.57, Accepted throughput: 32.59 tokens/s, Drafted throughput: 41.39 tokens/s, Accepted: 326 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.831, 0.744, Avg Draft acceptance rate: 78.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:03 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.61, Accepted throughput: 33.40 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 334 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.846, 0.760, Avg Draft acceptance rate: 80.3%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:13 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 26.70 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 267 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.764, 0.519, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:23 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 52.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 31.80 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 318 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.822, 0.707, Avg Draft acceptance rate: 76.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:33 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 53.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.57, Accepted throughput: 32.90 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 329 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.866, 0.708, Avg Draft acceptance rate: 78.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:43 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 49.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 28.90 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 289 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.794, 0.589, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:53 [loggers.py:257] Engine 000: Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:17:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.13, Accepted throughput: 23.70 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 237 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.665, 0.469, Avg Draft acceptance rate: 56.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:03 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 30.30 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 303 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.804, 0.646, Avg Draft acceptance rate: 72.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:13 [loggers.py:257] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 26.70 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 267 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.721, 0.562, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:23 [loggers.py:257] Engine 000: Avg prompt throughput: 23.0 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.68, Accepted throughput: 34.90 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 349 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.885, 0.793, Avg Draft acceptance rate: 83.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:33 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 27.90 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 279 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.756, 0.579, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:43 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.57, Accepted throughput: 32.50 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 325 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.845, 0.725, Avg Draft acceptance rate: 78.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:53 [loggers.py:257] Engine 000: Avg prompt throughput: 15.2 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:18:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.67, Accepted throughput: 34.49 tokens/s, Drafted throughput: 41.39 tokens/s, Accepted: 345 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.879, 0.787, Avg Draft acceptance rate: 83.3%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:03 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.69, Accepted throughput: 35.20 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 352 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.885, 0.808, Avg Draft acceptance rate: 84.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:13 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 29.50 tokens/s, Drafted throughput: 41.79 tokens/s, Accepted: 295 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.770, 0.641, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:23 [loggers.py:257] Engine 000: Avg prompt throughput: 25.5 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 31.80 tokens/s, Drafted throughput: 41.40 tokens/s, Accepted: 318 tokens, Drafted: 414 tokens, Per-position acceptance rate: 0.821, 0.715, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:33 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 53.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.58, Accepted throughput: 33.00 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 330 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.847, 0.732, Avg Draft acceptance rate: 78.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:43 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 49.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 28.20 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 282 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.755, 0.601, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:53 [loggers.py:257] Engine 000: Avg prompt throughput: 25.2 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:19:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 32.20 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 322 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.846, 0.702, Avg Draft acceptance rate: 77.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:03 [loggers.py:257] Engine 000: Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 28.00 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 280 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.769, 0.577, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:13 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 24.10 tokens/s, Drafted throughput: 41.79 tokens/s, Accepted: 241 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.684, 0.469, Avg Draft acceptance rate: 57.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:23 [loggers.py:257] Engine 000: Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 28.00 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 280 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.751, 0.589, Avg Draft acceptance rate: 67.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:33 [loggers.py:257] Engine 000: Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 31.30 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 313 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.799, 0.699, Avg Draft acceptance rate: 74.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:43 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 31.10 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 311 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.846, 0.649, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:53 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 51.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:20:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 30.20 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 302 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.798, 0.654, Avg Draft acceptance rate: 72.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:03 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 49.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 28.50 tokens/s, Drafted throughput: 41.59 tokens/s, Accepted: 285 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.760, 0.611, Avg Draft acceptance rate: 68.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:13 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 27.90 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 279 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.740, 0.601, Avg Draft acceptance rate: 67.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:23 [loggers.py:257] Engine 000: Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 53.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 32.40 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 324 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.833, 0.718, Avg Draft acceptance rate: 77.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:33 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 30.20 tokens/s, Drafted throughput: 42.00 tokens/s, Accepted: 302 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.800, 0.638, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:43 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 30.90 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 309 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.823, 0.656, Avg Draft acceptance rate: 73.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:53 [loggers.py:257] Engine 000: Avg prompt throughput: 17.0 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:21:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.64, Accepted throughput: 34.20 tokens/s, Drafted throughput: 41.60 tokens/s, Accepted: 342 tokens, Drafted: 416 tokens, Per-position acceptance rate: 0.880, 0.764, Avg Draft acceptance rate: 82.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:03 [loggers.py:257] Engine 000: Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 32.00 tokens/s, Drafted throughput: 42.00 tokens/s, Accepted: 320 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.819, 0.705, Avg Draft acceptance rate: 76.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:13 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 26.40 tokens/s, Drafted throughput: 42.00 tokens/s, Accepted: 264 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.719, 0.538, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:23 [loggers.py:257] Engine 000: Avg prompt throughput: 37.8 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 31.00 tokens/s, Drafted throughput: 41.99 tokens/s, Accepted: 310 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.800, 0.676, Avg Draft acceptance rate: 73.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:33 [loggers.py:257] Engine 000: Avg prompt throughput: 30.9 tokens/s, Avg generation throughput: 56.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 35.89 tokens/s, Drafted throughput: 41.79 tokens/s, Accepted: 359 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.895, 0.823, Avg Draft acceptance rate: 85.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:43 [loggers.py:257] Engine 000: Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 51.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 31.00 tokens/s, Drafted throughput: 41.80 tokens/s, Accepted: 310 tokens, Drafted: 418 tokens, Per-position acceptance rate: 0.818, 0.665, Avg Draft acceptance rate: 74.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:53 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:22:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 32.00 tokens/s, Drafted throughput: 42.00 tokens/s, Accepted: 320 tokens, Drafted: 420 tokens, Per-position acceptance rate: 0.829, 0.695, Avg Draft acceptance rate: 76.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  398.87    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.20      
Output token throughput (tok/s):         51.35     
Peak output token throughput (tok/s):    22.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          66.58     
---------------Time to First Token----------------
Mean TTFT (ms):                          59.88     
Median TTFT (ms):                        58.45     
P99 TTFT (ms):                           72.67     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.28     
Median TPOT (ms):                        19.20     
P99 TPOT (ms):                           22.50     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.38     
Median ITL (ms):                         47.43     
P99 ITL (ms):                            47.97     
---------------Speculative Decoding---------------
Acceptance rate (%):                     73.28     
Acceptance length:                       2.47      
Drafts:                                  8300      
Draft tokens:                            16600     
Accepted tokens:                         12165     
Per-position acceptance (%):
  Position 0:                            80.37     
  Position 1:                            66.19     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 9.10 tokens/s, Drafted throughput: 15.20 tokens/s, Accepted: 91 tokens, Drafted: 152 tokens, Per-position acceptance rate: 0.697, 0.500, Avg Draft acceptance rate: 59.9%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fcc5cf1efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a606ee77-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:13 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 4.50 tokens/s, Drafted throughput: 4.60 tokens/s, Accepted: 45 tokens, Drafted: 46 tokens, Per-position acceptance rate: 1.000, 0.957, Avg Draft acceptance rate: 97.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:23 [loggers.py:257] Engine 000: Avg prompt throughput: 34.7 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 50.20 tokens/s, Drafted throughput: 63.20 tokens/s, Accepted: 502 tokens, Drafted: 632 tokens, Per-position acceptance rate: 0.845, 0.744, Avg Draft acceptance rate: 79.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:33 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 59.89 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 599 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.792, 0.655, Avg Draft acceptance rate: 72.3%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:43 [loggers.py:257] Engine 000: Avg prompt throughput: 56.7 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 63.99 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 640 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.852, 0.701, Avg Draft acceptance rate: 77.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:53 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:23:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 62.89 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 629 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.829, 0.691, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:03 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 53.40 tokens/s, Drafted throughput: 83.20 tokens/s, Accepted: 534 tokens, Drafted: 832 tokens, Per-position acceptance rate: 0.731, 0.553, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:13 [loggers.py:257] Engine 000: Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 57.59 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 576 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.783, 0.609, Avg Draft acceptance rate: 69.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:23 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 62.70 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 627 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.814, 0.700, Avg Draft acceptance rate: 75.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:33 [loggers.py:257] Engine 000: Avg prompt throughput: 48.6 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 62.69 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 627 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.828, 0.694, Avg Draft acceptance rate: 76.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:43 [loggers.py:257] Engine 000: Avg prompt throughput: 20.2 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.62, Accepted throughput: 66.89 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 669 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.862, 0.754, Avg Draft acceptance rate: 80.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:53 [loggers.py:257] Engine 000: Avg prompt throughput: 41.2 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:24:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.56, Accepted throughput: 64.00 tokens/s, Drafted throughput: 82.00 tokens/s, Accepted: 640 tokens, Drafted: 820 tokens, Per-position acceptance rate: 0.841, 0.720, Avg Draft acceptance rate: 78.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:03 [loggers.py:257] Engine 000: Avg prompt throughput: 29.7 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 57.70 tokens/s, Drafted throughput: 83.20 tokens/s, Accepted: 577 tokens, Drafted: 832 tokens, Per-position acceptance rate: 0.776, 0.611, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:13 [loggers.py:257] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 56.09 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 561 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.763, 0.592, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:23 [loggers.py:257] Engine 000: Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 102.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 60.39 tokens/s, Drafted throughput: 83.19 tokens/s, Accepted: 604 tokens, Drafted: 832 tokens, Per-position acceptance rate: 0.810, 0.642, Avg Draft acceptance rate: 72.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:33 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 59.89 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 599 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.803, 0.650, Avg Draft acceptance rate: 72.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:43 [loggers.py:257] Engine 000: Avg prompt throughput: 20.8 tokens/s, Avg generation throughput: 98.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 56.60 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 566 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.763, 0.604, Avg Draft acceptance rate: 68.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:53 [loggers.py:257] Engine 000: Avg prompt throughput: 21.1 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:25:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 61.89 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 619 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.819, 0.676, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:03 [loggers.py:257] Engine 000: Avg prompt throughput: 36.6 tokens/s, Avg generation throughput: 104.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 63.40 tokens/s, Drafted throughput: 82.40 tokens/s, Accepted: 634 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.828, 0.711, Avg Draft acceptance rate: 76.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:13 [loggers.py:257] Engine 000: Avg prompt throughput: 15.9 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 59.69 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 597 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.795, 0.647, Avg Draft acceptance rate: 72.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:23 [loggers.py:257] Engine 000: Avg prompt throughput: 71.4 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 60.59 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 606 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.806, 0.665, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:33 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 63.59 tokens/s, Drafted throughput: 82.39 tokens/s, Accepted: 636 tokens, Drafted: 824 tokens, Per-position acceptance rate: 0.840, 0.704, Avg Draft acceptance rate: 77.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  202.20    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.40      
Output token throughput (tok/s):         101.29    
Peak output token throughput (tok/s):    44.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          131.35    
---------------Time to First Token----------------
Mean TTFT (ms):                          97.87     
Median TTFT (ms):                        97.33     
P99 TTFT (ms):                           109.96    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.26     
Median TPOT (ms):                        19.25     
P99 TPOT (ms):                           22.67     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.42     
Median ITL (ms):                         47.38     
P99 ITL (ms):                            48.51     
---------------Speculative Decoding---------------
Acceptance rate (%):                     73.45     
Acceptance length:                       2.47      
Drafts:                                  8287      
Draft tokens:                            16574     
Accepted tokens:                         12174     
Per-position acceptance (%):
  Position 0:                            80.67     
  Position 1:                            66.24     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 43.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 25.10 tokens/s, Drafted throughput: 36.60 tokens/s, Accepted: 251 tokens, Drafted: 366 tokens, Per-position acceptance rate: 0.760, 0.612, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:26:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fa29ed0afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-59052906-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:03 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 114.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 34.25 tokens/s, Drafted throughput: 45.00 tokens/s, Accepted: 685 tokens, Drafted: 900 tokens, Per-position acceptance rate: 0.824, 0.698, Avg Draft acceptance rate: 76.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:13 [loggers.py:257] Engine 000: Avg prompt throughput: 81.3 tokens/s, Avg generation throughput: 203.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 122.78 tokens/s, Drafted throughput: 162.37 tokens/s, Accepted: 1228 tokens, Drafted: 1624 tokens, Per-position acceptance rate: 0.828, 0.685, Avg Draft acceptance rate: 75.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:23 [loggers.py:257] Engine 000: Avg prompt throughput: 32.0 tokens/s, Avg generation throughput: 193.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 112.68 tokens/s, Drafted throughput: 162.38 tokens/s, Accepted: 1127 tokens, Drafted: 1624 tokens, Per-position acceptance rate: 0.791, 0.597, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:33 [loggers.py:257] Engine 000: Avg prompt throughput: 73.0 tokens/s, Avg generation throughput: 200.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 119.59 tokens/s, Drafted throughput: 162.39 tokens/s, Accepted: 1196 tokens, Drafted: 1624 tokens, Per-position acceptance rate: 0.812, 0.661, Avg Draft acceptance rate: 73.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:43 [loggers.py:257] Engine 000: Avg prompt throughput: 58.5 tokens/s, Avg generation throughput: 210.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 128.88 tokens/s, Drafted throughput: 162.38 tokens/s, Accepted: 1289 tokens, Drafted: 1624 tokens, Per-position acceptance rate: 0.852, 0.735, Avg Draft acceptance rate: 79.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:53 [loggers.py:257] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 197.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:27:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 116.59 tokens/s, Drafted throughput: 161.58 tokens/s, Accepted: 1166 tokens, Drafted: 1616 tokens, Per-position acceptance rate: 0.790, 0.653, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:03 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 196.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 115.19 tokens/s, Drafted throughput: 163.19 tokens/s, Accepted: 1152 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.786, 0.626, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:13 [loggers.py:257] Engine 000: Avg prompt throughput: 61.7 tokens/s, Avg generation throughput: 196.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 115.29 tokens/s, Drafted throughput: 161.79 tokens/s, Accepted: 1153 tokens, Drafted: 1618 tokens, Per-position acceptance rate: 0.794, 0.632, Avg Draft acceptance rate: 71.3%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:23 [loggers.py:257] Engine 000: Avg prompt throughput: 55.8 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 123.29 tokens/s, Drafted throughput: 162.58 tokens/s, Accepted: 1233 tokens, Drafted: 1626 tokens, Per-position acceptance rate: 0.830, 0.686, Avg Draft acceptance rate: 75.8%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:33 [loggers.py:257] Engine 000: Avg prompt throughput: 81.5 tokens/s, Avg generation throughput: 202.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 120.89 tokens/s, Drafted throughput: 162.38 tokens/s, Accepted: 1209 tokens, Drafted: 1624 tokens, Per-position acceptance rate: 0.818, 0.671, Avg Draft acceptance rate: 74.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  104.55    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.77      
Output token throughput (tok/s):         195.89    
Peak output token throughput (tok/s):    84.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          254.02    
---------------Time to First Token----------------
Mean TTFT (ms):                          98.23     
Median TTFT (ms):                        98.26     
P99 TTFT (ms):                           110.47    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.59     
Median TPOT (ms):                        19.60     
P99 TPOT (ms):                           22.76     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.31     
Median ITL (ms):                         48.21     
P99 ITL (ms):                            52.26     
---------------Speculative Decoding---------------
Acceptance rate (%):                     73.71     
Acceptance length:                       2.47      
Drafts:                                  8272      
Draft tokens:                            16544     
Accepted tokens:                         12195     
Per-position acceptance (%):
  Position 0:                            81.14     
  Position 1:                            66.28     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:43 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 152.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 92.09 tokens/s, Drafted throughput: 121.39 tokens/s, Accepted: 921 tokens, Drafted: 1214 tokens, Per-position acceptance rate: 0.824, 0.694, Avg Draft acceptance rate: 75.9%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:28:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc677ff6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d88302d1-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:03 [loggers.py:257] Engine 000: Avg prompt throughput: 91.3 tokens/s, Avg generation throughput: 38.7 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 12.20 tokens/s, Drafted throughput: 13.50 tokens/s, Accepted: 244 tokens, Drafted: 270 tokens, Per-position acceptance rate: 0.926, 0.881, Avg Draft acceptance rate: 90.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:13 [loggers.py:257] Engine 000: Avg prompt throughput: 89.0 tokens/s, Avg generation throughput: 398.3 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 237.67 tokens/s, Drafted throughput: 320.96 tokens/s, Accepted: 2377 tokens, Drafted: 3210 tokens, Per-position acceptance rate: 0.821, 0.660, Avg Draft acceptance rate: 74.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:23 [loggers.py:257] Engine 000: Avg prompt throughput: 104.0 tokens/s, Avg generation throughput: 394.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 234.39 tokens/s, Drafted throughput: 319.19 tokens/s, Accepted: 2344 tokens, Drafted: 3192 tokens, Per-position acceptance rate: 0.811, 0.657, Avg Draft acceptance rate: 73.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:33 [loggers.py:257] Engine 000: Avg prompt throughput: 100.2 tokens/s, Avg generation throughput: 394.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 234.56 tokens/s, Drafted throughput: 319.94 tokens/s, Accepted: 2346 tokens, Drafted: 3200 tokens, Per-position acceptance rate: 0.806, 0.660, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:43 [loggers.py:257] Engine 000: Avg prompt throughput: 113.7 tokens/s, Avg generation throughput: 390.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 231.08 tokens/s, Drafted throughput: 319.97 tokens/s, Accepted: 2311 tokens, Drafted: 3200 tokens, Per-position acceptance rate: 0.797, 0.647, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:53 [loggers.py:257] Engine 000: Avg prompt throughput: 127.6 tokens/s, Avg generation throughput: 395.2 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:29:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 236.08 tokens/s, Drafted throughput: 317.38 tokens/s, Accepted: 2361 tokens, Drafted: 3174 tokens, Per-position acceptance rate: 0.811, 0.677, Avg Draft acceptance rate: 74.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  54.58     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.47      
Output token throughput (tok/s):         375.23    
Peak output token throughput (tok/s):    168.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          486.59    
---------------Time to First Token----------------
Mean TTFT (ms):                          101.63    
Median TTFT (ms):                        100.54    
P99 TTFT (ms):                           141.88    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.92     
Median TPOT (ms):                        19.76     
P99 TPOT (ms):                           23.75     
---------------Inter-token Latency----------------
Mean ITL (ms):                           49.02     
Median ITL (ms):                         48.77     
P99 ITL (ms):                            54.04     
---------------Speculative Decoding---------------
Acceptance rate (%):                     73.52     
Acceptance length:                       2.47      
Drafts:                                  8288      
Draft tokens:                            16576     
Accepted tokens:                         12187     
Per-position acceptance (%):
  Position 0:                            80.92     
  Position 1:                            66.12     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 36.80 tokens/s, Drafted throughput: 51.20 tokens/s, Accepted: 368 tokens, Drafted: 512 tokens, Per-position acceptance rate: 0.785, 0.652, Avg Draft acceptance rate: 71.9%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fccaacd2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-13a9f11c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:13 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 5.90 tokens/s, Drafted throughput: 6.00 tokens/s, Accepted: 59 tokens, Drafted: 60 tokens, Per-position acceptance rate: 1.000, 0.967, Avg Draft acceptance rate: 98.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:23 [loggers.py:257] Engine 000: Avg prompt throughput: 257.8 tokens/s, Avg generation throughput: 505.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 302.93 tokens/s, Drafted throughput: 401.71 tokens/s, Accepted: 3030 tokens, Drafted: 4018 tokens, Per-position acceptance rate: 0.824, 0.684, Avg Draft acceptance rate: 75.4%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:33 [loggers.py:257] Engine 000: Avg prompt throughput: 173.2 tokens/s, Avg generation throughput: 750.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 445.72 tokens/s, Drafted throughput: 610.29 tokens/s, Accepted: 4458 tokens, Drafted: 6104 tokens, Per-position acceptance rate: 0.803, 0.658, Avg Draft acceptance rate: 73.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:43 [loggers.py:257] Engine 000: Avg prompt throughput: 176.7 tokens/s, Avg generation throughput: 748.7 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 448.84 tokens/s, Drafted throughput: 599.72 tokens/s, Accepted: 4489 tokens, Drafted: 5998 tokens, Per-position acceptance rate: 0.822, 0.675, Avg Draft acceptance rate: 74.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  29.86     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              2.68      
Output token throughput (tok/s):         685.87    
Peak output token throughput (tok/s):    320.00    
Peak concurrent requests:                27.00     
Total token throughput (tok/s):          889.43    
---------------Time to First Token----------------
Mean TTFT (ms):                          102.98    
Median TTFT (ms):                        104.78    
P99 TTFT (ms):                           123.97    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.90     
Median TPOT (ms):                        20.80     
P99 TPOT (ms):                           24.29     
---------------Inter-token Latency----------------
Mean ITL (ms):                           51.58     
Median ITL (ms):                         51.21     
P99 ITL (ms):                            61.89     
---------------Speculative Decoding---------------
Acceptance rate (%):                     73.87     
Acceptance length:                       2.48      
Drafts:                                  8265      
Draft tokens:                            16530     
Accepted tokens:                         12210     
Per-position acceptance (%):
  Position 0:                            81.20     
  Position 1:                            66.53     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:30:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 33.80 tokens/s, Drafted throughput: 53.20 tokens/s, Accepted: 338 tokens, Drafted: 532 tokens, Per-position acceptance rate: 0.729, 0.541, Avg Draft acceptance rate: 63.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f43ea876fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0fab82ea-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:03 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 13.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 9.00 tokens/s, Drafted throughput: 9.60 tokens/s, Accepted: 90 tokens, Drafted: 96 tokens, Per-position acceptance rate: 0.958, 0.917, Avg Draft acceptance rate: 93.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:13 [loggers.py:257] Engine 000: Avg prompt throughput: 470.0 tokens/s, Avg generation throughput: 1048.2 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 625.96 tokens/s, Drafted throughput: 837.41 tokens/s, Accepted: 6261 tokens, Drafted: 8376 tokens, Per-position acceptance rate: 0.816, 0.679, Avg Draft acceptance rate: 74.7%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:23 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 1009.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 593.48 tokens/s, Drafted throughput: 835.98 tokens/s, Accepted: 5935 tokens, Drafted: 8360 tokens, Per-position acceptance rate: 0.791, 0.629, Avg Draft acceptance rate: 71.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  18.34     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.36      
Output token throughput (tok/s):         1116.93   
Peak output token throughput (tok/s):    608.00    
Peak concurrent requests:                52.00     
Total token throughput (tok/s):          1448.40   
---------------Time to First Token----------------
Mean TTFT (ms):                          109.36    
Median TTFT (ms):                        111.32    
P99 TTFT (ms):                           171.83    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.22     
Median TPOT (ms):                        22.06     
P99 TPOT (ms):                           26.56     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.38     
Median ITL (ms):                         53.92     
P99 ITL (ms):                            68.68     
---------------Speculative Decoding---------------
Acceptance rate (%):                     72.76     
Acceptance length:                       2.46      
Drafts:                                  8337      
Draft tokens:                            16674     
Accepted tokens:                         12132     
Per-position acceptance (%):
  Position 0:                            80.27     
  Position 1:                            65.25     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.83, Accepted throughput: 1.00 tokens/s, Drafted throughput: 2.40 tokens/s, Accepted: 10 tokens, Drafted: 24 tokens, Per-position acceptance rate: 0.500, 0.333, Avg Draft acceptance rate: 41.7%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb8515cefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8d5b3ad9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:43 [loggers.py:257] Engine 000: Avg prompt throughput: 36.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 16.40 tokens/s, Drafted throughput: 18.20 tokens/s, Accepted: 164 tokens, Drafted: 182 tokens, Per-position acceptance rate: 0.923, 0.879, Avg Draft acceptance rate: 90.1%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:53 [loggers.py:257] Engine 000: Avg prompt throughput: 589.7 tokens/s, Avg generation throughput: 1906.0 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:31:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 1141.12 tokens/s, Drafted throughput: 1526.16 tokens/s, Accepted: 11413 tokens, Drafted: 15264 tokens, Per-position acceptance rate: 0.821, 0.675, Avg Draft acceptance rate: 74.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  12.81     
Total input tokens:                      6078      
Total generated tokens:                  20436     
Request throughput (req/s):              6.25      
Output token throughput (tok/s):         1595.73   
Peak output token throughput (tok/s):    1088.00   
Peak concurrent requests:                78.00     
Total token throughput (tok/s):          2070.32   
---------------Time to First Token----------------
Mean TTFT (ms):                          144.78    
Median TTFT (ms):                        137.53    
P99 TTFT (ms):                           245.14    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.35     
Median TPOT (ms):                        24.49     
P99 TPOT (ms):                           29.33     
---------------Inter-token Latency----------------
Mean ITL (ms):                           60.27     
Median ITL (ms):                         58.83     
P99 ITL (ms):                            116.89    
---------------Speculative Decoding---------------
Acceptance rate (%):                     74.24     
Acceptance length:                       2.48      
Drafts:                                  8222      
Draft tokens:                            16444     
Accepted tokens:                         12208     
Per-position acceptance (%):
  Position 0:                            81.60     
  Position 1:                            66.88     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 137.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 79.49 tokens/s, Drafted throughput: 117.99 tokens/s, Accepted: 795 tokens, Drafted: 1180 tokens, Per-position acceptance rate: 0.754, 0.593, Avg Draft acceptance rate: 67.4%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3a1b18afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15011, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c5c48b82-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:13 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 9.20 tokens/s, Drafted throughput: 9.80 tokens/s, Accepted: 92 tokens, Drafted: 98 tokens, Per-position acceptance rate: 0.959, 0.918, Avg Draft acceptance rate: 93.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:23 [loggers.py:257] Engine 000: Avg prompt throughput: 607.8 tokens/s, Avg generation throughput: 1912.7 tokens/s, Running: 41 reqs, Waiting: 0 reqs, GPU KV cache usage: 65.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 1139.25 tokens/s, Drafted throughput: 1532.73 tokens/s, Accepted: 11393 tokens, Drafted: 15328 tokens, Per-position acceptance rate: 0.817, 0.670, Avg Draft acceptance rate: 74.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  10.96     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              7.30      
Output token throughput (tok/s):         1869.15   
Peak output token throughput (tok/s):    1280.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2423.87   
---------------Time to First Token----------------
Mean TTFT (ms):                          175.97    
Median TTFT (ms):                        170.57    
P99 TTFT (ms):                           293.03    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          29.12     
Median TPOT (ms):                        28.92     
P99 TPOT (ms):                           36.26     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.52     
Median ITL (ms):                         62.44     
P99 ITL (ms):                            181.96    
---------------Speculative Decoding---------------
Acceptance rate (%):                     73.42     
Acceptance length:                       2.47      
Drafts:                                  8284      
Draft tokens:                            16568     
Accepted tokens:                         12164     
Per-position acceptance (%):
  Position 0:                            80.98     
  Position 1:                            65.86     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k2-t0.0-tp1...
[0;36m(APIServer pid=2757613)[0;0m INFO 01-23 12:32:26 [launcher.py:110] Shutting down FastAPI HTTP server.
