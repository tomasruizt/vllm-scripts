Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k3-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k3-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2760772
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:05 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:05 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15012, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 3, 'max_model_len': 5000}}
[0;36m(APIServer pid=2760772)[0;0m WARNING 01-23 12:22:05 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:06 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:06 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:07 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:07 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:07 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2760772)[0;0m WARNING 01-23 12:22:07 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:22:07 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9f6f9f2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d4c35e8b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:22:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:22:18 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=3), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:22:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:22:20 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:36473 backend=nccl
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:22:20 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2760867)[0;0m WARNING 01-23 12:22:20 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:22:20 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:22:21 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:22:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:22:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:23:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:04 [default_loader.py:291] Loading weights took 41.88 seconds
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:04 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:04 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:04 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 12:23:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:10 [default_loader.py:291] Loading weights took 4.93 seconds
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:11 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 49.610798 seconds
WARNING 01-23 12:23:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:23:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:23 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:23 [backends.py:704] Dynamo bytecode transform time: 11.79 s
WARNING 01-23 12:23:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:23:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:23:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:38 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.414 s
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:38 [monitor.py:34] torch.compile takes 14.20 s in total
WARNING 01-23 12:23:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:23:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:44 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:44 [backends.py:704] Dynamo bytecode transform time: 6.07 s
WARNING 01-23 12:23:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:52 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.483 s
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:52 [monitor.py:34] torch.compile takes 21.76 s in total
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:53 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:53 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:23:53 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 12:23:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:23:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:24:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
WARNING 01-23 12:24:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:24:10 [gpu_model_runner.py:4880] Graph capturing finished in 15 secs, took 0.07 GiB
[0;36m(EngineCore_DP0 pid=2760867)[0;0m INFO 01-23 12:24:10 [core.py:272] init engine (profile, create kv cache, warmup model) took 58.62 seconds
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:12 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2760772)[0;0m WARNING 01-23 12:24:12 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:12 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:12 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:12 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [serving.py:221] Chat template warmup completed in 1705.6ms
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15012
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:14 [launcher.py:46] Route: /pooling, Methods: POST
WARNING 01-23 12:24:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15012)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15012 ssl:default [Connect call failed (\'127.0.0.1\', 15012)]\n''
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:24 [loggers.py:257] Engine 000: Avg prompt throughput: 27.9 tokens/s, Avg generation throughput: 19.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 14.12 tokens/s, Drafted throughput: 17.46 tokens/s, Accepted: 182 tokens, Drafted: 225 tokens, Per-position acceptance rate: 0.867, 0.800, 0.760, Avg Draft acceptance rate: 80.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:34 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 38.30 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 383 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.812, 0.672, 0.575, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:44 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 49.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.69, Accepted throughput: 31.30 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 313 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.757, 0.530, 0.405, Avg Draft acceptance rate: 56.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:54 [loggers.py:257] Engine 000: Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:24:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 41.90 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 419 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.844, 0.742, 0.667, Avg Draft acceptance rate: 75.1%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:04 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 43.20 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 432 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.855, 0.758, 0.710, Avg Draft acceptance rate: 77.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:14 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.56, Accepted throughput: 28.80 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 288 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.686, 0.503, 0.368, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:24 [loggers.py:257] Engine 000: Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 36.50 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 365 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.795, 0.649, 0.530, Avg Draft acceptance rate: 65.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:34 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 40.80 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 408 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.880, 0.734, 0.603, Avg Draft acceptance rate: 73.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:44 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 50.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.71, Accepted throughput: 31.80 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 318 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.737, 0.548, 0.425, Avg Draft acceptance rate: 57.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:54 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:25:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 28.50 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 285 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.694, 0.478, 0.360, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:04 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 36.20 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 362 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.817, 0.634, 0.495, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:14 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 35.00 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 350 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.762, 0.611, 0.519, Avg Draft acceptance rate: 63.1%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:24 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 43.20 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 432 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.849, 0.785, 0.688, Avg Draft acceptance rate: 77.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:34 [loggers.py:257] Engine 000: Avg prompt throughput: 38.1 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 33.70 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 337 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.751, 0.605, 0.465, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:44 [loggers.py:257] Engine 000: Avg prompt throughput: 14.5 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 43.00 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 430 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.891, 0.772, 0.674, Avg Draft acceptance rate: 77.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:54 [loggers.py:257] Engine 000: Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 65.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:26:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 47.10 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 471 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.914, 0.839, 0.780, Avg Draft acceptance rate: 84.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:04 [loggers.py:257] Engine 000: Avg prompt throughput: 8.2 tokens/s, Avg generation throughput: 54.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 36.00 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 360 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.784, 0.632, 0.530, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:14 [loggers.py:257] Engine 000: Avg prompt throughput: 31.5 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 39.70 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 397 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.842, 0.707, 0.609, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:24 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 39.90 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 399 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.812, 0.715, 0.618, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:34 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.80, Accepted throughput: 33.40 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 334 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.769, 0.586, 0.441, Avg Draft acceptance rate: 59.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:44 [loggers.py:257] Engine 000: Avg prompt throughput: 25.2 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 41.20 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 412 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.838, 0.746, 0.643, Avg Draft acceptance rate: 74.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:54 [loggers.py:257] Engine 000: Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:27:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 32.69 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 327 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.757, 0.557, 0.454, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:04 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.68, Accepted throughput: 31.20 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 312 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.726, 0.538, 0.414, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:14 [loggers.py:257] Engine 000: Avg prompt throughput: 6.9 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 39.40 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 394 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.860, 0.688, 0.570, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:24 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 36.80 tokens/s, Drafted throughput: 55.20 tokens/s, Accepted: 368 tokens, Drafted: 552 tokens, Per-position acceptance rate: 0.810, 0.658, 0.533, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:34 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 34.70 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 347 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.763, 0.608, 0.495, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:44 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 32.90 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 329 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.762, 0.568, 0.449, Avg Draft acceptance rate: 59.3%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:54 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:28:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.78, Accepted throughput: 33.10 tokens/s, Drafted throughput: 55.80 tokens/s, Accepted: 331 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.763, 0.570, 0.446, Avg Draft acceptance rate: 59.3%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:04 [loggers.py:257] Engine 000: Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 37.70 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 377 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.822, 0.659, 0.557, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:14 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.06, Accepted throughput: 38.30 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 383 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.823, 0.667, 0.570, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:24 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 34.60 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 346 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.800, 0.611, 0.459, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:34 [loggers.py:257] Engine 000: Avg prompt throughput: 17.0 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 40.99 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 410 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.859, 0.714, 0.643, Avg Draft acceptance rate: 73.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:44 [loggers.py:257] Engine 000: Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 40.40 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 404 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.838, 0.730, 0.616, Avg Draft acceptance rate: 72.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:54 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:29:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.61, Accepted throughput: 29.90 tokens/s, Drafted throughput: 55.79 tokens/s, Accepted: 299 tokens, Drafted: 558 tokens, Per-position acceptance rate: 0.715, 0.538, 0.355, Avg Draft acceptance rate: 53.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:04 [loggers.py:257] Engine 000: Avg prompt throughput: 37.8 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 40.20 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 402 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.876, 0.724, 0.573, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:14 [loggers.py:257] Engine 000: Avg prompt throughput: 30.9 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 40.30 tokens/s, Drafted throughput: 55.50 tokens/s, Accepted: 403 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.849, 0.697, 0.632, Avg Draft acceptance rate: 72.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:24 [loggers.py:257] Engine 000: Avg prompt throughput: 26.6 tokens/s, Avg generation throughput: 60.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.24, Accepted throughput: 41.50 tokens/s, Drafted throughput: 55.49 tokens/s, Accepted: 415 tokens, Drafted: 555 tokens, Per-position acceptance rate: 0.865, 0.735, 0.643, Avg Draft acceptance rate: 74.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  369.43    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         55.44     
Peak output token throughput (tok/s):    19.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          71.89     
---------------Time to First Token----------------
Mean TTFT (ms):                          65.49     
Median TTFT (ms):                        64.10     
P99 TTFT (ms):                           79.06     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.81     
Median TPOT (ms):                        17.56     
P99 TPOT (ms):                           21.45     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.20     
Median ITL (ms):                         53.23     
P99 ITL (ms):                            53.77     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.65     
Acceptance length:                       3.00      
Drafts:                                  6830      
Draft tokens:                            20490     
Accepted tokens:                         13656     
Per-position acceptance (%):
  Position 0:                            80.50     
  Position 1:                            65.30     
  Position 2:                            54.14     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:34 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 47.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 31.40 tokens/s, Drafted throughput: 47.40 tokens/s, Accepted: 314 tokens, Drafted: 474 tokens, Per-position acceptance rate: 0.810, 0.658, 0.519, Avg Draft acceptance rate: 66.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f65aa84afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3251c151-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:54 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:30:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 16.95 tokens/s, Drafted throughput: 22.35 tokens/s, Accepted: 339 tokens, Drafted: 447 tokens, Per-position acceptance rate: 0.839, 0.745, 0.691, Avg Draft acceptance rate: 75.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:04 [loggers.py:257] Engine 000: Avg prompt throughput: 45.0 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 70.29 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 703 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.791, 0.624, 0.516, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:14 [loggers.py:257] Engine 000: Avg prompt throughput: 29.6 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 77.70 tokens/s, Drafted throughput: 109.20 tokens/s, Accepted: 777 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.824, 0.698, 0.613, Avg Draft acceptance rate: 71.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:24 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 66.69 tokens/s, Drafted throughput: 110.38 tokens/s, Accepted: 667 tokens, Drafted: 1104 tokens, Per-position acceptance rate: 0.750, 0.592, 0.470, Avg Draft acceptance rate: 60.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:34 [loggers.py:257] Engine 000: Avg prompt throughput: 17.0 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 68.00 tokens/s, Drafted throughput: 110.09 tokens/s, Accepted: 680 tokens, Drafted: 1101 tokens, Per-position acceptance rate: 0.771, 0.597, 0.485, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:44 [loggers.py:257] Engine 000: Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.76, Accepted throughput: 64.50 tokens/s, Drafted throughput: 110.10 tokens/s, Accepted: 645 tokens, Drafted: 1101 tokens, Per-position acceptance rate: 0.757, 0.564, 0.436, Avg Draft acceptance rate: 58.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:54 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 113.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:31:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.11, Accepted throughput: 77.09 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 771 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.801, 0.702, 0.604, Avg Draft acceptance rate: 70.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:04 [loggers.py:257] Engine 000: Avg prompt throughput: 52.2 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 74.90 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 749 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.818, 0.691, 0.561, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:14 [loggers.py:257] Engine 000: Avg prompt throughput: 22.6 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 86.49 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 865 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.891, 0.779, 0.694, Avg Draft acceptance rate: 78.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:24 [loggers.py:257] Engine 000: Avg prompt throughput: 40.6 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 76.19 tokens/s, Drafted throughput: 109.78 tokens/s, Accepted: 762 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.822, 0.678, 0.582, Avg Draft acceptance rate: 69.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:35 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 113.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 76.59 tokens/s, Drafted throughput: 109.79 tokens/s, Accepted: 766 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.831, 0.689, 0.574, Avg Draft acceptance rate: 69.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:45 [loggers.py:257] Engine 000: Avg prompt throughput: 13.7 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.65, Accepted throughput: 60.79 tokens/s, Drafted throughput: 110.39 tokens/s, Accepted: 608 tokens, Drafted: 1104 tokens, Per-position acceptance rate: 0.726, 0.533, 0.394, Avg Draft acceptance rate: 55.1%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:55 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:32:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 70.39 tokens/s, Drafted throughput: 109.78 tokens/s, Accepted: 704 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.781, 0.620, 0.522, Avg Draft acceptance rate: 64.1%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:05 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 69.10 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 691 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.773, 0.615, 0.500, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:15 [loggers.py:257] Engine 000: Avg prompt throughput: 19.4 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 70.90 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 709 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.803, 0.623, 0.511, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:25 [loggers.py:257] Engine 000: Avg prompt throughput: 44.1 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.96, Accepted throughput: 71.39 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 714 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.794, 0.635, 0.533, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:35 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 116.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 79.39 tokens/s, Drafted throughput: 110.39 tokens/s, Accepted: 794 tokens, Drafted: 1104 tokens, Per-position acceptance rate: 0.840, 0.717, 0.601, Avg Draft acceptance rate: 71.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:45 [loggers.py:257] Engine 000: Avg prompt throughput: 44.3 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 73.80 tokens/s, Drafted throughput: 109.80 tokens/s, Accepted: 738 tokens, Drafted: 1098 tokens, Per-position acceptance rate: 0.822, 0.672, 0.522, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:55 [loggers.py:257] Engine 000: Avg prompt throughput: 57.5 tokens/s, Avg generation throughput: 114.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:33:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 78.40 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 784 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.852, 0.701, 0.602, Avg Draft acceptance rate: 71.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  189.62    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.42      
Output token throughput (tok/s):         108.01    
Peak output token throughput (tok/s):    38.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          140.06    
---------------Time to First Token----------------
Mean TTFT (ms):                          109.00    
Median TTFT (ms):                        108.90    
P99 TTFT (ms):                           121.37    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.93     
Median TPOT (ms):                        17.85     
P99 TPOT (ms):                           22.89     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.39     
Median ITL (ms):                         53.34     
P99 ITL (ms):                            56.14     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.48     
Acceptance length:                       2.99      
Drafts:                                  6850      
Draft tokens:                            20550     
Accepted tokens:                         13662     
Per-position acceptance (%):
  Position 0:                            80.25     
  Position 1:                            65.20     
  Position 2:                            54.00     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:05 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 37.79 tokens/s, Drafted throughput: 57.59 tokens/s, Accepted: 378 tokens, Drafted: 576 tokens, Per-position acceptance rate: 0.797, 0.656, 0.516, Avg Draft acceptance rate: 65.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f54e4e9efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d7dd2e59-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:25 [loggers.py:257] Engine 000: Avg prompt throughput: 52.7 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 34.15 tokens/s, Drafted throughput: 50.84 tokens/s, Accepted: 683 tokens, Drafted: 1017 tokens, Per-position acceptance rate: 0.782, 0.649, 0.584, Avg Draft acceptance rate: 67.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:35 [loggers.py:257] Engine 000: Avg prompt throughput: 103.0 tokens/s, Avg generation throughput: 216.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.00, Accepted throughput: 144.58 tokens/s, Drafted throughput: 216.87 tokens/s, Accepted: 1446 tokens, Drafted: 2169 tokens, Per-position acceptance rate: 0.797, 0.651, 0.552, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:45 [loggers.py:257] Engine 000: Avg prompt throughput: 32.0 tokens/s, Avg generation throughput: 206.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.84, Accepted throughput: 133.39 tokens/s, Drafted throughput: 217.18 tokens/s, Accepted: 1334 tokens, Drafted: 2172 tokens, Per-position acceptance rate: 0.789, 0.595, 0.459, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:55 [loggers.py:257] Engine 000: Avg prompt throughput: 73.0 tokens/s, Avg generation throughput: 216.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:34:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 144.19 tokens/s, Drafted throughput: 217.18 tokens/s, Accepted: 1442 tokens, Drafted: 2172 tokens, Per-position acceptance rate: 0.797, 0.657, 0.537, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:05 [loggers.py:257] Engine 000: Avg prompt throughput: 68.3 tokens/s, Avg generation throughput: 229.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 158.20 tokens/s, Drafted throughput: 216.00 tokens/s, Accepted: 1582 tokens, Drafted: 2160 tokens, Per-position acceptance rate: 0.851, 0.724, 0.622, Avg Draft acceptance rate: 73.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:15 [loggers.py:257] Engine 000: Avg prompt throughput: 49.4 tokens/s, Avg generation throughput: 213.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 140.59 tokens/s, Drafted throughput: 217.18 tokens/s, Accepted: 1406 tokens, Drafted: 2172 tokens, Per-position acceptance rate: 0.790, 0.630, 0.522, Avg Draft acceptance rate: 64.7%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:25 [loggers.py:257] Engine 000: Avg prompt throughput: 52.5 tokens/s, Avg generation throughput: 209.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.87, Accepted throughput: 136.28 tokens/s, Drafted throughput: 218.37 tokens/s, Accepted: 1363 tokens, Drafted: 2184 tokens, Per-position acceptance rate: 0.769, 0.602, 0.501, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:35 [loggers.py:257] Engine 000: Avg prompt throughput: 57.2 tokens/s, Avg generation throughput: 214.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 142.30 tokens/s, Drafted throughput: 216.60 tokens/s, Accepted: 1423 tokens, Drafted: 2166 tokens, Per-position acceptance rate: 0.812, 0.634, 0.525, Avg Draft acceptance rate: 65.7%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:45 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 227.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 154.18 tokens/s, Drafted throughput: 217.17 tokens/s, Accepted: 1542 tokens, Drafted: 2172 tokens, Per-position acceptance rate: 0.847, 0.702, 0.581, Avg Draft acceptance rate: 71.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:55 [loggers.py:257] Engine 000: Avg prompt throughput: 97.8 tokens/s, Avg generation throughput: 213.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:35:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 145.18 tokens/s, Drafted throughput: 207.57 tokens/s, Accepted: 1452 tokens, Drafted: 2076 tokens, Per-position acceptance rate: 0.834, 0.688, 0.577, Avg Draft acceptance rate: 69.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  98.12     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.82      
Output token throughput (tok/s):         208.72    
Peak output token throughput (tok/s):    76.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          270.66    
---------------Time to First Token----------------
Mean TTFT (ms):                          109.50    
Median TTFT (ms):                        109.83    
P99 TTFT (ms):                           123.00    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.07     
Median TPOT (ms):                        17.67     
P99 TPOT (ms):                           22.29     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.96     
Median ITL (ms):                         53.82     
P99 ITL (ms):                            58.64     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.59     
Acceptance length:                       3.00      
Drafts:                                  6830      
Draft tokens:                            20490     
Accepted tokens:                         13645     
Per-position acceptance (%):
  Position 0:                            80.63     
  Position 1:                            65.10     
  Position 2:                            54.06     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 15.40 tokens/s, Drafted throughput: 25.50 tokens/s, Accepted: 154 tokens, Drafted: 255 tokens, Per-position acceptance rate: 0.729, 0.600, 0.482, Avg Draft acceptance rate: 60.4%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f991a38afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b13e17fd-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:15 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 6.50 tokens/s, Drafted throughput: 7.20 tokens/s, Accepted: 65 tokens, Drafted: 72 tokens, Per-position acceptance rate: 0.917, 0.917, 0.875, Avg Draft acceptance rate: 90.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:25 [loggers.py:257] Engine 000: Avg prompt throughput: 153.4 tokens/s, Avg generation throughput: 307.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 207.57 tokens/s, Drafted throughput: 298.15 tokens/s, Accepted: 2076 tokens, Drafted: 2982 tokens, Per-position acceptance rate: 0.818, 0.678, 0.593, Avg Draft acceptance rate: 69.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:35 [loggers.py:257] Engine 000: Avg prompt throughput: 104.5 tokens/s, Avg generation throughput: 406.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 266.05 tokens/s, Drafted throughput: 422.32 tokens/s, Accepted: 2661 tokens, Drafted: 4224 tokens, Per-position acceptance rate: 0.771, 0.612, 0.508, Avg Draft acceptance rate: 63.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:45 [loggers.py:257] Engine 000: Avg prompt throughput: 105.5 tokens/s, Avg generation throughput: 437.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 298.05 tokens/s, Drafted throughput: 422.32 tokens/s, Accepted: 2981 tokens, Drafted: 4224 tokens, Per-position acceptance rate: 0.835, 0.697, 0.585, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:55 [loggers.py:257] Engine 000: Avg prompt throughput: 106.7 tokens/s, Avg generation throughput: 411.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:36:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 270.56 tokens/s, Drafted throughput: 422.34 tokens/s, Accepted: 2706 tokens, Drafted: 4224 tokens, Per-position acceptance rate: 0.792, 0.619, 0.511, Avg Draft acceptance rate: 64.1%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:05 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 428.7 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 290.57 tokens/s, Drafted throughput: 416.06 tokens/s, Accepted: 2906 tokens, Drafted: 4161 tokens, Per-position acceptance rate: 0.831, 0.693, 0.572, Avg Draft acceptance rate: 69.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  51.58     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.55      
Output token throughput (tok/s):         397.04    
Peak output token throughput (tok/s):    152.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          514.88    
---------------Time to First Token----------------
Mean TTFT (ms):                          112.87    
Median TTFT (ms):                        112.93    
P99 TTFT (ms):                           142.63    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.50     
Median TPOT (ms):                        18.41     
P99 TPOT (ms):                           22.51     
---------------Inter-token Latency----------------
Mean ITL (ms):                           55.46     
Median ITL (ms):                         55.19     
P99 ITL (ms):                            60.66     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.03     
Acceptance length:                       3.01      
Drafts:                                  6805      
Draft tokens:                            20415     
Accepted tokens:                         13684     
Per-position acceptance (%):
  Position 0:                            80.71     
  Position 1:                            65.60     
  Position 2:                            54.78     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 47.10 tokens/s, Drafted throughput: 75.29 tokens/s, Accepted: 471 tokens, Drafted: 753 tokens, Per-position acceptance rate: 0.777, 0.610, 0.490, Avg Draft acceptance rate: 62.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f4fbd98efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e8fd0260-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:25 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 5.00 tokens/s, Drafted throughput: 5.70 tokens/s, Accepted: 50 tokens, Drafted: 57 tokens, Per-position acceptance rate: 0.895, 0.895, 0.842, Avg Draft acceptance rate: 87.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:35 [loggers.py:257] Engine 000: Avg prompt throughput: 257.9 tokens/s, Avg generation throughput: 538.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 362.08 tokens/s, Drafted throughput: 525.87 tokens/s, Accepted: 3621 tokens, Drafted: 5259 tokens, Per-position acceptance rate: 0.811, 0.675, 0.580, Avg Draft acceptance rate: 68.9%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:45 [loggers.py:257] Engine 000: Avg prompt throughput: 193.8 tokens/s, Avg generation throughput: 809.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 537.48 tokens/s, Drafted throughput: 817.03 tokens/s, Accepted: 5376 tokens, Drafted: 8172 tokens, Per-position acceptance rate: 0.793, 0.644, 0.537, Avg Draft acceptance rate: 65.8%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:55 [loggers.py:257] Engine 000: Avg prompt throughput: 156.1 tokens/s, Avg generation throughput: 706.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:37:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 468.07 tokens/s, Drafted throughput: 720.85 tokens/s, Accepted: 4681 tokens, Drafted: 7209 tokens, Per-position acceptance rate: 0.790, 0.634, 0.524, Avg Draft acceptance rate: 64.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  28.02     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              2.86      
Output token throughput (tok/s):         729.35    
Peak output token throughput (tok/s):    288.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          946.29    
---------------Time to First Token----------------
Mean TTFT (ms):                          115.16    
Median TTFT (ms):                        116.62    
P99 TTFT (ms):                           142.02    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.38     
Median TPOT (ms):                        19.30     
P99 TPOT (ms):                           23.89     
---------------Inter-token Latency----------------
Mean ITL (ms):                           57.57     
Median ITL (ms):                         57.05     
P99 ITL (ms):                            67.34     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.10     
Acceptance length:                       2.98      
Drafts:                                  6854      
Draft tokens:                            20562     
Accepted tokens:                         13592     
Per-position acceptance (%):
  Position 0:                            79.53     
  Position 1:                            64.69     
  Position 2:                            54.09     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 4.60 tokens/s, Drafted throughput: 9.00 tokens/s, Accepted: 46 tokens, Drafted: 90 tokens, Per-position acceptance rate: 0.633, 0.500, 0.400, Avg Draft acceptance rate: 51.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f01810d6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-97f216e4-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:15 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 25.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 17.60 tokens/s, Drafted throughput: 21.90 tokens/s, Accepted: 176 tokens, Drafted: 219 tokens, Per-position acceptance rate: 0.863, 0.795, 0.753, Avg Draft acceptance rate: 80.4%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:25 [loggers.py:257] Engine 000: Avg prompt throughput: 487.7 tokens/s, Avg generation throughput: 1393.2 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 39.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.01, Accepted throughput: 928.59 tokens/s, Drafted throughput: 1388.83 tokens/s, Accepted: 9287 tokens, Drafted: 13890 tokens, Per-position acceptance rate: 0.798, 0.655, 0.552, Avg Draft acceptance rate: 66.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  17.21     
Total input tokens:                      6078      
Total generated tokens:                  20450     
Request throughput (req/s):              4.65      
Output token throughput (tok/s):         1188.38   
Peak output token throughput (tok/s):    544.00    
Peak concurrent requests:                46.00     
Total token throughput (tok/s):          1541.58   
---------------Time to First Token----------------
Mean TTFT (ms):                          120.59    
Median TTFT (ms):                        126.04    
P99 TTFT (ms):                           196.44    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.70     
Median TPOT (ms):                        20.55     
P99 TPOT (ms):                           26.88     
---------------Inter-token Latency----------------
Mean ITL (ms):                           61.71     
Median ITL (ms):                         61.16     
P99 ITL (ms):                            78.70     
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.42     
Acceptance length:                       2.99      
Drafts:                                  6835      
Draft tokens:                            20505     
Accepted tokens:                         13619     
Per-position acceptance (%):
  Position 0:                            79.99     
  Position 1:                            64.97     
  Position 2:                            54.29     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:35 [loggers.py:257] Engine 000: Avg prompt throughput: 120.0 tokens/s, Avg generation throughput: 652.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 433.79 tokens/s, Drafted throughput: 662.09 tokens/s, Accepted: 4338 tokens, Drafted: 6621 tokens, Per-position acceptance rate: 0.803, 0.639, 0.524, Avg Draft acceptance rate: 65.5%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0eef0c6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a67ed6e3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:55 [loggers.py:257] Engine 000: Avg prompt throughput: 487.9 tokens/s, Avg generation throughput: 915.7 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 76.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:38:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 309.54 tokens/s, Drafted throughput: 435.67 tokens/s, Accepted: 6192 tokens, Drafted: 8715 tokens, Per-position acceptance rate: 0.826, 0.698, 0.608, Avg Draft acceptance rate: 71.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  12.45     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.42      
Output token throughput (tok/s):         1644.35   
Peak output token throughput (tok/s):    896.00    
Peak concurrent requests:                76.00     
Total token throughput (tok/s):          2132.35   
---------------Time to First Token----------------
Mean TTFT (ms):                          180.00    
Median TTFT (ms):                        165.06    
P99 TTFT (ms):                           276.18    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.89     
Median TPOT (ms):                        23.95     
P99 TPOT (ms):                           30.07     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.07     
Median ITL (ms):                         70.29     
P99 ITL (ms):                            146.22    
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.35     
Acceptance length:                       2.99      
Drafts:                                  6856      
Draft tokens:                            20568     
Accepted tokens:                         13646     
Per-position acceptance (%):
  Position 0:                            80.15     
  Position 1:                            64.89     
  Position 2:                            54.00     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:39:05 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 1157.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:39:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.90, Accepted throughput: 763.56 tokens/s, Drafted throughput: 1207.73 tokens/s, Accepted: 7636 tokens, Drafted: 12078 tokens, Per-position acceptance rate: 0.785, 0.616, 0.495, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:39:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f585410efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15012, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-66ee5a38-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:39:25 [loggers.py:257] Engine 000: Avg prompt throughput: 625.8 tokens/s, Avg generation throughput: 596.4 tokens/s, Running: 80 reqs, Waiting: 0 reqs, GPU KV cache usage: 68.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:39:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 206.03 tokens/s, Drafted throughput: 264.57 tokens/s, Accepted: 4121 tokens, Drafted: 5292 tokens, Per-position acceptance rate: 0.870, 0.769, 0.697, Avg Draft acceptance rate: 77.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  10.49     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              7.62      
Output token throughput (tok/s):         1951.96   
Peak output token throughput (tok/s):    1021.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2531.26   
---------------Time to First Token----------------
Mean TTFT (ms):                          209.37    
Median TTFT (ms):                        208.68    
P99 TTFT (ms):                           403.87    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          28.73     
Median TPOT (ms):                        29.00     
P99 TPOT (ms):                           36.59     
---------------Inter-token Latency----------------
Mean ITL (ms):                           85.58     
Median ITL (ms):                         77.34     
P99 ITL (ms):                            272.75    
---------------Speculative Decoding---------------
Acceptance rate (%):                     66.60     
Acceptance length:                       3.00      
Drafts:                                  6830      
Draft tokens:                            20490     
Accepted tokens:                         13647     
Per-position acceptance (%):
  Position 0:                            79.85     
  Position 1:                            65.23     
  Position 2:                            54.73     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k3-t0.0-tp1...
[0;36m(APIServer pid=2760772)[0;0m INFO 01-23 12:39:33 [launcher.py:110] Shutting down FastAPI HTTP server.
