Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k4-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k4-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 144851
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:15 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:15 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15013, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 4, 'max_model_len': 5000}}
[0;36m(APIServer pid=144851)[0;0m WARNING 01-23 12:23:15 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:16 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:16 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:17 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:17 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:17 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=144851)[0;0m WARNING 01-23 12:23:17 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:23:17 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fb9f8562fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-ec624443-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:23:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:23:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:23:28 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=4), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:23:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:23:30 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.67:49527 backend=nccl
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:23:30 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=145170)[0;0m WARNING 01-23 12:23:31 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:23:31 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:23:32 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:23:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:23:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:23:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:23:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:23:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:27 [default_loader.py:291] Loading weights took 53.84 seconds
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:27 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:27 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:27 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 12:24:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:35 [default_loader.py:291] Loading weights took 7.26 seconds
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:36 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 64.192770 seconds
WARNING 01-23 12:24:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:24:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:50 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:24:50 [backends.py:704] Dynamo bytecode transform time: 13.45 s
WARNING 01-23 12:24:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:25:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:25:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:08 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.521 s
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:08 [monitor.py:34] torch.compile takes 17.97 s in total
WARNING 01-23 12:25:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:14 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:14 [backends.py:704] Dynamo bytecode transform time: 6.07 s
WARNING 01-23 12:25:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:25:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:22 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.118 s
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:22 [monitor.py:34] torch.compile takes 26.15 s in total
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:24 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:24 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:24 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 12:25:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:25:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:25:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
WARNING 01-23 12:25:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:42 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 0.03 GiB
[0;36m(EngineCore_DP0 pid=145170)[0;0m INFO 01-23 12:25:42 [core.py:272] init engine (profile, create kv cache, warmup model) took 66.18 seconds
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:44 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=144851)[0;0m WARNING 01-23 12:25:45 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:45 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:45 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:45 [serving.py:185] Warming up chat template processing...
WARNING 01-23 12:25:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15013)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15013 ssl:default [Connect call failed (\'127.0.0.1\', 15013)]\n''
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [serving.py:221] Chat template warmup completed in 2029.7ms
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15013
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:47 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:58 [loggers.py:257] Engine 000: Avg prompt throughput: 27.1 tokens/s, Avg generation throughput: 35.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:25:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.23, Accepted throughput: 26.90 tokens/s, Drafted throughput: 33.36 tokens/s, Accepted: 358 tokens, Drafted: 444 tokens, Per-position acceptance rate: 0.937, 0.829, 0.766, 0.694, Avg Draft acceptance rate: 80.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:08 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 36.70 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 367 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.796, 0.623, 0.461, 0.317, Avg Draft acceptance rate: 54.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:18 [loggers.py:257] Engine 000: Avg prompt throughput: 39.1 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 40.09 tokens/s, Drafted throughput: 65.99 tokens/s, Accepted: 401 tokens, Drafted: 660 tokens, Per-position acceptance rate: 0.812, 0.630, 0.539, 0.448, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:28 [loggers.py:257] Engine 000: Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 67.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.07, Accepted throughput: 51.20 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 512 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.868, 0.802, 0.725, 0.671, Avg Draft acceptance rate: 76.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:38 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.20, Accepted throughput: 36.80 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 368 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.796, 0.587, 0.467, 0.353, Avg Draft acceptance rate: 55.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:48 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.12, Accepted throughput: 35.20 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 352 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.747, 0.566, 0.464, 0.343, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:58 [loggers.py:257] Engine 000: Avg prompt throughput: 17.6 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:26:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 40.79 tokens/s, Drafted throughput: 65.99 tokens/s, Accepted: 408 tokens, Drafted: 660 tokens, Per-position acceptance rate: 0.800, 0.673, 0.545, 0.455, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:08 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 41.39 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 414 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.820, 0.689, 0.563, 0.407, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:18 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 47.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 30.60 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 306 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.719, 0.509, 0.353, 0.251, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:28 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.03, Accepted throughput: 33.90 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 339 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.737, 0.533, 0.407, 0.353, Avg Draft acceptance rate: 50.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:38 [loggers.py:257] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 35.60 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 356 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.737, 0.593, 0.491, 0.311, Avg Draft acceptance rate: 53.3%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:48 [loggers.py:257] Engine 000: Avg prompt throughput: 23.0 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 44.90 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 449 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.819, 0.711, 0.614, 0.560, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:58 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:27:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 38.50 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 385 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.760, 0.635, 0.497, 0.413, Avg Draft acceptance rate: 57.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:08 [loggers.py:257] Engine 000: Avg prompt throughput: 43.5 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 45.39 tokens/s, Drafted throughput: 65.99 tokens/s, Accepted: 454 tokens, Drafted: 660 tokens, Per-position acceptance rate: 0.836, 0.709, 0.642, 0.564, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:18 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.99, Accepted throughput: 49.99 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 500 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.874, 0.814, 0.689, 0.617, Avg Draft acceptance rate: 74.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:28 [loggers.py:257] Engine 000: Avg prompt throughput: 16.6 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 43.80 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 438 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.880, 0.717, 0.590, 0.452, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:38 [loggers.py:257] Engine 000: Avg prompt throughput: 26.7 tokens/s, Avg generation throughput: 62.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 46.00 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 460 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.880, 0.735, 0.639, 0.518, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:48 [loggers.py:257] Engine 000: Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 41.50 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 415 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.760, 0.659, 0.581, 0.485, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:58 [loggers.py:257] Engine 000: Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:28:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 36.30 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 363 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.784, 0.605, 0.437, 0.347, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:08 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.82, Accepted throughput: 46.80 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 468 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.849, 0.759, 0.663, 0.548, Avg Draft acceptance rate: 70.5%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:18 [loggers.py:257] Engine 000: Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 36.10 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 361 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.760, 0.587, 0.467, 0.347, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:28 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.94, Accepted throughput: 32.20 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 322 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.723, 0.494, 0.410, 0.313, Avg Draft acceptance rate: 48.5%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:38 [loggers.py:257] Engine 000: Avg prompt throughput: 6.9 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 34.70 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 347 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.743, 0.551, 0.419, 0.365, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:48 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 42.79 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 428 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.790, 0.671, 0.599, 0.503, Avg Draft acceptance rate: 64.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:58 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:29:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 38.49 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 385 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.772, 0.605, 0.491, 0.437, Avg Draft acceptance rate: 57.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:08 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.34, Accepted throughput: 39.00 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 390 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.778, 0.629, 0.503, 0.425, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:18 [loggers.py:257] Engine 000: Avg prompt throughput: 14.8 tokens/s, Avg generation throughput: 50.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.04, Accepted throughput: 33.80 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 338 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.747, 0.560, 0.434, 0.295, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:28 [loggers.py:257] Engine 000: Avg prompt throughput: 12.6 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 42.69 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 427 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.819, 0.681, 0.584, 0.488, Avg Draft acceptance rate: 64.3%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:38 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 42.40 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 424 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.832, 0.689, 0.569, 0.449, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:48 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.17, Accepted throughput: 36.00 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 360 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.783, 0.590, 0.452, 0.343, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:58 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 60.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:30:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 44.09 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 441 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.820, 0.683, 0.617, 0.521, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:08 [loggers.py:257] Engine 000: Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 60.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 44.30 tokens/s, Drafted throughput: 66.40 tokens/s, Accepted: 443 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.843, 0.723, 0.590, 0.512, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:18 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 32.60 tokens/s, Drafted throughput: 66.79 tokens/s, Accepted: 326 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.731, 0.527, 0.401, 0.293, Avg Draft acceptance rate: 48.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:28 [loggers.py:257] Engine 000: Avg prompt throughput: 37.8 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 45.89 tokens/s, Drafted throughput: 66.39 tokens/s, Accepted: 459 tokens, Drafted: 664 tokens, Per-position acceptance rate: 0.843, 0.747, 0.633, 0.542, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:38 [loggers.py:257] Engine 000: Avg prompt throughput: 33.1 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 43.90 tokens/s, Drafted throughput: 66.00 tokens/s, Accepted: 439 tokens, Drafted: 660 tokens, Per-position acceptance rate: 0.830, 0.703, 0.600, 0.527, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:48 [loggers.py:257] Engine 000: Avg prompt throughput: 24.4 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 46.20 tokens/s, Drafted throughput: 66.80 tokens/s, Accepted: 462 tokens, Drafted: 668 tokens, Per-position acceptance rate: 0.850, 0.713, 0.623, 0.581, Avg Draft acceptance rate: 69.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  361.05    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         56.72     
Peak output token throughput (tok/s):    18.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          73.56     
---------------Time to First Token----------------
Mean TTFT (ms):                          71.62     
Median TTFT (ms):                        69.90     
P99 TTFT (ms):                           88.76     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.38     
Median TPOT (ms):                        17.30     
P99 TPOT (ms):                           22.56     
---------------Inter-token Latency----------------
Mean ITL (ms):                           59.12     
Median ITL (ms):                         59.17     
P99 ITL (ms):                            59.70     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.58     
Acceptance length:                       3.42      
Drafts:                                  5996      
Draft tokens:                            23984     
Accepted tokens:                         14530     
Per-position acceptance (%):
  Position 0:                            79.89     
  Position 1:                            64.86     
  Position 2:                            53.65     
  Position 3:                            43.93     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:58 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 38.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:31:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 26.30 tokens/s, Drafted throughput: 48.00 tokens/s, Accepted: 263 tokens, Drafted: 480 tokens, Per-position acceptance rate: 0.767, 0.583, 0.475, 0.367, Avg Draft acceptance rate: 54.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f957dda6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-84e0e7be-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:18 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.78, Accepted throughput: 4.35 tokens/s, Drafted throughput: 4.60 tokens/s, Accepted: 87 tokens, Drafted: 92 tokens, Per-position acceptance rate: 1.000, 0.957, 0.957, 0.870, Avg Draft acceptance rate: 94.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:28 [loggers.py:257] Engine 000: Avg prompt throughput: 34.7 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 69.29 tokens/s, Drafted throughput: 106.78 tokens/s, Accepted: 693 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.824, 0.693, 0.588, 0.491, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:38 [loggers.py:257] Engine 000: Avg prompt throughput: 46.0 tokens/s, Avg generation throughput: 121.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 89.59 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 896 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.831, 0.721, 0.641, 0.555, Avg Draft acceptance rate: 68.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:48 [loggers.py:257] Engine 000: Avg prompt throughput: 57.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 76.79 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 768 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.776, 0.629, 0.534, 0.417, Avg Draft acceptance rate: 58.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:58 [loggers.py:257] Engine 000: Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:32:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 83.69 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 837 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.853, 0.706, 0.567, 0.442, Avg Draft acceptance rate: 64.2%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:08 [loggers.py:257] Engine 000: Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 97.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.99, Accepted throughput: 65.30 tokens/s, Drafted throughput: 131.19 tokens/s, Accepted: 653 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.729, 0.540, 0.412, 0.311, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:18 [loggers.py:257] Engine 000: Avg prompt throughput: 30.0 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 80.99 tokens/s, Drafted throughput: 131.18 tokens/s, Accepted: 810 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.784, 0.649, 0.558, 0.479, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:28 [loggers.py:257] Engine 000: Avg prompt throughput: 47.5 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.46, Accepted throughput: 80.09 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 801 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.804, 0.653, 0.531, 0.469, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:38 [loggers.py:257] Engine 000: Avg prompt throughput: 25.3 tokens/s, Avg generation throughput: 124.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 92.58 tokens/s, Drafted throughput: 130.37 tokens/s, Accepted: 926 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.868, 0.761, 0.653, 0.558, Avg Draft acceptance rate: 71.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:48 [loggers.py:257] Engine 000: Avg prompt throughput: 41.2 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 84.59 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 846 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.819, 0.693, 0.595, 0.488, Avg Draft acceptance rate: 64.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:58 [loggers.py:257] Engine 000: Avg prompt throughput: 37.1 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:33:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 83.90 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 839 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.828, 0.696, 0.580, 0.469, Avg Draft acceptance rate: 64.3%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:08 [loggers.py:257] Engine 000: Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 97.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 64.29 tokens/s, Drafted throughput: 131.98 tokens/s, Accepted: 643 tokens, Drafted: 1320 tokens, Per-position acceptance rate: 0.730, 0.518, 0.400, 0.300, Avg Draft acceptance rate: 48.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:18 [loggers.py:257] Engine 000: Avg prompt throughput: 16.3 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 75.60 tokens/s, Drafted throughput: 130.40 tokens/s, Accepted: 756 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.770, 0.613, 0.512, 0.423, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:28 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 112.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.42, Accepted throughput: 79.49 tokens/s, Drafted throughput: 131.18 tokens/s, Accepted: 795 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.793, 0.652, 0.534, 0.445, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:38 [loggers.py:257] Engine 000: Avg prompt throughput: 19.4 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 76.09 tokens/s, Drafted throughput: 131.19 tokens/s, Accepted: 761 tokens, Drafted: 1312 tokens, Per-position acceptance rate: 0.787, 0.622, 0.512, 0.399, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:48 [loggers.py:257] Engine 000: Avg prompt throughput: 36.5 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.48, Accepted throughput: 80.68 tokens/s, Drafted throughput: 129.98 tokens/s, Accepted: 807 tokens, Drafted: 1300 tokens, Per-position acceptance rate: 0.812, 0.671, 0.542, 0.458, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:58 [loggers.py:257] Engine 000: Avg prompt throughput: 23.3 tokens/s, Avg generation throughput: 120.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:34:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 87.49 tokens/s, Drafted throughput: 131.59 tokens/s, Accepted: 875 tokens, Drafted: 1316 tokens, Per-position acceptance rate: 0.827, 0.705, 0.599, 0.529, Avg Draft acceptance rate: 66.5%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:08 [loggers.py:257] Engine 000: Avg prompt throughput: 44.3 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.18, Accepted throughput: 71.19 tokens/s, Drafted throughput: 130.39 tokens/s, Accepted: 712 tokens, Drafted: 1304 tokens, Per-position acceptance rate: 0.761, 0.592, 0.466, 0.365, Avg Draft acceptance rate: 54.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:18 [loggers.py:257] Engine 000: Avg prompt throughput: 57.5 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 82.69 tokens/s, Drafted throughput: 130.78 tokens/s, Accepted: 827 tokens, Drafted: 1308 tokens, Per-position acceptance rate: 0.801, 0.664, 0.563, 0.502, Avg Draft acceptance rate: 63.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  184.87    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.43      
Output token throughput (tok/s):         110.78    
Peak output token throughput (tok/s):    34.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          143.66    
---------------Time to First Token----------------
Mean TTFT (ms):                          121.20    
Median TTFT (ms):                        121.16    
P99 TTFT (ms):                           145.13    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.40     
Median TPOT (ms):                        17.26     
P99 TPOT (ms):                           22.63     
---------------Inter-token Latency----------------
Mean ITL (ms):                           59.54     
Median ITL (ms):                         59.50     
P99 ITL (ms):                            62.20     
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.12     
Acceptance length:                       3.44      
Drafts:                                  5961      
Draft tokens:                            23844     
Accepted tokens:                         14574     
Per-position acceptance (%):
  Position 0:                            79.92     
  Position 1:                            65.32     
  Position 2:                            54.32     
  Position 3:                            44.93     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:28 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 60.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 44.00 tokens/s, Drafted throughput: 70.00 tokens/s, Accepted: 440 tokens, Drafted: 700 tokens, Per-position acceptance rate: 0.811, 0.657, 0.571, 0.474, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fdba6282fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9eea7e6d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:48 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 111.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.73, Accepted throughput: 40.75 tokens/s, Drafted throughput: 59.60 tokens/s, Accepted: 815 tokens, Drafted: 1192 tokens, Per-position acceptance rate: 0.849, 0.725, 0.624, 0.537, Avg Draft acceptance rate: 68.4%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:58 [loggers.py:257] Engine 000: Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 227.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:35:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 163.09 tokens/s, Drafted throughput: 256.79 tokens/s, Accepted: 1631 tokens, Drafted: 2568 tokens, Per-position acceptance rate: 0.804, 0.667, 0.587, 0.483, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:08 [loggers.py:257] Engine 000: Avg prompt throughput: 27.4 tokens/s, Avg generation throughput: 207.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 143.58 tokens/s, Drafted throughput: 257.96 tokens/s, Accepted: 1436 tokens, Drafted: 2580 tokens, Per-position acceptance rate: 0.774, 0.595, 0.479, 0.378, Avg Draft acceptance rate: 55.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:18 [loggers.py:257] Engine 000: Avg prompt throughput: 79.5 tokens/s, Avg generation throughput: 224.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 159.68 tokens/s, Drafted throughput: 258.76 tokens/s, Accepted: 1597 tokens, Drafted: 2588 tokens, Per-position acceptance rate: 0.791, 0.665, 0.552, 0.461, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:28 [loggers.py:257] Engine 000: Avg prompt throughput: 71.9 tokens/s, Avg generation throughput: 237.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.73, Accepted throughput: 174.39 tokens/s, Drafted throughput: 255.19 tokens/s, Accepted: 1744 tokens, Drafted: 2552 tokens, Per-position acceptance rate: 0.846, 0.730, 0.632, 0.525, Avg Draft acceptance rate: 68.3%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:38 [loggers.py:257] Engine 000: Avg prompt throughput: 45.4 tokens/s, Avg generation throughput: 209.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 145.78 tokens/s, Drafted throughput: 259.17 tokens/s, Accepted: 1458 tokens, Drafted: 2592 tokens, Per-position acceptance rate: 0.764, 0.585, 0.495, 0.406, Avg Draft acceptance rate: 56.2%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:48 [loggers.py:257] Engine 000: Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 215.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.32, Accepted throughput: 150.59 tokens/s, Drafted throughput: 259.18 tokens/s, Accepted: 1506 tokens, Drafted: 2592 tokens, Per-position acceptance rate: 0.778, 0.623, 0.498, 0.424, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:58 [loggers.py:257] Engine 000: Avg prompt throughput: 55.9 tokens/s, Avg generation throughput: 216.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:36:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.36, Accepted throughput: 152.18 tokens/s, Drafted throughput: 257.56 tokens/s, Accepted: 1522 tokens, Drafted: 2576 tokens, Per-position acceptance rate: 0.792, 0.640, 0.511, 0.421, Avg Draft acceptance rate: 59.1%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:08 [loggers.py:257] Engine 000: Avg prompt throughput: 67.6 tokens/s, Avg generation throughput: 228.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 163.79 tokens/s, Drafted throughput: 258.39 tokens/s, Accepted: 1638 tokens, Drafted: 2584 tokens, Per-position acceptance rate: 0.819, 0.680, 0.565, 0.472, Avg Draft acceptance rate: 63.4%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:18 [loggers.py:257] Engine 000: Avg prompt throughput: 60.0 tokens/s, Avg generation throughput: 189.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 137.48 tokens/s, Drafted throughput: 211.17 tokens/s, Accepted: 1375 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.820, 0.688, 0.595, 0.502, Avg Draft acceptance rate: 65.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  95.51     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.84      
Output token throughput (tok/s):         214.43    
Peak output token throughput (tok/s):    68.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          278.07    
---------------Time to First Token----------------
Mean TTFT (ms):                          122.66    
Median TTFT (ms):                        122.45    
P99 TTFT (ms):                           136.40    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.59     
Median TPOT (ms):                        17.51     
P99 TPOT (ms):                           22.57     
---------------Inter-token Latency----------------
Mean ITL (ms):                           60.38     
Median ITL (ms):                         60.24     
P99 ITL (ms):                            64.67     
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.27     
Acceptance length:                       3.45      
Drafts:                                  5943      
Draft tokens:                            23772     
Accepted tokens:                         14566     
Per-position acceptance (%):
  Position 0:                            79.93     
  Position 1:                            65.30     
  Position 2:                            54.62     
  Position 3:                            45.25     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.10, Accepted throughput: 4.20 tokens/s, Drafted throughput: 8.00 tokens/s, Accepted: 42 tokens, Drafted: 80 tokens, Per-position acceptance rate: 0.800, 0.550, 0.450, 0.300, Avg Draft acceptance rate: 52.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3c17df6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-17df1f06-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:38 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.27, Accepted throughput: 20.90 tokens/s, Drafted throughput: 25.60 tokens/s, Accepted: 209 tokens, Drafted: 256 tokens, Per-position acceptance rate: 0.938, 0.844, 0.781, 0.703, Avg Draft acceptance rate: 81.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:48 [loggers.py:257] Engine 000: Avg prompt throughput: 143.2 tokens/s, Avg generation throughput: 422.9 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 303.06 tokens/s, Drafted throughput: 483.14 tokens/s, Accepted: 3031 tokens, Drafted: 4832 tokens, Per-position acceptance rate: 0.810, 0.669, 0.562, 0.468, Avg Draft acceptance rate: 62.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:58 [loggers.py:257] Engine 000: Avg prompt throughput: 141.0 tokens/s, Avg generation throughput: 427.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:37:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.47, Accepted throughput: 305.07 tokens/s, Drafted throughput: 494.74 tokens/s, Accepted: 3051 tokens, Drafted: 4948 tokens, Per-position acceptance rate: 0.802, 0.665, 0.550, 0.450, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:08 [loggers.py:257] Engine 000: Avg prompt throughput: 73.7 tokens/s, Avg generation throughput: 409.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.28, Accepted throughput: 285.17 tokens/s, Drafted throughput: 499.95 tokens/s, Accepted: 2852 tokens, Drafted: 5000 tokens, Per-position acceptance rate: 0.762, 0.610, 0.502, 0.408, Avg Draft acceptance rate: 57.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:18 [loggers.py:257] Engine 000: Avg prompt throughput: 123.8 tokens/s, Avg generation throughput: 423.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 300.55 tokens/s, Drafted throughput: 495.12 tokens/s, Accepted: 3006 tokens, Drafted: 4952 tokens, Per-position acceptance rate: 0.792, 0.644, 0.537, 0.456, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:28 [loggers.py:257] Engine 000: Avg prompt throughput: 104.3 tokens/s, Avg generation throughput: 357.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 254.88 tokens/s, Drafted throughput: 419.97 tokens/s, Accepted: 2549 tokens, Drafted: 4200 tokens, Per-position acceptance rate: 0.808, 0.649, 0.527, 0.445, Avg Draft acceptance rate: 60.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  51.41     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.56      
Output token throughput (tok/s):         398.35    
Peak output token throughput (tok/s):    128.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          516.57    
---------------Time to First Token----------------
Mean TTFT (ms):                          126.86    
Median TTFT (ms):                        126.61    
P99 TTFT (ms):                           159.84    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.46     
Median TPOT (ms):                        18.09     
P99 TPOT (ms):                           24.15     
---------------Inter-token Latency----------------
Mean ITL (ms):                           62.74     
Median ITL (ms):                         62.50     
P99 ITL (ms):                            68.40     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.52     
Acceptance length:                       3.42      
Drafts:                                  6003      
Draft tokens:                            24012     
Accepted tokens:                         14532     
Per-position acceptance (%):
  Position 0:                            79.41     
  Position 1:                            64.67     
  Position 2:                            53.54     
  Position 3:                            44.46     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.88, Accepted throughput: 3.20 tokens/s, Drafted throughput: 6.80 tokens/s, Accepted: 32 tokens, Drafted: 68 tokens, Per-position acceptance rate: 0.765, 0.529, 0.353, 0.235, Avg Draft acceptance rate: 47.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f66c2b7efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b68fc07c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:48 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 29.2 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.26, Accepted throughput: 22.20 tokens/s, Drafted throughput: 27.20 tokens/s, Accepted: 222 tokens, Drafted: 272 tokens, Per-position acceptance rate: 0.941, 0.838, 0.779, 0.706, Avg Draft acceptance rate: 81.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:58 [loggers.py:257] Engine 000: Avg prompt throughput: 218.1 tokens/s, Avg generation throughput: 774.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:38:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 547.22 tokens/s, Drafted throughput: 908.27 tokens/s, Accepted: 5473 tokens, Drafted: 9084 tokens, Per-position acceptance rate: 0.797, 0.648, 0.529, 0.435, Avg Draft acceptance rate: 60.2%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:08 [loggers.py:257] Engine 000: Avg prompt throughput: 216.9 tokens/s, Avg generation throughput: 789.8 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:08 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 559.76 tokens/s, Drafted throughput: 933.93 tokens/s, Accepted: 5598 tokens, Drafted: 9340 tokens, Per-position acceptance rate: 0.795, 0.646, 0.528, 0.429, Avg Draft acceptance rate: 59.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  29.38     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              2.72      
Output token throughput (tok/s):         697.03    
Peak output token throughput (tok/s):    240.00    
Peak concurrent requests:                23.00     
Total token throughput (tok/s):          903.90    
---------------Time to First Token----------------
Mean TTFT (ms):                          132.25    
Median TTFT (ms):                        132.22    
P99 TTFT (ms):                           164.52    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.60     
Median TPOT (ms):                        19.43     
P99 TPOT (ms):                           25.21     
---------------Inter-token Latency----------------
Mean ITL (ms):                           66.41     
Median ITL (ms):                         66.21     
P99 ITL (ms):                            81.93     
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.26     
Acceptance length:                       3.41      
Drafts:                                  6021      
Draft tokens:                            24084     
Accepted tokens:                         14514     
Per-position acceptance (%):
  Position 0:                            79.69     
  Position 1:                            64.67     
  Position 2:                            53.03     
  Position 3:                            43.66     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:18 [loggers.py:257] Engine 000: Avg prompt throughput: 116.4 tokens/s, Avg generation throughput: 480.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 341.88 tokens/s, Drafted throughput: 563.16 tokens/s, Accepted: 3419 tokens, Drafted: 5632 tokens, Per-position acceptance rate: 0.798, 0.645, 0.535, 0.450, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:28 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efaf8f26fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-40c6852b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:38 [loggers.py:257] Engine 000: Avg prompt throughput: 275.8 tokens/s, Avg generation throughput: 345.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:38 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.82, Accepted throughput: 126.58 tokens/s, Drafted throughput: 179.38 tokens/s, Accepted: 2532 tokens, Drafted: 3588 tokens, Per-position acceptance rate: 0.858, 0.755, 0.661, 0.548, Avg Draft acceptance rate: 70.6%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:48 [loggers.py:257] Engine 000: Avg prompt throughput: 349.9 tokens/s, Avg generation throughput: 1460.6 tokens/s, Running: 22 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.44, Accepted throughput: 1040.44 tokens/s, Drafted throughput: 1705.50 tokens/s, Accepted: 10405 tokens, Drafted: 17056 tokens, Per-position acceptance rate: 0.799, 0.656, 0.537, 0.447, Avg Draft acceptance rate: 61.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  17.62     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              4.54      
Output token throughput (tok/s):         1159.60   
Peak output token throughput (tok/s):    477.00    
Peak concurrent requests:                43.00     
Total token throughput (tok/s):          1504.50   
---------------Time to First Token----------------
Mean TTFT (ms):                          144.79    
Median TTFT (ms):                        143.30    
P99 TTFT (ms):                           215.20    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.74     
Median TPOT (ms):                        20.55     
P99 TPOT (ms):                           26.77     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.55     
Median ITL (ms):                         69.84     
P99 ITL (ms):                            116.27    
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.83     
Acceptance length:                       3.47      
Drafts:                                  5900      
Draft tokens:                            23600     
Accepted tokens:                         14591     
Per-position acceptance (%):
  Position 0:                            80.34     
  Position 1:                            66.36     
  Position 2:                            54.85     
  Position 3:                            45.76     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:58 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 262.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:39:58 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.31, Accepted throughput: 185.18 tokens/s, Drafted throughput: 319.96 tokens/s, Accepted: 1852 tokens, Drafted: 3200 tokens, Per-position acceptance rate: 0.774, 0.613, 0.499, 0.430, Avg Draft acceptance rate: 57.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:08 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7e8e1a6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8811e13a-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:18 [loggers.py:257] Engine 000: Avg prompt throughput: 517.3 tokens/s, Avg generation throughput: 1377.5 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:18 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 494.00 tokens/s, Drafted throughput: 768.52 tokens/s, Accepted: 9881 tokens, Drafted: 15372 tokens, Per-position acceptance rate: 0.818, 0.686, 0.579, 0.487, Avg Draft acceptance rate: 64.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  12.87     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.21      
Output token throughput (tok/s):         1590.98   
Peak output token throughput (tok/s):    768.00    
Peak concurrent requests:                75.00     
Total token throughput (tok/s):          2063.15   
---------------Time to First Token----------------
Mean TTFT (ms):                          198.13    
Median TTFT (ms):                        191.15    
P99 TTFT (ms):                           348.23    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.83     
Median TPOT (ms):                        24.56     
P99 TPOT (ms):                           32.82     
---------------Inter-token Latency----------------
Mean ITL (ms):                           84.87     
Median ITL (ms):                         84.31     
P99 ITL (ms):                            143.39    
---------------Speculative Decoding---------------
Acceptance rate (%):                     61.02     
Acceptance length:                       3.44      
Drafts:                                  5969      
Draft tokens:                            23876     
Accepted tokens:                         14569     
Per-position acceptance (%):
  Position 0:                            79.85     
  Position 1:                            65.29     
  Position 2:                            53.84     
  Position 3:                            45.10     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:28 [loggers.py:257] Engine 000: Avg prompt throughput: 108.4 tokens/s, Avg generation throughput: 695.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:28 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.23, Accepted throughput: 488.57 tokens/s, Drafted throughput: 874.75 tokens/s, Accepted: 4886 tokens, Drafted: 8748 tokens, Per-position acceptance rate: 0.768, 0.599, 0.473, 0.394, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:38 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7faa4aaeefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15013, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-f875cc15-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:48 [loggers.py:257] Engine 000: Avg prompt throughput: 625.8 tokens/s, Avg generation throughput: 1063.4 tokens/s, Running: 80 reqs, Waiting: 0 reqs, GPU KV cache usage: 93.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:48 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 384.93 tokens/s, Drafted throughput: 571.56 tokens/s, Accepted: 7699 tokens, Drafted: 11432 tokens, Per-position acceptance rate: 0.839, 0.718, 0.616, 0.522, Avg Draft acceptance rate: 67.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  11.42     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              7.01      
Output token throughput (tok/s):         1793.94   
Peak output token throughput (tok/s):    824.00    
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2326.34   
---------------Time to First Token----------------
Mean TTFT (ms):                          238.08    
Median TTFT (ms):                        235.22    
P99 TTFT (ms):                           440.21    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          31.41     
Median TPOT (ms):                        31.29     
P99 TPOT (ms):                           39.23     
---------------Inter-token Latency----------------
Mean ITL (ms):                           106.49    
Median ITL (ms):                         103.75    
P99 ITL (ms):                            219.51    
---------------Speculative Decoding---------------
Acceptance rate (%):                     60.48     
Acceptance length:                       3.42      
Drafts:                                  5999      
Draft tokens:                            23996     
Accepted tokens:                         14513     
Per-position acceptance (%):
  Position 0:                            79.65     
  Position 1:                            64.53     
  Position 2:                            53.53     
  Position 3:                            44.22     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k4-t0.0-tp1...
[0;36m(APIServer pid=144851)[0;0m INFO 01-23 12:40:55 [launcher.py:110] Shutting down FastAPI HTTP server.
