Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k5-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k5-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 3701664
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:11 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:11 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15014, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=3701664)[0;0m WARNING 01-23 12:27:11 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:12 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:12 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:13 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:13 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:13 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=3701664)[0;0m WARNING 01-23 12:27:13 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:27:13 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efaa699afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-3e3cc27e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:27:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:27:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:27:24 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=5), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:27:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:27:26 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.45:57915 backend=nccl
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:27:26 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=3701757)[0;0m WARNING 01-23 12:27:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:27:27 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:27:27 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:27:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:27:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:27:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:27:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:27:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:27:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:19 [default_loader.py:291] Loading weights took 50.39 seconds
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:19 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:19 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:19 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 12:28:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:36 [default_loader.py:291] Loading weights took 15.83 seconds
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:37 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 69.186251 seconds
WARNING 01-23 12:28:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:49 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:28:49 [backends.py:704] Dynamo bytecode transform time: 11.81 s
WARNING 01-23 12:28:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:28:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:29:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:29:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:06 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.232 s
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:06 [monitor.py:34] torch.compile takes 16.04 s in total
WARNING 01-23 12:29:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:12 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:12 [backends.py:704] Dynamo bytecode transform time: 6.12 s
WARNING 01-23 12:29:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:21 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 1.884 s
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:21 [monitor.py:34] torch.compile takes 24.05 s in total
WARNING 01-23 12:29:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:22 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:22 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:22 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 12:29:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:29:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
WARNING 01-23 12:29:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:41 [gpu_model_runner.py:4880] Graph capturing finished in 17 secs, took 0.02 GiB
WARNING 01-23 12:29:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
[0;36m(EngineCore_DP0 pid=3701757)[0;0m INFO 01-23 12:29:41 [core.py:272] init engine (profile, create kv cache, warmup model) took 64.10 seconds
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:43 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=3701664)[0;0m WARNING 01-23 12:29:43 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:43 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:43 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:43 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:45 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:45 [serving.py:221] Chat template warmup completed in 2154.8ms
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15014
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:46 [launcher.py:46] Route: /pooling, Methods: POST
WARNING 01-23 12:29:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15014)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15014 ssl:default [Connect call failed (\'127.0.0.1\', 15014)]\n''
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:56 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:29:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.80, Accepted throughput: 18.44 tokens/s, Drafted throughput: 24.26 tokens/s, Accepted: 247 tokens, Drafted: 325 tokens, Per-position acceptance rate: 0.938, 0.831, 0.723, 0.692, 0.615, Avg Draft acceptance rate: 76.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 57.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.79, Accepted throughput: 42.20 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 422 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.834, 0.656, 0.530, 0.430, 0.344, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:16 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.45, Accepted throughput: 37.00 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 370 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.781, 0.589, 0.457, 0.344, 0.278, Avg Draft acceptance rate: 49.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:26 [loggers.py:257] Engine 000: Avg prompt throughput: 19.7 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.35, Accepted throughput: 50.30 tokens/s, Drafted throughput: 74.99 tokens/s, Accepted: 503 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.847, 0.753, 0.680, 0.567, 0.507, Avg Draft acceptance rate: 67.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:36 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.10, Accepted throughput: 47.10 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 471 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.816, 0.697, 0.599, 0.526, 0.461, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:46 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 47.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.13, Accepted throughput: 32.10 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 321 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.709, 0.530, 0.424, 0.291, 0.172, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:56 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:30:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.15, Accepted throughput: 47.60 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 476 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.854, 0.748, 0.616, 0.517, 0.417, Avg Draft acceptance rate: 63.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:06 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 57.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 42.09 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 421 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.862, 0.664, 0.539, 0.414, 0.289, Avg Draft acceptance rate: 55.4%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:16 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 33.10 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 331 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.762, 0.510, 0.397, 0.291, 0.232, Avg Draft acceptance rate: 43.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:26 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 32.70 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 327 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.678, 0.493, 0.375, 0.316, 0.289, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:36 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.40, Accepted throughput: 36.30 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 363 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.755, 0.589, 0.450, 0.338, 0.272, Avg Draft acceptance rate: 48.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:46 [loggers.py:257] Engine 000: Avg prompt throughput: 25.6 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 45.80 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 458 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.815, 0.702, 0.609, 0.490, 0.417, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:56 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:31:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.98, Accepted throughput: 44.99 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 450 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.768, 0.682, 0.583, 0.523, 0.424, Avg Draft acceptance rate: 59.6%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:06 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 57.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 42.30 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 423 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.801, 0.629, 0.536, 0.450, 0.384, Avg Draft acceptance rate: 56.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:16 [loggers.py:257] Engine 000: Avg prompt throughput: 15.2 tokens/s, Avg generation throughput: 67.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.50, Accepted throughput: 52.50 tokens/s, Drafted throughput: 74.99 tokens/s, Accepted: 525 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.873, 0.793, 0.673, 0.613, 0.547, Avg Draft acceptance rate: 70.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:26 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 62.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.11, Accepted throughput: 47.29 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 473 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.849, 0.711, 0.605, 0.500, 0.447, Avg Draft acceptance rate: 62.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:36 [loggers.py:257] Engine 000: Avg prompt throughput: 30.1 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.93, Accepted throughput: 43.90 tokens/s, Drafted throughput: 75.00 tokens/s, Accepted: 439 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.767, 0.667, 0.580, 0.500, 0.413, Avg Draft acceptance rate: 58.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:46 [loggers.py:257] Engine 000: Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.19, Accepted throughput: 48.19 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 482 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.834, 0.728, 0.616, 0.523, 0.490, Avg Draft acceptance rate: 63.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:56 [loggers.py:257] Engine 000: Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:32:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.41, Accepted throughput: 36.60 tokens/s, Drafted throughput: 76.00 tokens/s, Accepted: 366 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.770, 0.625, 0.441, 0.329, 0.243, Avg Draft acceptance rate: 48.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:06 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 65.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.41, Accepted throughput: 51.09 tokens/s, Drafted throughput: 74.99 tokens/s, Accepted: 511 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.900, 0.793, 0.647, 0.560, 0.507, Avg Draft acceptance rate: 68.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:16 [loggers.py:257] Engine 000: Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 53.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.55, Accepted throughput: 38.50 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 385 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.755, 0.589, 0.503, 0.404, 0.298, Avg Draft acceptance rate: 51.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:26 [loggers.py:257] Engine 000: Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 43.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.82, Accepted throughput: 27.90 tokens/s, Drafted throughput: 76.50 tokens/s, Accepted: 279 tokens, Drafted: 765 tokens, Per-position acceptance rate: 0.673, 0.477, 0.301, 0.209, 0.163, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:36 [loggers.py:257] Engine 000: Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 33.30 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 333 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.704, 0.546, 0.388, 0.316, 0.237, Avg Draft acceptance rate: 43.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:46 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.19, Accepted throughput: 47.90 tokens/s, Drafted throughput: 74.99 tokens/s, Accepted: 479 tokens, Drafted: 750 tokens, Per-position acceptance rate: 0.807, 0.727, 0.633, 0.540, 0.487, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:56 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:33:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 42.90 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 429 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.815, 0.656, 0.536, 0.483, 0.351, Avg Draft acceptance rate: 56.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 55.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 40.40 tokens/s, Drafted throughput: 76.00 tokens/s, Accepted: 404 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.776, 0.625, 0.493, 0.408, 0.355, Avg Draft acceptance rate: 53.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:16 [loggers.py:257] Engine 000: Avg prompt throughput: 14.8 tokens/s, Avg generation throughput: 48.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 33.10 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 331 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.742, 0.536, 0.391, 0.285, 0.238, Avg Draft acceptance rate: 43.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:26 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 53.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 38.70 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 387 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.755, 0.596, 0.483, 0.391, 0.338, Avg Draft acceptance rate: 51.3%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:36 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.09, Accepted throughput: 46.60 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 466 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.834, 0.709, 0.603, 0.490, 0.450, Avg Draft acceptance rate: 61.7%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:46 [loggers.py:257] Engine 000: Avg prompt throughput: 28.5 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 39.40 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 394 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.788, 0.576, 0.483, 0.404, 0.358, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:56 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:34:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.17, Accepted throughput: 47.90 tokens/s, Drafted throughput: 75.50 tokens/s, Accepted: 479 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.874, 0.702, 0.583, 0.517, 0.497, Avg Draft acceptance rate: 63.4%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:06 [loggers.py:257] Engine 000: Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.92, Accepted throughput: 44.40 tokens/s, Drafted throughput: 75.99 tokens/s, Accepted: 444 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.836, 0.678, 0.546, 0.461, 0.401, Avg Draft acceptance rate: 58.4%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:16 [loggers.py:257] Engine 000: Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 46.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.07, Accepted throughput: 31.20 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 312 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.702, 0.530, 0.351, 0.272, 0.212, Avg Draft acceptance rate: 41.3%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:26 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 58.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.86, Accepted throughput: 43.50 tokens/s, Drafted throughput: 76.00 tokens/s, Accepted: 435 tokens, Drafted: 760 tokens, Per-position acceptance rate: 0.803, 0.697, 0.592, 0.434, 0.336, Avg Draft acceptance rate: 57.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:36 [loggers.py:257] Engine 000: Avg prompt throughput: 65.9 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.56, Accepted throughput: 53.00 tokens/s, Drafted throughput: 74.50 tokens/s, Accepted: 530 tokens, Drafted: 745 tokens, Per-position acceptance rate: 0.906, 0.792, 0.711, 0.604, 0.544, Avg Draft acceptance rate: 71.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:46 [loggers.py:257] Engine 000: Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 43.99 tokens/s, Drafted throughput: 75.49 tokens/s, Accepted: 440 tokens, Drafted: 755 tokens, Per-position acceptance rate: 0.821, 0.642, 0.576, 0.464, 0.411, Avg Draft acceptance rate: 58.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  360.68    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         56.78     
Peak output token throughput (tok/s):    16.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          73.63     
---------------Time to First Token----------------
Mean TTFT (ms):                          77.31     
Median TTFT (ms):                        75.91     
P99 TTFT (ms):                           90.71     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.34     
Median TPOT (ms):                        16.98     
P99 TPOT (ms):                           23.54     
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.05     
Median ITL (ms):                         65.10     
P99 ITL (ms):                            65.64     
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.55     
Acceptance length:                       3.78      
Drafts:                                  5437      
Draft tokens:                            27185     
Accepted tokens:                         15101     
Per-position acceptance (%):
  Position 0:                            79.60     
  Position 1:                            64.80     
  Position 2:                            53.15     
  Position 3:                            43.57     
  Position 4:                            36.62     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:56 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 52.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:35:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.89, Accepted throughput: 39.89 tokens/s, Drafted throughput: 68.99 tokens/s, Accepted: 399 tokens, Drafted: 690 tokens, Per-position acceptance rate: 0.783, 0.674, 0.601, 0.471, 0.362, Avg Draft acceptance rate: 57.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9fe995efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c627798f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:16 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 43.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.71, Accepted throughput: 17.25 tokens/s, Drafted throughput: 23.25 tokens/s, Accepted: 345 tokens, Drafted: 465 tokens, Per-position acceptance rate: 0.925, 0.828, 0.720, 0.656, 0.581, Avg Draft acceptance rate: 74.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:26 [loggers.py:257] Engine 000: Avg prompt throughput: 45.0 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 77.29 tokens/s, Drafted throughput: 148.98 tokens/s, Accepted: 773 tokens, Drafted: 1490 tokens, Per-position acceptance rate: 0.775, 0.607, 0.487, 0.399, 0.326, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:36 [loggers.py:257] Engine 000: Avg prompt throughput: 29.6 tokens/s, Avg generation throughput: 121.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.15, Accepted throughput: 92.50 tokens/s, Drafted throughput: 147.00 tokens/s, Accepted: 925 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.827, 0.704, 0.616, 0.531, 0.469, Avg Draft acceptance rate: 62.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:46 [loggers.py:257] Engine 000: Avg prompt throughput: 57.0 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.85, Accepted throughput: 83.80 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 838 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.830, 0.667, 0.551, 0.459, 0.344, Avg Draft acceptance rate: 57.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:56 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:36:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.37, Accepted throughput: 71.09 tokens/s, Drafted throughput: 149.99 tokens/s, Accepted: 711 tokens, Drafted: 1500 tokens, Per-position acceptance rate: 0.777, 0.547, 0.447, 0.337, 0.263, Avg Draft acceptance rate: 47.4%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 65.39 tokens/s, Drafted throughput: 148.99 tokens/s, Accepted: 654 tokens, Drafted: 1490 tokens, Per-position acceptance rate: 0.701, 0.520, 0.409, 0.309, 0.255, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:16 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 118.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.01, Accepted throughput: 88.99 tokens/s, Drafted throughput: 147.98 tokens/s, Accepted: 890 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.804, 0.696, 0.595, 0.500, 0.412, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:26 [loggers.py:257] Engine 000: Avg prompt throughput: 52.2 tokens/s, Avg generation throughput: 121.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.12, Accepted throughput: 91.79 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 918 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.837, 0.711, 0.599, 0.517, 0.459, Avg Draft acceptance rate: 62.4%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:36 [loggers.py:257] Engine 000: Avg prompt throughput: 43.3 tokens/s, Avg generation throughput: 122.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.16, Accepted throughput: 92.80 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 928 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.820, 0.690, 0.622, 0.541, 0.483, Avg Draft acceptance rate: 63.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:46 [loggers.py:257] Engine 000: Avg prompt throughput: 19.9 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 86.19 tokens/s, Drafted throughput: 147.98 tokens/s, Accepted: 862 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.787, 0.689, 0.571, 0.470, 0.395, Avg Draft acceptance rate: 58.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:56 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 105.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:37:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 77.00 tokens/s, Drafted throughput: 148.00 tokens/s, Accepted: 770 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.784, 0.605, 0.486, 0.402, 0.324, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:06 [loggers.py:257] Engine 000: Avg prompt throughput: 13.7 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.15, Accepted throughput: 64.09 tokens/s, Drafted throughput: 148.98 tokens/s, Accepted: 641 tokens, Drafted: 1490 tokens, Per-position acceptance rate: 0.681, 0.527, 0.393, 0.326, 0.225, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:16 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 82.19 tokens/s, Drafted throughput: 147.98 tokens/s, Accepted: 822 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.774, 0.635, 0.544, 0.456, 0.368, Avg Draft acceptance rate: 55.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:26 [loggers.py:257] Engine 000: Avg prompt throughput: 51.4 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.65, Accepted throughput: 77.80 tokens/s, Drafted throughput: 147.00 tokens/s, Accepted: 778 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.772, 0.622, 0.490, 0.408, 0.354, Avg Draft acceptance rate: 52.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:36 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 80.39 tokens/s, Drafted throughput: 148.98 tokens/s, Accepted: 804 tokens, Drafted: 1490 tokens, Per-position acceptance rate: 0.792, 0.607, 0.510, 0.419, 0.369, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:46 [loggers.py:257] Engine 000: Avg prompt throughput: 36.1 tokens/s, Avg generation throughput: 120.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.07, Accepted throughput: 90.99 tokens/s, Drafted throughput: 147.99 tokens/s, Accepted: 910 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.861, 0.713, 0.584, 0.500, 0.416, Avg Draft acceptance rate: 61.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:56 [loggers.py:257] Engine 000: Avg prompt throughput: 19.5 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:38:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.74, Accepted throughput: 81.09 tokens/s, Drafted throughput: 147.98 tokens/s, Accepted: 811 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.787, 0.639, 0.517, 0.436, 0.361, Avg Draft acceptance rate: 54.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:06 [loggers.py:257] Engine 000: Avg prompt throughput: 71.4 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 83.40 tokens/s, Drafted throughput: 147.00 tokens/s, Accepted: 834 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.820, 0.673, 0.551, 0.429, 0.364, Avg Draft acceptance rate: 56.7%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:16 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 123.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.21, Accepted throughput: 94.89 tokens/s, Drafted throughput: 147.99 tokens/s, Accepted: 949 tokens, Drafted: 1480 tokens, Per-position acceptance rate: 0.834, 0.723, 0.639, 0.544, 0.466, Avg Draft acceptance rate: 64.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  186.62    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.43      
Output token throughput (tok/s):         109.74    
Peak output token throughput (tok/s):    32.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          142.31    
---------------Time to First Token----------------
Mean TTFT (ms):                          132.94    
Median TTFT (ms):                        132.94    
P99 TTFT (ms):                           145.64    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.48     
Median TPOT (ms):                        17.23     
P99 TPOT (ms):                           24.03     
---------------Inter-token Latency----------------
Mean ITL (ms):                           65.62     
Median ITL (ms):                         65.59     
P99 ITL (ms):                            68.19     
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.60     
Acceptance length:                       3.78      
Drafts:                                  5435      
Draft tokens:                            27175     
Accepted tokens:                         15108     
Per-position acceptance (%):
  Position 0:                            79.19     
  Position 1:                            64.31     
  Position 2:                            53.39     
  Position 3:                            44.25     
  Position 4:                            36.84     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.16, Accepted throughput: 14.90 tokens/s, Drafted throughput: 34.50 tokens/s, Accepted: 149 tokens, Drafted: 345 tokens, Per-position acceptance rate: 0.696, 0.536, 0.449, 0.290, 0.188, Avg Draft acceptance rate: 43.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fce450defc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7b3cfcbe-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:36 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.33, Accepted throughput: 3.90 tokens/s, Drafted throughput: 4.50 tokens/s, Accepted: 39 tokens, Drafted: 45 tokens, Per-position acceptance rate: 1.000, 1.000, 0.778, 0.778, 0.778, Avg Draft acceptance rate: 86.7%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:46 [loggers.py:257] Engine 000: Avg prompt throughput: 76.1 tokens/s, Avg generation throughput: 167.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 125.78 tokens/s, Drafted throughput: 207.97 tokens/s, Accepted: 1258 tokens, Drafted: 2080 tokens, Per-position acceptance rate: 0.851, 0.690, 0.579, 0.495, 0.409, Avg Draft acceptance rate: 60.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:56 [loggers.py:257] Engine 000: Avg prompt throughput: 80.2 tokens/s, Avg generation throughput: 226.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:39:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 168.68 tokens/s, Drafted throughput: 290.97 tokens/s, Accepted: 1687 tokens, Drafted: 2910 tokens, Per-position acceptance rate: 0.825, 0.675, 0.552, 0.467, 0.380, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:06 [loggers.py:257] Engine 000: Avg prompt throughput: 36.7 tokens/s, Avg generation throughput: 204.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.50, Accepted throughput: 146.58 tokens/s, Drafted throughput: 292.97 tokens/s, Accepted: 1466 tokens, Drafted: 2930 tokens, Per-position acceptance rate: 0.775, 0.594, 0.488, 0.367, 0.278, Avg Draft acceptance rate: 50.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:16 [loggers.py:257] Engine 000: Avg prompt throughput: 73.3 tokens/s, Avg generation throughput: 234.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 176.48 tokens/s, Drafted throughput: 290.97 tokens/s, Accepted: 1765 tokens, Drafted: 2910 tokens, Per-position acceptance rate: 0.826, 0.687, 0.584, 0.503, 0.431, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:26 [loggers.py:257] Engine 000: Avg prompt throughput: 58.3 tokens/s, Avg generation throughput: 235.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 177.28 tokens/s, Drafted throughput: 292.47 tokens/s, Accepted: 1773 tokens, Drafted: 2925 tokens, Per-position acceptance rate: 0.832, 0.715, 0.585, 0.480, 0.419, Avg Draft acceptance rate: 60.6%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:36 [loggers.py:257] Engine 000: Avg prompt throughput: 49.1 tokens/s, Avg generation throughput: 207.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.57, Accepted throughput: 150.08 tokens/s, Drafted throughput: 291.46 tokens/s, Accepted: 1501 tokens, Drafted: 2915 tokens, Per-position acceptance rate: 0.758, 0.599, 0.484, 0.401, 0.333, Avg Draft acceptance rate: 51.5%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:46 [loggers.py:257] Engine 000: Avg prompt throughput: 67.7 tokens/s, Avg generation throughput: 215.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.71, Accepted throughput: 157.97 tokens/s, Drafted throughput: 290.95 tokens/s, Accepted: 1580 tokens, Drafted: 2910 tokens, Per-position acceptance rate: 0.782, 0.641, 0.510, 0.419, 0.363, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:56 [loggers.py:257] Engine 000: Avg prompt throughput: 55.8 tokens/s, Avg generation throughput: 234.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:40:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 176.18 tokens/s, Drafted throughput: 293.97 tokens/s, Accepted: 1762 tokens, Drafted: 2940 tokens, Per-position acceptance rate: 0.820, 0.672, 0.570, 0.495, 0.440, Avg Draft acceptance rate: 59.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:06 [loggers.py:257] Engine 000: Avg prompt throughput: 83.7 tokens/s, Avg generation throughput: 221.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 163.49 tokens/s, Drafted throughput: 290.98 tokens/s, Accepted: 1635 tokens, Drafted: 2910 tokens, Per-position acceptance rate: 0.813, 0.660, 0.531, 0.440, 0.366, Avg Draft acceptance rate: 56.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  95.52     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.84      
Output token throughput (tok/s):         214.41    
Peak output token throughput (tok/s):    64.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          278.04    
---------------Time to First Token----------------
Mean TTFT (ms):                          133.99    
Median TTFT (ms):                        134.75    
P99 TTFT (ms):                           146.76    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.46     
Median TPOT (ms):                        17.14     
P99 TPOT (ms):                           24.23     
---------------Inter-token Latency----------------
Mean ITL (ms):                           66.51     
Median ITL (ms):                         66.36     
P99 ITL (ms):                            70.61     
---------------Speculative Decoding---------------
Acceptance rate (%):                     56.61     
Acceptance length:                       3.83      
Drafts:                                  5357      
Draft tokens:                            26785     
Accepted tokens:                         15162     
Per-position acceptance (%):
  Position 0:                            80.60     
  Position 1:                            65.71     
  Position 2:                            54.10     
  Position 3:                            44.89     
  Position 4:                            37.73     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:16 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 120.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.84, Accepted throughput: 89.99 tokens/s, Drafted throughput: 158.49 tokens/s, Accepted: 900 tokens, Drafted: 1585 tokens, Per-position acceptance rate: 0.795, 0.656, 0.558, 0.451, 0.379, Avg Draft acceptance rate: 56.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1d625defc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c9821831-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:36 [loggers.py:257] Engine 000: Avg prompt throughput: 94.1 tokens/s, Avg generation throughput: 141.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.38, Accepted throughput: 54.30 tokens/s, Drafted throughput: 80.24 tokens/s, Accepted: 1086 tokens, Drafted: 1605 tokens, Per-position acceptance rate: 0.872, 0.763, 0.660, 0.583, 0.505, Avg Draft acceptance rate: 67.7%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:46 [loggers.py:257] Engine 000: Avg prompt throughput: 116.9 tokens/s, Avg generation throughput: 419.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.69, Accepted throughput: 306.05 tokens/s, Drafted throughput: 567.90 tokens/s, Accepted: 3061 tokens, Drafted: 5680 tokens, Per-position acceptance rate: 0.790, 0.629, 0.516, 0.426, 0.333, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:56 [loggers.py:257] Engine 000: Avg prompt throughput: 128.1 tokens/s, Avg generation throughput: 441.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:41:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.91, Accepted throughput: 329.67 tokens/s, Drafted throughput: 566.95 tokens/s, Accepted: 3297 tokens, Drafted: 5670 tokens, Per-position acceptance rate: 0.809, 0.670, 0.563, 0.474, 0.392, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:06 [loggers.py:257] Engine 000: Avg prompt throughput: 112.6 tokens/s, Avg generation throughput: 412.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 300.56 tokens/s, Drafted throughput: 565.92 tokens/s, Accepted: 3006 tokens, Drafted: 5660 tokens, Per-position acceptance rate: 0.775, 0.617, 0.503, 0.417, 0.345, Avg Draft acceptance rate: 53.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:16 [loggers.py:257] Engine 000: Avg prompt throughput: 145.0 tokens/s, Avg generation throughput: 435.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.88, Accepted throughput: 323.77 tokens/s, Drafted throughput: 562.95 tokens/s, Accepted: 3238 tokens, Drafted: 5630 tokens, Per-position acceptance rate: 0.806, 0.654, 0.551, 0.470, 0.396, Avg Draft acceptance rate: 57.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  51.89     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.54      
Output token throughput (tok/s):         394.71    
Peak output token throughput (tok/s):    120.00    
Peak concurrent requests:                12.00     
Total token throughput (tok/s):          511.86    
---------------Time to First Token----------------
Mean TTFT (ms):                          136.99    
Median TTFT (ms):                        138.36    
P99 TTFT (ms):                           154.29    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.21     
Median TPOT (ms):                        17.91     
P99 TPOT (ms):                           24.99     
---------------Inter-token Latency----------------
Mean ITL (ms):                           68.55     
Median ITL (ms):                         68.28     
P99 ITL (ms):                            74.46     
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.80     
Acceptance length:                       3.79      
Drafts:                                  5418      
Draft tokens:                            27090     
Accepted tokens:                         15115     
Per-position acceptance (%):
  Position 0:                            79.59     
  Position 1:                            64.41     
  Position 2:                            53.53     
  Position 3:                            44.68     
  Position 4:                            36.77     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:26 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 223.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.61, Accepted throughput: 163.09 tokens/s, Drafted throughput: 311.98 tokens/s, Accepted: 1631 tokens, Drafted: 3120 tokens, Per-position acceptance rate: 0.776, 0.609, 0.502, 0.399, 0.329, Avg Draft acceptance rate: 52.3%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f51ed35afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-788ee9d4-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:46 [loggers.py:257] Engine 000: Avg prompt throughput: 171.4 tokens/s, Avg generation throughput: 198.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.26, Accepted throughput: 75.30 tokens/s, Drafted throughput: 115.50 tokens/s, Accepted: 1506 tokens, Drafted: 2310 tokens, Per-position acceptance rate: 0.861, 0.734, 0.643, 0.554, 0.468, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:56 [loggers.py:257] Engine 000: Avg prompt throughput: 207.0 tokens/s, Avg generation throughput: 809.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:42:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.79, Accepted throughput: 597.66 tokens/s, Drafted throughput: 1072.92 tokens/s, Accepted: 5977 tokens, Drafted: 10730 tokens, Per-position acceptance rate: 0.800, 0.645, 0.532, 0.444, 0.364, Avg Draft acceptance rate: 55.7%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:06 [loggers.py:257] Engine 000: Avg prompt throughput: 241.9 tokens/s, Avg generation throughput: 810.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 594.83 tokens/s, Drafted throughput: 1080.86 tokens/s, Accepted: 5949 tokens, Drafted: 10810 tokens, Per-position acceptance rate: 0.796, 0.634, 0.524, 0.434, 0.363, Avg Draft acceptance rate: 55.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  28.78     
Total input tokens:                      6078      
Total generated tokens:                  20471     
Request throughput (req/s):              2.78      
Output token throughput (tok/s):         711.23    
Peak output token throughput (tok/s):    239.00    
Peak concurrent requests:                25.00     
Total token throughput (tok/s):          922.40    
---------------Time to First Token----------------
Mean TTFT (ms):                          142.40    
Median TTFT (ms):                        144.04    
P99 TTFT (ms):                           163.49    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.13     
Median TPOT (ms):                        18.86     
P99 TPOT (ms):                           26.57     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.81     
Median ITL (ms):                         71.51     
P99 ITL (ms):                            83.09     
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.54     
Acceptance length:                       3.78      
Drafts:                                  5431      
Draft tokens:                            27155     
Accepted tokens:                         15081     
Per-position acceptance (%):
  Position 0:                            80.15     
  Position 1:                            64.19     
  Position 2:                            52.97     
  Position 3:                            43.99     
  Position 4:                            36.38     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:16 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 255.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.59, Accepted throughput: 185.30 tokens/s, Drafted throughput: 357.99 tokens/s, Accepted: 1853 tokens, Drafted: 3580 tokens, Per-position acceptance rate: 0.793, 0.609, 0.480, 0.390, 0.316, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f90dcbbefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-a5d9aa8b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:36 [loggers.py:257] Engine 000: Avg prompt throughput: 333.7 tokens/s, Avg generation throughput: 771.9 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 288.78 tokens/s, Drafted throughput: 480.71 tokens/s, Accepted: 5776 tokens, Drafted: 9615 tokens, Per-position acceptance rate: 0.827, 0.686, 0.580, 0.492, 0.418, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:46 [loggers.py:257] Engine 000: Avg prompt throughput: 292.0 tokens/s, Avg generation throughput: 1269.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 931.96 tokens/s, Drafted throughput: 1725.75 tokens/s, Accepted: 9321 tokens, Drafted: 17260 tokens, Per-position acceptance rate: 0.790, 0.628, 0.510, 0.424, 0.349, Avg Draft acceptance rate: 54.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  17.77     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.50      
Output token throughput (tok/s):         1152.26   
Peak output token throughput (tok/s):    448.00    
Peak concurrent requests:                43.00     
Total token throughput (tok/s):          1494.22   
---------------Time to First Token----------------
Mean TTFT (ms):                          150.44    
Median TTFT (ms):                        156.82    
P99 TTFT (ms):                           200.59    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.50     
Median TPOT (ms):                        20.00     
P99 TPOT (ms):                           28.45     
---------------Inter-token Latency----------------
Mean ITL (ms):                           76.97     
Median ITL (ms):                         75.81     
P99 ITL (ms):                            111.29    
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.65     
Acceptance length:                       3.78      
Drafts:                                  5432      
Draft tokens:                            27160     
Accepted tokens:                         15114     
Per-position acceptance (%):
  Position 0:                            79.97     
  Position 1:                            64.43     
  Position 2:                            52.98     
  Position 3:                            44.15     
  Position 4:                            36.71     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:56 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:43:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.97, Accepted throughput: 22.10 tokens/s, Drafted throughput: 56.00 tokens/s, Accepted: 221 tokens, Drafted: 560 tokens, Per-position acceptance rate: 0.696, 0.491, 0.375, 0.241, 0.170, Avg Draft acceptance rate: 39.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f442b42efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e2344a9d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:06 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.68, Accepted throughput: 12.50 tokens/s, Drafted throughput: 17.00 tokens/s, Accepted: 125 tokens, Drafted: 170 tokens, Per-position acceptance rate: 0.941, 0.794, 0.676, 0.647, 0.618, Avg Draft acceptance rate: 73.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:16 [loggers.py:257] Engine 000: Avg prompt throughput: 607.7 tokens/s, Avg generation throughput: 1837.9 tokens/s, Running: 23 reqs, Waiting: 0 reqs, GPU KV cache usage: 30.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.81, Accepted throughput: 1357.54 tokens/s, Drafted throughput: 2417.21 tokens/s, Accepted: 13577 tokens, Drafted: 24175 tokens, Per-position acceptance rate: 0.800, 0.644, 0.537, 0.449, 0.379, Avg Draft acceptance rate: 56.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  12.56     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              6.37      
Output token throughput (tok/s):         1626.59   
Peak output token throughput (tok/s):    704.00    
Peak concurrent requests:                75.00     
Total token throughput (tok/s):          2110.39   
---------------Time to First Token----------------
Mean TTFT (ms):                          212.11    
Median TTFT (ms):                        203.26    
P99 TTFT (ms):                           337.69    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.53     
Median TPOT (ms):                        24.69     
P99 TPOT (ms):                           33.43     
---------------Inter-token Latency----------------
Mean ITL (ms):                           91.96     
Median ITL (ms):                         90.94     
P99 ITL (ms):                            146.32    
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.46     
Acceptance length:                       3.77      
Drafts:                                  5431      
Draft tokens:                            27155     
Accepted tokens:                         15060     
Per-position acceptance (%):
  Position 0:                            79.40     
  Position 1:                            63.60     
  Position 2:                            52.84     
  Position 3:                            44.25     
  Position 4:                            37.21     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 156.19 tokens/s, Drafted throughput: 308.47 tokens/s, Accepted: 1562 tokens, Drafted: 3085 tokens, Per-position acceptance rate: 0.754, 0.580, 0.473, 0.400, 0.324, Avg Draft acceptance rate: 50.6%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc36f602fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15014, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0a3b526e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:36 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.42, Accepted throughput: 5.30 tokens/s, Drafted throughput: 6.00 tokens/s, Accepted: 53 tokens, Drafted: 60 tokens, Per-position acceptance rate: 1.000, 1.000, 0.833, 0.833, 0.750, Avg Draft acceptance rate: 88.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:46 [loggers.py:257] Engine 000: Avg prompt throughput: 607.8 tokens/s, Avg generation throughput: 1724.3 tokens/s, Running: 63 reqs, Waiting: 1 reqs, GPU KV cache usage: 97.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.95, Accepted throughput: 1282.93 tokens/s, Drafted throughput: 2176.55 tokens/s, Accepted: 12829 tokens, Drafted: 21765 tokens, Per-position acceptance rate: 0.820, 0.673, 0.566, 0.481, 0.409, Avg Draft acceptance rate: 58.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  10.98     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              7.29      
Output token throughput (tok/s):         1865.89   
Peak output token throughput (tok/s):    800.00    
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2419.64   
---------------Time to First Token----------------
Mean TTFT (ms):                          270.93    
Median TTFT (ms):                        265.74    
P99 TTFT (ms):                           482.89    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          28.85     
Median TPOT (ms):                        28.64     
P99 TPOT (ms):                           36.63     
---------------Inter-token Latency----------------
Mean ITL (ms):                           108.15    
Median ITL (ms):                         99.37     
P99 ITL (ms):                            278.66    
---------------Speculative Decoding---------------
Acceptance rate (%):                     55.64     
Acceptance length:                       3.78      
Drafts:                                  5427      
Draft tokens:                            27135     
Accepted tokens:                         15098     
Per-position acceptance (%):
  Position 0:                            79.92     
  Position 1:                            64.36     
  Position 2:                            52.96     
  Position 3:                            44.15     
  Position 4:                            36.82     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k5-t0.0-tp1...
[0;36m(APIServer pid=3701664)[0;0m INFO 01-23 12:44:50 [launcher.py:110] Shutting down FastAPI HTTP server.
