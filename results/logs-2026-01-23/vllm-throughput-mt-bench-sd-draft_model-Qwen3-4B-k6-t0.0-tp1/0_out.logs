Removing any existing container named vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k6-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k6-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 267046
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:06 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:06 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15017, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'draft_model', 'model': 'Qwen/Qwen3-4B', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=267046)[0;0m WARNING 01-23 20:25:06 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:07 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:07 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:08 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:08 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:08 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=267046)[0;0m WARNING 01-23 20:25:08 [vllm.py:589] Async scheduling not supported with draft_model-based speculative decoding and will be disabled.
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:25:08 [vllm.py:618] Asynchronous scheduling is disabled.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efc0e4befc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-70254d57-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 20:25:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:25:19 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='draft_model', model='Qwen/Qwen3-4B', num_spec_tokens=6), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [9216], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:25:20 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.68:55891 backend=nccl
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:25:20 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
WARNING 01-23 20:25:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m WARNING 01-23 20:25:21 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:25:21 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:25:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 20:25:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:25:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:14 [default_loader.py:291] Loading weights took 50.74 seconds
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:14 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:14 [vllm.py:618] Asynchronous scheduling is disabled.
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:14 [draft_model.py:165] Starting to load draft model Qwen/Qwen3-4B. TP=1, rank=0
WARNING 01-23 20:26:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:22 [default_loader.py:291] Loading weights took 6.64 seconds
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:23 [gpu_model_runner.py:3921] Model loading took 68.58 GiB memory and 60.384202 seconds
WARNING 01-23 20:26:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:35 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:35 [backends.py:704] Dynamo bytecode transform time: 12.57 s
WARNING 01-23 20:26:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:26:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:52 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 4.369 s
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:52 [monitor.py:34] torch.compile takes 16.94 s in total
WARNING 01-23 20:26:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:59 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/f24996e9cc/rank_0_0/draft_model for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:26:59 [backends.py:704] Dynamo bytecode transform time: 6.07 s
WARNING 01-23 20:27:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:27:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:07 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 9216) from the cache, took 2.121 s
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:07 [monitor.py:34] torch.compile takes 25.12 s in total
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:09 [gpu_worker.py:355] Available KV cache memory: 7.08 GiB
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:09 [kv_cache_utils.py:1307] GPU KV cache size: 18,560 tokens
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:09 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 3.71x
WARNING 01-23 20:27:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:27:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:27:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
WARNING 01-23 20:27:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:29 [gpu_model_runner.py:4880] Graph capturing finished in 18 secs, took 0.02 GiB
[0;36m(EngineCore_DP0 pid=267166)[0;0m INFO 01-23 20:27:29 [core.py:272] init engine (profile, create kv cache, warmup model) took 66.42 seconds
WARNING 01-23 20:27:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15017)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15017 ssl:default [Connect call failed (\'127.0.0.1\', 15017)]\n''
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:31 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=267046)[0;0m WARNING 01-23 20:27:31 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:31 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:31 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:31 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [serving.py:221] Chat template warmup completed in 1720.2ms
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15017
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:33 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:44 [loggers.py:257] Engine 000: Avg prompt throughput: 27.7 tokens/s, Avg generation throughput: 39.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.04, Accepted throughput: 31.65 tokens/s, Drafted throughput: 47.02 tokens/s, Accepted: 412 tokens, Drafted: 612 tokens, Per-position acceptance rate: 0.863, 0.765, 0.686, 0.627, 0.549, 0.549, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:54 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:27:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.53, Accepted throughput: 34.90 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 349 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.783, 0.601, 0.420, 0.319, 0.232, 0.174, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:04 [loggers.py:257] Engine 000: Avg prompt throughput: 39.1 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.24, Accepted throughput: 44.40 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 444 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.839, 0.693, 0.569, 0.460, 0.387, 0.292, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:14 [loggers.py:257] Engine 000: Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.09, Accepted throughput: 56.50 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 565 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.862, 0.797, 0.696, 0.630, 0.565, 0.543, Avg Draft acceptance rate: 68.2%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:24 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.67, Accepted throughput: 37.10 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 371 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.784, 0.532, 0.439, 0.353, 0.302, 0.259, Avg Draft acceptance rate: 44.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:34 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.75, Accepted throughput: 37.70 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 377 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.796, 0.613, 0.526, 0.380, 0.248, 0.190, Avg Draft acceptance rate: 45.9%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:44 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 53.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 40.30 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 403 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.763, 0.604, 0.475, 0.396, 0.345, 0.317, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:54 [loggers.py:257] Engine 000: Avg prompt throughput: 14.0 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:28:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 40.50 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 405 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.825, 0.679, 0.540, 0.387, 0.285, 0.241, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:04 [loggers.py:257] Engine 000: Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.19, Accepted throughput: 30.70 tokens/s, Drafted throughput: 84.00 tokens/s, Accepted: 307 tokens, Drafted: 840 tokens, Per-position acceptance rate: 0.707, 0.479, 0.371, 0.257, 0.214, 0.164, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:14 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.43, Accepted throughput: 33.60 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 336 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.681, 0.522, 0.377, 0.341, 0.297, 0.217, Avg Draft acceptance rate: 40.6%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:24 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 49.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.56, Accepted throughput: 35.60 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 356 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.755, 0.583, 0.468, 0.309, 0.252, 0.194, Avg Draft acceptance rate: 42.7%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:34 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.25, Accepted throughput: 44.80 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 448 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.812, 0.674, 0.580, 0.478, 0.370, 0.333, Avg Draft acceptance rate: 54.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:44 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.50, Accepted throughput: 47.90 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 479 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.796, 0.730, 0.606, 0.526, 0.460, 0.380, Avg Draft acceptance rate: 58.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:54 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:29:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.77, Accepted throughput: 38.20 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 382 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.746, 0.594, 0.500, 0.370, 0.304, 0.254, Avg Draft acceptance rate: 46.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:04 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 67.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.87, Accepted throughput: 53.80 tokens/s, Drafted throughput: 83.40 tokens/s, Accepted: 538 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.885, 0.784, 0.662, 0.583, 0.504, 0.453, Avg Draft acceptance rate: 64.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:14 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 65.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.81, Accepted throughput: 52.20 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 522 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.861, 0.752, 0.664, 0.555, 0.511, 0.467, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:24 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.31, Accepted throughput: 45.69 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 457 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.819, 0.681, 0.565, 0.471, 0.406, 0.370, Avg Draft acceptance rate: 55.2%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:34 [loggers.py:257] Engine 000: Avg prompt throughput: 30.5 tokens/s, Avg generation throughput: 62.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.57, Accepted throughput: 48.90 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 489 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.832, 0.701, 0.620, 0.533, 0.489, 0.394, Avg Draft acceptance rate: 59.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:44 [loggers.py:257] Engine 000: Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.64, Accepted throughput: 36.70 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 367 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.719, 0.597, 0.468, 0.360, 0.273, 0.223, Avg Draft acceptance rate: 44.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:54 [loggers.py:257] Engine 000: Avg prompt throughput: 24.3 tokens/s, Avg generation throughput: 62.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:30:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.56, Accepted throughput: 49.10 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 491 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.848, 0.703, 0.609, 0.514, 0.478, 0.406, Avg Draft acceptance rate: 59.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:04 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.05, Accepted throughput: 42.09 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 421 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.819, 0.674, 0.543, 0.420, 0.326, 0.268, Avg Draft acceptance rate: 50.8%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:14 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 40.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 27.10 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 271 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.691, 0.439, 0.317, 0.223, 0.165, 0.115, Avg Draft acceptance rate: 32.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:24 [loggers.py:257] Engine 000: Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.25, Accepted throughput: 31.00 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 310 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.710, 0.514, 0.384, 0.297, 0.188, 0.152, Avg Draft acceptance rate: 37.4%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:34 [loggers.py:257] Engine 000: Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.65, Accepted throughput: 50.69 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 507 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.856, 0.748, 0.640, 0.540, 0.489, 0.374, Avg Draft acceptance rate: 60.8%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:44 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 40.80 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 408 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.812, 0.638, 0.522, 0.435, 0.319, 0.232, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:54 [loggers.py:257] Engine 000: Avg prompt throughput: 36.6 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:31:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.04, Accepted throughput: 41.70 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 417 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.781, 0.628, 0.533, 0.460, 0.387, 0.255, Avg Draft acceptance rate: 50.7%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:04 [loggers.py:257] Engine 000: Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 49.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.54, Accepted throughput: 35.50 tokens/s, Drafted throughput: 83.99 tokens/s, Accepted: 355 tokens, Drafted: 840 tokens, Per-position acceptance rate: 0.764, 0.557, 0.407, 0.329, 0.264, 0.214, Avg Draft acceptance rate: 42.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:14 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.51, Accepted throughput: 34.60 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 346 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.732, 0.551, 0.457, 0.326, 0.268, 0.174, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:24 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.64, Accepted throughput: 49.90 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 499 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.847, 0.708, 0.628, 0.555, 0.467, 0.438, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:34 [loggers.py:257] Engine 000: Avg prompt throughput: 18.4 tokens/s, Avg generation throughput: 51.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.70, Accepted throughput: 37.30 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 373 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.775, 0.558, 0.457, 0.341, 0.319, 0.254, Avg Draft acceptance rate: 45.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:44 [loggers.py:257] Engine 000: Avg prompt throughput: 17.7 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.56, Accepted throughput: 49.50 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 495 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.899, 0.727, 0.583, 0.496, 0.468, 0.388, Avg Draft acceptance rate: 59.4%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:54 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 63.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:32:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.61, Accepted throughput: 49.39 tokens/s, Drafted throughput: 82.19 tokens/s, Accepted: 494 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.810, 0.708, 0.628, 0.540, 0.489, 0.431, Avg Draft acceptance rate: 60.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:04 [loggers.py:257] Engine 000: Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 48.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.49, Accepted throughput: 34.40 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 344 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.746, 0.543, 0.391, 0.326, 0.254, 0.232, Avg Draft acceptance rate: 41.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:14 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 41.10 tokens/s, Drafted throughput: 83.39 tokens/s, Accepted: 411 tokens, Drafted: 834 tokens, Per-position acceptance rate: 0.813, 0.647, 0.489, 0.396, 0.331, 0.281, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:24 [loggers.py:257] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.74, Accepted throughput: 51.60 tokens/s, Drafted throughput: 82.80 tokens/s, Accepted: 516 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.862, 0.739, 0.630, 0.551, 0.500, 0.457, Avg Draft acceptance rate: 62.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:34 [loggers.py:257] Engine 000: Avg prompt throughput: 37.9 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.39, Accepted throughput: 46.50 tokens/s, Drafted throughput: 82.20 tokens/s, Accepted: 465 tokens, Drafted: 822 tokens, Per-position acceptance rate: 0.818, 0.686, 0.591, 0.511, 0.431, 0.358, Avg Draft acceptance rate: 56.6%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:44 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 58.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.26, Accepted throughput: 45.00 tokens/s, Drafted throughput: 82.79 tokens/s, Accepted: 450 tokens, Drafted: 828 tokens, Per-position acceptance rate: 0.819, 0.688, 0.565, 0.457, 0.399, 0.333, Avg Draft acceptance rate: 54.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  367.60    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         55.71     
Peak output token throughput (tok/s):    15.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          72.25     
---------------Time to First Token----------------
Mean TTFT (ms):                          83.24     
Median TTFT (ms):                        81.62     
P99 TTFT (ms):                           97.16     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.65     
Median TPOT (ms):                        17.41     
P99 TPOT (ms):                           25.10     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.09     
Median ITL (ms):                         71.16     
P99 ITL (ms):                            71.74     
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.89     
Acceptance length:                       4.05      
Drafts:                                  5065      
Draft tokens:                            30390     
Accepted tokens:                         15464     
Per-position acceptance (%):
  Position 0:                            79.61     
  Position 1:                            64.07     
  Position 2:                            52.52     
  Position 3:                            42.82     
  Position 4:                            36.09     
  Position 5:                            30.21     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:33:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.09, Accepted throughput: 9.00 tokens/s, Drafted throughput: 25.80 tokens/s, Accepted: 90 tokens, Drafted: 258 tokens, Per-position acceptance rate: 0.698, 0.488, 0.326, 0.209, 0.186, 0.186, Avg Draft acceptance rate: 34.9%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f5234ed2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-58e90532-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:04 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.52, Accepted throughput: 10.40 tokens/s, Drafted throughput: 13.80 tokens/s, Accepted: 104 tokens, Drafted: 138 tokens, Per-position acceptance rate: 0.957, 0.870, 0.783, 0.696, 0.609, 0.609, Avg Draft acceptance rate: 75.4%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:14 [loggers.py:257] Engine 000: Avg prompt throughput: 34.7 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.12, Accepted throughput: 70.09 tokens/s, Drafted throughput: 134.98 tokens/s, Accepted: 701 tokens, Drafted: 1350 tokens, Per-position acceptance rate: 0.800, 0.636, 0.516, 0.458, 0.382, 0.324, Avg Draft acceptance rate: 51.9%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:24 [loggers.py:257] Engine 000: Avg prompt throughput: 46.0 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.54, Accepted throughput: 94.89 tokens/s, Drafted throughput: 160.78 tokens/s, Accepted: 949 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.832, 0.713, 0.616, 0.522, 0.455, 0.403, Avg Draft acceptance rate: 59.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:34 [loggers.py:257] Engine 000: Avg prompt throughput: 52.1 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.68, Accepted throughput: 73.49 tokens/s, Drafted throughput: 164.38 tokens/s, Accepted: 735 tokens, Drafted: 1644 tokens, Per-position acceptance rate: 0.770, 0.577, 0.485, 0.369, 0.266, 0.215, Avg Draft acceptance rate: 44.7%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:44 [loggers.py:257] Engine 000: Avg prompt throughput: 23.5 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.10, Accepted throughput: 83.09 tokens/s, Drafted throughput: 160.78 tokens/s, Accepted: 831 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.806, 0.653, 0.537, 0.455, 0.340, 0.310, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:54 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 92.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:34:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.38, Accepted throughput: 65.10 tokens/s, Drafted throughput: 164.39 tokens/s, Accepted: 651 tokens, Drafted: 1644 tokens, Per-position acceptance rate: 0.734, 0.515, 0.420, 0.303, 0.226, 0.179, Avg Draft acceptance rate: 39.6%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:04 [loggers.py:257] Engine 000: Avg prompt throughput: 32.3 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.79, Accepted throughput: 75.19 tokens/s, Drafted throughput: 161.98 tokens/s, Accepted: 752 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.744, 0.593, 0.481, 0.381, 0.319, 0.267, Avg Draft acceptance rate: 46.4%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:14 [loggers.py:257] Engine 000: Avg prompt throughput: 45.4 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.13, Accepted throughput: 84.60 tokens/s, Drafted throughput: 162.00 tokens/s, Accepted: 846 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.793, 0.663, 0.552, 0.448, 0.370, 0.307, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:24 [loggers.py:257] Engine 000: Avg prompt throughput: 28.4 tokens/s, Avg generation throughput: 132.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.96, Accepted throughput: 106.79 tokens/s, Drafted throughput: 161.98 tokens/s, Accepted: 1068 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.896, 0.774, 0.670, 0.607, 0.522, 0.485, Avg Draft acceptance rate: 65.9%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:34 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 114.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.27, Accepted throughput: 87.60 tokens/s, Drafted throughput: 160.80 tokens/s, Accepted: 876 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.813, 0.664, 0.578, 0.481, 0.407, 0.325, Avg Draft acceptance rate: 54.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:44 [loggers.py:257] Engine 000: Avg prompt throughput: 34.4 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.90, Accepted throughput: 78.79 tokens/s, Drafted throughput: 163.17 tokens/s, Accepted: 788 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.765, 0.614, 0.511, 0.393, 0.331, 0.283, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:54 [loggers.py:257] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:35:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.86, Accepted throughput: 77.79 tokens/s, Drafted throughput: 163.18 tokens/s, Accepted: 778 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.772, 0.596, 0.485, 0.404, 0.335, 0.268, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:04 [loggers.py:257] Engine 000: Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.60, Accepted throughput: 70.69 tokens/s, Drafted throughput: 163.18 tokens/s, Accepted: 707 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.750, 0.577, 0.441, 0.331, 0.276, 0.224, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:14 [loggers.py:257] Engine 000: Avg prompt throughput: 32.9 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.03, Accepted throughput: 82.09 tokens/s, Drafted throughput: 162.59 tokens/s, Accepted: 821 tokens, Drafted: 1626 tokens, Per-position acceptance rate: 0.764, 0.635, 0.535, 0.461, 0.354, 0.280, Avg Draft acceptance rate: 50.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:24 [loggers.py:257] Engine 000: Avg prompt throughput: 24.5 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.83, Accepted throughput: 77.49 tokens/s, Drafted throughput: 164.38 tokens/s, Accepted: 775 tokens, Drafted: 1644 tokens, Per-position acceptance rate: 0.759, 0.613, 0.511, 0.391, 0.314, 0.241, Avg Draft acceptance rate: 47.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:34 [loggers.py:257] Engine 000: Avg prompt throughput: 23.7 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.09, Accepted throughput: 83.50 tokens/s, Drafted throughput: 162.00 tokens/s, Accepted: 835 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.785, 0.630, 0.530, 0.452, 0.374, 0.322, Avg Draft acceptance rate: 51.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:44 [loggers.py:257] Engine 000: Avg prompt throughput: 42.4 tokens/s, Avg generation throughput: 119.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.47, Accepted throughput: 92.89 tokens/s, Drafted throughput: 160.78 tokens/s, Accepted: 929 tokens, Drafted: 1608 tokens, Per-position acceptance rate: 0.851, 0.705, 0.575, 0.504, 0.444, 0.388, Avg Draft acceptance rate: 57.8%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:54 [loggers.py:257] Engine 000: Avg prompt throughput: 12.8 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:36:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 82.09 tokens/s, Drafted throughput: 163.18 tokens/s, Accepted: 821 tokens, Drafted: 1632 tokens, Per-position acceptance rate: 0.809, 0.614, 0.515, 0.423, 0.357, 0.301, Avg Draft acceptance rate: 50.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:04 [loggers.py:257] Engine 000: Avg prompt throughput: 68.7 tokens/s, Avg generation throughput: 113.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.23, Accepted throughput: 87.19 tokens/s, Drafted throughput: 161.99 tokens/s, Accepted: 872 tokens, Drafted: 1620 tokens, Per-position acceptance rate: 0.815, 0.693, 0.556, 0.459, 0.385, 0.322, Avg Draft acceptance rate: 53.8%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:14 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.40, Accepted throughput: 82.00 tokens/s, Drafted throughput: 144.60 tokens/s, Accepted: 820 tokens, Drafted: 1446 tokens, Per-position acceptance rate: 0.826, 0.701, 0.598, 0.502, 0.427, 0.349, Avg Draft acceptance rate: 56.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  190.25    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.42      
Output token throughput (tok/s):         107.65    
Peak output token throughput (tok/s):    30.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          139.59    
---------------Time to First Token----------------
Mean TTFT (ms):                          144.15    
Median TTFT (ms):                        144.49    
P99 TTFT (ms):                           164.51    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.80     
Median TPOT (ms):                        17.83     
P99 TPOT (ms):                           24.53     
---------------Inter-token Latency----------------
Mean ITL (ms):                           71.58     
Median ITL (ms):                         71.53     
P99 ITL (ms):                            73.88     
---------------Speculative Decoding---------------
Acceptance rate (%):                     51.02     
Acceptance length:                       4.06      
Drafts:                                  5072      
Draft tokens:                            30432     
Accepted tokens:                         15527     
Per-position acceptance (%):
  Position 0:                            79.28     
  Position 1:                            63.88     
  Position 2:                            53.08     
  Position 3:                            43.67     
  Position 4:                            35.98     
  Position 5:                            30.24     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.39, Accepted throughput: 7.40 tokens/s, Drafted throughput: 18.60 tokens/s, Accepted: 74 tokens, Drafted: 186 tokens, Per-position acceptance rate: 0.677, 0.548, 0.452, 0.290, 0.226, 0.194, Avg Draft acceptance rate: 39.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f314184efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-739f7a83-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:34 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.26, Accepted throughput: 14.90 tokens/s, Drafted throughput: 21.00 tokens/s, Accepted: 149 tokens, Drafted: 210 tokens, Per-position acceptance rate: 0.914, 0.771, 0.714, 0.657, 0.600, 0.600, Avg Draft acceptance rate: 71.0%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:44 [loggers.py:257] Engine 000: Avg prompt throughput: 80.7 tokens/s, Avg generation throughput: 201.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.43, Accepted throughput: 155.69 tokens/s, Drafted throughput: 272.38 tokens/s, Accepted: 1557 tokens, Drafted: 2724 tokens, Per-position acceptance rate: 0.844, 0.703, 0.590, 0.504, 0.414, 0.374, Avg Draft acceptance rate: 57.2%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:54 [loggers.py:257] Engine 000: Avg prompt throughput: 78.6 tokens/s, Avg generation throughput: 217.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:37:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.10, Accepted throughput: 165.38 tokens/s, Drafted throughput: 320.36 tokens/s, Accepted: 1654 tokens, Drafted: 3204 tokens, Per-position acceptance rate: 0.815, 0.652, 0.549, 0.442, 0.341, 0.300, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:04 [loggers.py:257] Engine 000: Avg prompt throughput: 38.0 tokens/s, Avg generation throughput: 202.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.80, Accepted throughput: 150.38 tokens/s, Drafted throughput: 322.75 tokens/s, Accepted: 1504 tokens, Drafted: 3228 tokens, Per-position acceptance rate: 0.762, 0.600, 0.489, 0.377, 0.305, 0.262, Avg Draft acceptance rate: 46.6%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:14 [loggers.py:257] Engine 000: Avg prompt throughput: 77.2 tokens/s, Avg generation throughput: 233.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.48, Accepted throughput: 183.28 tokens/s, Drafted throughput: 316.17 tokens/s, Accepted: 1833 tokens, Drafted: 3162 tokens, Per-position acceptance rate: 0.835, 0.715, 0.596, 0.510, 0.438, 0.383, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:24 [loggers.py:257] Engine 000: Avg prompt throughput: 75.3 tokens/s, Avg generation throughput: 220.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.13, Accepted throughput: 167.18 tokens/s, Drafted throughput: 320.97 tokens/s, Accepted: 1672 tokens, Drafted: 3210 tokens, Per-position acceptance rate: 0.783, 0.654, 0.548, 0.454, 0.376, 0.310, Avg Draft acceptance rate: 52.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:34 [loggers.py:257] Engine 000: Avg prompt throughput: 23.9 tokens/s, Avg generation throughput: 195.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 141.60 tokens/s, Drafted throughput: 322.80 tokens/s, Accepted: 1416 tokens, Drafted: 3228 tokens, Per-position acceptance rate: 0.766, 0.556, 0.448, 0.351, 0.284, 0.227, Avg Draft acceptance rate: 43.9%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:44 [loggers.py:257] Engine 000: Avg prompt throughput: 60.0 tokens/s, Avg generation throughput: 213.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 160.58 tokens/s, Drafted throughput: 318.57 tokens/s, Accepted: 1606 tokens, Drafted: 3186 tokens, Per-position acceptance rate: 0.778, 0.629, 0.537, 0.446, 0.367, 0.267, Avg Draft acceptance rate: 50.4%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:54 [loggers.py:257] Engine 000: Avg prompt throughput: 65.7 tokens/s, Avg generation throughput: 226.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:38:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.24, Accepted throughput: 172.78 tokens/s, Drafted throughput: 319.76 tokens/s, Accepted: 1728 tokens, Drafted: 3198 tokens, Per-position acceptance rate: 0.814, 0.664, 0.553, 0.475, 0.392, 0.343, Avg Draft acceptance rate: 54.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:04 [loggers.py:257] Engine 000: Avg prompt throughput: 81.5 tokens/s, Avg generation throughput: 220.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.12, Accepted throughput: 167.50 tokens/s, Drafted throughput: 321.59 tokens/s, Accepted: 1675 tokens, Drafted: 3216 tokens, Per-position acceptance rate: 0.804, 0.632, 0.534, 0.438, 0.379, 0.338, Avg Draft acceptance rate: 52.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  98.12     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.82      
Output token throughput (tok/s):         208.72    
Peak output token throughput (tok/s):    56.00     
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          270.67    
---------------Time to First Token----------------
Mean TTFT (ms):                          145.91    
Median TTFT (ms):                        147.11    
P99 TTFT (ms):                           160.48    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.90     
Median TPOT (ms):                        17.35     
P99 TPOT (ms):                           25.42     
---------------Inter-token Latency----------------
Mean ITL (ms):                           72.58     
Median ITL (ms):                         72.43     
P99 ITL (ms):                            76.87     
---------------Speculative Decoding---------------
Acceptance rate (%):                     51.46     
Acceptance length:                       4.09      
Drafts:                                  5031      
Draft tokens:                            30186     
Accepted tokens:                         15533     
Per-position acceptance (%):
  Position 0:                            79.75     
  Position 1:                            64.36     
  Position 2:                            53.43     
  Position 3:                            44.05     
  Position 4:                            36.37     
  Position 5:                            30.79     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:14 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 125.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.94, Accepted throughput: 94.50 tokens/s, Drafted throughput: 192.59 tokens/s, Accepted: 945 tokens, Drafted: 1926 tokens, Per-position acceptance rate: 0.769, 0.645, 0.498, 0.408, 0.346, 0.277, Avg Draft acceptance rate: 49.1%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f6a963b2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-33a081f8-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:34 [loggers.py:257] Engine 000: Avg prompt throughput: 94.1 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.86, Accepted throughput: 40.35 tokens/s, Drafted throughput: 62.69 tokens/s, Accepted: 807 tokens, Drafted: 1254 tokens, Per-position acceptance rate: 0.885, 0.756, 0.660, 0.574, 0.498, 0.488, Avg Draft acceptance rate: 64.4%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:44 [loggers.py:257] Engine 000: Avg prompt throughput: 98.2 tokens/s, Avg generation throughput: 410.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 308.05 tokens/s, Drafted throughput: 625.09 tokens/s, Accepted: 3081 tokens, Drafted: 6252 tokens, Per-position acceptance rate: 0.788, 0.620, 0.519, 0.415, 0.332, 0.283, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:54 [loggers.py:257] Engine 000: Avg prompt throughput: 146.8 tokens/s, Avg generation throughput: 429.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:39:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.13, Accepted throughput: 327.14 tokens/s, Drafted throughput: 626.29 tokens/s, Accepted: 3272 tokens, Drafted: 6264 tokens, Per-position acceptance rate: 0.784, 0.649, 0.549, 0.453, 0.378, 0.321, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:04 [loggers.py:257] Engine 000: Avg prompt throughput: 91.8 tokens/s, Avg generation throughput: 405.2 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.86, Accepted throughput: 300.67 tokens/s, Drafted throughput: 631.14 tokens/s, Accepted: 3007 tokens, Drafted: 6312 tokens, Per-position acceptance rate: 0.767, 0.610, 0.496, 0.396, 0.332, 0.257, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:14 [loggers.py:257] Engine 000: Avg prompt throughput: 99.9 tokens/s, Avg generation throughput: 425.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.09, Accepted throughput: 322.26 tokens/s, Drafted throughput: 626.32 tokens/s, Accepted: 3223 tokens, Drafted: 6264 tokens, Per-position acceptance rate: 0.783, 0.647, 0.534, 0.441, 0.370, 0.314, Avg Draft acceptance rate: 51.5%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:24 [loggers.py:257] Engine 000: Avg prompt throughput: 95.0 tokens/s, Avg generation throughput: 300.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.97, Accepted throughput: 226.47 tokens/s, Drafted throughput: 457.15 tokens/s, Accepted: 2265 tokens, Drafted: 4572 tokens, Per-position acceptance rate: 0.787, 0.617, 0.500, 0.427, 0.346, 0.295, Avg Draft acceptance rate: 49.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  52.26     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.53      
Output token throughput (tok/s):         391.92    
Peak output token throughput (tok/s):    112.00    
Peak concurrent requests:                12.00     
Total token throughput (tok/s):          508.23    
---------------Time to First Token----------------
Mean TTFT (ms):                          149.57    
Median TTFT (ms):                        149.29    
P99 TTFT (ms):                           203.10    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.57     
Median TPOT (ms):                        18.18     
P99 TPOT (ms):                           26.25     
---------------Inter-token Latency----------------
Mean ITL (ms):                           74.22     
Median ITL (ms):                         73.83     
P99 ITL (ms):                            80.70     
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.46     
Acceptance length:                       4.03      
Drafts:                                  5104      
Draft tokens:                            30624     
Accepted tokens:                         15452     
Per-position acceptance (%):
  Position 0:                            78.47     
  Position 1:                            63.30     
  Position 2:                            52.47     
  Position 3:                            43.01     
  Position 4:                            35.58     
  Position 5:                            29.92     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 0.30 tokens/s, Drafted throughput: 1.20 tokens/s, Accepted: 3 tokens, Drafted: 12 tokens, Per-position acceptance rate: 0.500, 0.500, 0.500, 0.000, 0.000, 0.000, Avg Draft acceptance rate: 25.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f12dc55afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-32c35099-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:44 [loggers.py:257] Engine 000: Avg prompt throughput: 171.4 tokens/s, Avg generation throughput: 77.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 5.02, Accepted throughput: 61.10 tokens/s, Drafted throughput: 91.20 tokens/s, Accepted: 611 tokens, Drafted: 912 tokens, Per-position acceptance rate: 0.901, 0.743, 0.678, 0.599, 0.553, 0.546, Avg Draft acceptance rate: 67.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:54 [loggers.py:257] Engine 000: Avg prompt throughput: 157.6 tokens/s, Avg generation throughput: 787.9 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:40:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.02, Accepted throughput: 594.40 tokens/s, Drafted throughput: 1180.61 tokens/s, Accepted: 5945 tokens, Drafted: 11808 tokens, Per-position acceptance rate: 0.790, 0.633, 0.523, 0.424, 0.350, 0.301, Avg Draft acceptance rate: 50.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:04 [loggers.py:257] Engine 000: Avg prompt throughput: 192.5 tokens/s, Avg generation throughput: 791.2 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.00, Accepted throughput: 596.06 tokens/s, Drafted throughput: 1192.71 tokens/s, Accepted: 5961 tokens, Drafted: 11928 tokens, Per-position acceptance rate: 0.781, 0.631, 0.522, 0.427, 0.354, 0.284, Avg Draft acceptance rate: 50.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  29.33     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              2.73      
Output token throughput (tok/s):         698.27    
Peak output token throughput (tok/s):    208.00    
Peak concurrent requests:                23.00     
Total token throughput (tok/s):          905.50    
---------------Time to First Token----------------
Mean TTFT (ms):                          154.75    
Median TTFT (ms):                        157.33    
P99 TTFT (ms):                           187.71    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.50     
Median TPOT (ms):                        19.01     
P99 TPOT (ms):                           27.86     
---------------Inter-token Latency----------------
Mean ITL (ms):                           78.36     
Median ITL (ms):                         77.67     
P99 ITL (ms):                            100.83    
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.87     
Acceptance length:                       4.05      
Drafts:                                  5077      
Draft tokens:                            30462     
Accepted tokens:                         15495     
Per-position acceptance (%):
  Position 0:                            79.02     
  Position 1:                            63.60     
  Position 2:                            52.91     
  Position 3:                            43.19     
  Position 4:                            36.04     
  Position 5:                            30.43     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:14 [loggers.py:257] Engine 000: Avg prompt throughput: 104.3 tokens/s, Avg generation throughput: 416.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.12, Accepted throughput: 318.38 tokens/s, Drafted throughput: 611.96 tokens/s, Accepted: 3184 tokens, Drafted: 6120 tokens, Per-position acceptance rate: 0.797, 0.642, 0.539, 0.443, 0.374, 0.326, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2fc4ed6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-42fc6f97-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:34 [loggers.py:257] Engine 000: Avg prompt throughput: 275.9 tokens/s, Avg generation throughput: 484.5 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.41, Accepted throughput: 186.13 tokens/s, Drafted throughput: 327.27 tokens/s, Accepted: 3723 tokens, Drafted: 6546 tokens, Per-position acceptance rate: 0.823, 0.700, 0.594, 0.497, 0.420, 0.379, Avg Draft acceptance rate: 56.9%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:44 [loggers.py:257] Engine 000: Avg prompt throughput: 349.9 tokens/s, Avg generation throughput: 1389.9 tokens/s, Running: 21 reqs, Waiting: 0 reqs, GPU KV cache usage: 29.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.96, Accepted throughput: 1041.41 tokens/s, Drafted throughput: 2114.22 tokens/s, Accepted: 10415 tokens, Drafted: 21144 tokens, Per-position acceptance rate: 0.774, 0.617, 0.509, 0.421, 0.347, 0.287, Avg Draft acceptance rate: 49.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  19.06     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.20      
Output token throughput (tok/s):         1074.68   
Peak output token throughput (tok/s):    384.00    
Peak concurrent requests:                43.00     
Total token throughput (tok/s):          1393.61   
---------------Time to First Token----------------
Mean TTFT (ms):                          169.23    
Median TTFT (ms):                        171.83    
P99 TTFT (ms):                           234.98    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.56     
Median TPOT (ms):                        21.02     
P99 TPOT (ms):                           31.71     
---------------Inter-token Latency----------------
Mean ITL (ms):                           85.84     
Median ITL (ms):                         84.24     
P99 ITL (ms):                            139.32    
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.11     
Acceptance length:                       4.01      
Drafts:                                  5123      
Draft tokens:                            30738     
Accepted tokens:                         15402     
Per-position acceptance (%):
  Position 0:                            78.12     
  Position 1:                            62.78     
  Position 2:                            51.69     
  Position 3:                            42.83     
  Position 4:                            35.37     
  Position 5:                            29.87     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:41:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.63, Accepted throughput: 147.00 tokens/s, Drafted throughput: 335.40 tokens/s, Accepted: 1470 tokens, Drafted: 3354 tokens, Per-position acceptance rate: 0.751, 0.569, 0.433, 0.356, 0.284, 0.236, Avg Draft acceptance rate: 43.8%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe8cae0efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2ffd6d42-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:14 [loggers.py:257] Engine 000: Avg prompt throughput: 505.8 tokens/s, Avg generation throughput: 1267.7 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 89.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.23, Accepted throughput: 482.27 tokens/s, Drafted throughput: 894.85 tokens/s, Accepted: 9646 tokens, Drafted: 17898 tokens, Per-position acceptance rate: 0.804, 0.662, 0.562, 0.472, 0.396, 0.338, Avg Draft acceptance rate: 53.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  13.73     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              5.83      
Output token throughput (tok/s):         1491.31   
Peak output token throughput (tok/s):    640.00    
Peak concurrent requests:                74.00     
Total token throughput (tok/s):          1933.90   
---------------Time to First Token----------------
Mean TTFT (ms):                          230.82    
Median TTFT (ms):                        230.03    
P99 TTFT (ms):                           407.18    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          26.47     
Median TPOT (ms):                        26.34     
P99 TPOT (ms):                           37.31     
---------------Inter-token Latency----------------
Mean ITL (ms):                           105.80    
Median ITL (ms):                         106.20    
P99 ITL (ms):                            194.83    
---------------Speculative Decoding---------------
Acceptance rate (%):                     50.57     
Acceptance length:                       4.03      
Drafts:                                  5104      
Draft tokens:                            30624     
Accepted tokens:                         15486     
Per-position acceptance (%):
  Position 0:                            78.90     
  Position 1:                            63.30     
  Position 2:                            52.21     
  Position 3:                            43.14     
  Position 4:                            35.64     
  Position 5:                            30.21     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:24 [loggers.py:257] Engine 000: Avg prompt throughput: 120.0 tokens/s, Avg generation throughput: 805.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.78, Accepted throughput: 604.52 tokens/s, Drafted throughput: 1303.03 tokens/s, Accepted: 6046 tokens, Drafted: 13032 tokens, Per-position acceptance rate: 0.770, 0.596, 0.472, 0.380, 0.307, 0.258, Avg Draft acceptance rate: 46.4%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9af7f4efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15017, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8b68effd-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:44 [loggers.py:257] Engine 000: Avg prompt throughput: 625.7 tokens/s, Avg generation throughput: 836.5 tokens/s, Running: 80 reqs, Waiting: 0 reqs, GPU KV cache usage: 83.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 4.62, Accepted throughput: 324.61 tokens/s, Drafted throughput: 538.13 tokens/s, Accepted: 6493 tokens, Drafted: 10764 tokens, Per-position acceptance rate: 0.838, 0.722, 0.640, 0.537, 0.469, 0.411, Avg Draft acceptance rate: 60.3%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1234.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.66, Accepted throughput: 909.11 tokens/s, Drafted throughput: 2047.82 tokens/s, Accepted: 9091 tokens, Drafted: 20478 tokens, Per-position acceptance rate: 0.749, 0.574, 0.452, 0.363, 0.292, 0.234, Avg Draft acceptance rate: 44.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  14.44     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              5.54      
Output token throughput (tok/s):         1417.84   
Peak output token throughput (tok/s):    720.00    
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          1838.62   
---------------Time to First Token----------------
Mean TTFT (ms):                          268.87    
Median TTFT (ms):                        250.99    
P99 TTFT (ms):                           465.86    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          31.56     
Median TPOT (ms):                        31.67     
P99 TPOT (ms):                           43.95     
---------------Inter-token Latency----------------
Mean ITL (ms):                           124.32    
Median ITL (ms):                         118.95    
P99 ITL (ms):                            252.94    
---------------Speculative Decoding---------------
Acceptance rate (%):                     49.66     
Acceptance length:                       3.98      
Drafts:                                  5167      
Draft tokens:                            31002     
Accepted tokens:                         15397     
Per-position acceptance (%):
  Position 0:                            77.88     
  Position 1:                            62.36     
  Position 2:                            51.50     
  Position 3:                            42.02     
  Position 4:                            35.07     
  Position 5:                            29.17     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-draft_model-Qwen3-4B-k6-t0.0-tp1...
[0;36m(APIServer pid=267046)[0;0m INFO 01-23 20:42:55 [launcher.py:110] Shutting down FastAPI HTTP server.
