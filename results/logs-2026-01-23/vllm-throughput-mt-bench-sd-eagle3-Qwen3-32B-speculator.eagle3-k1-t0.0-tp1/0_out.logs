Removing any existing container named vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k1-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k1-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2764618
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:47 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:47 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15020, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 1, 'max_model_len': 5000}}
[0;36m(APIServer pid=2764618)[0;0m WARNING 01-23 12:31:47 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:48 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:48 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:49 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:49 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:49 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:49 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:31:49 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f040fd4afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e274b83c-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:31:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:31:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:00 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=1), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:32:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:01 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:50407 backend=nccl
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:01 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2764900)[0;0m WARNING 01-23 12:32:02 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:02 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:03 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:32:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:32:46 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:46 [default_loader.py:291] Loading weights took 41.98 seconds
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:46 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:47 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 12:32:51 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:53 [default_loader.py:291] Loading weights took 6.31 seconds
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:55 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
WARNING 01-23 12:32:56 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:57 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:32:57 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 54.175907 seconds
WARNING 01-23 12:33:01 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:33:06 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:10 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:10 [backends.py:704] Dynamo bytecode transform time: 11.88 s
WARNING 01-23 12:33:11 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:33:16 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:33:21 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:33:26 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:27 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.599 s
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:27 [monitor.py:34] torch.compile takes 16.47 s in total
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:28 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:28 [backends.py:704] Dynamo bytecode transform time: 0.49 s
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:28 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.116 s
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:28 [monitor.py:34] torch.compile takes 17.08 s in total
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:29 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:29 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:29 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 12:33:31 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:33:36 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
WARNING 01-23 12:33:41 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15020)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15020 ssl:default [Connect call failed (\'127.0.0.1\', 15020)]\n''
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:41 [gpu_model_runner.py:4880] Graph capturing finished in 12 secs, took -0.61 GiB
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:41 [core.py:272] init engine (profile, create kv cache, warmup model) took 43.87 seconds
[0;36m(EngineCore_DP0 pid=2764900)[0;0m INFO 01-23 12:33:43 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:43 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2764618)[0;0m WARNING 01-23 12:33:43 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:43 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:43 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:43 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [serving.py:221] Chat template warmup completed in 1710.9ms
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15020
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:45 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:56 [loggers.py:257] Engine 000: Avg prompt throughput: 27.8 tokens/s, Avg generation throughput: 32.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:33:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 13.84 tokens/s, Drafted throughput: 18.55 tokens/s, Accepted: 179 tokens, Drafted: 240 tokens, Per-position acceptance rate: 0.746, Avg Draft acceptance rate: 74.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 17.90 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 179 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.646, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:16 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 43.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.59, Accepted throughput: 16.10 tokens/s, Drafted throughput: 27.50 tokens/s, Accepted: 161 tokens, Drafted: 275 tokens, Per-position acceptance rate: 0.585, Avg Draft acceptance rate: 58.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:26 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 19.90 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 199 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.718, Avg Draft acceptance rate: 71.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 19.50 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 195 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.704, Avg Draft acceptance rate: 70.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:46 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.80 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 178 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.645, Avg Draft acceptance rate: 64.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:56 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 47.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:34:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.20 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 192 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.693, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 46.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.20 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 192 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.693, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:16 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 45.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 17.20 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 172 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.621, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:26 [loggers.py:257] Engine 000: Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 46.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 18.60 tokens/s, Drafted throughput: 27.90 tokens/s, Accepted: 186 tokens, Drafted: 279 tokens, Per-position acceptance rate: 0.667, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:36 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 44.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.60, Accepted throughput: 16.60 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 166 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.597, Avg Draft acceptance rate: 59.7%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:46 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 45.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 17.50 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 175 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.632, Avg Draft acceptance rate: 63.2%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:56 [loggers.py:257] Engine 000: Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:35:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.60 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 176 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.635, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:06 [loggers.py:257] Engine 000: Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 46.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 18.80 tokens/s, Drafted throughput: 27.90 tokens/s, Accepted: 188 tokens, Drafted: 279 tokens, Per-position acceptance rate: 0.674, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:16 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 48.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 20.70 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 207 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.745, Avg Draft acceptance rate: 74.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:26 [loggers.py:257] Engine 000: Avg prompt throughput: 38.1 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.58, Accepted throughput: 15.90 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 159 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.576, Avg Draft acceptance rate: 57.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 20.00 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 200 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.722, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:46 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 18.70 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 187 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.675, Avg Draft acceptance rate: 67.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:56 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:36:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.77, Accepted throughput: 21.20 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 212 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.768, Avg Draft acceptance rate: 76.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:06 [loggers.py:257] Engine 000: Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 18.30 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 183 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.658, Avg Draft acceptance rate: 65.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:16 [loggers.py:257] Engine 000: Avg prompt throughput: 26.7 tokens/s, Avg generation throughput: 46.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 18.60 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 186 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.674, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:26 [loggers.py:257] Engine 000: Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 20.00 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 200 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.722, Avg Draft acceptance rate: 72.2%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:36 [loggers.py:257] Engine 000: Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 45.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 18.00 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 180 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.650, Avg Draft acceptance rate: 65.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:46 [loggers.py:257] Engine 000: Avg prompt throughput: 24.3 tokens/s, Avg generation throughput: 48.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 21.00 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 210 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.758, Avg Draft acceptance rate: 75.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:56 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 48.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:37:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 21.00 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 210 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.761, Avg Draft acceptance rate: 76.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:06 [loggers.py:257] Engine 000: Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.56, Accepted throughput: 15.60 tokens/s, Drafted throughput: 27.90 tokens/s, Accepted: 156 tokens, Drafted: 279 tokens, Per-position acceptance rate: 0.559, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:16 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 18.00 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 180 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.652, Avg Draft acceptance rate: 65.2%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:26 [loggers.py:257] Engine 000: Avg prompt throughput: 6.9 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 19.80 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 198 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.712, Avg Draft acceptance rate: 71.2%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:36 [loggers.py:257] Engine 000: Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 45.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 18.10 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 181 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.653, Avg Draft acceptance rate: 65.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:46 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 19.80 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 198 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.715, Avg Draft acceptance rate: 71.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:56 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:38:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.70 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 177 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.639, Avg Draft acceptance rate: 63.9%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:06 [loggers.py:257] Engine 000: Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 18.30 tokens/s, Drafted throughput: 27.90 tokens/s, Accepted: 183 tokens, Drafted: 279 tokens, Per-position acceptance rate: 0.656, Avg Draft acceptance rate: 65.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:16 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 47.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.20 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 192 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.693, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:26 [loggers.py:257] Engine 000: Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.64, Accepted throughput: 17.60 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 176 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.635, Avg Draft acceptance rate: 63.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:36 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 47.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 19.90 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 199 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.716, Avg Draft acceptance rate: 71.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:46 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 44.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.61, Accepted throughput: 17.00 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 170 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.614, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:56 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:39:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 18.80 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 188 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.676, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 19.60 tokens/s, Drafted throughput: 27.80 tokens/s, Accepted: 196 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.705, Avg Draft acceptance rate: 70.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:16 [loggers.py:257] Engine 000: Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 46.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 19.10 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 191 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.690, Avg Draft acceptance rate: 69.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:26 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 46.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 18.80 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 188 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.679, Avg Draft acceptance rate: 67.9%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:36 [loggers.py:257] Engine 000: Avg prompt throughput: 37.8 tokens/s, Avg generation throughput: 44.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 17.20 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 172 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.621, Avg Draft acceptance rate: 62.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:46 [loggers.py:257] Engine 000: Avg prompt throughput: 30.9 tokens/s, Avg generation throughput: 49.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.80, Accepted throughput: 22.10 tokens/s, Drafted throughput: 27.60 tokens/s, Accepted: 221 tokens, Drafted: 276 tokens, Per-position acceptance rate: 0.801, Avg Draft acceptance rate: 80.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:56 [loggers.py:257] Engine 000: Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:40:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 20.50 tokens/s, Drafted throughput: 27.90 tokens/s, Accepted: 205 tokens, Drafted: 279 tokens, Per-position acceptance rate: 0.735, Avg Draft acceptance rate: 73.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:06 [loggers.py:257] Engine 000: Avg prompt throughput: 24.4 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 19.60 tokens/s, Drafted throughput: 27.70 tokens/s, Accepted: 196 tokens, Drafted: 277 tokens, Per-position acceptance rate: 0.708, Avg Draft acceptance rate: 70.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  440.99    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.18      
Output token throughput (tok/s):         46.44     
Peak output token throughput (tok/s):    29.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          60.22     
---------------Time to First Token----------------
Mean TTFT (ms):                          57.07     
Median TTFT (ms):                        50.83     
P99 TTFT (ms):                           77.46     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.35     
Median TPOT (ms):                        21.43     
P99 TPOT (ms):                           23.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           35.71     
Median ITL (ms):                         35.72     
P99 ITL (ms):                            36.00     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.59     
Acceptance length:                       1.68      
Drafts:                                  12198     
Draft tokens:                            12198     
Accepted tokens:                         8245      
Per-position acceptance (%):
  Position 0:                            67.59     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:16 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 30.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 11.50 tokens/s, Drafted throughput: 18.60 tokens/s, Accepted: 115 tokens, Drafted: 186 tokens, Per-position acceptance rate: 0.618, Avg Draft acceptance rate: 61.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f3fba14afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-78d9d40e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:36 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 9.90 tokens/s, Drafted throughput: 13.90 tokens/s, Accepted: 198 tokens, Drafted: 278 tokens, Per-position acceptance rate: 0.712, Avg Draft acceptance rate: 71.2%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:46 [loggers.py:257] Engine 000: Avg prompt throughput: 45.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 34.10 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 341 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.627, Avg Draft acceptance rate: 62.7%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:56 [loggers.py:257] Engine 000: Avg prompt throughput: 13.9 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:41:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.72, Accepted throughput: 39.60 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 396 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.724, Avg Draft acceptance rate: 72.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:06 [loggers.py:257] Engine 000: Avg prompt throughput: 52.1 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 35.49 tokens/s, Drafted throughput: 54.49 tokens/s, Accepted: 355 tokens, Drafted: 545 tokens, Per-position acceptance rate: 0.651, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:16 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 35.40 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 354 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.651, Avg Draft acceptance rate: 65.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:26 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.63, Accepted throughput: 34.30 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 343 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.631, Avg Draft acceptance rate: 63.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 35.90 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 359 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.656, Avg Draft acceptance rate: 65.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:46 [loggers.py:257] Engine 000: Avg prompt throughput: 31.4 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 38.30 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 383 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.704, Avg Draft acceptance rate: 70.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:56 [loggers.py:257] Engine 000: Avg prompt throughput: 43.5 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:42:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 36.30 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 363 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.664, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:06 [loggers.py:257] Engine 000: Avg prompt throughput: 21.9 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 39.60 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 396 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.728, Avg Draft acceptance rate: 72.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:16 [loggers.py:257] Engine 000: Avg prompt throughput: 30.1 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 36.80 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 368 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.676, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:26 [loggers.py:257] Engine 000: Avg prompt throughput: 19.9 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 37.50 tokens/s, Drafted throughput: 54.49 tokens/s, Accepted: 375 tokens, Drafted: 545 tokens, Per-position acceptance rate: 0.688, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:36 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 39.89 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 399 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.733, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:46 [loggers.py:257] Engine 000: Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.62, Accepted throughput: 33.90 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 339 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.620, Avg Draft acceptance rate: 62.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:56 [loggers.py:257] Engine 000: Avg prompt throughput: 14.3 tokens/s, Avg generation throughput: 91.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:43:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 37.20 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 372 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.684, Avg Draft acceptance rate: 68.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:06 [loggers.py:257] Engine 000: Avg prompt throughput: 35.2 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 36.10 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 361 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.660, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:16 [loggers.py:257] Engine 000: Avg prompt throughput: 20.8 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 36.10 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 361 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.666, Avg Draft acceptance rate: 66.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:26 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 36.10 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 361 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.660, Avg Draft acceptance rate: 66.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:36 [loggers.py:257] Engine 000: Avg prompt throughput: 36.1 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 35.40 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 354 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.653, Avg Draft acceptance rate: 65.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:46 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 39.99 tokens/s, Drafted throughput: 54.59 tokens/s, Accepted: 400 tokens, Drafted: 546 tokens, Per-position acceptance rate: 0.733, Avg Draft acceptance rate: 73.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:56 [loggers.py:257] Engine 000: Avg prompt throughput: 44.3 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:44:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 35.49 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 355 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.653, Avg Draft acceptance rate: 65.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:06 [loggers.py:257] Engine 000: Avg prompt throughput: 33.1 tokens/s, Avg generation throughput: 93.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 38.70 tokens/s, Drafted throughput: 54.49 tokens/s, Accepted: 387 tokens, Drafted: 545 tokens, Per-position acceptance rate: 0.710, Avg Draft acceptance rate: 71.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:16 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.73, Accepted throughput: 39.70 tokens/s, Drafted throughput: 54.70 tokens/s, Accepted: 397 tokens, Drafted: 547 tokens, Per-position acceptance rate: 0.726, Avg Draft acceptance rate: 72.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  225.25    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.36      
Output token throughput (tok/s):         90.92     
Peak output token throughput (tok/s):    56.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          117.91    
---------------Time to First Token----------------
Mean TTFT (ms):                          109.76    
Median TTFT (ms):                        109.74    
P99 TTFT (ms):                           119.25    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.50     
Median TPOT (ms):                        21.59     
P99 TPOT (ms):                           23.37     
---------------Inter-token Latency----------------
Mean ITL (ms):                           35.98     
Median ITL (ms):                         35.95     
P99 ITL (ms):                            36.96     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.63     
Acceptance length:                       1.68      
Drafts:                                  12190     
Draft tokens:                            12190     
Accepted tokens:                         8244      
Per-position acceptance (%):
  Position 0:                            67.63     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.52, Accepted throughput: 3.60 tokens/s, Drafted throughput: 6.90 tokens/s, Accepted: 36 tokens, Drafted: 69 tokens, Per-position acceptance rate: 0.522, Avg Draft acceptance rate: 52.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe7c94bafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-be7623a9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:36 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 6.60 tokens/s, Drafted throughput: 8.90 tokens/s, Accepted: 66 tokens, Drafted: 89 tokens, Per-position acceptance rate: 0.742, Avg Draft acceptance rate: 74.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:46 [loggers.py:257] Engine 000: Avg prompt throughput: 76.1 tokens/s, Avg generation throughput: 141.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 57.00 tokens/s, Drafted throughput: 84.10 tokens/s, Accepted: 570 tokens, Drafted: 841 tokens, Per-position acceptance rate: 0.678, Avg Draft acceptance rate: 67.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:56 [loggers.py:257] Engine 000: Avg prompt throughput: 66.2 tokens/s, Avg generation throughput: 182.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:45:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 74.30 tokens/s, Drafted throughput: 107.60 tokens/s, Accepted: 743 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.691, Avg Draft acceptance rate: 69.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:06 [loggers.py:257] Engine 000: Avg prompt throughput: 22.7 tokens/s, Avg generation throughput: 180.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 70.49 tokens/s, Drafted throughput: 109.19 tokens/s, Accepted: 705 tokens, Drafted: 1092 tokens, Per-position acceptance rate: 0.646, Avg Draft acceptance rate: 64.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:16 [loggers.py:257] Engine 000: Avg prompt throughput: 74.8 tokens/s, Avg generation throughput: 181.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 73.79 tokens/s, Drafted throughput: 107.59 tokens/s, Accepted: 738 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.686, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:26 [loggers.py:257] Engine 000: Avg prompt throughput: 40.7 tokens/s, Avg generation throughput: 183.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.70, Accepted throughput: 75.09 tokens/s, Drafted throughput: 107.59 tokens/s, Accepted: 751 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.698, Avg Draft acceptance rate: 69.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:36 [loggers.py:257] Engine 000: Avg prompt throughput: 64.9 tokens/s, Avg generation throughput: 181.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 72.69 tokens/s, Drafted throughput: 107.89 tokens/s, Accepted: 727 tokens, Drafted: 1079 tokens, Per-position acceptance rate: 0.674, Avg Draft acceptance rate: 67.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:46 [loggers.py:257] Engine 000: Avg prompt throughput: 21.1 tokens/s, Avg generation throughput: 181.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 72.10 tokens/s, Drafted throughput: 108.59 tokens/s, Accepted: 721 tokens, Drafted: 1086 tokens, Per-position acceptance rate: 0.664, Avg Draft acceptance rate: 66.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:56 [loggers.py:257] Engine 000: Avg prompt throughput: 57.8 tokens/s, Avg generation throughput: 181.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:46:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 73.79 tokens/s, Drafted throughput: 107.59 tokens/s, Accepted: 738 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.686, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:06 [loggers.py:257] Engine 000: Avg prompt throughput: 45.8 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 72.19 tokens/s, Drafted throughput: 108.29 tokens/s, Accepted: 722 tokens, Drafted: 1083 tokens, Per-position acceptance rate: 0.667, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:16 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 181.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 73.98 tokens/s, Drafted throughput: 107.57 tokens/s, Accepted: 740 tokens, Drafted: 1076 tokens, Per-position acceptance rate: 0.688, Avg Draft acceptance rate: 68.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:26 [loggers.py:257] Engine 000: Avg prompt throughput: 92.3 tokens/s, Avg generation throughput: 183.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 74.99 tokens/s, Drafted throughput: 108.19 tokens/s, Accepted: 750 tokens, Drafted: 1082 tokens, Per-position acceptance rate: 0.693, Avg Draft acceptance rate: 69.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  114.98    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.70      
Output token throughput (tok/s):         178.11    
Peak output token throughput (tok/s):    112.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          230.97    
---------------Time to First Token----------------
Mean TTFT (ms):                          110.78    
Median TTFT (ms):                        110.75    
P99 TTFT (ms):                           120.73    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.67     
Median TPOT (ms):                        21.77     
P99 TPOT (ms):                           23.48     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.30     
Median ITL (ms):                         36.23     
P99 ITL (ms):                            43.38     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.84     
Acceptance length:                       1.68      
Drafts:                                  12175     
Draft tokens:                            12175     
Accepted tokens:                         8260      
Per-position acceptance (%):
  Position 0:                            67.84     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:36 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 39.80 tokens/s, Drafted throughput: 59.00 tokens/s, Accepted: 398 tokens, Drafted: 590 tokens, Per-position acceptance rate: 0.675, Avg Draft acceptance rate: 67.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:46 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f26c3acafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8f648df9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:56 [loggers.py:257] Engine 000: Avg prompt throughput: 94.0 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:47:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 21.04 tokens/s, Drafted throughput: 30.28 tokens/s, Accepted: 421 tokens, Drafted: 606 tokens, Per-position acceptance rate: 0.695, Avg Draft acceptance rate: 69.5%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:06 [loggers.py:257] Engine 000: Avg prompt throughput: 116.9 tokens/s, Avg generation throughput: 359.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 145.82 tokens/s, Drafted throughput: 212.69 tokens/s, Accepted: 1459 tokens, Drafted: 2128 tokens, Per-position acceptance rate: 0.686, Avg Draft acceptance rate: 68.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:16 [loggers.py:257] Engine 000: Avg prompt throughput: 73.3 tokens/s, Avg generation throughput: 355.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.65, Accepted throughput: 140.49 tokens/s, Drafted throughput: 214.89 tokens/s, Accepted: 1405 tokens, Drafted: 2149 tokens, Per-position acceptance rate: 0.654, Avg Draft acceptance rate: 65.4%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:26 [loggers.py:257] Engine 000: Avg prompt throughput: 97.1 tokens/s, Avg generation throughput: 357.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 143.47 tokens/s, Drafted throughput: 213.05 tokens/s, Accepted: 1435 tokens, Drafted: 2131 tokens, Per-position acceptance rate: 0.673, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:36 [loggers.py:257] Engine 000: Avg prompt throughput: 106.7 tokens/s, Avg generation throughput: 355.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 142.00 tokens/s, Drafted throughput: 212.80 tokens/s, Accepted: 1420 tokens, Drafted: 2128 tokens, Per-position acceptance rate: 0.667, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:46 [loggers.py:257] Engine 000: Avg prompt throughput: 132.2 tokens/s, Avg generation throughput: 360.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 145.98 tokens/s, Drafted throughput: 213.36 tokens/s, Accepted: 1460 tokens, Drafted: 2134 tokens, Per-position acceptance rate: 0.684, Avg Draft acceptance rate: 68.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  59.23     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.35      
Output token throughput (tok/s):         345.75    
Peak output token throughput (tok/s):    224.00    
Peak concurrent requests:                16.00     
Total token throughput (tok/s):          448.36    
---------------Time to First Token----------------
Mean TTFT (ms):                          112.85    
Median TTFT (ms):                        111.81    
P99 TTFT (ms):                           134.66    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.00     
Median TPOT (ms):                        22.03     
P99 TPOT (ms):                           24.08     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.74     
Median ITL (ms):                         36.60     
P99 ITL (ms):                            44.90     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.31     
Acceptance length:                       1.67      
Drafts:                                  12213     
Draft tokens:                            12213     
Accepted tokens:                         8221      
Per-position acceptance (%):
  Position 0:                            67.31     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:56 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 181.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:48:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 72.99 tokens/s, Drafted throughput: 108.38 tokens/s, Accepted: 730 tokens, Drafted: 1084 tokens, Per-position acceptance rate: 0.673, Avg Draft acceptance rate: 67.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe183066fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7ad8caf3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:16 [loggers.py:257] Engine 000: Avg prompt throughput: 171.4 tokens/s, Avg generation throughput: 214.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.69, Accepted throughput: 43.50 tokens/s, Drafted throughput: 62.79 tokens/s, Accepted: 870 tokens, Drafted: 1256 tokens, Per-position acceptance rate: 0.693, Avg Draft acceptance rate: 69.3%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:26 [loggers.py:257] Engine 000: Avg prompt throughput: 207.0 tokens/s, Avg generation throughput: 682.2 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 272.15 tokens/s, Drafted throughput: 408.23 tokens/s, Accepted: 2722 tokens, Drafted: 4083 tokens, Per-position acceptance rate: 0.667, Avg Draft acceptance rate: 66.7%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:36 [loggers.py:257] Engine 000: Avg prompt throughput: 119.8 tokens/s, Avg generation throughput: 692.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:36 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 276.68 tokens/s, Drafted throughput: 415.17 tokens/s, Accepted: 2767 tokens, Drafted: 4152 tokens, Per-position acceptance rate: 0.666, Avg Draft acceptance rate: 66.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  31.64     
Total input tokens:                      6078      
Total generated tokens:                  20439     
Request throughput (req/s):              2.53      
Output token throughput (tok/s):         646.06    
Peak output token throughput (tok/s):    432.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          838.18    
---------------Time to First Token----------------
Mean TTFT (ms):                          118.22    
Median TTFT (ms):                        115.79    
P99 TTFT (ms):                           189.55    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.85     
Median TPOT (ms):                        22.87     
P99 TPOT (ms):                           25.07     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.11     
Median ITL (ms):                         37.83     
P99 ITL (ms):                            48.28     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.04     
Acceptance length:                       1.67      
Drafts:                                  12210     
Draft tokens:                            12210     
Accepted tokens:                         8186      
Per-position acceptance (%):
  Position 0:                            67.04     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:46 [loggers.py:257] Engine 000: Avg prompt throughput: 127.6 tokens/s, Avg generation throughput: 480.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 193.59 tokens/s, Drafted throughput: 286.58 tokens/s, Accepted: 1936 tokens, Drafted: 2866 tokens, Per-position acceptance rate: 0.676, Avg Draft acceptance rate: 67.6%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:49:56 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f33bc87afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8e7fc052-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:06 [loggers.py:257] Engine 000: Avg prompt throughput: 275.8 tokens/s, Avg generation throughput: 131.0 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 27.34 tokens/s, Drafted throughput: 36.54 tokens/s, Accepted: 547 tokens, Drafted: 731 tokens, Per-position acceptance rate: 0.748, Avg Draft acceptance rate: 74.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:16 [loggers.py:257] Engine 000: Avg prompt throughput: 212.1 tokens/s, Avg generation throughput: 1355.8 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 544.53 tokens/s, Drafted throughput: 809.35 tokens/s, Accepted: 5447 tokens, Drafted: 8096 tokens, Per-position acceptance rate: 0.673, Avg Draft acceptance rate: 67.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  19.23     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.16      
Output token throughput (tok/s):         1064.82   
Peak output token throughput (tok/s):    832.00    
Peak concurrent requests:                50.00     
Total token throughput (tok/s):          1380.83   
---------------Time to First Token----------------
Mean TTFT (ms):                          128.46    
Median TTFT (ms):                        128.18    
P99 TTFT (ms):                           197.74    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.40     
Median TPOT (ms):                        23.56     
P99 TPOT (ms):                           25.37     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.22     
Median ITL (ms):                         38.69     
P99 ITL (ms):                            53.17     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.59     
Acceptance length:                       1.68      
Drafts:                                  12195     
Draft tokens:                            12195     
Accepted tokens:                         8243      
Per-position acceptance (%):
  Position 0:                            67.59     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:26 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 586.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:26 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 235.77 tokens/s, Drafted throughput: 351.46 tokens/s, Accepted: 2358 tokens, Drafted: 3515 tokens, Per-position acceptance rate: 0.671, Avg Draft acceptance rate: 67.1%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fd4fc3fafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-91780961-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:46 [loggers.py:257] Engine 000: Avg prompt throughput: 487.9 tokens/s, Avg generation throughput: 426.4 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:46 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.74, Accepted throughput: 89.18 tokens/s, Drafted throughput: 120.87 tokens/s, Accepted: 1784 tokens, Drafted: 2418 tokens, Per-position acceptance rate: 0.738, Avg Draft acceptance rate: 73.8%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:56 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 1608.6 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:50:56 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.66, Accepted throughput: 639.51 tokens/s, Drafted throughput: 969.97 tokens/s, Accepted: 6396 tokens, Drafted: 9701 tokens, Per-position acceptance rate: 0.659, Avg Draft acceptance rate: 65.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  13.33     
Total input tokens:                      6078      
Total generated tokens:                  20450     
Request throughput (req/s):              6.00      
Output token throughput (tok/s):         1533.82   
Peak output token throughput (tok/s):    1536.00   
Peak concurrent requests:                76.00     
Total token throughput (tok/s):          1989.68   
---------------Time to First Token----------------
Mean TTFT (ms):                          163.13    
Median TTFT (ms):                        164.61    
P99 TTFT (ms):                           234.37    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          25.27     
Median TPOT (ms):                        25.53     
P99 TPOT (ms):                           27.94     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.25     
Median ITL (ms):                         41.85     
P99 ITL (ms):                            81.22     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.41     
Acceptance length:                       1.67      
Drafts:                                  12185     
Draft tokens:                            12185     
Accepted tokens:                         8214      
Per-position acceptance (%):
  Position 0:                            67.41     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:06 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.67, Accepted throughput: 14.30 tokens/s, Drafted throughput: 21.30 tokens/s, Accepted: 143 tokens, Drafted: 213 tokens, Per-position acceptance rate: 0.671, Avg Draft acceptance rate: 67.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fced7feefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15020, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1c9425c3-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:16 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 21.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:16 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.76, Accepted throughput: 9.00 tokens/s, Drafted throughput: 11.90 tokens/s, Accepted: 90 tokens, Drafted: 119 tokens, Per-position acceptance rate: 0.756, Avg Draft acceptance rate: 75.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  8.27      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              9.68      
Output token throughput (tok/s):         2477.69   
Peak output token throughput (tok/s):    1840.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          3213.02   
---------------Time to First Token----------------
Mean TTFT (ms):                          208.86    
Median TTFT (ms):                        176.76    
P99 TTFT (ms):                           395.12    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          26.98     
Median TPOT (ms):                        26.90     
P99 TPOT (ms):                           29.40     
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.12     
Median ITL (ms):                         44.15     
P99 ITL (ms):                            95.55     
---------------Speculative Decoding---------------
Acceptance rate (%):                     67.53     
Acceptance length:                       1.68      
Drafts:                                  12196     
Draft tokens:                            12196     
Accepted tokens:                         8236      
Per-position acceptance (%):
  Position 0:                            67.53     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k1-t0.0-tp1...
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:26 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:27 [loggers.py:257] Engine 000: Avg prompt throughput: 562.1 tokens/s, Avg generation throughput: 1898.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2764618)[0;0m INFO 01-23 12:51:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.68, Accepted throughput: 763.41 tokens/s, Drafted throughput: 1130.46 tokens/s, Accepted: 8255 tokens, Drafted: 12224 tokens, Per-position acceptance rate: 0.675, Avg Draft acceptance rate: 67.5%
