Removing any existing container named vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k2-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k2-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2765666
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:19 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:19 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15021, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 2, 'max_model_len': 5000}}
[0;36m(APIServer pid=2765666)[0;0m WARNING 01-23 12:33:19 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:20 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:20 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:21 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:21 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:21 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:21 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:33:21 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0553bcafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-8af05cc2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:33:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:33:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:33:32 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=2), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:33:33 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:59663 backend=nccl
WARNING 01-23 12:33:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:33:33 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2765937)[0;0m WARNING 01-23 12:33:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:33:34 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:33:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:33:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:33:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:33:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:33:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:33:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:22 [default_loader.py:291] Loading weights took 46.33 seconds
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:22 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:23 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 12:34:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:25 [default_loader.py:291] Loading weights took 2.33 seconds
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:28 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
WARNING 01-23 12:34:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:29 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:30 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 54.492266 seconds
WARNING 01-23 12:34:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:42 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:42 [backends.py:704] Dynamo bytecode transform time: 11.74 s
WARNING 01-23 12:34:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:34:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:57 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 2.440 s
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:57 [monitor.py:34] torch.compile takes 14.18 s in total
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:57 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:57 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:58 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.058 s
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:58 [monitor.py:34] torch.compile takes 14.71 s in total
WARNING 01-23 12:34:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:59 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:59 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:34:59 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 12:35:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
WARNING 01-23 12:35:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:35:10 [gpu_model_runner.py:4880] Graph capturing finished in 10 secs, took -0.65 GiB
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:35:10 [core.py:272] init engine (profile, create kv cache, warmup model) took 40.10 seconds
[0;36m(EngineCore_DP0 pid=2765937)[0;0m INFO 01-23 12:35:11 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:11 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2765666)[0;0m WARNING 01-23 12:35:12 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:12 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:12 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:12 [serving.py:185] Warming up chat template processing...
WARNING 01-23 12:35:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15021)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15021 ssl:default [Connect call failed (\'127.0.0.1\', 15021)]\n''
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [serving.py:221] Chat template warmup completed in 1707.8ms
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15021
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:14 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:24 [loggers.py:257] Engine 000: Avg prompt throughput: 27.8 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 13.92 tokens/s, Drafted throughput: 20.57 tokens/s, Accepted: 180 tokens, Drafted: 266 tokens, Per-position acceptance rate: 0.737, 0.617, Avg Draft acceptance rate: 67.7%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:34 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 29.40 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 294 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.672, 0.413, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:44 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 27.70 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 277 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.662, 0.357, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:54 [loggers.py:257] Engine 000: Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:35:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 32.90 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 329 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.723, 0.491, Avg Draft acceptance rate: 60.7%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:04 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 27.70 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 277 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.638, 0.384, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:14 [loggers.py:257] Engine 000: Avg prompt throughput: 46.3 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 27.60 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 276 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.656, 0.367, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:24 [loggers.py:257] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.12, Accepted throughput: 30.40 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 304 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.668, 0.454, Avg Draft acceptance rate: 56.1%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:34 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 27.00 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 270 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.631, 0.365, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:44 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.95, Accepted throughput: 25.90 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 259 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.599, 0.353, Avg Draft acceptance rate: 47.6%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:54 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:36:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 28.30 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 283 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.640, 0.401, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:04 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 27.30 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 273 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.647, 0.357, Avg Draft acceptance rate: 50.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:14 [loggers.py:257] Engine 000: Avg prompt throughput: 25.6 tokens/s, Avg generation throughput: 57.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.13, Accepted throughput: 30.20 tokens/s, Drafted throughput: 53.60 tokens/s, Accepted: 302 tokens, Drafted: 536 tokens, Per-position acceptance rate: 0.675, 0.451, Avg Draft acceptance rate: 56.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:24 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 57.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 29.60 tokens/s, Drafted throughput: 54.59 tokens/s, Accepted: 296 tokens, Drafted: 546 tokens, Per-position acceptance rate: 0.670, 0.414, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:34 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 26.20 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 262 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.615, 0.356, Avg Draft acceptance rate: 48.5%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:44 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 32.50 tokens/s, Drafted throughput: 54.60 tokens/s, Accepted: 325 tokens, Drafted: 546 tokens, Per-position acceptance rate: 0.714, 0.476, Avg Draft acceptance rate: 59.5%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:54 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:37:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 32.90 tokens/s, Drafted throughput: 53.60 tokens/s, Accepted: 329 tokens, Drafted: 536 tokens, Per-position acceptance rate: 0.754, 0.474, Avg Draft acceptance rate: 61.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:04 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 28.30 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 283 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.662, 0.379, Avg Draft acceptance rate: 52.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:14 [loggers.py:257] Engine 000: Avg prompt throughput: 25.5 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 29.20 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 292 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.664, 0.413, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:24 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 58.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 31.50 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 315 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.701, 0.461, Avg Draft acceptance rate: 58.1%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:34 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 27.20 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 272 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.629, 0.371, Avg Draft acceptance rate: 50.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:44 [loggers.py:257] Engine 000: Avg prompt throughput: 28.2 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 33.80 tokens/s, Drafted throughput: 53.80 tokens/s, Accepted: 338 tokens, Drafted: 538 tokens, Per-position acceptance rate: 0.755, 0.502, Avg Draft acceptance rate: 62.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:54 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 51.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:38:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.87, Accepted throughput: 23.70 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 237 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.562, 0.309, Avg Draft acceptance rate: 43.6%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:04 [loggers.py:257] Engine 000: Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.95, Accepted throughput: 25.80 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 258 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.618, 0.331, Avg Draft acceptance rate: 47.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:14 [loggers.py:257] Engine 000: Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.13, Accepted throughput: 30.70 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 307 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.654, 0.474, Avg Draft acceptance rate: 56.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:24 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 55.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 28.40 tokens/s, Drafted throughput: 54.39 tokens/s, Accepted: 284 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.640, 0.404, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:34 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 28.20 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 282 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.636, 0.401, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:44 [loggers.py:257] Engine 000: Avg prompt throughput: 18.2 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 27.70 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 277 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.642, 0.380, Avg Draft acceptance rate: 51.1%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:54 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:39:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.99, Accepted throughput: 26.80 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 268 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.621, 0.364, Avg Draft acceptance rate: 49.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:04 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 56.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.07, Accepted throughput: 28.90 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 289 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.686, 0.380, Avg Draft acceptance rate: 53.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:14 [loggers.py:257] Engine 000: Avg prompt throughput: 18.4 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.95, Accepted throughput: 25.80 tokens/s, Drafted throughput: 54.40 tokens/s, Accepted: 258 tokens, Drafted: 544 tokens, Per-position acceptance rate: 0.610, 0.338, Avg Draft acceptance rate: 47.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:24 [loggers.py:257] Engine 000: Avg prompt throughput: 17.7 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 28.10 tokens/s, Drafted throughput: 54.20 tokens/s, Accepted: 281 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.657, 0.380, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:34 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 61.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 34.30 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 343 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.745, 0.520, Avg Draft acceptance rate: 63.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:44 [loggers.py:257] Engine 000: Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 54.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 26.90 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 269 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.611, 0.385, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:54 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:40:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.06, Accepted throughput: 28.70 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 287 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.661, 0.399, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:04 [loggers.py:257] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 32.20 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 322 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.701, 0.487, Avg Draft acceptance rate: 59.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:14 [loggers.py:257] Engine 000: Avg prompt throughput: 16.5 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 31.50 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 315 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.707, 0.459, Avg Draft acceptance rate: 58.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:24 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 57.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.12, Accepted throughput: 30.30 tokens/s, Drafted throughput: 54.19 tokens/s, Accepted: 303 tokens, Drafted: 542 tokens, Per-position acceptance rate: 0.675, 0.443, Avg Draft acceptance rate: 55.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  365.87    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.22      
Output token throughput (tok/s):         55.98     
Peak output token throughput (tok/s):    28.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          72.59     
---------------Time to First Token----------------
Mean TTFT (ms):                          62.54     
Median TTFT (ms):                        54.75     
P99 TTFT (ms):                           81.98     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.65     
Median TPOT (ms):                        17.70     
P99 TPOT (ms):                           20.14     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.38     
Median ITL (ms):                         36.39     
P99 ITL (ms):                            36.73     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.34     
Acceptance length:                       2.07      
Drafts:                                  9897      
Draft tokens:                            19794     
Accepted tokens:                         10559     
Per-position acceptance (%):
  Position 0:                            66.00     
  Position 1:                            40.69     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.86, Accepted throughput: 9.30 tokens/s, Drafted throughput: 21.60 tokens/s, Accepted: 93 tokens, Drafted: 216 tokens, Per-position acceptance rate: 0.583, 0.278, Avg Draft acceptance rate: 43.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f6d0646afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2707fd01-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:44 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 3.70 tokens/s, Drafted throughput: 6.40 tokens/s, Accepted: 37 tokens, Drafted: 64 tokens, Per-position acceptance rate: 0.625, 0.531, Avg Draft acceptance rate: 57.8%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:54 [loggers.py:257] Engine 000: Avg prompt throughput: 34.7 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:41:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 48.40 tokens/s, Drafted throughput: 84.40 tokens/s, Accepted: 484 tokens, Drafted: 844 tokens, Per-position acceptance rate: 0.690, 0.457, Avg Draft acceptance rate: 57.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:04 [loggers.py:257] Engine 000: Avg prompt throughput: 41.4 tokens/s, Avg generation throughput: 111.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 58.79 tokens/s, Drafted throughput: 106.39 tokens/s, Accepted: 588 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.667, 0.438, Avg Draft acceptance rate: 55.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:14 [loggers.py:257] Engine 000: Avg prompt throughput: 56.7 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 54.70 tokens/s, Drafted throughput: 106.39 tokens/s, Accepted: 547 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.654, 0.374, Avg Draft acceptance rate: 51.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:24 [loggers.py:257] Engine 000: Avg prompt throughput: 23.5 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 57.19 tokens/s, Drafted throughput: 106.19 tokens/s, Accepted: 572 tokens, Drafted: 1062 tokens, Per-position acceptance rate: 0.663, 0.414, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:34 [loggers.py:257] Engine 000: Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 52.29 tokens/s, Drafted throughput: 106.38 tokens/s, Accepted: 523 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.609, 0.374, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:44 [loggers.py:257] Engine 000: Avg prompt throughput: 27.6 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 57.60 tokens/s, Drafted throughput: 106.79 tokens/s, Accepted: 576 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.669, 0.410, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:54 [loggers.py:257] Engine 000: Avg prompt throughput: 45.4 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:42:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 54.20 tokens/s, Drafted throughput: 106.39 tokens/s, Accepted: 542 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.641, 0.378, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:04 [loggers.py:257] Engine 000: Avg prompt throughput: 28.4 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 64.29 tokens/s, Drafted throughput: 106.19 tokens/s, Accepted: 643 tokens, Drafted: 1062 tokens, Per-position acceptance rate: 0.736, 0.475, Avg Draft acceptance rate: 60.5%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:14 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 61.40 tokens/s, Drafted throughput: 105.80 tokens/s, Accepted: 614 tokens, Drafted: 1058 tokens, Per-position acceptance rate: 0.722, 0.439, Avg Draft acceptance rate: 58.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:24 [loggers.py:257] Engine 000: Avg prompt throughput: 34.4 tokens/s, Avg generation throughput: 113.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.13, Accepted throughput: 60.10 tokens/s, Drafted throughput: 106.79 tokens/s, Accepted: 601 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.678, 0.448, Avg Draft acceptance rate: 56.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:34 [loggers.py:257] Engine 000: Avg prompt throughput: 15.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 55.70 tokens/s, Drafted throughput: 106.79 tokens/s, Accepted: 557 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.655, 0.388, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:44 [loggers.py:257] Engine 000: Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.07, Accepted throughput: 57.19 tokens/s, Drafted throughput: 106.79 tokens/s, Accepted: 572 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.654, 0.418, Avg Draft acceptance rate: 53.6%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:54 [loggers.py:257] Engine 000: Avg prompt throughput: 42.6 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:43:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 56.99 tokens/s, Drafted throughput: 105.78 tokens/s, Accepted: 570 tokens, Drafted: 1058 tokens, Per-position acceptance rate: 0.652, 0.425, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:04 [loggers.py:257] Engine 000: Avg prompt throughput: 25.1 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.98, Accepted throughput: 52.50 tokens/s, Drafted throughput: 106.80 tokens/s, Accepted: 525 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.624, 0.360, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:14 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.03, Accepted throughput: 54.89 tokens/s, Drafted throughput: 106.39 tokens/s, Accepted: 549 tokens, Drafted: 1064 tokens, Per-position acceptance rate: 0.648, 0.383, Avg Draft acceptance rate: 51.6%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:24 [loggers.py:257] Engine 000: Avg prompt throughput: 27.1 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 64.00 tokens/s, Drafted throughput: 106.79 tokens/s, Accepted: 640 tokens, Drafted: 1068 tokens, Per-position acceptance rate: 0.717, 0.481, Avg Draft acceptance rate: 59.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:34 [loggers.py:257] Engine 000: Avg prompt throughput: 15.6 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 55.09 tokens/s, Drafted throughput: 105.79 tokens/s, Accepted: 551 tokens, Drafted: 1058 tokens, Per-position acceptance rate: 0.654, 0.388, Avg Draft acceptance rate: 52.1%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:44 [loggers.py:257] Engine 000: Avg prompt throughput: 89.5 tokens/s, Avg generation throughput: 114.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 61.80 tokens/s, Drafted throughput: 105.79 tokens/s, Accepted: 618 tokens, Drafted: 1058 tokens, Per-position acceptance rate: 0.701, 0.467, Avg Draft acceptance rate: 58.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  186.74    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.43      
Output token throughput (tok/s):         109.67    
Peak output token throughput (tok/s):    56.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          142.22    
---------------Time to First Token----------------
Mean TTFT (ms):                          111.29    
Median TTFT (ms):                        111.14    
P99 TTFT (ms):                           120.99    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.68     
Median TPOT (ms):                        17.68     
P99 TPOT (ms):                           20.37     
---------------Inter-token Latency----------------
Mean ITL (ms):                           36.68     
Median ITL (ms):                         36.66     
P99 ITL (ms):                            38.03     
---------------Speculative Decoding---------------
Acceptance rate (%):                     54.09     
Acceptance length:                       2.08      
Drafts:                                  9831      
Draft tokens:                            19662     
Accepted tokens:                         10636     
Per-position acceptance (%):
  Position 0:                            66.73     
  Position 1:                            41.46     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:54 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 91.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:44:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 47.70 tokens/s, Drafted throughput: 87.99 tokens/s, Accepted: 477 tokens, Drafted: 880 tokens, Per-position acceptance rate: 0.675, 0.409, Avg Draft acceptance rate: 54.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f40cc1aafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-960f2493-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:14 [loggers.py:257] Engine 000: Avg prompt throughput: 36.0 tokens/s, Avg generation throughput: 26.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 7.65 tokens/s, Drafted throughput: 10.80 tokens/s, Accepted: 153 tokens, Drafted: 216 tokens, Per-position acceptance rate: 0.778, 0.639, Avg Draft acceptance rate: 70.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:24 [loggers.py:257] Engine 000: Avg prompt throughput: 68.5 tokens/s, Avg generation throughput: 212.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 110.20 tokens/s, Drafted throughput: 204.60 tokens/s, Accepted: 1102 tokens, Drafted: 2046 tokens, Per-position acceptance rate: 0.654, 0.423, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:34 [loggers.py:257] Engine 000: Avg prompt throughput: 78.5 tokens/s, Avg generation throughput: 214.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 109.09 tokens/s, Drafted throughput: 209.19 tokens/s, Accepted: 1091 tokens, Drafted: 2092 tokens, Per-position acceptance rate: 0.649, 0.394, Avg Draft acceptance rate: 52.2%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:44 [loggers.py:257] Engine 000: Avg prompt throughput: 74.8 tokens/s, Avg generation throughput: 212.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.02, Accepted throughput: 107.08 tokens/s, Drafted throughput: 210.37 tokens/s, Accepted: 1071 tokens, Drafted: 2104 tokens, Per-position acceptance rate: 0.633, 0.385, Avg Draft acceptance rate: 50.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:54 [loggers.py:257] Engine 000: Avg prompt throughput: 40.7 tokens/s, Avg generation throughput: 226.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:45:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 120.49 tokens/s, Drafted throughput: 210.38 tokens/s, Accepted: 1205 tokens, Drafted: 2104 tokens, Per-position acceptance rate: 0.711, 0.434, Avg Draft acceptance rate: 57.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:04 [loggers.py:257] Engine 000: Avg prompt throughput: 72.3 tokens/s, Avg generation throughput: 223.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.12, Accepted throughput: 117.79 tokens/s, Drafted throughput: 210.58 tokens/s, Accepted: 1178 tokens, Drafted: 2106 tokens, Per-position acceptance rate: 0.674, 0.444, Avg Draft acceptance rate: 55.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:14 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 212.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.01, Accepted throughput: 106.98 tokens/s, Drafted throughput: 211.17 tokens/s, Accepted: 1070 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.625, 0.388, Avg Draft acceptance rate: 50.7%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:24 [loggers.py:257] Engine 000: Avg prompt throughput: 64.0 tokens/s, Avg generation throughput: 214.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 108.88 tokens/s, Drafted throughput: 210.37 tokens/s, Accepted: 1089 tokens, Drafted: 2104 tokens, Per-position acceptance rate: 0.650, 0.385, Avg Draft acceptance rate: 51.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:34 [loggers.py:257] Engine 000: Avg prompt throughput: 59.8 tokens/s, Avg generation throughput: 221.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 115.98 tokens/s, Drafted throughput: 209.76 tokens/s, Accepted: 1160 tokens, Drafted: 2098 tokens, Per-position acceptance rate: 0.665, 0.440, Avg Draft acceptance rate: 55.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:44 [loggers.py:257] Engine 000: Avg prompt throughput: 98.8 tokens/s, Avg generation throughput: 221.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.10, Accepted throughput: 115.79 tokens/s, Drafted throughput: 211.18 tokens/s, Accepted: 1158 tokens, Drafted: 2112 tokens, Per-position acceptance rate: 0.671, 0.425, Avg Draft acceptance rate: 54.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  96.26     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.83      
Output token throughput (tok/s):         212.77    
Peak output token throughput (tok/s):    112.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          275.91    
---------------Time to First Token----------------
Mean TTFT (ms):                          113.11    
Median TTFT (ms):                        112.15    
P99 TTFT (ms):                           130.98    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.94     
Median TPOT (ms):                        17.96     
P99 TPOT (ms):                           20.82     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.09     
Median ITL (ms):                         37.05     
P99 ITL (ms):                            44.43     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.66     
Acceptance length:                       2.07      
Drafts:                                  9864      
Draft tokens:                            19728     
Accepted tokens:                         10586     
Per-position acceptance (%):
  Position 0:                            66.04     
  Position 1:                            41.28     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:54 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 87.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:46:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 45.90 tokens/s, Drafted throughput: 84.40 tokens/s, Accepted: 459 tokens, Drafted: 844 tokens, Per-position acceptance rate: 0.685, 0.403, Avg Draft acceptance rate: 54.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fdbe4c76fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0a33b5fb-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:14 [loggers.py:257] Engine 000: Avg prompt throughput: 114.4 tokens/s, Avg generation throughput: 233.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 61.94 tokens/s, Drafted throughput: 108.19 tokens/s, Accepted: 1239 tokens, Drafted: 2164 tokens, Per-position acceptance rate: 0.689, 0.457, Avg Draft acceptance rate: 57.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:24 [loggers.py:257] Engine 000: Avg prompt throughput: 146.3 tokens/s, Avg generation throughput: 415.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.00, Accepted throughput: 206.58 tokens/s, Drafted throughput: 415.16 tokens/s, Accepted: 2066 tokens, Drafted: 4152 tokens, Per-position acceptance rate: 0.624, 0.371, Avg Draft acceptance rate: 49.8%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:34 [loggers.py:257] Engine 000: Avg prompt throughput: 117.7 tokens/s, Avg generation throughput: 437.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 229.57 tokens/s, Drafted throughput: 414.75 tokens/s, Accepted: 2296 tokens, Drafted: 4148 tokens, Per-position acceptance rate: 0.688, 0.419, Avg Draft acceptance rate: 55.4%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:44 [loggers.py:257] Engine 000: Avg prompt throughput: 109.7 tokens/s, Avg generation throughput: 426.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.05, Accepted throughput: 218.86 tokens/s, Drafted throughput: 415.33 tokens/s, Accepted: 2189 tokens, Drafted: 4154 tokens, Per-position acceptance rate: 0.654, 0.400, Avg Draft acceptance rate: 52.7%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:54 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 427.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:47:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.05, Accepted throughput: 218.14 tokens/s, Drafted throughput: 414.55 tokens/s, Accepted: 2189 tokens, Drafted: 4160 tokens, Per-position acceptance rate: 0.662, 0.390, Avg Draft acceptance rate: 52.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  49.87     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              1.60      
Output token throughput (tok/s):         409.76    
Peak output token throughput (tok/s):    216.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          531.64    
---------------Time to First Token----------------
Mean TTFT (ms):                          113.43    
Median TTFT (ms):                        113.99    
P99 TTFT (ms):                           136.73    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.28     
Median TPOT (ms):                        18.61     
P99 TPOT (ms):                           20.74     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.58     
Median ITL (ms):                         37.42     
P99 ITL (ms):                            46.89     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.07     
Acceptance length:                       2.06      
Drafts:                                  9900      
Draft tokens:                            19800     
Accepted tokens:                         10508     
Per-position acceptance (%):
  Position 0:                            66.04     
  Position 1:                            40.10     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.10, Accepted throughput: 67.88 tokens/s, Drafted throughput: 123.17 tokens/s, Accepted: 679 tokens, Drafted: 1232 tokens, Per-position acceptance rate: 0.675, 0.427, Avg Draft acceptance rate: 55.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1d82ff6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-247a3a29-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:14 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.75, Accepted throughput: 0.30 tokens/s, Drafted throughput: 0.80 tokens/s, Accepted: 3 tokens, Drafted: 8 tokens, Per-position acceptance rate: 0.500, 0.250, Avg Draft acceptance rate: 37.5%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:24 [loggers.py:257] Engine 000: Avg prompt throughput: 254.3 tokens/s, Avg generation throughput: 489.2 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 253.56 tokens/s, Drafted throughput: 466.93 tokens/s, Accepted: 2536 tokens, Drafted: 4670 tokens, Per-position acceptance rate: 0.666, 0.420, Avg Draft acceptance rate: 54.3%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:34 [loggers.py:257] Engine 000: Avg prompt throughput: 189.4 tokens/s, Avg generation throughput: 830.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:34 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 430.51 tokens/s, Drafted throughput: 798.03 tokens/s, Accepted: 4306 tokens, Drafted: 7982 tokens, Per-position acceptance rate: 0.665, 0.413, Avg Draft acceptance rate: 53.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:44 [loggers.py:257] Engine 000: Avg prompt throughput: 164.1 tokens/s, Avg generation throughput: 748.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.06, Accepted throughput: 384.77 tokens/s, Drafted throughput: 727.74 tokens/s, Accepted: 3848 tokens, Drafted: 7278 tokens, Per-position acceptance rate: 0.659, 0.399, Avg Draft acceptance rate: 52.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  27.14     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              2.95      
Output token throughput (tok/s):         754.57    
Peak output token throughput (tok/s):    416.00    
Peak concurrent requests:                28.00     
Total token throughput (tok/s):          978.51    
---------------Time to First Token----------------
Mean TTFT (ms):                          117.54    
Median TTFT (ms):                        118.24    
P99 TTFT (ms):                           138.39    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.99     
Median TPOT (ms):                        19.05     
P99 TPOT (ms):                           21.55     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.18     
Median ITL (ms):                         38.94     
P99 ITL (ms):                            49.54     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.41     
Acceptance length:                       2.07      
Drafts:                                  9889      
Draft tokens:                            19778     
Accepted tokens:                         10564     
Per-position acceptance (%):
  Position 0:                            66.18     
  Position 1:                            40.64     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:48:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.84, Accepted throughput: 2.10 tokens/s, Drafted throughput: 5.00 tokens/s, Accepted: 21 tokens, Drafted: 50 tokens, Per-position acceptance rate: 0.640, 0.200, Avg Draft acceptance rate: 42.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f43af436fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-358839c2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:04 [loggers.py:257] Engine 000: Avg prompt throughput: 150.8 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:04 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 24.60 tokens/s, Drafted throughput: 35.20 tokens/s, Accepted: 246 tokens, Drafted: 352 tokens, Per-position acceptance rate: 0.756, 0.642, Avg Draft acceptance rate: 69.9%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:14 [loggers.py:257] Engine 000: Avg prompt throughput: 374.4 tokens/s, Avg generation throughput: 1544.1 tokens/s, Running: 31 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.07, Accepted throughput: 798.15 tokens/s, Drafted throughput: 1486.34 tokens/s, Accepted: 7984 tokens, Drafted: 14868 tokens, Per-position acceptance rate: 0.661, 0.413, Avg Draft acceptance rate: 53.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  16.70     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.79      
Output token throughput (tok/s):         1226.22   
Peak output token throughput (tok/s):    800.00    
Peak concurrent requests:                47.00     
Total token throughput (tok/s):          1590.14   
---------------Time to First Token----------------
Mean TTFT (ms):                          127.38    
Median TTFT (ms):                        127.30    
P99 TTFT (ms):                           165.24    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.76     
Median TPOT (ms):                        19.82     
P99 TPOT (ms):                           22.36     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.90     
Median ITL (ms):                         40.68     
P99 ITL (ms):                            75.09     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.77     
Acceptance length:                       2.08      
Drafts:                                  9859      
Draft tokens:                            19718     
Accepted tokens:                         10603     
Per-position acceptance (%):
  Position 0:                            66.21     
  Position 1:                            41.33     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:24 [loggers.py:257] Engine 000: Avg prompt throughput: 100.5 tokens/s, Avg generation throughput: 485.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:24 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.07, Accepted throughput: 252.30 tokens/s, Drafted throughput: 470.81 tokens/s, Accepted: 2523 tokens, Drafted: 4708 tokens, Per-position acceptance rate: 0.663, 0.409, Avg Draft acceptance rate: 53.6%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1dcfb1afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-0acfc6e9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:44 [loggers.py:257] Engine 000: Avg prompt throughput: 498.2 tokens/s, Avg generation throughput: 1334.5 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 38.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:44 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.10, Accepted throughput: 347.22 tokens/s, Drafted throughput: 633.35 tokens/s, Accepted: 6945 tokens, Drafted: 12668 tokens, Per-position acceptance rate: 0.675, 0.422, Avg Draft acceptance rate: 54.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  11.70     
Total input tokens:                      6078      
Total generated tokens:                  20450     
Request throughput (req/s):              6.83      
Output token throughput (tok/s):         1747.17   
Peak output token throughput (tok/s):    1472.00   
Peak concurrent requests:                79.00     
Total token throughput (tok/s):          2266.45   
---------------Time to First Token----------------
Mean TTFT (ms):                          174.17    
Median TTFT (ms):                        180.35    
P99 TTFT (ms):                           270.10    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          21.30     
Median TPOT (ms):                        21.57     
P99 TPOT (ms):                           24.87     
---------------Inter-token Latency----------------
Mean ITL (ms):                           44.15     
Median ITL (ms):                         43.47     
P99 ITL (ms):                            88.69     
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.99     
Acceptance length:                       2.08      
Drafts:                                  9825      
Draft tokens:                            19650     
Accepted tokens:                         10610     
Per-position acceptance (%):
  Position 0:                            66.83     
  Position 1:                            41.16     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:54 [loggers.py:257] Engine 000: Avg prompt throughput: 127.6 tokens/s, Avg generation throughput: 736.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:49:54 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.06, Accepted throughput: 381.49 tokens/s, Drafted throughput: 719.18 tokens/s, Accepted: 3815 tokens, Drafted: 7192 tokens, Per-position acceptance rate: 0.660, 0.400, Avg Draft acceptance rate: 53.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:50:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f35dc54efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15021, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2c6a75e7-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:50:14 [loggers.py:257] Engine 000: Avg prompt throughput: 625.8 tokens/s, Avg generation throughput: 1368.8 tokens/s, Running: 80 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:50:14 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 357.98 tokens/s, Drafted throughput: 644.77 tokens/s, Accepted: 7160 tokens, Drafted: 12896 tokens, Per-position acceptance rate: 0.676, 0.435, Avg Draft acceptance rate: 55.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  7.46      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              10.73     
Output token throughput (tok/s):         2746.70   
Peak output token throughput (tok/s):    1760.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          3561.85   
---------------Time to First Token----------------
Mean TTFT (ms):                          193.21    
Median TTFT (ms):                        175.39    
P99 TTFT (ms):                           363.03    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.95     
Median TPOT (ms):                        22.92     
P99 TPOT (ms):                           25.98     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.36     
Median ITL (ms):                         46.23     
P99 ITL (ms):                            129.99    
---------------Speculative Decoding---------------
Acceptance rate (%):                     53.45     
Acceptance length:                       2.07      
Drafts:                                  9885      
Draft tokens:                            19770     
Accepted tokens:                         10568     
Per-position acceptance (%):
  Position 0:                            66.22     
  Position 1:                            40.69     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k2-t0.0-tp1...
[0;36m(APIServer pid=2765666)[0;0m INFO 01-23 12:50:18 [launcher.py:110] Shutting down FastAPI HTTP server.
