Removing any existing container named vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k3-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k3-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 922122
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:43 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:43 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15022, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 3, 'max_model_len': 5000}}
[0;36m(APIServer pid=922122)[0;0m WARNING 01-23 12:38:43 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:44 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:44 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:46 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:46 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:46 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:46 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:38:46 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f89fd3defc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-d66f0655-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:38:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:38:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:38:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:38:58 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=3), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:39:00 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.69:47387 backend=nccl
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:39:01 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
WARNING 01-23 12:39:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m WARNING 01-23 12:39:05 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:39:05 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
WARNING 01-23 12:39:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:39:08 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:39:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:39:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:18 [default_loader.py:291] Loading weights took 68.20 seconds
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:18 [gpu_model_runner.py:3851] Loading drafter model...
WARNING 01-23 12:40:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:18 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 12:40:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:25 [default_loader.py:291] Loading weights took 6.39 seconds
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:27 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
WARNING 01-23 12:40:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:28 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:29 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 82.882892 seconds
WARNING 01-23 12:40:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:47 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:40:47 [backends.py:704] Dynamo bytecode transform time: 17.66 s
WARNING 01-23 12:40:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:40:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:41:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:06 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 5.053 s
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:06 [monitor.py:34] torch.compile takes 22.71 s in total
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:06 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:06 [backends.py:704] Dynamo bytecode transform time: 0.50 s
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:07 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.116 s
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:07 [monitor.py:34] torch.compile takes 23.33 s in total
WARNING 01-23 12:41:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:08 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:08 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:08 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 12:41:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
WARNING 01-23 12:41:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:19 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.63 GiB
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:20 [core.py:272] init engine (profile, create kv cache, warmup model) took 50.41 seconds
[0;36m(EngineCore_DP0 pid=922493)[0;0m INFO 01-23 12:41:21 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:22 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=922122)[0;0m WARNING 01-23 12:41:22 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:22 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:22 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:22 [serving.py:185] Warming up chat template processing...
WARNING 01-23 12:41:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15022)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15022 ssl:default [Connect call failed (\'127.0.0.1\', 15022)]\n''
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [serving.py:221] Chat template warmup completed in 1818.3ms
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15022
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:24 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:35 [loggers.py:257] Engine 000: Avg prompt throughput: 26.5 tokens/s, Avg generation throughput: 28.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.64, Accepted throughput: 17.73 tokens/s, Drafted throughput: 32.45 tokens/s, Accepted: 241 tokens, Drafted: 441 tokens, Per-position acceptance rate: 0.707, 0.531, 0.401, Avg Draft acceptance rate: 54.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:45 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 31.80 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 318 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.619, 0.370, 0.211, Avg Draft acceptance rate: 40.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:55 [loggers.py:257] Engine 000: Avg prompt throughput: 39.1 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:41:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 33.99 tokens/s, Drafted throughput: 78.58 tokens/s, Accepted: 340 tokens, Drafted: 786 tokens, Per-position acceptance rate: 0.672, 0.401, 0.225, Avg Draft acceptance rate: 43.3%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:05 [loggers.py:257] Engine 000: Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 38.40 tokens/s, Drafted throughput: 79.50 tokens/s, Accepted: 384 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.709, 0.483, 0.257, Avg Draft acceptance rate: 48.3%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:15 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 56.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.11, Accepted throughput: 29.50 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 295 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.615, 0.317, 0.181, Avg Draft acceptance rate: 37.1%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:25 [loggers.py:257] Engine 000: Avg prompt throughput: 51.2 tokens/s, Avg generation throughput: 62.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 35.80 tokens/s, Drafted throughput: 78.90 tokens/s, Accepted: 358 tokens, Drafted: 789 tokens, Per-position acceptance rate: 0.726, 0.407, 0.228, Avg Draft acceptance rate: 45.4%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:35 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 57.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 30.80 tokens/s, Drafted throughput: 79.79 tokens/s, Accepted: 308 tokens, Drafted: 798 tokens, Per-position acceptance rate: 0.624, 0.368, 0.165, Avg Draft acceptance rate: 38.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:45 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 56.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 30.50 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 305 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.636, 0.337, 0.182, Avg Draft acceptance rate: 38.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:55 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:42:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.08, Accepted throughput: 28.50 tokens/s, Drafted throughput: 79.20 tokens/s, Accepted: 285 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.564, 0.337, 0.178, Avg Draft acceptance rate: 36.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:05 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 32.50 tokens/s, Drafted throughput: 78.89 tokens/s, Accepted: 325 tokens, Drafted: 789 tokens, Per-position acceptance rate: 0.635, 0.376, 0.224, Avg Draft acceptance rate: 41.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:15 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 31.40 tokens/s, Drafted throughput: 79.50 tokens/s, Accepted: 314 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.626, 0.358, 0.200, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:25 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 35.80 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 358 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.686, 0.428, 0.242, Avg Draft acceptance rate: 45.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:35 [loggers.py:257] Engine 000: Avg prompt throughput: 41.0 tokens/s, Avg generation throughput: 57.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 30.60 tokens/s, Drafted throughput: 79.20 tokens/s, Accepted: 306 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.625, 0.360, 0.174, Avg Draft acceptance rate: 38.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:45 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 35.19 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 352 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.687, 0.415, 0.226, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:55 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:43:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 41.70 tokens/s, Drafted throughput: 78.90 tokens/s, Accepted: 417 tokens, Drafted: 789 tokens, Per-position acceptance rate: 0.757, 0.498, 0.331, Avg Draft acceptance rate: 52.9%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:05 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 56.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.14, Accepted throughput: 30.20 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 302 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.614, 0.330, 0.201, Avg Draft acceptance rate: 38.1%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:15 [loggers.py:257] Engine 000: Avg prompt throughput: 25.5 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 32.50 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 325 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.663, 0.371, 0.197, Avg Draft acceptance rate: 41.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:25 [loggers.py:257] Engine 000: Avg prompt throughput: 15.1 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 38.50 tokens/s, Drafted throughput: 79.20 tokens/s, Accepted: 385 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.693, 0.473, 0.292, Avg Draft acceptance rate: 48.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:35 [loggers.py:257] Engine 000: Avg prompt throughput: 24.3 tokens/s, Avg generation throughput: 70.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.66, Accepted throughput: 43.79 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 438 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.777, 0.534, 0.348, Avg Draft acceptance rate: 55.3%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:45 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 33.60 tokens/s, Drafted throughput: 78.59 tokens/s, Accepted: 336 tokens, Drafted: 786 tokens, Per-position acceptance rate: 0.656, 0.397, 0.229, Avg Draft acceptance rate: 42.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:55 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:44:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 29.00 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 290 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.585, 0.317, 0.192, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:05 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 37.80 tokens/s, Drafted throughput: 79.20 tokens/s, Accepted: 378 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.689, 0.455, 0.288, Avg Draft acceptance rate: 47.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:15 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 34.10 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 341 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.657, 0.385, 0.245, Avg Draft acceptance rate: 42.9%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:25 [loggers.py:257] Engine 000: Avg prompt throughput: 33.2 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 33.10 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 331 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.630, 0.381, 0.238, Avg Draft acceptance rate: 41.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:35 [loggers.py:257] Engine 000: Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 56.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 30.40 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 304 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.649, 0.340, 0.158, Avg Draft acceptance rate: 38.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:45 [loggers.py:257] Engine 000: Avg prompt throughput: 17.1 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 32.20 tokens/s, Drafted throughput: 79.19 tokens/s, Accepted: 322 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.633, 0.379, 0.208, Avg Draft acceptance rate: 40.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:55 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:45:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 31.80 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 318 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.634, 0.351, 0.215, Avg Draft acceptance rate: 40.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:05 [loggers.py:257] Engine 000: Avg prompt throughput: 18.4 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 31.70 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 317 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.608, 0.377, 0.211, Avg Draft acceptance rate: 39.9%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:15 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 34.80 tokens/s, Drafted throughput: 78.89 tokens/s, Accepted: 348 tokens, Drafted: 789 tokens, Per-position acceptance rate: 0.677, 0.399, 0.247, Avg Draft acceptance rate: 44.1%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:25 [loggers.py:257] Engine 000: Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 37.70 tokens/s, Drafted throughput: 79.49 tokens/s, Accepted: 377 tokens, Drafted: 795 tokens, Per-position acceptance rate: 0.675, 0.457, 0.291, Avg Draft acceptance rate: 47.4%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:35 [loggers.py:257] Engine 000: Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.14, Accepted throughput: 30.15 tokens/s, Drafted throughput: 79.08 tokens/s, Accepted: 302 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.625, 0.356, 0.163, Avg Draft acceptance rate: 38.1%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:45 [loggers.py:257] Engine 000: Avg prompt throughput: 40.4 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 33.70 tokens/s, Drafted throughput: 79.20 tokens/s, Accepted: 337 tokens, Drafted: 792 tokens, Per-position acceptance rate: 0.640, 0.402, 0.235, Avg Draft acceptance rate: 42.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:55 [loggers.py:257] Engine 000: Avg prompt throughput: 30.9 tokens/s, Avg generation throughput: 67.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:46:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 40.79 tokens/s, Drafted throughput: 78.89 tokens/s, Accepted: 408 tokens, Drafted: 789 tokens, Per-position acceptance rate: 0.734, 0.494, 0.323, Avg Draft acceptance rate: 51.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:05 [loggers.py:257] Engine 000: Avg prompt throughput: 26.6 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 38.30 tokens/s, Drafted throughput: 78.89 tokens/s, Accepted: 383 tokens, Drafted: 789 tokens, Per-position acceptance rate: 0.722, 0.430, 0.304, Avg Draft acceptance rate: 48.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  340.27    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.24      
Output token throughput (tok/s):         60.19     
Peak output token throughput (tok/s):    27.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          78.05     
---------------Time to First Token----------------
Mean TTFT (ms):                          65.11     
Median TTFT (ms):                        62.61     
P99 TTFT (ms):                           81.80     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.38     
Median TPOT (ms):                        16.44     
P99 TPOT (ms):                           19.18     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.28     
Median ITL (ms):                         37.29     
P99 ITL (ms):                            37.96     
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.81     
Acceptance length:                       2.28      
Drafts:                                  8964      
Draft tokens:                            26892     
Accepted tokens:                         11513     
Per-position acceptance (%):
  Position 0:                            65.81     
  Position 1:                            39.64     
  Position 2:                            22.99     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:15 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.15, Accepted throughput: 22.70 tokens/s, Drafted throughput: 59.10 tokens/s, Accepted: 227 tokens, Drafted: 591 tokens, Per-position acceptance rate: 0.619, 0.365, 0.168, Avg Draft acceptance rate: 38.4%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fc31c712fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9600d487-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:35 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 42.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 13.45 tokens/s, Drafted throughput: 23.40 tokens/s, Accepted: 269 tokens, Drafted: 468 tokens, Per-position acceptance rate: 0.731, 0.564, 0.429, Avg Draft acceptance rate: 57.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:45 [loggers.py:257] Engine 000: Avg prompt throughput: 45.0 tokens/s, Avg generation throughput: 117.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 65.29 tokens/s, Drafted throughput: 154.78 tokens/s, Accepted: 653 tokens, Drafted: 1548 tokens, Per-position acceptance rate: 0.647, 0.395, 0.223, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:55 [loggers.py:257] Engine 000: Avg prompt throughput: 29.6 tokens/s, Avg generation throughput: 122.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:47:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 70.58 tokens/s, Drafted throughput: 155.65 tokens/s, Accepted: 706 tokens, Drafted: 1557 tokens, Per-position acceptance rate: 0.682, 0.439, 0.239, Avg Draft acceptance rate: 45.3%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:05 [loggers.py:257] Engine 000: Avg prompt throughput: 54.0 tokens/s, Avg generation throughput: 115.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 63.18 tokens/s, Drafted throughput: 155.95 tokens/s, Accepted: 632 tokens, Drafted: 1560 tokens, Per-position acceptance rate: 0.656, 0.365, 0.194, Avg Draft acceptance rate: 40.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:15 [loggers.py:257] Engine 000: Avg prompt throughput: 14.6 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 60.00 tokens/s, Drafted throughput: 155.09 tokens/s, Accepted: 600 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.619, 0.352, 0.190, Avg Draft acceptance rate: 38.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:25 [loggers.py:257] Engine 000: Avg prompt throughput: 28.0 tokens/s, Avg generation throughput: 114.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 62.09 tokens/s, Drafted throughput: 155.99 tokens/s, Accepted: 621 tokens, Drafted: 1560 tokens, Per-position acceptance rate: 0.629, 0.360, 0.206, Avg Draft acceptance rate: 39.8%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:35 [loggers.py:257] Engine 000: Avg prompt throughput: 49.7 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 63.79 tokens/s, Drafted throughput: 154.48 tokens/s, Accepted: 638 tokens, Drafted: 1545 tokens, Per-position acceptance rate: 0.650, 0.386, 0.202, Avg Draft acceptance rate: 41.3%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:45 [loggers.py:257] Engine 000: Avg prompt throughput: 28.4 tokens/s, Avg generation throughput: 128.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 76.69 tokens/s, Drafted throughput: 155.08 tokens/s, Accepted: 767 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.737, 0.460, 0.286, Avg Draft acceptance rate: 49.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:55 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 121.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:48:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 69.09 tokens/s, Drafted throughput: 155.09 tokens/s, Accepted: 691 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.675, 0.412, 0.250, Avg Draft acceptance rate: 44.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:05 [loggers.py:257] Engine 000: Avg prompt throughput: 34.4 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 76.69 tokens/s, Drafted throughput: 155.98 tokens/s, Accepted: 767 tokens, Drafted: 1560 tokens, Per-position acceptance rate: 0.719, 0.463, 0.292, Avg Draft acceptance rate: 49.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:15 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 112.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.18, Accepted throughput: 61.10 tokens/s, Drafted throughput: 155.10 tokens/s, Accepted: 611 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.625, 0.358, 0.199, Avg Draft acceptance rate: 39.4%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:25 [loggers.py:257] Engine 000: Avg prompt throughput: 16.3 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 67.29 tokens/s, Drafted throughput: 155.08 tokens/s, Accepted: 673 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.650, 0.398, 0.253, Avg Draft acceptance rate: 43.4%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:35 [loggers.py:257] Engine 000: Avg prompt throughput: 44.6 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 66.00 tokens/s, Drafted throughput: 155.39 tokens/s, Accepted: 660 tokens, Drafted: 1554 tokens, Per-position acceptance rate: 0.662, 0.388, 0.224, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:45 [loggers.py:257] Engine 000: Avg prompt throughput: 19.4 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.13, Accepted throughput: 58.69 tokens/s, Drafted throughput: 155.97 tokens/s, Accepted: 587 tokens, Drafted: 1560 tokens, Per-position acceptance rate: 0.606, 0.338, 0.185, Avg Draft acceptance rate: 37.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:55 [loggers.py:257] Engine 000: Avg prompt throughput: 44.1 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:49:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 62.59 tokens/s, Drafted throughput: 155.08 tokens/s, Accepted: 626 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.621, 0.369, 0.221, Avg Draft acceptance rate: 40.4%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:05 [loggers.py:257] Engine 000: Avg prompt throughput: 19.5 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 70.90 tokens/s, Drafted throughput: 155.09 tokens/s, Accepted: 709 tokens, Drafted: 1551 tokens, Per-position acceptance rate: 0.692, 0.422, 0.257, Avg Draft acceptance rate: 45.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:15 [loggers.py:257] Engine 000: Avg prompt throughput: 71.4 tokens/s, Avg generation throughput: 120.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 69.19 tokens/s, Drafted throughput: 154.48 tokens/s, Accepted: 692 tokens, Drafted: 1545 tokens, Per-position acceptance rate: 0.674, 0.433, 0.237, Avg Draft acceptance rate: 44.8%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:25 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 122.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 71.39 tokens/s, Drafted throughput: 153.89 tokens/s, Accepted: 714 tokens, Drafted: 1539 tokens, Per-position acceptance rate: 0.700, 0.425, 0.267, Avg Draft acceptance rate: 46.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  175.25    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.46      
Output token throughput (tok/s):         116.86    
Peak output token throughput (tok/s):    54.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          151.54    
---------------Time to First Token----------------
Mean TTFT (ms):                          114.22    
Median TTFT (ms):                        113.97    
P99 TTFT (ms):                           127.01    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.49     
Median TPOT (ms):                        16.67     
P99 TPOT (ms):                           19.35     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.62     
Median ITL (ms):                         37.59     
P99 ITL (ms):                            39.35     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.00     
Acceptance length:                       2.29      
Drafts:                                  8942      
Draft tokens:                            26826     
Accepted tokens:                         11535     
Per-position acceptance (%):
  Position 0:                            66.09     
  Position 1:                            39.81     
  Position 2:                            23.09     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.93, Accepted throughput: 8.10 tokens/s, Drafted throughput: 26.10 tokens/s, Accepted: 81 tokens, Drafted: 261 tokens, Per-position acceptance rate: 0.552, 0.276, 0.103, Avg Draft acceptance rate: 31.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f309efbefc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9ebd3947-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:45 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 3.40 tokens/s, Drafted throughput: 5.70 tokens/s, Accepted: 34 tokens, Drafted: 57 tokens, Per-position acceptance rate: 0.737, 0.632, 0.421, Avg Draft acceptance rate: 59.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:55 [loggers.py:257] Engine 000: Avg prompt throughput: 76.1 tokens/s, Avg generation throughput: 174.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:50:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 99.40 tokens/s, Drafted throughput: 222.38 tokens/s, Accepted: 995 tokens, Drafted: 2226 tokens, Per-position acceptance rate: 0.659, 0.419, 0.263, Avg Draft acceptance rate: 44.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:05 [loggers.py:257] Engine 000: Avg prompt throughput: 80.1 tokens/s, Avg generation throughput: 232.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 129.59 tokens/s, Drafted throughput: 307.48 tokens/s, Accepted: 1296 tokens, Drafted: 3075 tokens, Per-position acceptance rate: 0.666, 0.390, 0.208, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:15 [loggers.py:257] Engine 000: Avg prompt throughput: 41.0 tokens/s, Avg generation throughput: 231.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 128.69 tokens/s, Drafted throughput: 308.38 tokens/s, Accepted: 1287 tokens, Drafted: 3084 tokens, Per-position acceptance rate: 0.636, 0.393, 0.223, Avg Draft acceptance rate: 41.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:25 [loggers.py:257] Engine 000: Avg prompt throughput: 103.9 tokens/s, Avg generation throughput: 242.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 140.70 tokens/s, Drafted throughput: 304.80 tokens/s, Accepted: 1407 tokens, Drafted: 3048 tokens, Per-position acceptance rate: 0.711, 0.423, 0.251, Avg Draft acceptance rate: 46.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:35 [loggers.py:257] Engine 000: Avg prompt throughput: 51.6 tokens/s, Avg generation throughput: 242.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 138.99 tokens/s, Drafted throughput: 309.58 tokens/s, Accepted: 1390 tokens, Drafted: 3096 tokens, Per-position acceptance rate: 0.667, 0.423, 0.257, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:45 [loggers.py:257] Engine 000: Avg prompt throughput: 53.8 tokens/s, Avg generation throughput: 233.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 131.28 tokens/s, Drafted throughput: 307.45 tokens/s, Accepted: 1313 tokens, Drafted: 3075 tokens, Per-position acceptance rate: 0.654, 0.397, 0.230, Avg Draft acceptance rate: 42.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:55 [loggers.py:257] Engine 000: Avg prompt throughput: 48.2 tokens/s, Avg generation throughput: 226.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:51:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 123.60 tokens/s, Drafted throughput: 307.50 tokens/s, Accepted: 1236 tokens, Drafted: 3075 tokens, Per-position acceptance rate: 0.631, 0.367, 0.208, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:05 [loggers.py:257] Engine 000: Avg prompt throughput: 55.2 tokens/s, Avg generation throughput: 235.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 132.76 tokens/s, Drafted throughput: 308.61 tokens/s, Accepted: 1328 tokens, Drafted: 3087 tokens, Per-position acceptance rate: 0.657, 0.398, 0.235, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:15 [loggers.py:257] Engine 000: Avg prompt throughput: 97.8 tokens/s, Avg generation throughput: 228.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 130.50 tokens/s, Drafted throughput: 293.40 tokens/s, Accepted: 1305 tokens, Drafted: 2934 tokens, Per-position acceptance rate: 0.674, 0.413, 0.247, Avg Draft acceptance rate: 44.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  90.24     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.89      
Output token throughput (tok/s):         226.95    
Peak output token throughput (tok/s):    108.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          294.31    
---------------Time to First Token----------------
Mean TTFT (ms):                          115.43    
Median TTFT (ms):                        115.14    
P99 TTFT (ms):                           126.59    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.63     
Median TPOT (ms):                        16.73     
P99 TPOT (ms):                           19.94     
---------------Inter-token Latency----------------
Mean ITL (ms):                           37.97     
Median ITL (ms):                         37.90     
P99 ITL (ms):                            45.55     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.08     
Acceptance length:                       2.29      
Drafts:                                  8933      
Draft tokens:                            26799     
Accepted tokens:                         11544     
Per-position acceptance (%):
  Position 0:                            66.02     
  Position 1:                            40.00     
  Position 2:                            23.21     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.05, Accepted throughput: 11.50 tokens/s, Drafted throughput: 33.00 tokens/s, Accepted: 115 tokens, Drafted: 330 tokens, Per-position acceptance rate: 0.591, 0.309, 0.145, Avg Draft acceptance rate: 34.8%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f63ba26efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-bf72ca65-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.92, Accepted throughput: 5.00 tokens/s, Drafted throughput: 7.80 tokens/s, Accepted: 50 tokens, Drafted: 78 tokens, Per-position acceptance rate: 0.769, 0.654, 0.500, Avg Draft acceptance rate: 64.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:45 [loggers.py:257] Engine 000: Avg prompt throughput: 153.4 tokens/s, Avg generation throughput: 331.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 188.58 tokens/s, Drafted throughput: 426.26 tokens/s, Accepted: 1886 tokens, Drafted: 4263 tokens, Per-position acceptance rate: 0.672, 0.416, 0.239, Avg Draft acceptance rate: 44.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:55 [loggers.py:257] Engine 000: Avg prompt throughput: 112.9 tokens/s, Avg generation throughput: 454.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:52:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 253.97 tokens/s, Drafted throughput: 601.44 tokens/s, Accepted: 2540 tokens, Drafted: 6015 tokens, Per-position acceptance rate: 0.656, 0.392, 0.219, Avg Draft acceptance rate: 42.2%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:05 [loggers.py:257] Engine 000: Avg prompt throughput: 107.4 tokens/s, Avg generation throughput: 457.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 255.34 tokens/s, Drafted throughput: 600.22 tokens/s, Accepted: 2555 tokens, Drafted: 6006 tokens, Per-position acceptance rate: 0.664, 0.387, 0.225, Avg Draft acceptance rate: 42.5%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:15 [loggers.py:257] Engine 000: Avg prompt throughput: 125.6 tokens/s, Avg generation throughput: 448.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 249.65 tokens/s, Drafted throughput: 599.87 tokens/s, Accepted: 2497 tokens, Drafted: 6000 tokens, Per-position acceptance rate: 0.638, 0.391, 0.220, Avg Draft acceptance rate: 41.6%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:25 [loggers.py:257] Engine 000: Avg prompt throughput: 108.4 tokens/s, Avg generation throughput: 368.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 208.89 tokens/s, Drafted throughput: 482.08 tokens/s, Accepted: 2089 tokens, Drafted: 4821 tokens, Per-position acceptance rate: 0.674, 0.397, 0.229, Avg Draft acceptance rate: 43.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  47.51     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              1.68      
Output token throughput (tok/s):         430.11    
Peak output token throughput (tok/s):    208.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          558.04    
---------------Time to First Token----------------
Mean TTFT (ms):                          117.79    
Median TTFT (ms):                        118.05    
P99 TTFT (ms):                           138.80    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.14     
Median TPOT (ms):                        17.25     
P99 TPOT (ms):                           20.20     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.90     
Median ITL (ms):                         38.74     
P99 ITL (ms):                            48.27     
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.59     
Acceptance length:                       2.28      
Drafts:                                  8968      
Draft tokens:                            26904     
Accepted tokens:                         11459     
Per-position acceptance (%):
  Position 0:                            65.93     
  Position 1:                            39.44     
  Position 2:                            22.40     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 0.40 tokens/s, Drafted throughput: 0.90 tokens/s, Accepted: 4 tokens, Drafted: 9 tokens, Per-position acceptance rate: 0.667, 0.333, 0.333, Avg Draft acceptance rate: 44.4%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f67e76c2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c11d077e-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:45 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 23.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.62, Accepted throughput: 14.40 tokens/s, Drafted throughput: 26.70 tokens/s, Accepted: 144 tokens, Drafted: 267 tokens, Per-position acceptance rate: 0.708, 0.517, 0.393, Avg Draft acceptance rate: 53.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:55 [loggers.py:257] Engine 000: Avg prompt throughput: 310.9 tokens/s, Avg generation throughput: 809.6 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:53:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 459.12 tokens/s, Drafted throughput: 1050.43 tokens/s, Accepted: 4592 tokens, Drafted: 10506 tokens, Per-position acceptance rate: 0.666, 0.407, 0.238, Avg Draft acceptance rate: 43.7%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:05 [loggers.py:257] Engine 000: Avg prompt throughput: 267.6 tokens/s, Avg generation throughput: 893.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 504.49 tokens/s, Drafted throughput: 1166.45 tokens/s, Accepted: 5046 tokens, Drafted: 11667 tokens, Per-position acceptance rate: 0.652, 0.409, 0.237, Avg Draft acceptance rate: 43.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  25.61     
Total input tokens:                      6078      
Total generated tokens:                  20460     
Request throughput (req/s):              3.12      
Output token throughput (tok/s):         798.83    
Peak output token throughput (tok/s):    416.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          1036.14   
---------------Time to First Token----------------
Mean TTFT (ms):                          124.62    
Median TTFT (ms):                        121.04    
P99 TTFT (ms):                           214.84    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.55     
Median TPOT (ms):                        17.38     
P99 TPOT (ms):                           20.66     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.08     
Median ITL (ms):                         39.73     
P99 ITL (ms):                            50.79     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.14     
Acceptance length:                       2.29      
Drafts:                                  8923      
Draft tokens:                            26769     
Accepted tokens:                         11548     
Per-position acceptance (%):
  Position 0:                            65.74     
  Position 1:                            40.36     
  Position 2:                            23.32     
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:15 [loggers.py:257] Engine 000: Avg prompt throughput: 29.1 tokens/s, Avg generation throughput: 344.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 192.79 tokens/s, Drafted throughput: 461.67 tokens/s, Accepted: 1928 tokens, Drafted: 4617 tokens, Per-position acceptance rate: 0.650, 0.385, 0.218, Avg Draft acceptance rate: 41.8%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efc8d312fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9b57dfb2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:35 [loggers.py:257] Engine 000: Avg prompt throughput: 275.8 tokens/s, Avg generation throughput: 532.9 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 152.08 tokens/s, Drafted throughput: 338.80 tokens/s, Accepted: 3042 tokens, Drafted: 6777 tokens, Per-position acceptance rate: 0.678, 0.421, 0.247, Avg Draft acceptance rate: 44.9%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:45 [loggers.py:257] Engine 000: Avg prompt throughput: 349.9 tokens/s, Avg generation throughput: 1459.4 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 821.41 tokens/s, Drafted throughput: 1918.89 tokens/s, Accepted: 8215 tokens, Drafted: 19191 tokens, Per-position acceptance rate: 0.663, 0.393, 0.229, Avg Draft acceptance rate: 42.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  16.00     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              5.00      
Output token throughput (tok/s):         1277.47   
Peak output token throughput (tok/s):    768.00    
Peak concurrent requests:                51.00     
Total token throughput (tok/s):          1657.42   
---------------Time to First Token----------------
Mean TTFT (ms):                          142.52    
Median TTFT (ms):                        131.80    
P99 TTFT (ms):                           350.58    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.79     
Median TPOT (ms):                        18.99     
P99 TPOT (ms):                           22.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.01     
Median ITL (ms):                         42.33     
P99 ITL (ms):                            76.00     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.03     
Acceptance length:                       2.29      
Drafts:                                  8920      
Draft tokens:                            26760     
Accepted tokens:                         11516     
Per-position acceptance (%):
  Position 0:                            66.38     
  Position 1:                            39.73     
  Position 2:                            22.99     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:54:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 42.10 tokens/s, Drafted throughput: 107.99 tokens/s, Accepted: 421 tokens, Drafted: 1080 tokens, Per-position acceptance rate: 0.611, 0.364, 0.194, Avg Draft acceptance rate: 39.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0b8720efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-7ec0fd8d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:05 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 12.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.63, Accepted throughput: 7.50 tokens/s, Drafted throughput: 13.80 tokens/s, Accepted: 75 tokens, Drafted: 138 tokens, Per-position acceptance rate: 0.717, 0.543, 0.370, Avg Draft acceptance rate: 54.3%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:15 [loggers.py:257] Engine 000: Avg prompt throughput: 607.7 tokens/s, Avg generation throughput: 1877.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 1063.32 tokens/s, Drafted throughput: 2434.92 tokens/s, Accepted: 10634 tokens, Drafted: 24351 tokens, Per-position acceptance rate: 0.666, 0.408, 0.237, Avg Draft acceptance rate: 43.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  11.14     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              7.18      
Output token throughput (tok/s):         1834.66   
Peak output token throughput (tok/s):    1344.00   
Peak concurrent requests:                78.00     
Total token throughput (tok/s):          2380.34   
---------------Time to First Token----------------
Mean TTFT (ms):                          176.19    
Median TTFT (ms):                        181.10    
P99 TTFT (ms):                           246.90    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          20.68     
Median TPOT (ms):                        20.78     
P99 TPOT (ms):                           24.76     
---------------Inter-token Latency----------------
Mean ITL (ms):                           47.38     
Median ITL (ms):                         47.06     
P99 ITL (ms):                            94.12     
---------------Speculative Decoding---------------
Acceptance rate (%):                     43.27     
Acceptance length:                       2.30      
Drafts:                                  8886      
Draft tokens:                            26658     
Accepted tokens:                         11534     
Per-position acceptance (%):
  Position 0:                            66.35     
  Position 1:                            40.25     
  Position 2:                            23.19     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 178.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 98.69 tokens/s, Drafted throughput: 245.66 tokens/s, Accepted: 987 tokens, Drafted: 2457 tokens, Per-position acceptance rate: 0.647, 0.358, 0.200, Avg Draft acceptance rate: 40.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f1f6adfafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15022, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-13ea5cb1-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 5.20 tokens/s, Drafted throughput: 8.10 tokens/s, Accepted: 52 tokens, Drafted: 81 tokens, Per-position acceptance rate: 0.778, 0.667, 0.481, Avg Draft acceptance rate: 64.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:45 [loggers.py:257] Engine 000: Avg prompt throughput: 607.7 tokens/s, Avg generation throughput: 2049.2 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 1155.25 tokens/s, Drafted throughput: 2680.67 tokens/s, Accepted: 11553 tokens, Drafted: 26808 tokens, Per-position acceptance rate: 0.663, 0.397, 0.233, Avg Draft acceptance rate: 43.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  7.98      
Total input tokens:                      6078      
Total generated tokens:                  20439     
Request throughput (req/s):              10.02     
Output token throughput (tok/s):         2560.25   
Peak output token throughput (tok/s):    1520.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          3321.60   
---------------Time to First Token----------------
Mean TTFT (ms):                          197.67    
Median TTFT (ms):                        183.81    
P99 TTFT (ms):                           342.44    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.83     
Median TPOT (ms):                        23.84     
P99 TPOT (ms):                           27.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.31     
Median ITL (ms):                         53.68     
P99 ITL (ms):                            125.62    
---------------Speculative Decoding---------------
Acceptance rate (%):                     42.97     
Acceptance length:                       2.29      
Drafts:                                  8931      
Draft tokens:                            26793     
Accepted tokens:                         11512     
Per-position acceptance (%):
  Position 0:                            66.17     
  Position 1:                            39.60     
  Position 2:                            23.12     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k3-t0.0-tp1...
[0;36m(APIServer pid=922122)[0;0m INFO 01-23 12:55:46 [launcher.py:110] Shutting down FastAPI HTTP server.
