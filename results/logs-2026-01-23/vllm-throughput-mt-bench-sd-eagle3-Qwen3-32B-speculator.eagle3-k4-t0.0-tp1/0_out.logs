Removing any existing container named vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k4-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k4-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2774429
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:49 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:49 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15023, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 4, 'max_model_len': 5000}}
[0;36m(APIServer pid=2774429)[0;0m WARNING 01-23 12:56:49 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:50 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:50 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:51 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:51 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:51 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:51 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:56:51 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f68ce2c6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b071cc59-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:56:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:56:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:57:03 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=4), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:57:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:57:04 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:39187 backend=nccl
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:57:04 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2774762)[0;0m WARNING 01-23 12:57:04 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:57:04 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:57:05 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:57:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:57:59 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:58:04 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:05 [default_loader.py:291] Loading weights took 58.05 seconds
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:05 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:05 [weight_utils.py:550] No model.safetensors.index.json found in remote.
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:08 [default_loader.py:291] Loading weights took 2.38 seconds
WARNING 01-23 12:58:09 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:10 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:11 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:12 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 66.305642 seconds
WARNING 01-23 12:58:14 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:58:19 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:58:24 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:24 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:24 [backends.py:704] Dynamo bytecode transform time: 11.88 s
WARNING 01-23 12:58:29 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:58:34 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:58:39 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 2.348 s
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:39 [monitor.py:34] torch.compile takes 14.23 s in total
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:40 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:40 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:40 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.061 s
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:40 [monitor.py:34] torch.compile takes 14.76 s in total
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:41 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:41 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:41 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 12:58:44 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
WARNING 01-23 12:58:49 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:52 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.67 GiB
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:52 [core.py:272] init engine (profile, create kv cache, warmup model) took 40.54 seconds
WARNING 01-23 12:58:54 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15023)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15023 ssl:default [Connect call failed (\'127.0.0.1\', 15023)]\n''
[0;36m(EngineCore_DP0 pid=2774762)[0;0m INFO 01-23 12:58:54 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:54 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2774429)[0;0m WARNING 01-23 12:58:54 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:54 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:54 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:54 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [serving.py:221] Chat template warmup completed in 1672.2ms
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15023
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:58:56 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:07 [loggers.py:257] Engine 000: Avg prompt throughput: 30.7 tokens/s, Avg generation throughput: 41.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.89, Accepted throughput: 27.18 tokens/s, Drafted throughput: 57.60 tokens/s, Accepted: 353 tokens, Drafted: 748 tokens, Per-position acceptance rate: 0.674, 0.519, 0.390, 0.305, Avg Draft acceptance rate: 47.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:17 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.04, Accepted throughput: 26.80 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 268 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.572, 0.292, 0.125, 0.054, Avg Draft acceptance rate: 26.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:27 [loggers.py:257] Engine 000: Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 36.50 tokens/s, Drafted throughput: 103.20 tokens/s, Accepted: 365 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.667, 0.403, 0.236, 0.109, Avg Draft acceptance rate: 35.4%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:37 [loggers.py:257] Engine 000: Avg prompt throughput: 13.9 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.56, Accepted throughput: 39.89 tokens/s, Drafted throughput: 102.38 tokens/s, Accepted: 399 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.707, 0.477, 0.246, 0.129, Avg Draft acceptance rate: 39.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:47 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 54.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.12, Accepted throughput: 29.00 tokens/s, Drafted throughput: 103.59 tokens/s, Accepted: 290 tokens, Drafted: 1036 tokens, Per-position acceptance rate: 0.587, 0.301, 0.166, 0.066, Avg Draft acceptance rate: 28.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:57 [loggers.py:257] Engine 000: Avg prompt throughput: 41.3 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 12:59:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 37.70 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 377 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.689, 0.405, 0.226, 0.148, Avg Draft acceptance rate: 36.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:07 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 32.70 tokens/s, Drafted throughput: 102.80 tokens/s, Accepted: 327 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.630, 0.366, 0.191, 0.086, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:17 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 34.30 tokens/s, Drafted throughput: 103.19 tokens/s, Accepted: 343 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.659, 0.345, 0.221, 0.105, Avg Draft acceptance rate: 33.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:27 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 56.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.19, Accepted throughput: 30.60 tokens/s, Drafted throughput: 103.20 tokens/s, Accepted: 306 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.578, 0.357, 0.182, 0.070, Avg Draft acceptance rate: 29.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:37 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 33.30 tokens/s, Drafted throughput: 103.59 tokens/s, Accepted: 333 tokens, Drafted: 1036 tokens, Per-position acceptance rate: 0.672, 0.340, 0.193, 0.081, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:47 [loggers.py:257] Engine 000: Avg prompt throughput: 25.6 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 39.60 tokens/s, Drafted throughput: 101.99 tokens/s, Accepted: 396 tokens, Drafted: 1020 tokens, Per-position acceptance rate: 0.678, 0.455, 0.259, 0.161, Avg Draft acceptance rate: 38.8%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:57 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:00:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 33.70 tokens/s, Drafted throughput: 102.80 tokens/s, Accepted: 337 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.638, 0.362, 0.202, 0.109, Avg Draft acceptance rate: 32.8%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:07 [loggers.py:257] Engine 000: Avg prompt throughput: 43.5 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 38.70 tokens/s, Drafted throughput: 102.80 tokens/s, Accepted: 387 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.689, 0.436, 0.253, 0.128, Avg Draft acceptance rate: 37.6%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:17 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 38.20 tokens/s, Drafted throughput: 103.19 tokens/s, Accepted: 382 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.713, 0.407, 0.229, 0.132, Avg Draft acceptance rate: 37.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:27 [loggers.py:257] Engine 000: Avg prompt throughput: 16.6 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 34.80 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 348 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.665, 0.346, 0.218, 0.125, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:37 [loggers.py:257] Engine 000: Avg prompt throughput: 26.7 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 38.80 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 388 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.673, 0.444, 0.265, 0.128, Avg Draft acceptance rate: 37.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:47 [loggers.py:257] Engine 000: Avg prompt throughput: 14.5 tokens/s, Avg generation throughput: 67.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.63, Accepted throughput: 41.80 tokens/s, Drafted throughput: 102.40 tokens/s, Accepted: 418 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.738, 0.488, 0.301, 0.105, Avg Draft acceptance rate: 40.8%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:57 [loggers.py:257] Engine 000: Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:01:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.43, Accepted throughput: 37.00 tokens/s, Drafted throughput: 103.60 tokens/s, Accepted: 370 tokens, Drafted: 1036 tokens, Per-position acceptance rate: 0.680, 0.367, 0.251, 0.131, Avg Draft acceptance rate: 35.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:07 [loggers.py:257] Engine 000: Avg prompt throughput: 28.2 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.68, Accepted throughput: 43.10 tokens/s, Drafted throughput: 102.40 tokens/s, Accepted: 431 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.707, 0.469, 0.301, 0.207, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:17 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 29.90 tokens/s, Drafted throughput: 103.20 tokens/s, Accepted: 299 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.593, 0.341, 0.163, 0.062, Avg Draft acceptance rate: 29.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:27 [loggers.py:257] Engine 000: Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 33.50 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 335 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.634, 0.342, 0.206, 0.121, Avg Draft acceptance rate: 32.6%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:37 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 61.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 35.50 tokens/s, Drafted throughput: 102.40 tokens/s, Accepted: 355 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.613, 0.406, 0.230, 0.137, Avg Draft acceptance rate: 34.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:47 [loggers.py:257] Engine 000: Avg prompt throughput: 28.9 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 38.90 tokens/s, Drafted throughput: 103.59 tokens/s, Accepted: 389 tokens, Drafted: 1036 tokens, Per-position acceptance rate: 0.683, 0.402, 0.278, 0.139, Avg Draft acceptance rate: 37.5%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:57 [loggers.py:257] Engine 000: Avg prompt throughput: 17.7 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:02:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 34.20 tokens/s, Drafted throughput: 102.40 tokens/s, Accepted: 342 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.664, 0.379, 0.184, 0.109, Avg Draft acceptance rate: 33.4%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:07 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 31.30 tokens/s, Drafted throughput: 103.59 tokens/s, Accepted: 313 tokens, Drafted: 1036 tokens, Per-position acceptance rate: 0.606, 0.336, 0.174, 0.093, Avg Draft acceptance rate: 30.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:17 [loggers.py:257] Engine 000: Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 33.60 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 336 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.626, 0.370, 0.210, 0.101, Avg Draft acceptance rate: 32.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:27 [loggers.py:257] Engine 000: Avg prompt throughput: 26.4 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 32.80 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 328 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.626, 0.323, 0.206, 0.121, Avg Draft acceptance rate: 31.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:37 [loggers.py:257] Engine 000: Avg prompt throughput: 17.7 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 30.90 tokens/s, Drafted throughput: 103.19 tokens/s, Accepted: 309 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.605, 0.310, 0.182, 0.101, Avg Draft acceptance rate: 29.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:47 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 70.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 44.20 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 442 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.743, 0.479, 0.323, 0.175, Avg Draft acceptance rate: 43.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:57 [loggers.py:257] Engine 000: Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:03:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 35.10 tokens/s, Drafted throughput: 103.20 tokens/s, Accepted: 351 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.651, 0.415, 0.217, 0.078, Avg Draft acceptance rate: 34.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:07 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 33.50 tokens/s, Drafted throughput: 103.19 tokens/s, Accepted: 335 tokens, Drafted: 1032 tokens, Per-position acceptance rate: 0.636, 0.372, 0.205, 0.085, Avg Draft acceptance rate: 32.5%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:17 [loggers.py:257] Engine 000: Avg prompt throughput: 65.9 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.68, Accepted throughput: 43.09 tokens/s, Drafted throughput: 102.39 tokens/s, Accepted: 431 tokens, Drafted: 1024 tokens, Per-position acceptance rate: 0.695, 0.480, 0.316, 0.191, Avg Draft acceptance rate: 42.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:27 [loggers.py:257] Engine 000: Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 39.70 tokens/s, Drafted throughput: 102.79 tokens/s, Accepted: 397 tokens, Drafted: 1028 tokens, Per-position acceptance rate: 0.700, 0.444, 0.276, 0.125, Avg Draft acceptance rate: 38.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  333.67    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.24      
Output token throughput (tok/s):         61.38     
Peak output token throughput (tok/s):    27.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          79.59     
---------------Time to First Token----------------
Mean TTFT (ms):                          68.64     
Median TTFT (ms):                        79.15     
P99 TTFT (ms):                           92.39     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.05     
Median TPOT (ms):                        16.04     
P99 TPOT (ms):                           19.21     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.22     
Median ITL (ms):                         38.24     
P99 ITL (ms):                            38.74     
---------------Speculative Decoding---------------
Acceptance rate (%):                     34.77     
Acceptance length:                       2.39      
Drafts:                                  8564      
Draft tokens:                            34256     
Accepted tokens:                         11912     
Per-position acceptance (%):
  Position 0:                            65.59     
  Position 1:                            39.05     
  Position 2:                            22.69     
  Position 3:                            11.77     
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:37 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 52.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 29.90 tokens/s, Drafted throughput: 92.80 tokens/s, Accepted: 299 tokens, Drafted: 928 tokens, Per-position acceptance rate: 0.616, 0.384, 0.185, 0.103, Avg Draft acceptance rate: 32.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f343a00efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-831c99c2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:57 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 23.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:04:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.79, Accepted throughput: 7.50 tokens/s, Drafted throughput: 16.80 tokens/s, Accepted: 150 tokens, Drafted: 336 tokens, Per-position acceptance rate: 0.655, 0.500, 0.357, 0.274, Avg Draft acceptance rate: 44.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:07 [loggers.py:257] Engine 000: Avg prompt throughput: 56.4 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 63.29 tokens/s, Drafted throughput: 185.58 tokens/s, Accepted: 633 tokens, Drafted: 1856 tokens, Per-position acceptance rate: 0.634, 0.375, 0.220, 0.136, Avg Draft acceptance rate: 34.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:17 [loggers.py:257] Engine 000: Avg prompt throughput: 30.1 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 76.29 tokens/s, Drafted throughput: 201.18 tokens/s, Accepted: 763 tokens, Drafted: 2012 tokens, Per-position acceptance rate: 0.700, 0.443, 0.252, 0.121, Avg Draft acceptance rate: 37.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:27 [loggers.py:257] Engine 000: Avg prompt throughput: 63.9 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 69.79 tokens/s, Drafted throughput: 201.98 tokens/s, Accepted: 698 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.661, 0.392, 0.216, 0.113, Avg Draft acceptance rate: 34.6%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:37 [loggers.py:257] Engine 000: Avg prompt throughput: 11.9 tokens/s, Avg generation throughput: 116.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 65.40 tokens/s, Drafted throughput: 203.20 tokens/s, Accepted: 654 tokens, Drafted: 2032 tokens, Per-position acceptance rate: 0.640, 0.352, 0.197, 0.098, Avg Draft acceptance rate: 32.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:47 [loggers.py:257] Engine 000: Avg prompt throughput: 30.7 tokens/s, Avg generation throughput: 119.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 68.39 tokens/s, Drafted throughput: 201.98 tokens/s, Accepted: 684 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.638, 0.374, 0.222, 0.121, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:57 [loggers.py:257] Engine 000: Avg prompt throughput: 49.7 tokens/s, Avg generation throughput: 121.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:05:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 70.99 tokens/s, Drafted throughput: 201.18 tokens/s, Accepted: 710 tokens, Drafted: 2012 tokens, Per-position acceptance rate: 0.660, 0.398, 0.237, 0.117, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:07 [loggers.py:257] Engine 000: Avg prompt throughput: 28.4 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.61, Accepted throughput: 81.30 tokens/s, Drafted throughput: 201.99 tokens/s, Accepted: 813 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.741, 0.442, 0.271, 0.156, Avg Draft acceptance rate: 40.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:17 [loggers.py:257] Engine 000: Avg prompt throughput: 39.9 tokens/s, Avg generation throughput: 124.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 73.39 tokens/s, Drafted throughput: 201.98 tokens/s, Accepted: 734 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.673, 0.406, 0.253, 0.121, Avg Draft acceptance rate: 36.3%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:27 [loggers.py:257] Engine 000: Avg prompt throughput: 34.4 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 81.30 tokens/s, Drafted throughput: 202.79 tokens/s, Accepted: 813 tokens, Drafted: 2028 tokens, Per-position acceptance rate: 0.714, 0.456, 0.292, 0.142, Avg Draft acceptance rate: 40.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:37 [loggers.py:257] Engine 000: Avg prompt throughput: 21.1 tokens/s, Avg generation throughput: 114.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 63.89 tokens/s, Drafted throughput: 201.97 tokens/s, Accepted: 639 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.608, 0.364, 0.190, 0.103, Avg Draft acceptance rate: 31.6%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:47 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 124.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 73.20 tokens/s, Drafted throughput: 203.19 tokens/s, Accepted: 732 tokens, Drafted: 2032 tokens, Per-position acceptance rate: 0.663, 0.409, 0.240, 0.128, Avg Draft acceptance rate: 36.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:57 [loggers.py:257] Engine 000: Avg prompt throughput: 51.4 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:06:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 66.79 tokens/s, Drafted throughput: 201.98 tokens/s, Accepted: 668 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.648, 0.372, 0.196, 0.107, Avg Draft acceptance rate: 33.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:07 [loggers.py:257] Engine 000: Avg prompt throughput: 20.6 tokens/s, Avg generation throughput: 117.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 66.40 tokens/s, Drafted throughput: 203.19 tokens/s, Accepted: 664 tokens, Drafted: 2032 tokens, Per-position acceptance rate: 0.632, 0.354, 0.211, 0.110, Avg Draft acceptance rate: 32.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:17 [loggers.py:257] Engine 000: Avg prompt throughput: 39.7 tokens/s, Avg generation throughput: 121.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 71.20 tokens/s, Drafted throughput: 201.99 tokens/s, Accepted: 712 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.648, 0.388, 0.240, 0.135, Avg Draft acceptance rate: 35.2%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:27 [loggers.py:257] Engine 000: Avg prompt throughput: 18.6 tokens/s, Avg generation throughput: 124.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 73.79 tokens/s, Drafted throughput: 201.98 tokens/s, Accepted: 738 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.673, 0.424, 0.248, 0.117, Avg Draft acceptance rate: 36.5%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:37 [loggers.py:257] Engine 000: Avg prompt throughput: 70.9 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 74.59 tokens/s, Drafted throughput: 201.98 tokens/s, Accepted: 746 tokens, Drafted: 2020 tokens, Per-position acceptance rate: 0.665, 0.424, 0.250, 0.139, Avg Draft acceptance rate: 36.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:47 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 59.89 tokens/s, Drafted throughput: 171.17 tokens/s, Accepted: 599 tokens, Drafted: 1712 tokens, Per-position acceptance rate: 0.661, 0.409, 0.217, 0.112, Avg Draft acceptance rate: 35.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  170.46    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.47      
Output token throughput (tok/s):         120.14    
Peak output token throughput (tok/s):    54.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          155.80    
---------------Time to First Token----------------
Mean TTFT (ms):                          117.01    
Median TTFT (ms):                        116.13    
P99 TTFT (ms):                           148.48    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.99     
Median TPOT (ms):                        16.19     
P99 TPOT (ms):                           19.38     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.45     
Median ITL (ms):                         38.42     
P99 ITL (ms):                            40.79     
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.34     
Acceptance length:                       2.41      
Drafts:                                  8486      
Draft tokens:                            33944     
Accepted tokens:                         11995     
Per-position acceptance (%):
  Position 0:                            66.18     
  Position 1:                            39.84     
  Position 2:                            23.18     
  Position 3:                            12.15     
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:57 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:07:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.71, Accepted throughput: 1.20 tokens/s, Drafted throughput: 6.80 tokens/s, Accepted: 12 tokens, Drafted: 68 tokens, Per-position acceptance rate: 0.471, 0.235, 0.000, 0.000, Avg Draft acceptance rate: 17.6%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8d11c16fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-19ba1e91-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:07 [loggers.py:257] Engine 000: Avg prompt throughput: 36.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.83, Accepted throughput: 17.60 tokens/s, Drafted throughput: 38.40 tokens/s, Accepted: 176 tokens, Drafted: 384 tokens, Per-position acceptance rate: 0.667, 0.510, 0.375, 0.281, Avg Draft acceptance rate: 45.8%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:17 [loggers.py:257] Engine 000: Avg prompt throughput: 114.8 tokens/s, Avg generation throughput: 243.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 145.79 tokens/s, Drafted throughput: 389.57 tokens/s, Accepted: 1458 tokens, Drafted: 3896 tokens, Per-position acceptance rate: 0.672, 0.444, 0.251, 0.130, Avg Draft acceptance rate: 37.4%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:27 [loggers.py:257] Engine 000: Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 227.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 127.19 tokens/s, Drafted throughput: 399.98 tokens/s, Accepted: 1272 tokens, Drafted: 4000 tokens, Per-position acceptance rate: 0.625, 0.359, 0.193, 0.095, Avg Draft acceptance rate: 31.8%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:37 [loggers.py:257] Engine 000: Avg prompt throughput: 77.7 tokens/s, Avg generation throughput: 233.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.33, Accepted throughput: 133.19 tokens/s, Drafted throughput: 400.37 tokens/s, Accepted: 1332 tokens, Drafted: 4004 tokens, Per-position acceptance rate: 0.657, 0.366, 0.206, 0.102, Avg Draft acceptance rate: 33.3%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:47 [loggers.py:257] Engine 000: Avg prompt throughput: 68.3 tokens/s, Avg generation throughput: 250.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 151.09 tokens/s, Drafted throughput: 397.18 tokens/s, Accepted: 1511 tokens, Drafted: 3972 tokens, Per-position acceptance rate: 0.708, 0.423, 0.253, 0.138, Avg Draft acceptance rate: 38.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:57 [loggers.py:257] Engine 000: Avg prompt throughput: 55.5 tokens/s, Avg generation throughput: 239.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:08:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 139.68 tokens/s, Drafted throughput: 397.95 tokens/s, Accepted: 1397 tokens, Drafted: 3980 tokens, Per-position acceptance rate: 0.652, 0.404, 0.232, 0.116, Avg Draft acceptance rate: 35.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:07 [loggers.py:257] Engine 000: Avg prompt throughput: 64.6 tokens/s, Avg generation throughput: 241.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 141.39 tokens/s, Drafted throughput: 400.36 tokens/s, Accepted: 1414 tokens, Drafted: 4004 tokens, Per-position acceptance rate: 0.670, 0.408, 0.217, 0.118, Avg Draft acceptance rate: 35.3%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:17 [loggers.py:257] Engine 000: Avg prompt throughput: 60.3 tokens/s, Avg generation throughput: 226.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.27, Accepted throughput: 126.39 tokens/s, Drafted throughput: 398.78 tokens/s, Accepted: 1264 tokens, Drafted: 3988 tokens, Per-position acceptance rate: 0.614, 0.345, 0.206, 0.103, Avg Draft acceptance rate: 31.7%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:27 [loggers.py:257] Engine 000: Avg prompt throughput: 89.5 tokens/s, Avg generation throughput: 253.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 154.50 tokens/s, Drafted throughput: 397.60 tokens/s, Accepted: 1545 tokens, Drafted: 3976 tokens, Per-position acceptance rate: 0.697, 0.445, 0.268, 0.145, Avg Draft acceptance rate: 38.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  88.50     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.90      
Output token throughput (tok/s):         231.41    
Peak output token throughput (tok/s):    104.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          300.09    
---------------Time to First Token----------------
Mean TTFT (ms):                          118.62    
Median TTFT (ms):                        117.73    
P99 TTFT (ms):                           132.00    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.27     
Median TPOT (ms):                        16.22     
P99 TPOT (ms):                           19.44     
---------------Inter-token Latency----------------
Mean ITL (ms):                           38.99     
Median ITL (ms):                         38.93     
P99 ITL (ms):                            46.67     
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.09     
Acceptance length:                       2.40      
Drafts:                                  8516      
Draft tokens:                            34064     
Accepted tokens:                         11954     
Per-position acceptance (%):
  Position 0:                            66.10     
  Position 1:                            39.81     
  Position 2:                            22.70     
  Position 3:                            11.77     
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:37 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 75.19 tokens/s, Drafted throughput: 221.58 tokens/s, Accepted: 752 tokens, Drafted: 2216 tokens, Per-position acceptance rate: 0.648, 0.386, 0.213, 0.110, Avg Draft acceptance rate: 33.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2dfc0f6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c3f8f500-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:57 [loggers.py:257] Engine 000: Avg prompt throughput: 94.1 tokens/s, Avg generation throughput: 164.2 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:09:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.58, Accepted throughput: 50.05 tokens/s, Drafted throughput: 126.59 tokens/s, Accepted: 1001 tokens, Drafted: 2532 tokens, Per-position acceptance rate: 0.678, 0.450, 0.289, 0.164, Avg Draft acceptance rate: 39.5%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:07 [loggers.py:257] Engine 000: Avg prompt throughput: 121.2 tokens/s, Avg generation throughput: 438.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 246.45 tokens/s, Drafted throughput: 766.64 tokens/s, Accepted: 2465 tokens, Drafted: 7668 tokens, Per-position acceptance rate: 0.640, 0.365, 0.191, 0.091, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:17 [loggers.py:257] Engine 000: Avg prompt throughput: 152.5 tokens/s, Avg generation throughput: 462.9 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 272.97 tokens/s, Drafted throughput: 760.71 tokens/s, Accepted: 2730 tokens, Drafted: 7608 tokens, Per-position acceptance rate: 0.680, 0.406, 0.233, 0.116, Avg Draft acceptance rate: 35.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:27 [loggers.py:257] Engine 000: Avg prompt throughput: 105.0 tokens/s, Avg generation throughput: 453.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 261.56 tokens/s, Drafted throughput: 767.89 tokens/s, Accepted: 2616 tokens, Drafted: 7680 tokens, Per-position acceptance rate: 0.639, 0.381, 0.218, 0.125, Avg Draft acceptance rate: 34.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:37 [loggers.py:257] Engine 000: Avg prompt throughput: 153.0 tokens/s, Avg generation throughput: 457.3 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 267.66 tokens/s, Drafted throughput: 762.70 tokens/s, Accepted: 2677 tokens, Drafted: 7628 tokens, Per-position acceptance rate: 0.658, 0.391, 0.235, 0.121, Avg Draft acceptance rate: 35.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  48.11     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.66      
Output token throughput (tok/s):         425.72    
Peak output token throughput (tok/s):    200.00    
Peak concurrent requests:                14.00     
Total token throughput (tok/s):          552.07    
---------------Time to First Token----------------
Mean TTFT (ms):                          123.21    
Median TTFT (ms):                        122.36    
P99 TTFT (ms):                           159.91    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.16     
Median TPOT (ms):                        17.24     
P99 TPOT (ms):                           20.48     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.61     
Median ITL (ms):                         40.51     
P99 ITL (ms):                            51.99     
---------------Speculative Decoding---------------
Acceptance rate (%):                     34.41     
Acceptance length:                       2.38      
Drafts:                                  8622      
Draft tokens:                            34488     
Accepted tokens:                         11866     
Per-position acceptance (%):
  Position 0:                            65.37     
  Position 1:                            38.77     
  Position 2:                            22.12     
  Position 3:                            11.37     
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 54.40 tokens/s, Drafted throughput: 172.79 tokens/s, Accepted: 544 tokens, Drafted: 1728 tokens, Per-position acceptance rate: 0.620, 0.361, 0.190, 0.088, Avg Draft acceptance rate: 31.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f146120afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-2cc1f4de-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:57 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:10:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 1.10 tokens/s, Drafted throughput: 3.20 tokens/s, Accepted: 11 tokens, Drafted: 32 tokens, Per-position acceptance rate: 0.625, 0.500, 0.250, 0.000, Avg Draft acceptance rate: 34.4%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:07 [loggers.py:257] Engine 000: Avg prompt throughput: 257.8 tokens/s, Avg generation throughput: 560.9 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 329.64 tokens/s, Drafted throughput: 917.42 tokens/s, Accepted: 3297 tokens, Drafted: 9176 tokens, Per-position acceptance rate: 0.665, 0.409, 0.235, 0.129, Avg Draft acceptance rate: 35.9%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:17 [loggers.py:257] Engine 000: Avg prompt throughput: 212.2 tokens/s, Avg generation throughput: 880.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 517.42 tokens/s, Drafted throughput: 1452.58 tokens/s, Accepted: 5175 tokens, Drafted: 14528 tokens, Per-position acceptance rate: 0.661, 0.399, 0.237, 0.127, Avg Draft acceptance rate: 35.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  26.77     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              2.99      
Output token throughput (tok/s):         765.04    
Peak output token throughput (tok/s):    382.00    
Peak concurrent requests:                26.00     
Total token throughput (tok/s):          992.08    
---------------Time to First Token----------------
Mean TTFT (ms):                          130.35    
Median TTFT (ms):                        128.21    
P99 TTFT (ms):                           192.92    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.91     
Median TPOT (ms):                        18.09     
P99 TPOT (ms):                           21.59     
---------------Inter-token Latency----------------
Mean ITL (ms):                           42.91     
Median ITL (ms):                         42.92     
P99 ITL (ms):                            53.85     
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.15     
Acceptance length:                       2.41      
Drafts:                                  8516      
Draft tokens:                            34064     
Accepted tokens:                         11972     
Per-position acceptance (%):
  Position 0:                            65.68     
  Position 1:                            39.69     
  Position 2:                            23.02     
  Position 3:                            12.20     
==================================================
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:27 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 629.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 365.58 tokens/s, Drafted throughput: 1068.33 tokens/s, Accepted: 3656 tokens, Drafted: 10684 tokens, Per-position acceptance rate: 0.644, 0.387, 0.222, 0.115, Avg Draft acceptance rate: 34.2%
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fba0353afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-5b63d515-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:47 [loggers.py:257] Engine 000: Avg prompt throughput: 275.8 tokens/s, Avg generation throughput: 132.3 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:47 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 41.25 tokens/s, Drafted throughput: 93.19 tokens/s, Accepted: 825 tokens, Drafted: 1864 tokens, Per-position acceptance rate: 0.721, 0.494, 0.350, 0.206, Avg Draft acceptance rate: 44.3%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:57 [loggers.py:257] Engine 000: Avg prompt throughput: 349.8 tokens/s, Avg generation throughput: 1629.7 tokens/s, Running: 23 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:11:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 951.98 tokens/s, Drafted throughput: 2712.47 tokens/s, Accepted: 9521 tokens, Drafted: 27128 tokens, Per-position acceptance rate: 0.665, 0.396, 0.224, 0.119, Avg Draft acceptance rate: 35.1%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  16.12     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              4.96      
Output token throughput (tok/s):         1267.59   
Peak output token throughput (tok/s):    736.00    
Peak concurrent requests:                48.00     
Total token throughput (tok/s):          1644.61   
---------------Time to First Token----------------
Mean TTFT (ms):                          144.71    
Median TTFT (ms):                        143.31    
P99 TTFT (ms):                           217.78    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.88     
Median TPOT (ms):                        18.67     
P99 TPOT (ms):                           23.15     
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.46     
Median ITL (ms):                         44.84     
P99 ITL (ms):                            80.12     
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.42     
Acceptance length:                       2.42      
Drafts:                                  8453      
Draft tokens:                            33812     
Accepted tokens:                         11976     
Per-position acceptance (%):
  Position 0:                            66.54     
  Position 1:                            39.94     
  Position 2:                            22.80     
  Position 3:                            12.40     
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:07 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 306.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:07 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 179.70 tokens/s, Drafted throughput: 517.60 tokens/s, Accepted: 1797 tokens, Drafted: 5176 tokens, Per-position acceptance rate: 0.650, 0.391, 0.217, 0.131, Avg Draft acceptance rate: 34.7%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f50b04c2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-dc44e772-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:17 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:17 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 0.90 tokens/s, Drafted throughput: 2.80 tokens/s, Accepted: 9 tokens, Drafted: 28 tokens, Per-position acceptance rate: 0.571, 0.429, 0.286, 0.000, Avg Draft acceptance rate: 32.1%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:27 [loggers.py:257] Engine 000: Avg prompt throughput: 607.6 tokens/s, Avg generation throughput: 1670.8 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:27 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 976.62 tokens/s, Drafted throughput: 2754.94 tokens/s, Accepted: 9770 tokens, Drafted: 27560 tokens, Per-position acceptance rate: 0.662, 0.398, 0.232, 0.127, Avg Draft acceptance rate: 35.4%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  12.06     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.63      
Output token throughput (tok/s):         1698.09   
Peak output token throughput (tok/s):    1216.00   
Peak concurrent requests:                73.00     
Total token throughput (tok/s):          2202.05   
---------------Time to First Token----------------
Mean TTFT (ms):                          190.80    
Median TTFT (ms):                        184.60    
P99 TTFT (ms):                           286.17    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.36     
Median TPOT (ms):                        22.61     
P99 TPOT (ms):                           27.35     
---------------Inter-token Latency----------------
Mean ITL (ms):                           53.34     
Median ITL (ms):                         53.93     
P99 ITL (ms):                            112.92    
---------------Speculative Decoding---------------
Acceptance rate (%):                     34.84     
Acceptance length:                       2.39      
Drafts:                                  8552      
Draft tokens:                            34208     
Accepted tokens:                         11919     
Per-position acceptance (%):
  Position 0:                            65.49     
  Position 1:                            39.17     
  Position 2:                            22.49     
  Position 3:                            12.22     
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:37 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 400.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:37 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 230.70 tokens/s, Drafted throughput: 697.59 tokens/s, Accepted: 2307 tokens, Drafted: 6976 tokens, Per-position acceptance rate: 0.630, 0.372, 0.206, 0.115, Avg Draft acceptance rate: 33.1%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:47 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f8d4f98afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15023, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6e5389ab-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:57 [loggers.py:257] Engine 000: Avg prompt throughput: 625.8 tokens/s, Avg generation throughput: 1458.0 tokens/s, Running: 80 reqs, Waiting: 0 reqs, GPU KV cache usage: 46.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:12:57 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 431.58 tokens/s, Drafted throughput: 1173.55 tokens/s, Accepted: 8632 tokens, Drafted: 23472 tokens, Per-position acceptance rate: 0.674, 0.419, 0.245, 0.133, Avg Draft acceptance rate: 36.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  9.51      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              8.41      
Output token throughput (tok/s):         2153.24   
Peak output token throughput (tok/s):    1150.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2792.28   
---------------Time to First Token----------------
Mean TTFT (ms):                          219.60    
Median TTFT (ms):                        204.00    
P99 TTFT (ms):                           418.32    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          29.07     
Median TPOT (ms):                        29.28     
P99 TPOT (ms):                           33.54     
---------------Inter-token Latency----------------
Mean ITL (ms):                           69.81     
Median ITL (ms):                         73.45     
P99 ITL (ms):                            169.84    
---------------Speculative Decoding---------------
Acceptance rate (%):                     35.31     
Acceptance length:                       2.41      
Drafts:                                  8495      
Draft tokens:                            33980     
Accepted tokens:                         11998     
Per-position acceptance (%):
  Position 0:                            66.29     
  Position 1:                            40.08     
  Position 2:                            22.77     
  Position 3:                            12.10     
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k4-t0.0-tp1...
[0;36m(APIServer pid=2774429)[0;0m INFO 01-23 13:13:01 [launcher.py:110] Shutting down FastAPI HTTP server.
