Removing any existing container named vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k5-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k5-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 2774242
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:49 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:49 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15024, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 5, 'max_model_len': 5000}}
[0;36m(APIServer pid=2774242)[0;0m WARNING 01-23 12:56:49 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:50 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:50 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:51 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:51 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:51 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:51 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:56:51 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f30a7be6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-11b11421-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 12:56:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:56:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:57:02 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=5), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
WARNING 01-23 12:57:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:57:03 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.46:45149 backend=nccl
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:57:03 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2774711)[0;0m WARNING 01-23 12:57:04 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:57:04 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:57:05 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 12:57:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:57:58 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:00 [default_loader.py:291] Loading weights took 53.91 seconds
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:00 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:01 [weight_utils.py:510] Time spent downloading weights for RedHatAI/Qwen3-32B-speculator.eagle3: 0.503871 seconds
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:02 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 12:58:03 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:04 [default_loader.py:291] Loading weights took 1.89 seconds
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:06 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:07 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:08 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 62.331098 seconds
WARNING 01-23 12:58:08 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:58:13 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:58:18 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:20 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:20 [backends.py:704] Dynamo bytecode transform time: 11.85 s
WARNING 01-23 12:58:23 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:58:28 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:58:33 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:35 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 2.456 s
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:35 [monitor.py:34] torch.compile takes 14.31 s in total
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:36 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:36 [backends.py:704] Dynamo bytecode transform time: 0.47 s
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:36 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.059 s
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:36 [monitor.py:34] torch.compile takes 14.84 s in total
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:38 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:38 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:38 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 12:58:38 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:58:43 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
WARNING 01-23 12:58:48 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:48 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.67 GiB
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:48 [core.py:272] init engine (profile, create kv cache, warmup model) took 40.66 seconds
[0;36m(EngineCore_DP0 pid=2774711)[0;0m INFO 01-23 12:58:50 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:50 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2774242)[0;0m WARNING 01-23 12:58:50 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:50 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:51 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:51 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:52 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:52 [serving.py:221] Chat template warmup completed in 1933.3ms
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15024
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:58:53 [launcher.py:46] Route: /pooling, Methods: POST
WARNING 01-23 12:58:53 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15024)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15024 ssl:default [Connect call failed (\'127.0.0.1\', 15024)]\n''
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:03 [loggers.py:257] Engine 000: Avg prompt throughput: 27.4 tokens/s, Avg generation throughput: 23.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.02, Accepted throughput: 15.68 tokens/s, Drafted throughput: 38.83 tokens/s, Accepted: 206 tokens, Drafted: 510 tokens, Per-position acceptance rate: 0.676, 0.520, 0.363, 0.255, 0.206, Avg Draft acceptance rate: 40.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:13 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 36.20 tokens/s, Drafted throughput: 125.48 tokens/s, Accepted: 362 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.610, 0.378, 0.215, 0.131, 0.108, Avg Draft acceptance rate: 28.8%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:23 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 31.20 tokens/s, Drafted throughput: 126.00 tokens/s, Accepted: 312 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.643, 0.329, 0.163, 0.075, 0.028, Avg Draft acceptance rate: 24.8%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:33 [loggers.py:257] Engine 000: Avg prompt throughput: 19.7 tokens/s, Avg generation throughput: 66.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.67, Accepted throughput: 41.60 tokens/s, Drafted throughput: 124.49 tokens/s, Accepted: 416 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.695, 0.482, 0.269, 0.141, 0.084, Avg Draft acceptance rate: 33.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:43 [loggers.py:257] Engine 000: Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 32.40 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 324 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.615, 0.349, 0.190, 0.087, 0.044, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:53 [loggers.py:257] Engine 000: Avg prompt throughput: 51.2 tokens/s, Avg generation throughput: 63.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 12:59:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 38.10 tokens/s, Drafted throughput: 124.50 tokens/s, Accepted: 381 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.703, 0.378, 0.217, 0.141, 0.092, Avg Draft acceptance rate: 30.6%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:03 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 33.00 tokens/s, Drafted throughput: 126.49 tokens/s, Accepted: 330 tokens, Drafted: 1265 tokens, Per-position acceptance rate: 0.625, 0.387, 0.194, 0.067, 0.032, Avg Draft acceptance rate: 26.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:13 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 57.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 32.30 tokens/s, Drafted throughput: 125.50 tokens/s, Accepted: 323 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.657, 0.343, 0.175, 0.080, 0.032, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:23 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.22, Accepted throughput: 30.80 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 308 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.587, 0.325, 0.179, 0.079, 0.052, Avg Draft acceptance rate: 24.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:33 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 60.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 35.60 tokens/s, Drafted throughput: 125.49 tokens/s, Accepted: 356 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.645, 0.386, 0.219, 0.100, 0.068, Avg Draft acceptance rate: 28.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:43 [loggers.py:257] Engine 000: Avg prompt throughput: 21.3 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 34.40 tokens/s, Drafted throughput: 125.49 tokens/s, Accepted: 344 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.641, 0.378, 0.195, 0.100, 0.056, Avg Draft acceptance rate: 27.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:53 [loggers.py:257] Engine 000: Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:00:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 37.40 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 374 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.667, 0.393, 0.226, 0.123, 0.075, Avg Draft acceptance rate: 29.7%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:03 [loggers.py:257] Engine 000: Avg prompt throughput: 41.0 tokens/s, Avg generation throughput: 57.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 32.00 tokens/s, Drafted throughput: 124.98 tokens/s, Accepted: 320 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.616, 0.352, 0.180, 0.084, 0.048, Avg Draft acceptance rate: 25.6%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:13 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.57, Accepted throughput: 39.60 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 396 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.683, 0.425, 0.250, 0.143, 0.071, Avg Draft acceptance rate: 31.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:23 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 69.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 44.30 tokens/s, Drafted throughput: 125.00 tokens/s, Accepted: 443 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.736, 0.444, 0.320, 0.188, 0.084, Avg Draft acceptance rate: 35.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:33 [loggers.py:257] Engine 000: Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 33.20 tokens/s, Drafted throughput: 125.48 tokens/s, Accepted: 332 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.641, 0.347, 0.199, 0.092, 0.044, Avg Draft acceptance rate: 26.5%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:43 [loggers.py:257] Engine 000: Avg prompt throughput: 30.5 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.61, Accepted throughput: 40.20 tokens/s, Drafted throughput: 124.50 tokens/s, Accepted: 402 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.679, 0.462, 0.289, 0.133, 0.052, Avg Draft acceptance rate: 32.3%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:53 [loggers.py:257] Engine 000: Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:01:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.58, Accepted throughput: 39.90 tokens/s, Drafted throughput: 126.49 tokens/s, Accepted: 399 tokens, Drafted: 1265 tokens, Per-position acceptance rate: 0.676, 0.439, 0.277, 0.119, 0.067, Avg Draft acceptance rate: 31.5%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:03 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 45.10 tokens/s, Drafted throughput: 124.49 tokens/s, Accepted: 451 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.739, 0.490, 0.317, 0.173, 0.092, Avg Draft acceptance rate: 36.2%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:13 [loggers.py:257] Engine 000: Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 32.50 tokens/s, Drafted throughput: 126.49 tokens/s, Accepted: 325 tokens, Drafted: 1265 tokens, Per-position acceptance rate: 0.636, 0.364, 0.166, 0.083, 0.036, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:23 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 53.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.12, Accepted throughput: 28.30 tokens/s, Drafted throughput: 126.00 tokens/s, Accepted: 283 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.556, 0.294, 0.151, 0.079, 0.044, Avg Draft acceptance rate: 22.5%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:33 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 67.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.70, Accepted throughput: 42.50 tokens/s, Drafted throughput: 124.99 tokens/s, Accepted: 425 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.676, 0.468, 0.284, 0.160, 0.112, Avg Draft acceptance rate: 34.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:43 [loggers.py:257] Engine 000: Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 35.00 tokens/s, Drafted throughput: 125.48 tokens/s, Accepted: 350 tokens, Drafted: 1255 tokens, Per-position acceptance rate: 0.625, 0.367, 0.231, 0.124, 0.048, Avg Draft acceptance rate: 27.9%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:53 [loggers.py:257] Engine 000: Avg prompt throughput: 36.6 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:02:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 38.30 tokens/s, Drafted throughput: 124.99 tokens/s, Accepted: 383 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.652, 0.400, 0.252, 0.140, 0.088, Avg Draft acceptance rate: 30.6%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:03 [loggers.py:257] Engine 000: Avg prompt throughput: 14.8 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.21, Accepted throughput: 30.40 tokens/s, Drafted throughput: 126.00 tokens/s, Accepted: 304 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.631, 0.325, 0.143, 0.071, 0.036, Avg Draft acceptance rate: 24.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:13 [loggers.py:257] Engine 000: Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.31, Accepted throughput: 32.90 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 329 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.591, 0.349, 0.210, 0.107, 0.048, Avg Draft acceptance rate: 26.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:23 [loggers.py:257] Engine 000: Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 61.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 36.90 tokens/s, Drafted throughput: 124.99 tokens/s, Accepted: 369 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.676, 0.372, 0.220, 0.128, 0.080, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:33 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.16, Accepted throughput: 29.30 tokens/s, Drafted throughput: 126.49 tokens/s, Accepted: 293 tokens, Drafted: 1265 tokens, Per-position acceptance rate: 0.569, 0.296, 0.174, 0.083, 0.036, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:43 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 37.79 tokens/s, Drafted throughput: 125.98 tokens/s, Accepted: 378 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.679, 0.385, 0.238, 0.131, 0.067, Avg Draft acceptance rate: 30.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:53 [loggers.py:257] Engine 000: Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 65.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:03:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 40.09 tokens/s, Drafted throughput: 124.98 tokens/s, Accepted: 401 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.676, 0.436, 0.280, 0.124, 0.088, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:03 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 31.50 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 315 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.639, 0.361, 0.151, 0.063, 0.036, Avg Draft acceptance rate: 25.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:13 [loggers.py:257] Engine 000: Avg prompt throughput: 54.4 tokens/s, Avg generation throughput: 66.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.65, Accepted throughput: 41.00 tokens/s, Drafted throughput: 124.49 tokens/s, Accepted: 410 tokens, Drafted: 1245 tokens, Per-position acceptance rate: 0.643, 0.442, 0.297, 0.165, 0.100, Avg Draft acceptance rate: 32.9%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:23 [loggers.py:257] Engine 000: Avg prompt throughput: 16.5 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.67, Accepted throughput: 42.10 tokens/s, Drafted throughput: 125.99 tokens/s, Accepted: 421 tokens, Drafted: 1260 tokens, Per-position acceptance rate: 0.710, 0.429, 0.294, 0.143, 0.095, Avg Draft acceptance rate: 33.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:33 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.58, Accepted throughput: 39.49 tokens/s, Drafted throughput: 124.98 tokens/s, Accepted: 395 tokens, Drafted: 1250 tokens, Per-position acceptance rate: 0.696, 0.444, 0.252, 0.120, 0.068, Avg Draft acceptance rate: 31.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  335.61    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.24      
Output token throughput (tok/s):         61.02     
Peak output token throughput (tok/s):    26.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          79.13     
---------------Time to First Token----------------
Mean TTFT (ms):                          70.27     
Median TTFT (ms):                        81.11     
P99 TTFT (ms):                           86.23     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.13     
Median TPOT (ms):                        16.19     
P99 TPOT (ms):                           19.48     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.16     
Median ITL (ms):                         39.17     
P99 ITL (ms):                            39.68     
---------------Speculative Decoding---------------
Acceptance rate (%):                     28.76     
Acceptance length:                       2.44      
Drafts:                                  8405      
Draft tokens:                            42025     
Accepted tokens:                         12085     
Per-position acceptance (%):
  Position 0:                            65.04     
  Position 1:                            38.69     
  Position 2:                            22.32     
  Position 3:                            11.37     
  Position 4:                            6.35      
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.97, Accepted throughput: 9.90 tokens/s, Drafted throughput: 51.00 tokens/s, Accepted: 99 tokens, Drafted: 510 tokens, Per-position acceptance rate: 0.539, 0.245, 0.088, 0.059, 0.039, Avg Draft acceptance rate: 19.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:04:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f7f9fbe6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-1b91224f-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:03 [loggers.py:257] Engine 000: Avg prompt throughput: 52.7 tokens/s, Avg generation throughput: 93.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.63, Accepted throughput: 28.85 tokens/s, Drafted throughput: 88.49 tokens/s, Accepted: 577 tokens, Drafted: 1770 tokens, Per-position acceptance rate: 0.658, 0.410, 0.266, 0.167, 0.130, Avg Draft acceptance rate: 32.6%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:13 [loggers.py:257] Engine 000: Avg prompt throughput: 46.0 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 73.59 tokens/s, Drafted throughput: 245.47 tokens/s, Accepted: 736 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.672, 0.422, 0.230, 0.114, 0.061, Avg Draft acceptance rate: 30.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:23 [loggers.py:257] Engine 000: Avg prompt throughput: 57.0 tokens/s, Avg generation throughput: 119.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 69.59 tokens/s, Drafted throughput: 246.97 tokens/s, Accepted: 696 tokens, Drafted: 2470 tokens, Per-position acceptance rate: 0.664, 0.381, 0.202, 0.103, 0.059, Avg Draft acceptance rate: 28.2%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:33 [loggers.py:257] Engine 000: Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 118.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 69.09 tokens/s, Drafted throughput: 245.47 tokens/s, Accepted: 691 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.650, 0.389, 0.212, 0.106, 0.051, Avg Draft acceptance rate: 28.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:43 [loggers.py:257] Engine 000: Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 113.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.29, Accepted throughput: 63.99 tokens/s, Drafted throughput: 247.97 tokens/s, Accepted: 640 tokens, Drafted: 2480 tokens, Per-position acceptance rate: 0.619, 0.349, 0.192, 0.083, 0.048, Avg Draft acceptance rate: 25.8%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:53 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:05:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 73.09 tokens/s, Drafted throughput: 245.48 tokens/s, Accepted: 731 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.664, 0.395, 0.230, 0.126, 0.073, Avg Draft acceptance rate: 29.8%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:03 [loggers.py:257] Engine 000: Avg prompt throughput: 48.6 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 68.09 tokens/s, Drafted throughput: 246.98 tokens/s, Accepted: 681 tokens, Drafted: 2470 tokens, Per-position acceptance rate: 0.654, 0.377, 0.196, 0.101, 0.051, Avg Draft acceptance rate: 27.6%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:13 [loggers.py:257] Engine 000: Avg prompt throughput: 46.9 tokens/s, Avg generation throughput: 127.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.61, Accepted throughput: 78.79 tokens/s, Drafted throughput: 243.96 tokens/s, Accepted: 788 tokens, Drafted: 2440 tokens, Per-position acceptance rate: 0.721, 0.430, 0.254, 0.143, 0.066, Avg Draft acceptance rate: 32.3%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:23 [loggers.py:257] Engine 000: Avg prompt throughput: 19.9 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.65, Accepted throughput: 81.99 tokens/s, Drafted throughput: 248.98 tokens/s, Accepted: 820 tokens, Drafted: 2490 tokens, Per-position acceptance rate: 0.693, 0.454, 0.293, 0.141, 0.066, Avg Draft acceptance rate: 32.9%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:33 [loggers.py:257] Engine 000: Avg prompt throughput: 35.7 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 68.39 tokens/s, Drafted throughput: 245.47 tokens/s, Accepted: 684 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.629, 0.383, 0.224, 0.108, 0.049, Avg Draft acceptance rate: 27.9%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:43 [loggers.py:257] Engine 000: Avg prompt throughput: 16.9 tokens/s, Avg generation throughput: 112.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 63.89 tokens/s, Drafted throughput: 245.47 tokens/s, Accepted: 639 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.597, 0.342, 0.187, 0.114, 0.061, Avg Draft acceptance rate: 26.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:53 [loggers.py:257] Engine 000: Avg prompt throughput: 39.2 tokens/s, Avg generation throughput: 123.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:06:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 74.10 tokens/s, Drafted throughput: 247.98 tokens/s, Accepted: 741 tokens, Drafted: 2480 tokens, Per-position acceptance rate: 0.661, 0.413, 0.236, 0.113, 0.071, Avg Draft acceptance rate: 29.9%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:03 [loggers.py:257] Engine 000: Avg prompt throughput: 28.5 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 66.79 tokens/s, Drafted throughput: 245.48 tokens/s, Accepted: 668 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.652, 0.387, 0.189, 0.090, 0.043, Avg Draft acceptance rate: 27.2%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:13 [loggers.py:257] Engine 000: Avg prompt throughput: 28.7 tokens/s, Avg generation throughput: 118.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 69.50 tokens/s, Drafted throughput: 247.00 tokens/s, Accepted: 695 tokens, Drafted: 2470 tokens, Per-position acceptance rate: 0.615, 0.372, 0.229, 0.117, 0.073, Avg Draft acceptance rate: 28.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:23 [loggers.py:257] Engine 000: Avg prompt throughput: 33.4 tokens/s, Avg generation throughput: 124.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 75.99 tokens/s, Drafted throughput: 244.98 tokens/s, Accepted: 760 tokens, Drafted: 2450 tokens, Per-position acceptance rate: 0.667, 0.414, 0.255, 0.141, 0.073, Avg Draft acceptance rate: 31.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:33 [loggers.py:257] Engine 000: Avg prompt throughput: 44.3 tokens/s, Avg generation throughput: 113.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 64.30 tokens/s, Drafted throughput: 246.98 tokens/s, Accepted: 643 tokens, Drafted: 2470 tokens, Per-position acceptance rate: 0.634, 0.364, 0.186, 0.077, 0.040, Avg Draft acceptance rate: 26.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:43 [loggers.py:257] Engine 000: Avg prompt throughput: 57.5 tokens/s, Avg generation throughput: 133.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 84.29 tokens/s, Drafted throughput: 245.48 tokens/s, Accepted: 843 tokens, Drafted: 2455 tokens, Per-position acceptance rate: 0.703, 0.452, 0.299, 0.163, 0.100, Avg Draft acceptance rate: 34.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  172.72    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.46      
Output token throughput (tok/s):         118.57    
Peak output token throughput (tok/s):    52.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          153.76    
---------------Time to First Token----------------
Mean TTFT (ms):                          119.25    
Median TTFT (ms):                        118.93    
P99 TTFT (ms):                           142.30    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.18     
Median TPOT (ms):                        16.19     
P99 TPOT (ms):                           19.86     
---------------Inter-token Latency----------------
Mean ITL (ms):                           39.47     
Median ITL (ms):                         39.45     
P99 ITL (ms):                            41.09     
---------------Speculative Decoding---------------
Acceptance rate (%):                     28.98     
Acceptance length:                       2.45      
Drafts:                                  8365      
Draft tokens:                            41825     
Accepted tokens:                         12122     
Per-position acceptance (%):
  Position 0:                            65.42     
  Position 1:                            39.34     
  Position 2:                            22.45     
  Position 3:                            11.45     
  Position 4:                            6.25      
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:53 [loggers.py:257] Engine 000: Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:07:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.23, Accepted throughput: 26.30 tokens/s, Drafted throughput: 107.00 tokens/s, Accepted: 263 tokens, Drafted: 1070 tokens, Per-position acceptance rate: 0.593, 0.346, 0.159, 0.075, 0.056, Avg Draft acceptance rate: 24.6%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:03 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f37893bafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-9ea68543-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:13 [loggers.py:257] Engine 000: Avg prompt throughput: 84.8 tokens/s, Avg generation throughput: 143.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.58, Accepted throughput: 44.00 tokens/s, Drafted throughput: 139.49 tokens/s, Accepted: 880 tokens, Drafted: 2790 tokens, Per-position acceptance rate: 0.665, 0.432, 0.247, 0.136, 0.097, Avg Draft acceptance rate: 31.5%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:23 [loggers.py:257] Engine 000: Avg prompt throughput: 86.6 tokens/s, Avg generation throughput: 237.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 140.19 tokens/s, Drafted throughput: 482.97 tokens/s, Accepted: 1402 tokens, Drafted: 4830 tokens, Per-position acceptance rate: 0.649, 0.400, 0.231, 0.111, 0.061, Avg Draft acceptance rate: 29.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:33 [loggers.py:257] Engine 000: Avg prompt throughput: 43.9 tokens/s, Avg generation throughput: 225.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.32, Accepted throughput: 128.59 tokens/s, Drafted throughput: 486.46 tokens/s, Accepted: 1286 tokens, Drafted: 4865 tokens, Per-position acceptance rate: 0.616, 0.355, 0.198, 0.099, 0.054, Avg Draft acceptance rate: 26.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:43 [loggers.py:257] Engine 000: Avg prompt throughput: 69.0 tokens/s, Avg generation throughput: 244.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 146.70 tokens/s, Drafted throughput: 488.00 tokens/s, Accepted: 1467 tokens, Drafted: 4880 tokens, Per-position acceptance rate: 0.668, 0.399, 0.240, 0.130, 0.067, Avg Draft acceptance rate: 30.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:53 [loggers.py:257] Engine 000: Avg prompt throughput: 86.5 tokens/s, Avg generation throughput: 244.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:08:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 148.19 tokens/s, Drafted throughput: 479.95 tokens/s, Accepted: 1482 tokens, Drafted: 4800 tokens, Per-position acceptance rate: 0.683, 0.409, 0.258, 0.130, 0.062, Avg Draft acceptance rate: 30.9%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:03 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 245.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 147.30 tokens/s, Drafted throughput: 489.98 tokens/s, Accepted: 1473 tokens, Drafted: 4900 tokens, Per-position acceptance rate: 0.661, 0.417, 0.230, 0.127, 0.068, Avg Draft acceptance rate: 30.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:13 [loggers.py:257] Engine 000: Avg prompt throughput: 72.0 tokens/s, Avg generation throughput: 232.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 135.69 tokens/s, Drafted throughput: 484.47 tokens/s, Accepted: 1357 tokens, Drafted: 4845 tokens, Per-position acceptance rate: 0.646, 0.380, 0.214, 0.108, 0.053, Avg Draft acceptance rate: 28.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:23 [loggers.py:257] Engine 000: Avg prompt throughput: 58.3 tokens/s, Avg generation throughput: 239.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 142.88 tokens/s, Drafted throughput: 484.94 tokens/s, Accepted: 1429 tokens, Drafted: 4850 tokens, Per-position acceptance rate: 0.657, 0.389, 0.232, 0.125, 0.071, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:33 [loggers.py:257] Engine 000: Avg prompt throughput: 97.8 tokens/s, Avg generation throughput: 238.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 143.60 tokens/s, Drafted throughput: 473.51 tokens/s, Accepted: 1436 tokens, Drafted: 4735 tokens, Per-position acceptance rate: 0.674, 0.413, 0.235, 0.122, 0.072, Avg Draft acceptance rate: 30.3%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  88.58     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.90      
Output token throughput (tok/s):         231.20    
Peak output token throughput (tok/s):    104.00    
Peak concurrent requests:                8.00      
Total token throughput (tok/s):          299.82    
---------------Time to First Token----------------
Mean TTFT (ms):                          120.40    
Median TTFT (ms):                        120.85    
P99 TTFT (ms):                           132.09    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.31     
Median TPOT (ms):                        16.39     
P99 TPOT (ms):                           19.71     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.02     
Median ITL (ms):                         39.96     
P99 ITL (ms):                            47.83     
---------------Speculative Decoding---------------
Acceptance rate (%):                     29.26     
Acceptance length:                       2.46      
Drafts:                                  8316      
Draft tokens:                            41580     
Accepted tokens:                         12167     
Per-position acceptance (%):
  Position 0:                            65.60     
  Position 1:                            39.55     
  Position 2:                            22.91     
  Position 3:                            11.83     
  Position 4:                            6.42      
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.28, Accepted throughput: 12.90 tokens/s, Drafted throughput: 50.49 tokens/s, Accepted: 129 tokens, Drafted: 505 tokens, Per-position acceptance rate: 0.574, 0.327, 0.198, 0.099, 0.079, Avg Draft acceptance rate: 25.5%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f0ab9722fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-02266b7b-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:53 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:09:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.71, Accepted throughput: 8.90 tokens/s, Drafted throughput: 26.00 tokens/s, Accepted: 89 tokens, Drafted: 260 tokens, Per-position acceptance rate: 0.654, 0.462, 0.288, 0.173, 0.135, Avg Draft acceptance rate: 34.2%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:03 [loggers.py:257] Engine 000: Avg prompt throughput: 156.3 tokens/s, Avg generation throughput: 387.1 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 230.05 tokens/s, Drafted throughput: 784.33 tokens/s, Accepted: 2301 tokens, Drafted: 7845 tokens, Per-position acceptance rate: 0.653, 0.396, 0.221, 0.122, 0.075, Avg Draft acceptance rate: 29.3%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:13 [loggers.py:257] Engine 000: Avg prompt throughput: 124.2 tokens/s, Avg generation throughput: 444.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 255.68 tokens/s, Drafted throughput: 939.43 tokens/s, Accepted: 2557 tokens, Drafted: 9395 tokens, Per-position acceptance rate: 0.638, 0.359, 0.200, 0.104, 0.060, Avg Draft acceptance rate: 27.2%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:23 [loggers.py:257] Engine 000: Avg prompt throughput: 99.2 tokens/s, Avg generation throughput: 463.0 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 275.37 tokens/s, Drafted throughput: 939.90 tokens/s, Accepted: 2754 tokens, Drafted: 9400 tokens, Per-position acceptance rate: 0.670, 0.396, 0.227, 0.111, 0.061, Avg Draft acceptance rate: 29.3%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:33 [loggers.py:257] Engine 000: Avg prompt throughput: 130.3 tokens/s, Avg generation throughput: 459.5 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 271.07 tokens/s, Drafted throughput: 941.91 tokens/s, Accepted: 2711 tokens, Drafted: 9420 tokens, Per-position acceptance rate: 0.639, 0.373, 0.226, 0.131, 0.071, Avg Draft acceptance rate: 28.8%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  47.79     
Total input tokens:                      6078      
Total generated tokens:                  20435     
Request throughput (req/s):              1.67      
Output token throughput (tok/s):         427.64    
Peak output token throughput (tok/s):    200.00    
Peak concurrent requests:                13.00     
Total token throughput (tok/s):          554.84    
---------------Time to First Token----------------
Mean TTFT (ms):                          124.66    
Median TTFT (ms):                        124.33    
P99 TTFT (ms):                           150.37    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.10     
Median TPOT (ms):                        17.00     
P99 TPOT (ms):                           20.81     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.28     
Median ITL (ms):                         41.20     
P99 ITL (ms):                            50.97     
---------------Speculative Decoding---------------
Acceptance rate (%):                     28.51     
Acceptance length:                       2.43      
Drafts:                                  8430      
Draft tokens:                            42150     
Accepted tokens:                         12016     
Per-position acceptance (%):
  Position 0:                            64.88     
  Position 1:                            37.95     
  Position 2:                            21.63     
  Position 3:                            11.54     
  Position 4:                            6.55      
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:43 [loggers.py:257] Engine 000: Avg prompt throughput: 97.8 tokens/s, Avg generation throughput: 300.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 177.80 tokens/s, Drafted throughput: 625.00 tokens/s, Accepted: 1778 tokens, Drafted: 6250 tokens, Per-position acceptance rate: 0.644, 0.380, 0.213, 0.116, 0.070, Avg Draft acceptance rate: 28.4%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:10:53 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fbe3856efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c1d360e0-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:03 [loggers.py:257] Engine 000: Avg prompt throughput: 171.4 tokens/s, Avg generation throughput: 200.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.65, Accepted throughput: 61.94 tokens/s, Drafted throughput: 187.48 tokens/s, Accepted: 1239 tokens, Drafted: 3750 tokens, Per-position acceptance rate: 0.669, 0.453, 0.291, 0.141, 0.097, Avg Draft acceptance rate: 33.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:13 [loggers.py:257] Engine 000: Avg prompt throughput: 210.0 tokens/s, Avg generation throughput: 887.8 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:13 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 525.59 tokens/s, Drafted throughput: 1807.13 tokens/s, Accepted: 5257 tokens, Drafted: 18075 tokens, Per-position acceptance rate: 0.654, 0.391, 0.226, 0.119, 0.063, Avg Draft acceptance rate: 29.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:23 [loggers.py:257] Engine 000: Avg prompt throughput: 244.4 tokens/s, Avg generation throughput: 882.5 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 526.73 tokens/s, Drafted throughput: 1779.28 tokens/s, Accepted: 5268 tokens, Drafted: 17795 tokens, Per-position acceptance rate: 0.657, 0.398, 0.228, 0.127, 0.070, Avg Draft acceptance rate: 29.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  25.90     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              3.09      
Output token throughput (tok/s):         790.74    
Peak output token throughput (tok/s):    380.00    
Peak concurrent requests:                24.00     
Total token throughput (tok/s):          1025.41   
---------------Time to First Token----------------
Mean TTFT (ms):                          131.18    
Median TTFT (ms):                        128.47    
P99 TTFT (ms):                           181.31    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.52     
Median TPOT (ms):                        17.48     
P99 TPOT (ms):                           21.50     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.06     
Median ITL (ms):                         43.03     
P99 ITL (ms):                            57.24     
---------------Speculative Decoding---------------
Acceptance rate (%):                     29.32     
Acceptance length:                       2.47      
Drafts:                                  8300      
Draft tokens:                            41500     
Accepted tokens:                         12169     
Per-position acceptance (%):
  Position 0:                            65.37     
  Position 1:                            39.54     
  Position 2:                            22.87     
  Position 3:                            12.12     
  Position 4:                            6.71      
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:33 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 57.90 tokens/s, Drafted throughput: 229.98 tokens/s, Accepted: 579 tokens, Drafted: 2300 tokens, Per-position acceptance rate: 0.602, 0.335, 0.176, 0.091, 0.054, Avg Draft acceptance rate: 25.2%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f55d46c2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-cc30ed74-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:43 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:43 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.74, Accepted throughput: 8.20 tokens/s, Drafted throughput: 23.50 tokens/s, Accepted: 82 tokens, Drafted: 235 tokens, Per-position acceptance rate: 0.660, 0.489, 0.298, 0.170, 0.128, Avg Draft acceptance rate: 34.9%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:53 [loggers.py:257] Engine 000: Avg prompt throughput: 480.1 tokens/s, Avg generation throughput: 1327.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:11:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.49, Accepted throughput: 791.61 tokens/s, Drafted throughput: 2663.70 tokens/s, Accepted: 7917 tokens, Drafted: 26640 tokens, Per-position acceptance rate: 0.661, 0.399, 0.234, 0.123, 0.069, Avg Draft acceptance rate: 29.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  16.13     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.96      
Output token throughput (tok/s):         1269.58   
Peak output token throughput (tok/s):    736.00    
Peak concurrent requests:                51.00     
Total token throughput (tok/s):          1646.36   
---------------Time to First Token----------------
Mean TTFT (ms):                          140.30    
Median TTFT (ms):                        139.83    
P99 TTFT (ms):                           185.00    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.88     
Median TPOT (ms):                        18.93     
P99 TPOT (ms):                           23.64     
---------------Inter-token Latency----------------
Mean ITL (ms):                           45.62     
Median ITL (ms):                         45.19     
P99 ITL (ms):                            83.78     
---------------Speculative Decoding---------------
Acceptance rate (%):                     28.52     
Acceptance length:                       2.43      
Drafts:                                  8442      
Draft tokens:                            42210     
Accepted tokens:                         12039     
Per-position acceptance (%):
  Position 0:                            64.63     
  Position 1:                            38.30     
  Position 2:                            22.12     
  Position 3:                            11.37     
  Position 4:                            6.20      
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:03 [loggers.py:257] Engine 000: Avg prompt throughput: 127.6 tokens/s, Avg generation throughput: 732.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:03 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 421.39 tokens/s, Drafted throughput: 1575.45 tokens/s, Accepted: 4214 tokens, Drafted: 15755 tokens, Per-position acceptance rate: 0.622, 0.358, 0.202, 0.102, 0.054, Avg Draft acceptance rate: 26.7%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:13 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fe9feb76fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-b975c058-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:23 [loggers.py:257] Engine 000: Avg prompt throughput: 487.9 tokens/s, Avg generation throughput: 940.6 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 32.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:23 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.56, Accepted throughput: 284.93 tokens/s, Drafted throughput: 911.77 tokens/s, Accepted: 5700 tokens, Drafted: 18240 tokens, Per-position acceptance rate: 0.674, 0.416, 0.258, 0.136, 0.079, Avg Draft acceptance rate: 31.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  11.67     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.85      
Output token throughput (tok/s):         1754.84   
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                72.00     
Total token throughput (tok/s):          2275.64   
---------------Time to First Token----------------
Mean TTFT (ms):                          205.44    
Median TTFT (ms):                        204.17    
P99 TTFT (ms):                           330.71    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          22.51     
Median TPOT (ms):                        22.55     
P99 TPOT (ms):                           27.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           54.70     
Median ITL (ms):                         54.87     
P99 ITL (ms):                            106.93    
---------------Speculative Decoding---------------
Acceptance rate (%):                     28.79     
Acceptance length:                       2.44      
Drafts:                                  8396      
Draft tokens:                            41980     
Accepted tokens:                         12084     
Per-position acceptance (%):
  Position 0:                            65.08     
  Position 1:                            38.88     
  Position 2:                            22.17     
  Position 3:                            11.55     
  Position 4:                            6.25      
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:33 [loggers.py:257] Engine 000: Avg prompt throughput: 137.7 tokens/s, Avg generation throughput: 1132.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:33 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.36, Accepted throughput: 655.77 tokens/s, Drafted throughput: 2415.87 tokens/s, Accepted: 6558 tokens, Drafted: 24160 tokens, Per-position acceptance rate: 0.634, 0.370, 0.197, 0.103, 0.053, Avg Draft acceptance rate: 27.1%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:43 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7efb66ff2fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15024, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-52ef39f6-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:53 [loggers.py:257] Engine 000: Avg prompt throughput: 625.7 tokens/s, Avg generation throughput: 1015.5 tokens/s, Running: 80 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:53 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 309.87 tokens/s, Drafted throughput: 969.91 tokens/s, Accepted: 6198 tokens, Drafted: 19400 tokens, Per-position acceptance rate: 0.679, 0.427, 0.265, 0.141, 0.085, Avg Draft acceptance rate: 31.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  8.57      
Total input tokens:                      6078      
Total generated tokens:                  20471     
Request throughput (req/s):              9.33      
Output token throughput (tok/s):         2387.43   
Peak output token throughput (tok/s):    1280.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          3096.27   
---------------Time to First Token----------------
Mean TTFT (ms):                          218.71    
Median TTFT (ms):                        233.46    
P99 TTFT (ms):                           320.65    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          26.17     
Median TPOT (ms):                        26.53     
P99 TPOT (ms):                           30.28     
---------------Inter-token Latency----------------
Mean ITL (ms):                           64.00     
Median ITL (ms):                         64.13     
P99 ITL (ms):                            132.67    
---------------Speculative Decoding---------------
Acceptance rate (%):                     29.14     
Acceptance length:                       2.46      
Drafts:                                  8337      
Draft tokens:                            41685     
Accepted tokens:                         12149     
Per-position acceptance (%):
  Position 0:                            65.35     
  Position 1:                            39.25     
  Position 2:                            22.50     
  Position 3:                            11.97     
  Position 4:                            6.66      
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k5-t0.0-tp1...
[0;36m(APIServer pid=2774242)[0;0m INFO 01-23 13:12:58 [launcher.py:110] Shutting down FastAPI HTTP server.
