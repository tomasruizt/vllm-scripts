Removing any existing container named vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k6-t0.0-tp1...
Creating new container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k6-t0.0-tp1...
code
datasets

==========
== CUDA ==
==========

CUDA Version 12.4.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Starting vLLM server...
Server started with PID: 290507
Starting benchmark with MAX_CONCURRENCY = 1 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:51 [api_server.py:872] vLLM API server version 0.1.dev13107+ge1a34c3a5
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:51 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-32B', 'port': 15029, 'disable_uvicorn_access_log': True, 'model': 'Qwen/Qwen3-32B', 'max_model_len': 5000, 'enable_prefix_caching': False, 'speculative_config': {'method': 'eagle3', 'model': 'RedHatAI/Qwen3-32B-speculator.eagle3', 'num_speculative_tokens': 6, 'max_model_len': 5000}}
[0;36m(APIServer pid=290507)[0;0m WARNING 01-23 21:46:51 [system_utils.py:262] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:52 [model.py:541] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:52 [model.py:1559] Using max model len 5000
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:53 [model.py:541] Resolved architecture: Eagle3LlamaForCausalLM
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:53 [model.py:1559] Using max model len 40960
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:53 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:53 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:46:53 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f44c8d8efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=1, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=1.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-94841903-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
WARNING 01-23 21:46:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:04 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13107+ge1a34c3a5) with config: model='Qwen/Qwen3-32B', speculative_config=SpeculativeConfig(method='eagle3', model='RedHatAI/Qwen3-32B-speculator.eagle3', num_spec_tokens=6), tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:05 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.195.244.68:36773 backend=nccl
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:05 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
WARNING 01-23 21:47:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m WARNING 01-23 21:47:06 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:06 [gpu_model_runner.py:3824] Starting to load model Qwen/Qwen3-32B...
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:07 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
WARNING 01-23 21:47:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:47:55 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:59 [default_loader.py:291] Loading weights took 50.68 seconds
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:47:59 [gpu_model_runner.py:3851] Loading drafter model...
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:00 [weight_utils.py:550] No model.safetensors.index.json found in remote.
WARNING 01-23 21:48:00 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:02 [default_loader.py:291] Loading weights took 2.54 seconds
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:04 [eagle.py:1146] Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
WARNING 01-23 21:48:05 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:06 [eagle.py:1195] Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:06 [gpu_model_runner.py:3921] Model loading took 63.94 GiB memory and 59.236966 seconds
WARNING 01-23 21:48:10 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:48:15 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:20 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:20 [backends.py:704] Dynamo bytecode transform time: 12.85 s
WARNING 01-23 21:48:20 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:48:25 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:48:30 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:48:35 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:37 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.565 s
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:37 [monitor.py:34] torch.compile takes 17.41 s in total
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:37 [backends.py:644] Using cache directory: /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/.cache/vllm/torch_compile_cache/a2a295eb88/rank_0_0/eagle_head for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:37 [backends.py:704] Dynamo bytecode transform time: 0.46 s
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:38 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.108 s
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:38 [monitor.py:34] torch.compile takes 17.98 s in total
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:39 [gpu_worker.py:355] Available KV cache memory: 11.72 GiB
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:39 [kv_cache_utils.py:1307] GPU KV cache size: 47,264 tokens
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:39 [kv_cache_utils.py:1312] Maximum concurrency for 5,000 tokens per request: 9.44x
WARNING 01-23 21:48:40 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
WARNING 01-23 21:48:45 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:50 [gpu_model_runner.py:4880] Graph capturing finished in 11 secs, took -0.67 GiB
WARNING 01-23 21:48:50 [ready_checker.py:69] Endpoint is not ready. Error=''Traceback (most recent call last):\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 122, in start_connection\n    raise first_exception\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 638, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File "/opt/conda/lib/python3.11/asyncio/selector_events.py", line 678, in _sock_connect_cb\n    raise OSError(err, f\'Connect call failed {address}\')\nConnectionRefusedError: [Errno 111] Connect call failed (\'127.0.0.1\', 15029)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/workspace/code/vllm/vllm/benchmarks/lib/endpoint_request_func.py", line 187, in async_request_openai_completions\n    async with session.post(url=api_url, json=payload, headers=headers) as response:\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 1488, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/client.py", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1581, in _create_direct_connection\n    raise last_exc\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/ptvenv/lib/python3.11/site-packages/aiohttp/connector.py", line 1291, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:15029 ssl:default [Connect call failed (\'127.0.0.1\', 15029)]\n''
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:50 [core.py:272] init engine (profile, create kv cache, warmup model) took 43.93 seconds
[0;36m(EngineCore_DP0 pid=290979)[0;0m INFO 01-23 21:48:52 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:52 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=290507)[0;0m WARNING 01-23 21:48:52 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:52 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:53 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:53 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:54 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:54 [serving.py:221] Chat template warmup completed in 1726.6ms
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:15029
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:48:55 [launcher.py:46] Route: /pooling, Methods: POST
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 1.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 1
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:05 [loggers.py:257] Engine 000: Avg prompt throughput: 30.2 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.95, Accepted throughput: 32.50 tokens/s, Drafted throughput: 100.23 tokens/s, Accepted: 428 tokens, Drafted: 1320 tokens, Per-position acceptance rate: 0.627, 0.450, 0.305, 0.218, 0.186, 0.159, Avg Draft acceptance rate: 32.4%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:15 [loggers.py:257] Engine 000: Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 51.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.09, Accepted throughput: 26.90 tokens/s, Drafted throughput: 147.58 tokens/s, Accepted: 269 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.630, 0.256, 0.114, 0.049, 0.028, 0.016, Avg Draft acceptance rate: 18.2%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:25 [loggers.py:257] Engine 000: Avg prompt throughput: 32.1 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.57, Accepted throughput: 38.49 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 385 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.682, 0.449, 0.224, 0.110, 0.069, 0.037, Avg Draft acceptance rate: 26.2%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:35 [loggers.py:257] Engine 000: Avg prompt throughput: 13.9 tokens/s, Avg generation throughput: 60.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 37.00 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 370 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.678, 0.433, 0.237, 0.090, 0.049, 0.024, Avg Draft acceptance rate: 25.2%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:45 [loggers.py:257] Engine 000: Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 34.00 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 340 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.630, 0.386, 0.207, 0.093, 0.045, 0.020, Avg Draft acceptance rate: 23.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:55 [loggers.py:257] Engine 000: Avg prompt throughput: 45.9 tokens/s, Avg generation throughput: 61.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:49:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 37.30 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 373 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.668, 0.369, 0.217, 0.139, 0.090, 0.045, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:05 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 55.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.25, Accepted throughput: 30.70 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 307 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.633, 0.347, 0.151, 0.069, 0.037, 0.016, Avg Draft acceptance rate: 20.9%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:15 [loggers.py:257] Engine 000: Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.35, Accepted throughput: 33.40 tokens/s, Drafted throughput: 148.18 tokens/s, Accepted: 334 tokens, Drafted: 1482 tokens, Per-position acceptance rate: 0.628, 0.336, 0.198, 0.101, 0.061, 0.028, Avg Draft acceptance rate: 22.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:25 [loggers.py:257] Engine 000: Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 56.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 31.90 tokens/s, Drafted throughput: 147.58 tokens/s, Accepted: 319 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.585, 0.366, 0.179, 0.089, 0.049, 0.028, Avg Draft acceptance rate: 21.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:35 [loggers.py:257] Engine 000: Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.24, Accepted throughput: 30.40 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 304 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.630, 0.297, 0.171, 0.069, 0.037, 0.033, Avg Draft acceptance rate: 20.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:45 [loggers.py:257] Engine 000: Avg prompt throughput: 25.6 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 36.90 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 369 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.652, 0.402, 0.217, 0.127, 0.070, 0.045, Avg Draft acceptance rate: 25.2%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:55 [loggers.py:257] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 60.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:50:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 36.10 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 361 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.629, 0.380, 0.212, 0.114, 0.078, 0.061, Avg Draft acceptance rate: 24.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:05 [loggers.py:257] Engine 000: Avg prompt throughput: 37.0 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 36.50 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 365 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.667, 0.407, 0.220, 0.102, 0.057, 0.033, Avg Draft acceptance rate: 24.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:15 [loggers.py:257] Engine 000: Avg prompt throughput: 15.2 tokens/s, Avg generation throughput: 66.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.72, Accepted throughput: 41.90 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 419 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.705, 0.451, 0.270, 0.156, 0.078, 0.057, Avg Draft acceptance rate: 28.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:25 [loggers.py:257] Engine 000: Avg prompt throughput: 13.2 tokens/s, Avg generation throughput: 62.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 38.00 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 380 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.673, 0.392, 0.241, 0.139, 0.061, 0.045, Avg Draft acceptance rate: 25.9%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:35 [loggers.py:257] Engine 000: Avg prompt throughput: 30.1 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 38.89 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 389 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.694, 0.424, 0.249, 0.127, 0.061, 0.033, Avg Draft acceptance rate: 26.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:45 [loggers.py:257] Engine 000: Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.64, Accepted throughput: 40.10 tokens/s, Drafted throughput: 147.00 tokens/s, Accepted: 401 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.698, 0.457, 0.286, 0.114, 0.049, 0.033, Avg Draft acceptance rate: 27.3%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:55 [loggers.py:257] Engine 000: Avg prompt throughput: 13.6 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:51:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 34.80 tokens/s, Drafted throughput: 146.98 tokens/s, Accepted: 348 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.649, 0.347, 0.229, 0.114, 0.049, 0.033, Avg Draft acceptance rate: 23.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:05 [loggers.py:257] Engine 000: Avg prompt throughput: 28.2 tokens/s, Avg generation throughput: 71.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.93, Accepted throughput: 47.00 tokens/s, Drafted throughput: 146.39 tokens/s, Accepted: 470 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.754, 0.508, 0.316, 0.201, 0.094, 0.053, Avg Draft acceptance rate: 32.1%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:15 [loggers.py:257] Engine 000: Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 53.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 28.80 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 288 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.598, 0.333, 0.142, 0.061, 0.020, 0.016, Avg Draft acceptance rate: 19.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:25 [loggers.py:257] Engine 000: Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 54.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 29.50 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 295 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.581, 0.309, 0.150, 0.089, 0.041, 0.028, Avg Draft acceptance rate: 20.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:35 [loggers.py:257] Engine 000: Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 63.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 38.20 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 382 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.622, 0.407, 0.244, 0.134, 0.085, 0.061, Avg Draft acceptance rate: 25.9%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:45 [loggers.py:257] Engine 000: Avg prompt throughput: 32.9 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.65, Accepted throughput: 40.20 tokens/s, Drafted throughput: 146.40 tokens/s, Accepted: 402 tokens, Drafted: 1464 tokens, Per-position acceptance rate: 0.684, 0.443, 0.287, 0.148, 0.057, 0.029, Avg Draft acceptance rate: 27.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:55 [loggers.py:257] Engine 000: Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:52:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 35.50 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 355 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.634, 0.366, 0.195, 0.126, 0.077, 0.045, Avg Draft acceptance rate: 24.1%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:05 [loggers.py:257] Engine 000: Avg prompt throughput: 14.8 tokens/s, Avg generation throughput: 54.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.20, Accepted throughput: 29.60 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 296 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.634, 0.325, 0.146, 0.061, 0.028, 0.008, Avg Draft acceptance rate: 20.1%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:15 [loggers.py:257] Engine 000: Avg prompt throughput: 12.6 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 36.20 tokens/s, Drafted throughput: 146.99 tokens/s, Accepted: 362 tokens, Drafted: 1470 tokens, Per-position acceptance rate: 0.624, 0.392, 0.245, 0.131, 0.053, 0.033, Avg Draft acceptance rate: 24.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:25 [loggers.py:257] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.40, Accepted throughput: 34.40 tokens/s, Drafted throughput: 147.59 tokens/s, Accepted: 344 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.659, 0.325, 0.183, 0.106, 0.073, 0.053, Avg Draft acceptance rate: 23.3%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:35 [loggers.py:257] Engine 000: Avg prompt throughput: 25.4 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.12, Accepted throughput: 27.60 tokens/s, Drafted throughput: 148.18 tokens/s, Accepted: 276 tokens, Drafted: 1482 tokens, Per-position acceptance rate: 0.538, 0.287, 0.166, 0.073, 0.032, 0.020, Avg Draft acceptance rate: 18.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:45 [loggers.py:257] Engine 000: Avg prompt throughput: 11.2 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.53, Accepted throughput: 37.80 tokens/s, Drafted throughput: 148.18 tokens/s, Accepted: 378 tokens, Drafted: 1482 tokens, Per-position acceptance rate: 0.676, 0.393, 0.235, 0.121, 0.065, 0.040, Avg Draft acceptance rate: 25.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:55 [loggers.py:257] Engine 000: Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:53:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.68, Accepted throughput: 40.80 tokens/s, Drafted throughput: 145.79 tokens/s, Accepted: 408 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.679, 0.444, 0.296, 0.132, 0.082, 0.045, Avg Draft acceptance rate: 28.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:05 [loggers.py:257] Engine 000: Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.26, Accepted throughput: 31.10 tokens/s, Drafted throughput: 147.60 tokens/s, Accepted: 311 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.630, 0.370, 0.150, 0.057, 0.037, 0.020, Avg Draft acceptance rate: 21.1%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:15 [loggers.py:257] Engine 000: Avg prompt throughput: 54.4 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.77, Accepted throughput: 42.99 tokens/s, Drafted throughput: 145.78 tokens/s, Accepted: 430 tokens, Drafted: 1458 tokens, Per-position acceptance rate: 0.654, 0.449, 0.300, 0.173, 0.107, 0.086, Avg Draft acceptance rate: 29.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:25 [loggers.py:257] Engine 000: Avg prompt throughput: 16.5 tokens/s, Avg generation throughput: 66.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.70, Accepted throughput: 41.69 tokens/s, Drafted throughput: 147.57 tokens/s, Accepted: 417 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.711, 0.431, 0.280, 0.134, 0.089, 0.049, Avg Draft acceptance rate: 28.3%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:35 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 62.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 39.39 tokens/s, Drafted throughput: 147.58 tokens/s, Accepted: 394 tokens, Drafted: 1476 tokens, Per-position acceptance rate: 0.687, 0.427, 0.244, 0.126, 0.073, 0.045, Avg Draft acceptance rate: 26.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             1         
Request rate configured (RPS):           1.00      
Benchmark duration (s):                  340.19    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.24      
Output token throughput (tok/s):         60.20     
Peak output token throughput (tok/s):    26.00     
Peak concurrent requests:                2.00      
Total token throughput (tok/s):          78.07     
---------------Time to First Token----------------
Mean TTFT (ms):                          71.07     
Median TTFT (ms):                        82.42     
P99 TTFT (ms):                           88.02     
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.36     
Median TPOT (ms):                        16.20     
P99 TPOT (ms):                           19.90     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.08     
Median ITL (ms):                         40.09     
P99 ITL (ms):                            40.54     
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.39     
Acceptance length:                       2.46      
Drafts:                                  8325      
Draft tokens:                            49950     
Accepted tokens:                         12185     
Per-position acceptance (%):
  Position 0:                            64.95     
  Position 1:                            38.44     
  Position 2:                            21.80     
  Position 3:                            11.22     
  Position 4:                            6.15      
  Position 5:                            3.81      
==================================================
Starting benchmark with MAX_CONCURRENCY = 2 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.06, Accepted throughput: 9.50 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 95 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.556, 0.278, 0.111, 0.067, 0.044, 0.000, Avg Draft acceptance rate: 17.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:54:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f81196dafc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=2, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=2.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-5deffdc2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 2.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 2
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:05 [loggers.py:257] Engine 000: Avg prompt throughput: 45.7 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.70, Accepted throughput: 27.40 tokens/s, Drafted throughput: 96.90 tokens/s, Accepted: 548 tokens, Drafted: 1938 tokens, Per-position acceptance rate: 0.641, 0.390, 0.245, 0.167, 0.136, 0.118, Avg Draft acceptance rate: 28.3%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:15 [loggers.py:257] Engine 000: Avg prompt throughput: 48.4 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 72.09 tokens/s, Drafted throughput: 288.56 tokens/s, Accepted: 721 tokens, Drafted: 2886 tokens, Per-position acceptance rate: 0.676, 0.414, 0.222, 0.102, 0.054, 0.031, Avg Draft acceptance rate: 25.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:25 [loggers.py:257] Engine 000: Avg prompt throughput: 61.6 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.42, Accepted throughput: 68.20 tokens/s, Drafted throughput: 287.40 tokens/s, Accepted: 682 tokens, Drafted: 2874 tokens, Per-position acceptance rate: 0.643, 0.403, 0.209, 0.094, 0.052, 0.023, Avg Draft acceptance rate: 23.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:35 [loggers.py:257] Engine 000: Avg prompt throughput: 18.6 tokens/s, Avg generation throughput: 116.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 67.29 tokens/s, Drafted throughput: 290.38 tokens/s, Accepted: 673 tokens, Drafted: 2904 tokens, Per-position acceptance rate: 0.651, 0.347, 0.202, 0.114, 0.052, 0.025, Avg Draft acceptance rate: 23.2%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:45 [loggers.py:257] Engine 000: Avg prompt throughput: 15.4 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 62.70 tokens/s, Drafted throughput: 288.59 tokens/s, Accepted: 627 tokens, Drafted: 2886 tokens, Per-position acceptance rate: 0.613, 0.337, 0.187, 0.079, 0.052, 0.035, Avg Draft acceptance rate: 21.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:55 [loggers.py:257] Engine 000: Avg prompt throughput: 34.0 tokens/s, Avg generation throughput: 125.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:55:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.63, Accepted throughput: 77.99 tokens/s, Drafted throughput: 287.36 tokens/s, Accepted: 780 tokens, Drafted: 2874 tokens, Per-position acceptance rate: 0.685, 0.405, 0.244, 0.136, 0.090, 0.069, Avg Draft acceptance rate: 27.1%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:05 [loggers.py:257] Engine 000: Avg prompt throughput: 48.6 tokens/s, Avg generation throughput: 118.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 69.69 tokens/s, Drafted throughput: 290.36 tokens/s, Accepted: 697 tokens, Drafted: 2904 tokens, Per-position acceptance rate: 0.657, 0.376, 0.211, 0.107, 0.060, 0.029, Avg Draft acceptance rate: 24.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:15 [loggers.py:257] Engine 000: Avg prompt throughput: 20.2 tokens/s, Avg generation throughput: 124.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 76.69 tokens/s, Drafted throughput: 287.98 tokens/s, Accepted: 767 tokens, Drafted: 2880 tokens, Per-position acceptance rate: 0.698, 0.419, 0.246, 0.133, 0.060, 0.042, Avg Draft acceptance rate: 26.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:25 [loggers.py:257] Engine 000: Avg prompt throughput: 46.6 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.62, Accepted throughput: 77.88 tokens/s, Drafted throughput: 289.14 tokens/s, Accepted: 779 tokens, Drafted: 2892 tokens, Per-position acceptance rate: 0.685, 0.440, 0.268, 0.133, 0.062, 0.029, Avg Draft acceptance rate: 26.9%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:35 [loggers.py:257] Engine 000: Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.46, Accepted throughput: 70.49 tokens/s, Drafted throughput: 290.37 tokens/s, Accepted: 705 tokens, Drafted: 2904 tokens, Per-position acceptance rate: 0.632, 0.372, 0.236, 0.130, 0.058, 0.029, Avg Draft acceptance rate: 24.3%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:45 [loggers.py:257] Engine 000: Avg prompt throughput: 13.7 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 63.00 tokens/s, Drafted throughput: 289.81 tokens/s, Accepted: 630 tokens, Drafted: 2898 tokens, Per-position acceptance rate: 0.607, 0.335, 0.186, 0.095, 0.048, 0.033, Avg Draft acceptance rate: 21.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:55 [loggers.py:257] Engine 000: Avg prompt throughput: 40.1 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:56:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.55, Accepted throughput: 74.79 tokens/s, Drafted throughput: 289.17 tokens/s, Accepted: 748 tokens, Drafted: 2892 tokens, Per-position acceptance rate: 0.656, 0.409, 0.241, 0.135, 0.068, 0.044, Avg Draft acceptance rate: 25.9%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:05 [loggers.py:257] Engine 000: Avg prompt throughput: 27.1 tokens/s, Avg generation throughput: 117.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.47, Accepted throughput: 70.20 tokens/s, Drafted throughput: 287.38 tokens/s, Accepted: 702 tokens, Drafted: 2874 tokens, Per-position acceptance rate: 0.668, 0.407, 0.209, 0.104, 0.054, 0.023, Avg Draft acceptance rate: 24.4%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:15 [loggers.py:257] Engine 000: Avg prompt throughput: 36.4 tokens/s, Avg generation throughput: 117.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 69.19 tokens/s, Drafted throughput: 288.57 tokens/s, Accepted: 692 tokens, Drafted: 2886 tokens, Per-position acceptance rate: 0.638, 0.347, 0.225, 0.123, 0.064, 0.042, Avg Draft acceptance rate: 24.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:25 [loggers.py:257] Engine 000: Avg prompt throughput: 27.1 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 73.80 tokens/s, Drafted throughput: 290.39 tokens/s, Accepted: 738 tokens, Drafted: 2904 tokens, Per-position acceptance rate: 0.651, 0.397, 0.244, 0.124, 0.066, 0.043, Avg Draft acceptance rate: 25.4%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:35 [loggers.py:257] Engine 000: Avg prompt throughput: 15.6 tokens/s, Avg generation throughput: 113.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.37, Accepted throughput: 66.09 tokens/s, Drafted throughput: 289.77 tokens/s, Accepted: 661 tokens, Drafted: 2898 tokens, Per-position acceptance rate: 0.640, 0.371, 0.195, 0.087, 0.050, 0.027, Avg Draft acceptance rate: 22.8%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:45 [loggers.py:257] Engine 000: Avg prompt throughput: 89.5 tokens/s, Avg generation throughput: 135.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.81, Accepted throughput: 86.89 tokens/s, Drafted throughput: 287.38 tokens/s, Accepted: 869 tokens, Drafted: 2874 tokens, Per-position acceptance rate: 0.693, 0.468, 0.303, 0.167, 0.111, 0.073, Avg Draft acceptance rate: 30.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             2         
Request rate configured (RPS):           2.00      
Benchmark duration (s):                  174.13    
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.46      
Output token throughput (tok/s):         117.62    
Peak output token throughput (tok/s):    50.00     
Peak concurrent requests:                4.00      
Total token throughput (tok/s):          152.52    
---------------Time to First Token----------------
Mean TTFT (ms):                          120.64    
Median TTFT (ms):                        121.50    
P99 TTFT (ms):                           146.99    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.31     
Median TPOT (ms):                        16.34     
P99 TPOT (ms):                           20.29     
---------------Inter-token Latency----------------
Mean ITL (ms):                           40.35     
Median ITL (ms):                         40.34     
P99 ITL (ms):                            41.87     
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.74     
Acceptance length:                       2.48      
Drafts:                                  8246      
Draft tokens:                            49476     
Accepted tokens:                         12241     
Per-position acceptance (%):
  Position 0:                            65.45     
  Position 1:                            38.82     
  Position 2:                            22.40     
  Position 3:                            11.62     
  Position 4:                            6.34      
  Position 5:                            3.82      
==================================================
Starting benchmark with MAX_CONCURRENCY = 4 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:55 [loggers.py:257] Engine 000: Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 69.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:57:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 40.10 tokens/s, Drafted throughput: 179.38 tokens/s, Accepted: 401 tokens, Drafted: 1794 tokens, Per-position acceptance rate: 0.642, 0.361, 0.167, 0.094, 0.054, 0.023, Avg Draft acceptance rate: 22.4%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f77b893efc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=4, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=4.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-aafe2ce9-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 4.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:15 [loggers.py:257] Engine 000: Avg prompt throughput: 74.4 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.85, Accepted throughput: 30.90 tokens/s, Drafted throughput: 100.19 tokens/s, Accepted: 618 tokens, Drafted: 2004 tokens, Per-position acceptance rate: 0.656, 0.467, 0.296, 0.174, 0.141, 0.117, Avg Draft acceptance rate: 30.8%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:25 [loggers.py:257] Engine 000: Avg prompt throughput: 81.3 tokens/s, Avg generation throughput: 235.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 141.08 tokens/s, Drafted throughput: 571.10 tokens/s, Accepted: 1411 tokens, Drafted: 5712 tokens, Per-position acceptance rate: 0.666, 0.404, 0.215, 0.105, 0.061, 0.030, Avg Draft acceptance rate: 24.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:35 [loggers.py:257] Engine 000: Avg prompt throughput: 55.3 tokens/s, Avg generation throughput: 220.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.34, Accepted throughput: 125.88 tokens/s, Drafted throughput: 563.32 tokens/s, Accepted: 1259 tokens, Drafted: 5634 tokens, Per-position acceptance rate: 0.627, 0.360, 0.193, 0.085, 0.050, 0.026, Avg Draft acceptance rate: 22.3%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:45 [loggers.py:257] Engine 000: Avg prompt throughput: 64.9 tokens/s, Avg generation throughput: 236.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 141.30 tokens/s, Drafted throughput: 571.19 tokens/s, Accepted: 1413 tokens, Drafted: 5712 tokens, Per-position acceptance rate: 0.645, 0.389, 0.217, 0.116, 0.070, 0.047, Avg Draft acceptance rate: 24.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:55 [loggers.py:257] Engine 000: Avg prompt throughput: 63.2 tokens/s, Avg generation throughput: 240.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:58:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 145.88 tokens/s, Drafted throughput: 566.92 tokens/s, Accepted: 1459 tokens, Drafted: 5670 tokens, Per-position acceptance rate: 0.675, 0.407, 0.241, 0.122, 0.062, 0.036, Avg Draft acceptance rate: 25.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:05 [loggers.py:257] Engine 000: Avg prompt throughput: 45.4 tokens/s, Avg generation throughput: 229.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 134.49 tokens/s, Drafted throughput: 571.15 tokens/s, Accepted: 1345 tokens, Drafted: 5712 tokens, Per-position acceptance rate: 0.632, 0.361, 0.217, 0.113, 0.056, 0.033, Avg Draft acceptance rate: 23.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:15 [loggers.py:257] Engine 000: Avg prompt throughput: 67.2 tokens/s, Avg generation throughput: 238.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.52, Accepted throughput: 144.19 tokens/s, Drafted throughput: 567.57 tokens/s, Accepted: 1442 tokens, Drafted: 5676 tokens, Per-position acceptance rate: 0.651, 0.416, 0.226, 0.127, 0.068, 0.036, Avg Draft acceptance rate: 25.4%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:25 [loggers.py:257] Engine 000: Avg prompt throughput: 63.5 tokens/s, Avg generation throughput: 229.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.44, Accepted throughput: 135.89 tokens/s, Drafted throughput: 566.95 tokens/s, Accepted: 1359 tokens, Drafted: 5670 tokens, Per-position acceptance rate: 0.618, 0.367, 0.226, 0.120, 0.065, 0.042, Avg Draft acceptance rate: 24.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:35 [loggers.py:257] Engine 000: Avg prompt throughput: 83.7 tokens/s, Avg generation throughput: 244.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 150.28 tokens/s, Drafted throughput: 566.32 tokens/s, Accepted: 1503 tokens, Drafted: 5664 tokens, Per-position acceptance rate: 0.665, 0.416, 0.250, 0.127, 0.082, 0.052, Avg Draft acceptance rate: 26.5%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             4         
Request rate configured (RPS):           4.00      
Benchmark duration (s):                  90.32     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              0.89      
Output token throughput (tok/s):         226.74    
Peak output token throughput (tok/s):    100.00    
Peak concurrent requests:                7.00      
Total token throughput (tok/s):          294.03    
---------------Time to First Token----------------
Mean TTFT (ms):                          124.04    
Median TTFT (ms):                        124.08    
P99 TTFT (ms):                           149.47    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.62     
Median TPOT (ms):                        16.56     
P99 TPOT (ms):                           20.75     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.03     
Median ITL (ms):                         40.99     
P99 ITL (ms):                            48.71     
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.67     
Acceptance length:                       2.48      
Drafts:                                  8262      
Draft tokens:                            49572     
Accepted tokens:                         12228     
Per-position acceptance (%):
  Position 0:                            64.73     
  Position 1:                            39.14     
  Position 2:                            22.43     
  Position 3:                            11.41     
  Position 4:                            6.46      
  Position 5:                            3.82      
==================================================
Starting benchmark with MAX_CONCURRENCY = 8 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:45 [loggers.py:257] Engine 000: Avg prompt throughput: 26.9 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.38, Accepted throughput: 59.80 tokens/s, Drafted throughput: 260.39 tokens/s, Accepted: 598 tokens, Drafted: 2604 tokens, Per-position acceptance rate: 0.641, 0.373, 0.207, 0.092, 0.046, 0.018, Avg Draft acceptance rate: 23.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 21:59:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7fce1fd82fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=8, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=8.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-53a50b1d-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 8.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:05 [loggers.py:257] Engine 000: Avg prompt throughput: 150.8 tokens/s, Avg generation throughput: 241.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.68, Accepted throughput: 75.35 tokens/s, Drafted throughput: 269.70 tokens/s, Accepted: 1507 tokens, Drafted: 5394 tokens, Per-position acceptance rate: 0.677, 0.430, 0.259, 0.141, 0.097, 0.071, Avg Draft acceptance rate: 27.9%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:15 [loggers.py:257] Engine 000: Avg prompt throughput: 125.1 tokens/s, Avg generation throughput: 444.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.39, Accepted throughput: 257.86 tokens/s, Drafted throughput: 1115.84 tokens/s, Accepted: 2579 tokens, Drafted: 11160 tokens, Per-position acceptance rate: 0.637, 0.359, 0.192, 0.100, 0.061, 0.037, Avg Draft acceptance rate: 23.1%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:25 [loggers.py:257] Engine 000: Avg prompt throughput: 108.6 tokens/s, Avg generation throughput: 481.7 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.59, Accepted throughput: 295.85 tokens/s, Drafted throughput: 1116.41 tokens/s, Accepted: 2959 tokens, Drafted: 11166 tokens, Per-position acceptance rate: 0.693, 0.416, 0.250, 0.130, 0.063, 0.038, Avg Draft acceptance rate: 26.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:35 [loggers.py:257] Engine 000: Avg prompt throughput: 121.3 tokens/s, Avg generation throughput: 449.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.41, Accepted throughput: 263.25 tokens/s, Drafted throughput: 1121.19 tokens/s, Accepted: 2633 tokens, Drafted: 11214 tokens, Per-position acceptance rate: 0.640, 0.370, 0.204, 0.109, 0.053, 0.033, Avg Draft acceptance rate: 23.5%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:45 [loggers.py:257] Engine 000: Avg prompt throughput: 120.0 tokens/s, Avg generation throughput: 437.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.60, Accepted throughput: 270.36 tokens/s, Drafted throughput: 1010.86 tokens/s, Accepted: 2704 tokens, Drafted: 10110 tokens, Per-position acceptance rate: 0.677, 0.417, 0.250, 0.135, 0.077, 0.049, Avg Draft acceptance rate: 26.7%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             8         
Request rate configured (RPS):           8.00      
Benchmark duration (s):                  46.85     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              1.71      
Output token throughput (tok/s):         437.14    
Peak output token throughput (tok/s):    199.00    
Peak concurrent requests:                15.00     
Total token throughput (tok/s):          566.88    
---------------Time to First Token----------------
Mean TTFT (ms):                          126.85    
Median TTFT (ms):                        126.29    
P99 TTFT (ms):                           161.98    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.75     
Median TPOT (ms):                        16.96     
P99 TPOT (ms):                           20.32     
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.76     
Median ITL (ms):                         41.59     
P99 ITL (ms):                            51.96     
---------------Speculative Decoding---------------
Acceptance rate (%):                     25.07     
Acceptance length:                       2.50      
Drafts:                                  8183      
Draft tokens:                            49098     
Accepted tokens:                         12308     
Per-position acceptance (%):
  Position 0:                            66.19     
  Position 1:                            39.28     
  Position 2:                            22.53     
  Position 3:                            11.85     
  Position 4:                            6.51      
  Position 5:                            4.04      
==================================================
Starting benchmark with MAX_CONCURRENCY = 16 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:00:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.17, Accepted throughput: 10.50 tokens/s, Drafted throughput: 54.00 tokens/s, Accepted: 105 tokens, Drafted: 540 tokens, Per-position acceptance rate: 0.544, 0.333, 0.167, 0.067, 0.044, 0.011, Avg Draft acceptance rate: 19.4%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f6effd12fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=16.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-6274bfd4-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 16.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:05 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.21, Accepted throughput: 17.90 tokens/s, Drafted throughput: 48.59 tokens/s, Accepted: 179 tokens, Drafted: 486 tokens, Per-position acceptance rate: 0.667, 0.494, 0.346, 0.259, 0.235, 0.210, Avg Draft acceptance rate: 36.8%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:15 [loggers.py:257] Engine 000: Avg prompt throughput: 306.0 tokens/s, Avg generation throughput: 826.0 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 491.73 tokens/s, Drafted throughput: 1992.91 tokens/s, Accepted: 4918 tokens, Drafted: 19932 tokens, Per-position acceptance rate: 0.660, 0.391, 0.221, 0.108, 0.063, 0.037, Avg Draft acceptance rate: 24.7%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:25 [loggers.py:257] Engine 000: Avg prompt throughput: 206.8 tokens/s, Avg generation throughput: 867.8 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.45, Accepted throughput: 514.80 tokens/s, Drafted throughput: 2125.38 tokens/s, Accepted: 5149 tokens, Drafted: 21258 tokens, Per-position acceptance rate: 0.639, 0.382, 0.222, 0.118, 0.061, 0.032, Avg Draft acceptance rate: 24.2%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             16        
Request rate configured (RPS):           16.00     
Benchmark duration (s):                  26.57     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              3.01      
Output token throughput (tok/s):         770.90    
Peak output token throughput (tok/s):    368.00    
Peak concurrent requests:                24.00     
Total token throughput (tok/s):          999.68    
---------------Time to First Token----------------
Mean TTFT (ms):                          134.76    
Median TTFT (ms):                        132.12    
P99 TTFT (ms):                           175.11    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.73     
Median TPOT (ms):                        17.77     
P99 TPOT (ms):                           21.97     
---------------Inter-token Latency----------------
Mean ITL (ms):                           43.72     
Median ITL (ms):                         43.49     
P99 ITL (ms):                            54.36     
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.64     
Acceptance length:                       2.48      
Drafts:                                  8271      
Draft tokens:                            49626     
Accepted tokens:                         12229     
Per-position acceptance (%):
  Position 0:                            65.25     
  Position 1:                            38.88     
  Position 2:                            22.23     
  Position 3:                            11.52     
  Position 4:                            6.32      
  Position 5:                            3.64      
==================================================
Starting benchmark with MAX_CONCURRENCY = 32 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:35 [loggers.py:257] Engine 000: Avg prompt throughput: 95.0 tokens/s, Avg generation throughput: 353.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.54, Accepted throughput: 216.20 tokens/s, Drafted throughput: 843.61 tokens/s, Accepted: 2162 tokens, Drafted: 8436 tokens, Per-position acceptance rate: 0.668, 0.401, 0.228, 0.124, 0.071, 0.045, Avg Draft acceptance rate: 25.6%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f9d7b4c6fc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=32, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=32.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-e980c199-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 32.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:55 [loggers.py:257] Engine 000: Avg prompt throughput: 319.2 tokens/s, Avg generation throughput: 759.0 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:01:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.51, Accepted throughput: 227.38 tokens/s, Drafted throughput: 903.82 tokens/s, Accepted: 4548 tokens, Drafted: 18078 tokens, Per-position acceptance rate: 0.663, 0.395, 0.223, 0.116, 0.068, 0.044, Avg Draft acceptance rate: 25.2%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:05 [loggers.py:257] Engine 000: Avg prompt throughput: 306.6 tokens/s, Avg generation throughput: 1298.2 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.48, Accepted throughput: 776.73 tokens/s, Drafted throughput: 3158.73 tokens/s, Accepted: 7768 tokens, Drafted: 31590 tokens, Per-position acceptance rate: 0.649, 0.390, 0.222, 0.115, 0.062, 0.037, Avg Draft acceptance rate: 24.6%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             32        
Request rate configured (RPS):           32.00     
Benchmark duration (s):                  17.04     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              4.69      
Output token throughput (tok/s):         1201.77   
Peak output token throughput (tok/s):    670.00    
Peak concurrent requests:                46.00     
Total token throughput (tok/s):          1558.43   
---------------Time to First Token----------------
Mean TTFT (ms):                          155.89    
Median TTFT (ms):                        152.16    
P99 TTFT (ms):                           233.43    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.64     
Median TPOT (ms):                        19.34     
P99 TPOT (ms):                           24.74     
---------------Inter-token Latency----------------
Mean ITL (ms):                           48.34     
Median ITL (ms):                         47.88     
P99 ITL (ms):                            88.56     
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.56     
Acceptance length:                       2.47      
Drafts:                                  8288      
Draft tokens:                            49728     
Accepted tokens:                         12214     
Per-position acceptance (%):
  Position 0:                            65.13     
  Position 1:                            38.91     
  Position 2:                            22.02     
  Position 3:                            11.35     
  Position 4:                            6.19      
  Position 5:                            3.76      
==================================================
Starting benchmark with MAX_CONCURRENCY = 64 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:15 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 1.85, Accepted throughput: 7.70 tokens/s, Drafted throughput: 54.60 tokens/s, Accepted: 77 tokens, Drafted: 546 tokens, Per-position acceptance rate: 0.418, 0.220, 0.143, 0.066, 0.000, 0.000, Avg Draft acceptance rate: 14.1%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f719792afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=64, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=64.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-91fc50e5-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 64.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:25 [loggers.py:257] Engine 000: Avg prompt throughput: 39.8 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:25 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.22, Accepted throughput: 18.20 tokens/s, Drafted throughput: 49.19 tokens/s, Accepted: 182 tokens, Drafted: 492 tokens, Per-position acceptance rate: 0.671, 0.500, 0.354, 0.256, 0.232, 0.207, Avg Draft acceptance rate: 37.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:35 [loggers.py:257] Engine 000: Avg prompt throughput: 586.0 tokens/s, Avg generation throughput: 1921.1 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.7%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:35 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 1151.24 tokens/s, Drafted throughput: 4614.38 tokens/s, Accepted: 11513 tokens, Drafted: 46146 tokens, Per-position acceptance rate: 0.656, 0.392, 0.225, 0.117, 0.066, 0.041, Avg Draft acceptance rate: 24.9%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             64        
Request rate configured (RPS):           64.00     
Benchmark duration (s):                  12.84     
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              6.23      
Output token throughput (tok/s):         1595.13   
Peak output token throughput (tok/s):    960.00    
Peak concurrent requests:                73.00     
Total token throughput (tok/s):          2068.52   
---------------Time to First Token----------------
Mean TTFT (ms):                          215.39    
Median TTFT (ms):                        219.07    
P99 TTFT (ms):                           326.00    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          25.41     
Median TPOT (ms):                        26.05     
P99 TPOT (ms):                           32.25     
---------------Inter-token Latency----------------
Mean ITL (ms):                           62.83     
Median ITL (ms):                         65.74     
P99 ITL (ms):                            121.61    
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.73     
Acceptance length:                       2.48      
Drafts:                                  8252      
Draft tokens:                            49512     
Accepted tokens:                         12246     
Per-position acceptance (%):
  Position 0:                            65.40     
  Position 1:                            38.90     
  Position 2:                            22.18     
  Position 3:                            11.52     
  Position 4:                            6.43      
  Position 5:                            3.96      
==================================================
Starting benchmark with MAX_CONCURRENCY = 80 and NUM_PROMPTS = 80...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:45 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.30, Accepted throughput: 72.99 tokens/s, Drafted throughput: 335.97 tokens/s, Accepted: 730 tokens, Drafted: 3360 tokens, Per-position acceptance rate: 0.620, 0.350, 0.175, 0.086, 0.048, 0.025, Avg Draft acceptance rate: 21.7%
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x7f2e3ee4afc0>, seed=0, num_prompts=80, dataset_name='hf', no_stream=False, dataset_path='philschmid/mt-bench', no_oversample=False, skip_chat_template=False, disable_shuffle=False, custom_output_len=256, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, no_reranker=False, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 1}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='openai', base_url=None, host='127.0.0.1', port=15029, endpoint='/v1/completions', header=None, max_concurrency=80, model='Qwen/Qwen3-32B', input_len=None, output_len=None, tokenizer=None, tokenizer_mode='auto', use_beam_search=False, logprobs=None, request_rate=80.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, num_warmups=0, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics=None, metric_percentiles='99', goodput=None, request_id_prefix='bench-c0b6e9e2-', top_p=1.0, top_k=None, min_p=None, temperature=0.0, frequency_penalty=None, presence_penalty=None, repetition_penalty=None, served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600, extra_body=None)
Starting initial single prompt test run...
Waiting for endpoint to become up in 600 seconds
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:55 [loggers.py:257] Engine 000: Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 16.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:02:55 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 3.08, Accepted throughput: 11.00 tokens/s, Drafted throughput: 31.80 tokens/s, Accepted: 110 tokens, Drafted: 318 tokens, Per-position acceptance rate: 0.679, 0.472, 0.340, 0.226, 0.189, 0.170, Avg Draft acceptance rate: 34.6%
Initial test run completed.
Starting main benchmark run...
Traffic request rate: 80.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 80
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:03:05 [loggers.py:257] Engine 000: Avg prompt throughput: 607.8 tokens/s, Avg generation throughput: 2009.0 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.9%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:03:05 [metrics.py:100] SpecDecoding metrics: Mean acceptance length: 2.50, Accepted throughput: 1205.33 tokens/s, Drafted throughput: 4823.73 tokens/s, Accepted: 12054 tokens, Drafted: 48240 tokens, Per-position acceptance rate: 0.657, 0.395, 0.225, 0.116, 0.066, 0.041, Avg Draft acceptance rate: 25.0%
tip: install termplotlib and gnuplot to plot the metrics
============ Serving Benchmark Result ============
Successful requests:                     80        
Failed requests:                         0         
Maximum request concurrency:             80        
Request rate configured (RPS):           80.00     
Benchmark duration (s):                  9.97      
Total input tokens:                      6078      
Total generated tokens:                  20480     
Request throughput (req/s):              8.02      
Output token throughput (tok/s):         2054.21   
Peak output token throughput (tok/s):    1110.00   
Peak concurrent requests:                80.00     
Total token throughput (tok/s):          2663.85   
---------------Time to First Token----------------
Mean TTFT (ms):                          255.11    
Median TTFT (ms):                        234.59    
P99 TTFT (ms):                           505.19    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          29.92     
Median TPOT (ms):                        30.18     
P99 TPOT (ms):                           35.27     
---------------Inter-token Latency----------------
Mean ITL (ms):                           73.89     
Median ITL (ms):                         75.76     
P99 ITL (ms):                            149.04    
---------------Speculative Decoding---------------
Acceptance rate (%):                     24.71     
Acceptance length:                       2.48      
Drafts:                                  8261      
Draft tokens:                            49566     
Accepted tokens:                         12249     
Per-position acceptance (%):
  Position 0:                            65.44     
  Position 1:                            39.05     
  Position 2:                            22.18     
  Position 3:                            11.35     
  Position 4:                            6.36      
  Position 5:                            3.90      
==================================================
Benchmark completed, stopping server...
Cleaning up: removing container vllm-throughput-mt-bench-sd-eagle3-Qwen3-32B-speculator.eagle3-k6-t0.0-tp1...
[0;36m(APIServer pid=290507)[0;0m INFO 01-23 22:03:07 [launcher.py:110] Shutting down FastAPI HTTP server.
